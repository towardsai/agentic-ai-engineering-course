{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff02341e",
   "metadata": {},
   "source": [
    "# Lesson 18: Final Outputs and Agent Completion\n",
    "\n",
    "In our final lesson about the research agent implementation, we will complete the research agent's workflow by implementing the remaining tools that filter research results, scrape additional sources, and compile everything into a final research file. We'll test the complete end-to-end agent and analyze its output.\n",
    "\n",
    "Learning Objectives:\n",
    "- Learn how to filter and validate research sources for quality and trustworthiness\n",
    "- Understand how to select the most valuable sources for full content scraping\n",
    "- Implement the final research compilation tool that creates structured outputs\n",
    "- Test the complete agent workflow and analyze output quality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cdaa73",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "First, we define some standard Magic Python commands to autoreload Python packages whenever they change:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "708fa85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1b2878",
   "metadata": {},
   "source": [
    "### Set Up Python Environment\n",
    "\n",
    "To set up your Python virtual environment using `uv` and load it into the Notebook, follow the step-by-step instructions from the `Course Admin` lesson from the beginning of the course.\n",
    "\n",
    "**TL/DR:** Be sure the correct kernel pointing to your `uv` virtual environment is selected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d422eebe",
   "metadata": {},
   "source": [
    "### Configure Required APIs\n",
    "\n",
    "To run this lesson, you'll need several API keys configured:\n",
    "\n",
    "1. **Gemini API Key**, `GOOGLE_API_KEY` variable: Get your key from [Google AI Studio](https://aistudio.google.com/app/apikey).\n",
    "2. **Firecrawl API Key**, `FIRECRAWL_API_KEY` variable: Get your key from [Firecrawl](https://firecrawl.dev/). They have a free tier that allows you to scrape 500 pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f07bb55e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment variables loaded from `/Users/fabio/Desktop/course-ai-agents/.env`\n",
      "Environment variables loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "from utils import env\n",
    "\n",
    "env.load(required_env_vars=[\"GOOGLE_API_KEY\", \"FIRECRAWL_API_KEY\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b32778",
   "metadata": {},
   "source": [
    "### Import Key Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b24e052d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply() # Allow nested async usage in notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8943e9",
   "metadata": {},
   "source": [
    "## 2. Completing the Research Workflow\n",
    "\n",
    "As we've seen in previous lessons, our research agent follows a systematic workflow. We've covered the initial data ingestion (lesson 16) and the research loop with query generation (lesson 17). Now we need to implement the final steps that ensure quality and compile the results.\n",
    "\n",
    "The complete workflow includes these final steps:\n",
    "\n",
    "```markdown\n",
    "4. Filter Perplexity results by quality:\n",
    "    4.1 Run the \"select_research_sources_to_keep\" tool to automatically evaluate each source \n",
    "    for trustworthiness, authority and relevance.\n",
    "\n",
    "5. Identify which of the accepted sources deserve a full scrape:\n",
    "    5.1 Run the \"select_research_sources_to_scrape\" tool to choose up to 5 diverse, \n",
    "    authoritative sources whose full content will add most value.\n",
    "    5.2 Run the \"scrape_research_urls\" tool to scrape/clean each selected URL's full content.\n",
    "\n",
    "6. Write final research file:\n",
    "    6.1 Run the \"create_research_file\" tool to combine all research data into a \n",
    "    comprehensive research.md file.\n",
    "```\n",
    "\n",
    "Let's examine each of these final tools and understand their purpose in the workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6ae738",
   "metadata": {},
   "source": [
    "## 3. Filtering Research Sources for Quality\n",
    "\n",
    "The `select_research_sources_to_keep` tool addresses a critical problem we discovered during development: Perplexity results often include sources from untrustworthy blogs, SEO spam, or low-quality content that would pollute our research."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d0a8de",
   "metadata": {},
   "source": [
    "### 3.1 Understanding the Tool Implementation\n",
    "\n",
    "This tool takes a research directory as input and automatically filters Perplexity results for quality. It reads the article guidelines and raw Perplexity results, then uses an LLM to evaluate each source based on trustworthiness, authority, and relevance criteria. The tool outputs two files: a list of selected source IDs and a filtered markdown file containing only the approved sources. This automated filtering saves time while ensuring research quality.\n",
    "\n",
    "Source: _mcp_server/src/tools/select_research_sources_to_keep_tool.py_\n",
    "\n",
    "```python\n",
    "async def select_research_sources_to_keep_tool(research_directory: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Automatically select high-quality sources from Perplexity results.\n",
    "\n",
    "    Uses an LLM to evaluate each source in perplexity_results.md for trustworthiness,\n",
    "    authority, and relevance based on the article guidelines. Writes the comma-separated\n",
    "    IDs of accepted sources to perplexity_sources_selected.md and saves a filtered\n",
    "    markdown file perplexity_results_selected.md containing only the accepted sources.\n",
    "\n",
    "    Args:\n",
    "        research_directory: Path to the research directory containing article guidelines and research data\n",
    "\n",
    "    Returns:\n",
    "        Dict with status, selection results, and file paths\n",
    "    \"\"\"\n",
    "    # Convert to Path object\n",
    "    research_path = Path(research_directory)\n",
    "    nova_path = research_path / NOVA_FOLDER\n",
    "    \n",
    "    # Gather context from the research folder\n",
    "    guidelines_path = research_path / ARTICLE_GUIDELINE_FILE\n",
    "    results_path = nova_path / PERPLEXITY_RESULTS_FILE\n",
    "    \n",
    "    article_guidelines = read_file_safe(guidelines_path)\n",
    "    perplexity_results = read_file_safe(results_path)\n",
    "    \n",
    "    # Use LLM to select sources\n",
    "    selected_ids = await select_sources(\n",
    "        article_guidelines, perplexity_results, settings.source_selection_model\n",
    "    )\n",
    "    \n",
    "    # Write selected source IDs to file\n",
    "    sources_selected_path = nova_path / PERPLEXITY_SOURCES_SELECTED_FILE\n",
    "    sources_selected_path.write_text(\",\".join(map(str, selected_ids)), encoding=\"utf-8\")\n",
    "    \n",
    "    # Extract and save filtered content\n",
    "    filtered_content = extract_selected_blocks_content(selected_ids, perplexity_results)\n",
    "    results_selected_path = nova_path / PERPLEXITY_RESULTS_SELECTED_FILE\n",
    "    results_selected_path.write_text(filtered_content, encoding=\"utf-8\")\n",
    "    \n",
    "    return {\n",
    "        \"status\": \"success\",\n",
    "        \"sources_selected_count\": len(selected_ids),\n",
    "        \"selected_source_ids\": selected_ids,\n",
    "        \"sources_selected_path\": str(sources_selected_path.resolve()),\n",
    "        \"results_selected_path\": str(results_selected_path.resolve()),\n",
    "        \"message\": f\"Successfully selected {len(selected_ids)} high-quality sources...\"\n",
    "    }\n",
    "```\n",
    "\n",
    "The core of this tool is the `select_sources` function, which uses the `PROMPT_SELECT_SOURCES` prompt to evaluate each source and select the most relevant ones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4e3900",
   "metadata": {},
   "source": [
    "### 3.2 The Source Evaluation Prompt\n",
    "\n",
    "Here's the prompt used to evaluate the sources:\n",
    "\n",
    "Source: _mcp_server/src/config/prompts.py_\n",
    "\n",
    "```python\n",
    "PROMPT_SELECT_SOURCES = \"\"\"\n",
    "You are a research quality evaluator. Your task is to evaluate web sources for an upcoming article\n",
    "and select only the high-quality, trustworthy sources that are relevant to the article guidelines.\n",
    "\n",
    "<article_guidelines>\n",
    "{article_guidelines}\n",
    "</article_guidelines>\n",
    "\n",
    "Here are the sources to evaluate:\n",
    "<sources_to_evaluate>\n",
    "{sources_data}\n",
    "</sources_to_evaluate>\n",
    "\n",
    "**Selection Criteria:**\n",
    "- ACCEPT sources from trustworthy domains (e.g., .edu, .gov, established news sites,\n",
    "official documentation, reputable organizations)\n",
    "- ACCEPT sources with high-quality, relevant content that directly supports the article guidelines\n",
    "- REJECT sources from obscure, untrustworthy, or potentially biased websites\n",
    "- REJECT sources with low-quality, irrelevant, or superficial content\n",
    "- REJECT sources that seem to be marketing materials, advertisements, or self-promotional content\n",
    "\n",
    "Return your decision as a structured output with:\n",
    "1. selection_type: \"none\" if no sources meet the quality standards, \"all\" if all sources are acceptable,\n",
    "or \"specific\" for specific source IDs\n",
    "2. source_ids: List of selected source IDs\n",
    "\"\"\".strip()\n",
    "```\n",
    "\n",
    "This prompt serves as a quality gatekeeper, automatically filtering out unreliable sources that could compromise research quality. The key aspect is the structured selection criteria that balance domain reputation, content quality, and relevance. The prompt explicitly targets common quality issues like SEO spam, marketing content, and biased sources that often pollute web search results, ensuring only authoritative sources proceed to the next stage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f00c568",
   "metadata": {},
   "source": [
    "### 3.3 Testing the Source Selection Tool\n",
    "\n",
    "Let's test the source filtering tool to see how it evaluates and selects high-quality sources from our Perplexity results. The tool will analyze each source and provide feedback on which ones meet our quality standards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce42921",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fabio/Desktop/course-ai-agents/.venv/lib/python3.12/site-packages/pydantic/_internal/_fields.py:198: UserWarning: Field name \"schema\" in \"FewShotExampleStructuredOutputCompliance\" shadows an attribute in parent \"BaseModel\"\n",
      "  warnings.warn(\n",
      "/Users/fabio/Desktop/course-ai-agents/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\u001b[32m2025-09-23 12:39:00.001\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlogging\u001b[0m:\u001b[36mcallHandlers\u001b[0m:\u001b[36m1762\u001b[0m | ğŸ‘ All sources accepted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'status': 'success', 'sources_selected_count': 12, 'selected_source_ids': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12], 'sources_selected_path': '/Users/fabio/Desktop/course-ai-agents/lessons/research_agent_part_2/data/sample_research_folder/.nova/perplexity_sources_selected.md', 'results_selected_path': '/Users/fabio/Desktop/course-ai-agents/lessons/research_agent_part_2/data/sample_research_folder/.nova/perplexity_results_selected.md', 'message': 'âœ… Selected 12 source(s). IDs written to /Users/fabio/Desktop/course-ai-agents/lessons/research_agent_part_2/data/sample_research_folder/.nova/perplexity_sources_selected.md. Filtered results written to /Users/fabio/Desktop/course-ai-agents/lessons/research_agent_part_2/data/sample_research_folder/.nova/perplexity_results_selected.md.'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-23 12:41:50.878\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlogging\u001b[0m:\u001b[36mcallHandlers\u001b[0m:\u001b[36m1762\u001b[0m | ğŸ‘ 3 sources selected to scrape.\n",
      "\u001b[32m2025-09-23 12:43:45.044\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlogging\u001b[0m:\u001b[36mcallHandlers\u001b[0m:\u001b[36m1762\u001b[0m | HTTP Request: POST https://api.firecrawl.dev/v2/scrape \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2025-09-23 12:43:46.479\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlogging\u001b[0m:\u001b[36mcallHandlers\u001b[0m:\u001b[36m1762\u001b[0m | HTTP Request: POST https://api.firecrawl.dev/v2/scrape \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2025-09-23 12:44:23.510\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlogging\u001b[0m:\u001b[36mcallHandlers\u001b[0m:\u001b[36m1762\u001b[0m | HTTP Request: POST https://api.firecrawl.dev/v2/scrape \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2025-09-23 12:49:57.925\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlogging\u001b[0m:\u001b[36mcallHandlers\u001b[0m:\u001b[36m1762\u001b[0m | ğŸ“Š Opik monitoring disabled (missing configuration)\n",
      "\u001b[32m2025-09-23 12:49:57.940\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlogging\u001b[0m:\u001b[36mcallHandlers\u001b[0m:\u001b[36m1762\u001b[0m | ğŸš€ Starting MCP client with in-memory transport...\n",
      "\u001b[32m2025-09-23 12:49:57.949\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlogging\u001b[0m:\u001b[36mcallHandlers\u001b[0m:\u001b[36m1762\u001b[0m | Processing request of type ListToolsRequest\n",
      "\u001b[32m2025-09-23 12:49:57.951\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlogging\u001b[0m:\u001b[36mcallHandlers\u001b[0m:\u001b[36m1762\u001b[0m | Processing request of type ListResourcesRequest\n",
      "\u001b[32m2025-09-23 12:49:57.952\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlogging\u001b[0m:\u001b[36mcallHandlers\u001b[0m:\u001b[36m1762\u001b[0m | Processing request of type ListPromptsRequest\n",
      "\u001b[32m2025-09-23 12:50:12.191\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlogging\u001b[0m:\u001b[36mcallHandlers\u001b[0m:\u001b[36m1762\u001b[0m | Processing request of type GetPromptRequest\n",
      "\u001b[32m2025-09-23 12:52:13.135\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlogging\u001b[0m:\u001b[36mcallHandlers\u001b[0m:\u001b[36m1762\u001b[0m | Processing request of type CallToolRequest\n",
      "\u001b[32m2025-09-23 12:52:13.141\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlogging\u001b[0m:\u001b[36mcallHandlers\u001b[0m:\u001b[36m1762\u001b[0m | Processing request of type ListToolsRequest\n",
      "\u001b[32m2025-09-23 12:52:19.364\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlogging\u001b[0m:\u001b[36mcallHandlers\u001b[0m:\u001b[36m1762\u001b[0m | Processing request of type CallToolRequest\n",
      "\u001b[32m2025-09-23 12:53:44.751\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36mlogging\u001b[0m:\u001b[36mcallHandlers\u001b[0m:\u001b[36m1762\u001b[0m | Error scraping https://x.com/karpathy/status/1937902205765607626: Website Not Supported: Failed to scrape. This website is no longer supported, please reach out to help@firecrawl.com for more info on how to activate it on your account. - No additional error details provided.\n",
      "\u001b[33m\u001b[1mTraceback (most recent call last):\u001b[0m\n",
      "\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/Users/fabio/Desktop/course-ai-agents/.venv/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "    â”‚   â”” <bound method Application.launch_instance of <class 'ipykernel.kernelapp.IPKernelApp'>>\n",
      "    â”” <module 'ipykernel.kernelapp' from '/Users/fabio/Desktop/course-ai-agents/.venv/lib/python3.12/site-packages/ipykernel/kernel...\n",
      "  File \"/Users/fabio/Desktop/course-ai-agents/.venv/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "    â”‚   â”” <function IPKernelApp.start at 0x105b92a20>\n",
      "    â”” <ipykernel.kernelapp.IPKernelApp object at 0x102b1be30>\n",
      "  File \"/Users/fabio/Desktop/course-ai-agents/.venv/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "    â”‚    â”‚       â”” <function BaseAsyncIOLoop.start at 0x105b937e0>\n",
      "    â”‚    â”” <tornado.platform.asyncio.AsyncIOMainLoop object at 0x105ba65a0>\n",
      "    â”” <ipykernel.kernelapp.IPKernelApp object at 0x102b1be30>\n",
      "  File \"/Users/fabio/Desktop/course-ai-agents/.venv/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "    â”‚    â”‚            â”” <function _patch_loop.<locals>.run_forever at 0x121e667a0>\n",
      "    â”‚    â”” <_UnixSelectorEventLoop running=True closed=False debug=False>\n",
      "    â”” <tornado.platform.asyncio.AsyncIOMainLoop object at 0x105ba65a0>\n",
      "  File \"/Users/fabio/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/asyncio/base_events.py\", line 645, in run_forever\n",
      "    self._run_once()\n",
      "    â”‚    â”” <function _patch_loop.<locals>._run_once at 0x121e668e0>\n",
      "    â”” <_UnixSelectorEventLoop running=True closed=False debug=False>\n",
      "  File \"/Users/fabio/Desktop/course-ai-agents/.venv/lib/python3.12/site-packages/nest_asyncio.py\", line 133, in _run_once\n",
      "    handle._run()\n",
      "    â”‚      â”” <function Handle._run at 0x1031b8680>\n",
      "    â”” <Handle Task.__step()>\n",
      "  File \"/Users/fabio/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "    â”‚    â”‚            â”‚    â”‚           â”‚    â”” <member '_args' of 'Handle' objects>\n",
      "    â”‚    â”‚            â”‚    â”‚           â”” <Handle Task.__step()>\n",
      "    â”‚    â”‚            â”‚    â”” <member '_callback' of 'Handle' objects>\n",
      "    â”‚    â”‚            â”” <Handle Task.__step()>\n",
      "    â”‚    â”” <member '_context' of 'Handle' objects>\n",
      "    â”” <Handle Task.__step()>\n",
      "  File \"/Users/fabio/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/asyncio/tasks.py\", line 303, in __step\n",
      "    self.__step_run_and_handle_result(exc)\n",
      "    â”‚                                 â”” None\n",
      "    â”” <Task pending name='Task-81' coro=<scrape_urls_concurrently.<locals>.scrape_with_semaphore() running at /Users/fabio/Desktop/...\n",
      "  File \"/Users/fabio/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             â”‚    â”” <method 'send' of 'coroutine' objects>\n",
      "             â”” <coroutine object scrape_urls_concurrently.<locals>.scrape_with_semaphore at 0x132aee040>\n",
      "\n",
      "  File \"\u001b[32m/Users/fabio/Desktop/course-ai-agents/lessons/research_agent_part_2/mcp_server/src/app/\u001b[0m\u001b[32m\u001b[1mscraping_handler.py\u001b[0m\", line \u001b[33m189\u001b[0m, in \u001b[35mscrape_with_semaphore\u001b[0m\n",
      "    \u001b[35m\u001b[1mreturn\u001b[0m \u001b[35m\u001b[1mawait\u001b[0m \u001b[1mscrape_and_clean\u001b[0m\u001b[1m(\u001b[0m\u001b[1murl\u001b[0m\u001b[1m,\u001b[0m \u001b[1mguidelines\u001b[0m\u001b[1m,\u001b[0m \u001b[1mfirecrawl_app\u001b[0m\u001b[1m,\u001b[0m \u001b[1mchat_model\u001b[0m\u001b[1m)\u001b[0m\n",
      "    \u001b[36m             â”‚                â”‚    â”‚           â”‚              â”” \u001b[0m\u001b[36m\u001b[1mChatGoogleGenerativeAI(model='models/gemini-2.5-flash', google_api_key=SecretStr('**********'), temperature=1.0, max_retries=...\u001b[0m\n",
      "    \u001b[36m             â”‚                â”‚    â”‚           â”” \u001b[0m\u001b[36m\u001b[1m<firecrawl.client.AsyncFirecrawl object at 0x132b84a10>\u001b[0m\n",
      "    \u001b[36m             â”‚                â”‚    â”” \u001b[0m\u001b[36m\u001b[1m'## Global Context of the Lesson\\n\\n### What We Are Planning to Share\\n\\nWe will write a lesson on context engineering, the c...\u001b[0m\n",
      "    \u001b[36m             â”‚                â”” \u001b[0m\u001b[36m\u001b[1m'https://x.com/karpathy/status/1937902205765607626'\u001b[0m\n",
      "    \u001b[36m             â”” \u001b[0m\u001b[36m\u001b[1m<function scrape_and_clean at 0x132ac7600>\u001b[0m\n",
      "\n",
      "  File \"\u001b[32m/Users/fabio/Desktop/course-ai-agents/lessons/research_agent_part_2/mcp_server/src/app/\u001b[0m\u001b[32m\u001b[1mscraping_handler.py\u001b[0m\", line \u001b[33m152\u001b[0m, in \u001b[35mscrape_and_clean\u001b[0m\n",
      "    \u001b[1mscraped\u001b[0m \u001b[35m\u001b[1m=\u001b[0m \u001b[35m\u001b[1mawait\u001b[0m \u001b[1mscrape_url\u001b[0m\u001b[1m(\u001b[0m\u001b[1murl\u001b[0m\u001b[1m,\u001b[0m \u001b[1mfirecrawl_app\u001b[0m\u001b[1m)\u001b[0m\n",
      "    \u001b[36m                â”‚          â”‚    â”” \u001b[0m\u001b[36m\u001b[1m<firecrawl.client.AsyncFirecrawl object at 0x132b84a10>\u001b[0m\n",
      "    \u001b[36m                â”‚          â”” \u001b[0m\u001b[36m\u001b[1m'https://x.com/karpathy/status/1937902205765607626'\u001b[0m\n",
      "    \u001b[36m                â”” \u001b[0m\u001b[36m\u001b[1m<function scrape_url at 0x132ac7420>\u001b[0m\n",
      "\n",
      "> File \"\u001b[32m/Users/fabio/Desktop/course-ai-agents/lessons/research_agent_part_2/mcp_server/src/app/\u001b[0m\u001b[32m\u001b[1mscraping_handler.py\u001b[0m\", line \u001b[33m61\u001b[0m, in \u001b[35mscrape_url\u001b[0m\n",
      "    \u001b[1mres\u001b[0m \u001b[35m\u001b[1m=\u001b[0m \u001b[35m\u001b[1mawait\u001b[0m \u001b[1mfirecrawl_app\u001b[0m\u001b[35m\u001b[1m.\u001b[0m\u001b[1mscrape\u001b[0m\u001b[1m(\u001b[0m\n",
      "    \u001b[36m            â”‚             â”” \u001b[0m\u001b[36m\u001b[1m<bound method AsyncFirecrawlClient.scrape of <firecrawl.v2.client_async.AsyncFirecrawlClient object at 0x132b85640>>\u001b[0m\n",
      "    \u001b[36m            â”” \u001b[0m\u001b[36m\u001b[1m<firecrawl.client.AsyncFirecrawl object at 0x132b84a10>\u001b[0m\n",
      "\n",
      "  File \"/Users/fabio/Desktop/course-ai-agents/.venv/lib/python3.12/site-packages/firecrawl/v2/client_async.py\", line 65, in scrape\n",
      "    return await async_scrape.scrape(self.async_http_client, url, options)\n",
      "                 â”‚            â”‚      â”‚    â”‚                  â”‚    â”” ScrapeOptions(formats=['markdown'], headers=None, include_tags=None, exclude_tags=None, only_main_content=None, timeout=12000...\n",
      "                 â”‚            â”‚      â”‚    â”‚                  â”” 'https://x.com/karpathy/status/1937902205765607626'\n",
      "                 â”‚            â”‚      â”‚    â”” <firecrawl.v2.utils.http_client_async.AsyncHttpClient object at 0x132bac650>\n",
      "                 â”‚            â”‚      â”” <firecrawl.v2.client_async.AsyncFirecrawlClient object at 0x132b85640>\n",
      "                 â”‚            â”” <function scrape at 0x126babf60>\n",
      "                 â”” <module 'firecrawl.v2.methods.aio.scrape' from '/Users/fabio/Desktop/course-ai-agents/.venv/lib/python3.12/site-packages/fire...\n",
      "  File \"/Users/fabio/Desktop/course-ai-agents/.venv/lib/python3.12/site-packages/firecrawl/v2/methods/aio/scrape.py\", line 26, in scrape\n",
      "    handle_response_error(response, \"scrape\")\n",
      "    â”‚                     â”” <Response [403 Forbidden]>\n",
      "    â”” <function handle_response_error at 0x126c03b00>\n",
      "  File \"/Users/fabio/Desktop/course-ai-agents/.venv/lib/python3.12/site-packages/firecrawl/v2/utils/error_handler.py\", line 95, in handle_response_error\n",
      "    raise WebsiteNotSupportedError(message, response.status_code, response)\n",
      "          â”‚                        â”‚        â”‚        â”‚            â”” <Response [403 Forbidden]>\n",
      "          â”‚                        â”‚        â”‚        â”” 403\n",
      "          â”‚                        â”‚        â”” <Response [403 Forbidden]>\n",
      "          â”‚                        â”” 'Website Not Supported: Failed to scrape. This website is no longer supported, please reach out to help@firecrawl.com for mor...\n",
      "          â”” <class 'firecrawl.v2.utils.error_handler.WebsiteNotSupportedError'>\n",
      "\n",
      "\u001b[31m\u001b[1mfirecrawl.v2.utils.error_handler.WebsiteNotSupportedError\u001b[0m:\u001b[1m Website Not Supported: Failed to scrape. This website is no longer supported, please reach out to help@firecrawl.com for more info on how to activate it on your account. - No additional error details provided.\u001b[0m\n",
      "\u001b[32m2025-09-23 12:53:44.766\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mlogging\u001b[0m:\u001b[36mcallHandlers\u001b[0m:\u001b[36m1762\u001b[0m | âš ï¸ Error scraping https://x.com/karpathy/status/1937902205765607626 (attempt 1/3). Retrying in 5s...\n",
      "\u001b[32m2025-09-23 12:53:49.925\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36mlogging\u001b[0m:\u001b[36mcallHandlers\u001b[0m:\u001b[36m1762\u001b[0m | Error scraping https://x.com/karpathy/status/1937902205765607626: Website Not Supported: Failed to scrape. This website is no longer supported, please reach out to help@firecrawl.com for more info on how to activate it on your account. - No additional error details provided.\n",
      "\u001b[33m\u001b[1mTraceback (most recent call last):\u001b[0m\n",
      "\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/Users/fabio/Desktop/course-ai-agents/.venv/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "    â”‚   â”” <bound method Application.launch_instance of <class 'ipykernel.kernelapp.IPKernelApp'>>\n",
      "    â”” <module 'ipykernel.kernelapp' from '/Users/fabio/Desktop/course-ai-agents/.venv/lib/python3.12/site-packages/ipykernel/kernel...\n",
      "  File \"/Users/fabio/Desktop/course-ai-agents/.venv/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "    â”‚   â”” <function IPKernelApp.start at 0x105b92a20>\n",
      "    â”” <ipykernel.kernelapp.IPKernelApp object at 0x102b1be30>\n",
      "  File \"/Users/fabio/Desktop/course-ai-agents/.venv/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "    â”‚    â”‚       â”” <function BaseAsyncIOLoop.start at 0x105b937e0>\n",
      "    â”‚    â”” <tornado.platform.asyncio.AsyncIOMainLoop object at 0x105ba65a0>\n",
      "    â”” <ipykernel.kernelapp.IPKernelApp object at 0x102b1be30>\n",
      "  File \"/Users/fabio/Desktop/course-ai-agents/.venv/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "    â”‚    â”‚            â”” <function _patch_loop.<locals>.run_forever at 0x121e667a0>\n",
      "    â”‚    â”” <_UnixSelectorEventLoop running=True closed=False debug=False>\n",
      "    â”” <tornado.platform.asyncio.AsyncIOMainLoop object at 0x105ba65a0>\n",
      "  File \"/Users/fabio/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/asyncio/base_events.py\", line 645, in run_forever\n",
      "    self._run_once()\n",
      "    â”‚    â”” <function _patch_loop.<locals>._run_once at 0x121e668e0>\n",
      "    â”” <_UnixSelectorEventLoop running=True closed=False debug=False>\n",
      "  File \"/Users/fabio/Desktop/course-ai-agents/.venv/lib/python3.12/site-packages/nest_asyncio.py\", line 133, in _run_once\n",
      "    handle._run()\n",
      "    â”‚      â”” <function Handle._run at 0x1031b8680>\n",
      "    â”” <Handle Task.__step()>\n",
      "  File \"/Users/fabio/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "    â”‚    â”‚            â”‚    â”‚           â”‚    â”” <member '_args' of 'Handle' objects>\n",
      "    â”‚    â”‚            â”‚    â”‚           â”” <Handle Task.__step()>\n",
      "    â”‚    â”‚            â”‚    â”” <member '_callback' of 'Handle' objects>\n",
      "    â”‚    â”‚            â”” <Handle Task.__step()>\n",
      "    â”‚    â”” <member '_context' of 'Handle' objects>\n",
      "    â”” <Handle Task.__step()>\n",
      "  File \"/Users/fabio/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/asyncio/tasks.py\", line 303, in __step\n",
      "    self.__step_run_and_handle_result(exc)\n",
      "    â”‚                                 â”” None\n",
      "    â”” <Task pending name='Task-81' coro=<scrape_urls_concurrently.<locals>.scrape_with_semaphore() running at /Users/fabio/Desktop/...\n",
      "  File \"/Users/fabio/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             â”‚    â”” <method 'send' of 'coroutine' objects>\n",
      "             â”” <coroutine object scrape_urls_concurrently.<locals>.scrape_with_semaphore at 0x132aee040>\n",
      "\n",
      "  File \"\u001b[32m/Users/fabio/Desktop/course-ai-agents/lessons/research_agent_part_2/mcp_server/src/app/\u001b[0m\u001b[32m\u001b[1mscraping_handler.py\u001b[0m\", line \u001b[33m189\u001b[0m, in \u001b[35mscrape_with_semaphore\u001b[0m\n",
      "    \u001b[35m\u001b[1mreturn\u001b[0m \u001b[35m\u001b[1mawait\u001b[0m \u001b[1mscrape_and_clean\u001b[0m\u001b[1m(\u001b[0m\u001b[1murl\u001b[0m\u001b[1m,\u001b[0m \u001b[1mguidelines\u001b[0m\u001b[1m,\u001b[0m \u001b[1mfirecrawl_app\u001b[0m\u001b[1m,\u001b[0m \u001b[1mchat_model\u001b[0m\u001b[1m)\u001b[0m\n",
      "    \u001b[36m             â”‚                â”‚    â”‚           â”‚              â”” \u001b[0m\u001b[36m\u001b[1mChatGoogleGenerativeAI(model='models/gemini-2.5-flash', google_api_key=SecretStr('**********'), temperature=1.0, max_retries=...\u001b[0m\n",
      "    \u001b[36m             â”‚                â”‚    â”‚           â”” \u001b[0m\u001b[36m\u001b[1m<firecrawl.client.AsyncFirecrawl object at 0x132b84a10>\u001b[0m\n",
      "    \u001b[36m             â”‚                â”‚    â”” \u001b[0m\u001b[36m\u001b[1m'## Global Context of the Lesson\\n\\n### What We Are Planning to Share\\n\\nWe will write a lesson on context engineering, the c...\u001b[0m\n",
      "    \u001b[36m             â”‚                â”” \u001b[0m\u001b[36m\u001b[1m'https://x.com/karpathy/status/1937902205765607626'\u001b[0m\n",
      "    \u001b[36m             â”” \u001b[0m\u001b[36m\u001b[1m<function scrape_and_clean at 0x132ac7600>\u001b[0m\n",
      "\n",
      "  File \"\u001b[32m/Users/fabio/Desktop/course-ai-agents/lessons/research_agent_part_2/mcp_server/src/app/\u001b[0m\u001b[32m\u001b[1mscraping_handler.py\u001b[0m\", line \u001b[33m152\u001b[0m, in \u001b[35mscrape_and_clean\u001b[0m\n",
      "    \u001b[1mscraped\u001b[0m \u001b[35m\u001b[1m=\u001b[0m \u001b[35m\u001b[1mawait\u001b[0m \u001b[1mscrape_url\u001b[0m\u001b[1m(\u001b[0m\u001b[1murl\u001b[0m\u001b[1m,\u001b[0m \u001b[1mfirecrawl_app\u001b[0m\u001b[1m)\u001b[0m\n",
      "    \u001b[36m                â”‚          â”‚    â”” \u001b[0m\u001b[36m\u001b[1m<firecrawl.client.AsyncFirecrawl object at 0x132b84a10>\u001b[0m\n",
      "    \u001b[36m                â”‚          â”” \u001b[0m\u001b[36m\u001b[1m'https://x.com/karpathy/status/1937902205765607626'\u001b[0m\n",
      "    \u001b[36m                â”” \u001b[0m\u001b[36m\u001b[1m<function scrape_url at 0x132ac7420>\u001b[0m\n",
      "\n",
      "> File \"\u001b[32m/Users/fabio/Desktop/course-ai-agents/lessons/research_agent_part_2/mcp_server/src/app/\u001b[0m\u001b[32m\u001b[1mscraping_handler.py\u001b[0m\", line \u001b[33m61\u001b[0m, in \u001b[35mscrape_url\u001b[0m\n",
      "    \u001b[1mres\u001b[0m \u001b[35m\u001b[1m=\u001b[0m \u001b[35m\u001b[1mawait\u001b[0m \u001b[1mfirecrawl_app\u001b[0m\u001b[35m\u001b[1m.\u001b[0m\u001b[1mscrape\u001b[0m\u001b[1m(\u001b[0m\n",
      "    \u001b[36m            â”‚             â”” \u001b[0m\u001b[36m\u001b[1m<bound method AsyncFirecrawlClient.scrape of <firecrawl.v2.client_async.AsyncFirecrawlClient object at 0x132b85640>>\u001b[0m\n",
      "    \u001b[36m            â”” \u001b[0m\u001b[36m\u001b[1m<firecrawl.client.AsyncFirecrawl object at 0x132b84a10>\u001b[0m\n",
      "\n",
      "  File \"/Users/fabio/Desktop/course-ai-agents/.venv/lib/python3.12/site-packages/firecrawl/v2/client_async.py\", line 65, in scrape\n",
      "    return await async_scrape.scrape(self.async_http_client, url, options)\n",
      "                 â”‚            â”‚      â”‚    â”‚                  â”‚    â”” ScrapeOptions(formats=['markdown'], headers=None, include_tags=None, exclude_tags=None, only_main_content=None, timeout=12000...\n",
      "                 â”‚            â”‚      â”‚    â”‚                  â”” 'https://x.com/karpathy/status/1937902205765607626'\n",
      "                 â”‚            â”‚      â”‚    â”” <firecrawl.v2.utils.http_client_async.AsyncHttpClient object at 0x132bac650>\n",
      "                 â”‚            â”‚      â”” <firecrawl.v2.client_async.AsyncFirecrawlClient object at 0x132b85640>\n",
      "                 â”‚            â”” <function scrape at 0x126babf60>\n",
      "                 â”” <module 'firecrawl.v2.methods.aio.scrape' from '/Users/fabio/Desktop/course-ai-agents/.venv/lib/python3.12/site-packages/fire...\n",
      "  File \"/Users/fabio/Desktop/course-ai-agents/.venv/lib/python3.12/site-packages/firecrawl/v2/methods/aio/scrape.py\", line 26, in scrape\n",
      "    handle_response_error(response, \"scrape\")\n",
      "    â”‚                     â”” <Response [403 Forbidden]>\n",
      "    â”” <function handle_response_error at 0x126c03b00>\n",
      "  File \"/Users/fabio/Desktop/course-ai-agents/.venv/lib/python3.12/site-packages/firecrawl/v2/utils/error_handler.py\", line 95, in handle_response_error\n",
      "    raise WebsiteNotSupportedError(message, response.status_code, response)\n",
      "          â”‚                        â”‚        â”‚        â”‚            â”” <Response [403 Forbidden]>\n",
      "          â”‚                        â”‚        â”‚        â”” 403\n",
      "          â”‚                        â”‚        â”” <Response [403 Forbidden]>\n",
      "          â”‚                        â”” 'Website Not Supported: Failed to scrape. This website is no longer supported, please reach out to help@firecrawl.com for mor...\n",
      "          â”” <class 'firecrawl.v2.utils.error_handler.WebsiteNotSupportedError'>\n",
      "\n",
      "\u001b[31m\u001b[1mfirecrawl.v2.utils.error_handler.WebsiteNotSupportedError\u001b[0m:\u001b[1m Website Not Supported: Failed to scrape. This website is no longer supported, please reach out to help@firecrawl.com for more info on how to activate it on your account. - No additional error details provided.\u001b[0m\n",
      "\u001b[32m2025-09-23 12:53:49.929\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mlogging\u001b[0m:\u001b[36mcallHandlers\u001b[0m:\u001b[36m1762\u001b[0m | âš ï¸ Error scraping https://x.com/karpathy/status/1937902205765607626 (attempt 2/3). Retrying in 10s...\n",
      "\u001b[32m2025-09-23 12:53:51.647\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36mlogging\u001b[0m:\u001b[36mcallHandlers\u001b[0m:\u001b[36m1762\u001b[0m | Error scraping https://x.com/lenadroid/status/1943685060785524824: Website Not Supported: Failed to scrape. This website is no longer supported, please reach out to help@firecrawl.com for more info on how to activate it on your account. - No additional error details provided.\n",
      "\u001b[33m\u001b[1mTraceback (most recent call last):\u001b[0m\n",
      "\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/Users/fabio/Desktop/course-ai-agents/.venv/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "    â”‚   â”” <bound method Application.launch_instance of <class 'ipykernel.kernelapp.IPKernelApp'>>\n",
      "    â”” <module 'ipykernel.kernelapp' from '/Users/fabio/Desktop/course-ai-agents/.venv/lib/python3.12/site-packages/ipykernel/kernel...\n",
      "  File \"/Users/fabio/Desktop/course-ai-agents/.venv/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "    â”‚   â”” <function IPKernelApp.start at 0x105b92a20>\n",
      "    â”” <ipykernel.kernelapp.IPKernelApp object at 0x102b1be30>\n",
      "  File \"/Users/fabio/Desktop/course-ai-agents/.venv/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "    â”‚    â”‚       â”” <function BaseAsyncIOLoop.start at 0x105b937e0>\n",
      "    â”‚    â”” <tornado.platform.asyncio.AsyncIOMainLoop object at 0x105ba65a0>\n",
      "    â”” <ipykernel.kernelapp.IPKernelApp object at 0x102b1be30>\n",
      "  File \"/Users/fabio/Desktop/course-ai-agents/.venv/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "    â”‚    â”‚            â”” <function _patch_loop.<locals>.run_forever at 0x121e667a0>\n",
      "    â”‚    â”” <_UnixSelectorEventLoop running=True closed=False debug=False>\n",
      "    â”” <tornado.platform.asyncio.AsyncIOMainLoop object at 0x105ba65a0>\n",
      "  File \"/Users/fabio/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/asyncio/base_events.py\", line 645, in run_forever\n",
      "    self._run_once()\n",
      "    â”‚    â”” <function _patch_loop.<locals>._run_once at 0x121e668e0>\n",
      "    â”” <_UnixSelectorEventLoop running=True closed=False debug=False>\n",
      "  File \"/Users/fabio/Desktop/course-ai-agents/.venv/lib/python3.12/site-packages/nest_asyncio.py\", line 133, in _run_once\n",
      "    handle._run()\n",
      "    â”‚      â”” <function Handle._run at 0x1031b8680>\n",
      "    â”” <Handle Task.__step()>\n",
      "  File \"/Users/fabio/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "    â”‚    â”‚            â”‚    â”‚           â”‚    â”” <member '_args' of 'Handle' objects>\n",
      "    â”‚    â”‚            â”‚    â”‚           â”” <Handle Task.__step()>\n",
      "    â”‚    â”‚            â”‚    â”” <member '_callback' of 'Handle' objects>\n",
      "    â”‚    â”‚            â”” <Handle Task.__step()>\n",
      "    â”‚    â”” <member '_context' of 'Handle' objects>\n",
      "    â”” <Handle Task.__step()>\n",
      "  File \"/Users/fabio/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/asyncio/tasks.py\", line 303, in __step\n",
      "    self.__step_run_and_handle_result(exc)\n",
      "    â”‚                                 â”” None\n",
      "    â”” <Task pending name='Task-82' coro=<scrape_urls_concurrently.<locals>.scrape_with_semaphore() running at /Users/fabio/Desktop/...\n",
      "  File \"/Users/fabio/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             â”‚    â”” <method 'send' of 'coroutine' objects>\n",
      "             â”” <coroutine object scrape_urls_concurrently.<locals>.scrape_with_semaphore at 0x132aee440>\n",
      "\n",
      "  File \"\u001b[32m/Users/fabio/Desktop/course-ai-agents/lessons/research_agent_part_2/mcp_server/src/app/\u001b[0m\u001b[32m\u001b[1mscraping_handler.py\u001b[0m\", line \u001b[33m189\u001b[0m, in \u001b[35mscrape_with_semaphore\u001b[0m\n",
      "    \u001b[35m\u001b[1mreturn\u001b[0m \u001b[35m\u001b[1mawait\u001b[0m \u001b[1mscrape_and_clean\u001b[0m\u001b[1m(\u001b[0m\u001b[1murl\u001b[0m\u001b[1m,\u001b[0m \u001b[1mguidelines\u001b[0m\u001b[1m,\u001b[0m \u001b[1mfirecrawl_app\u001b[0m\u001b[1m,\u001b[0m \u001b[1mchat_model\u001b[0m\u001b[1m)\u001b[0m\n",
      "    \u001b[36m             â”‚                â”‚    â”‚           â”‚              â”” \u001b[0m\u001b[36m\u001b[1mChatGoogleGenerativeAI(model='models/gemini-2.5-flash', google_api_key=SecretStr('**********'), temperature=1.0, max_retries=...\u001b[0m\n",
      "    \u001b[36m             â”‚                â”‚    â”‚           â”” \u001b[0m\u001b[36m\u001b[1m<firecrawl.client.AsyncFirecrawl object at 0x132b84a10>\u001b[0m\n",
      "    \u001b[36m             â”‚                â”‚    â”” \u001b[0m\u001b[36m\u001b[1m'## Global Context of the Lesson\\n\\n### What We Are Planning to Share\\n\\nWe will write a lesson on context engineering, the c...\u001b[0m\n",
      "    \u001b[36m             â”‚                â”” \u001b[0m\u001b[36m\u001b[1m'https://x.com/lenadroid/status/1943685060785524824'\u001b[0m\n",
      "    \u001b[36m             â”” \u001b[0m\u001b[36m\u001b[1m<function scrape_and_clean at 0x132ac7600>\u001b[0m\n",
      "\n",
      "  File \"\u001b[32m/Users/fabio/Desktop/course-ai-agents/lessons/research_agent_part_2/mcp_server/src/app/\u001b[0m\u001b[32m\u001b[1mscraping_handler.py\u001b[0m\", line \u001b[33m152\u001b[0m, in \u001b[35mscrape_and_clean\u001b[0m\n",
      "    \u001b[1mscraped\u001b[0m \u001b[35m\u001b[1m=\u001b[0m \u001b[35m\u001b[1mawait\u001b[0m \u001b[1mscrape_url\u001b[0m\u001b[1m(\u001b[0m\u001b[1murl\u001b[0m\u001b[1m,\u001b[0m \u001b[1mfirecrawl_app\u001b[0m\u001b[1m)\u001b[0m\n",
      "    \u001b[36m                â”‚          â”‚    â”” \u001b[0m\u001b[36m\u001b[1m<firecrawl.client.AsyncFirecrawl object at 0x132b84a10>\u001b[0m\n",
      "    \u001b[36m                â”‚          â”” \u001b[0m\u001b[36m\u001b[1m'https://x.com/lenadroid/status/1943685060785524824'\u001b[0m\n",
      "    \u001b[36m                â”” \u001b[0m\u001b[36m\u001b[1m<function scrape_url at 0x132ac7420>\u001b[0m\n",
      "\n",
      "> File \"\u001b[32m/Users/fabio/Desktop/course-ai-agents/lessons/research_agent_part_2/mcp_server/src/app/\u001b[0m\u001b[32m\u001b[1mscraping_handler.py\u001b[0m\", line \u001b[33m61\u001b[0m, in \u001b[35mscrape_url\u001b[0m\n",
      "    \u001b[1mres\u001b[0m \u001b[35m\u001b[1m=\u001b[0m \u001b[35m\u001b[1mawait\u001b[0m \u001b[1mfirecrawl_app\u001b[0m\u001b[35m\u001b[1m.\u001b[0m\u001b[1mscrape\u001b[0m\u001b[1m(\u001b[0m\n",
      "    \u001b[36m            â”‚             â”” \u001b[0m\u001b[36m\u001b[1m<bound method AsyncFirecrawlClient.scrape of <firecrawl.v2.client_async.AsyncFirecrawlClient object at 0x132b85640>>\u001b[0m\n",
      "    \u001b[36m            â”” \u001b[0m\u001b[36m\u001b[1m<firecrawl.client.AsyncFirecrawl object at 0x132b84a10>\u001b[0m\n",
      "\n",
      "  File \"/Users/fabio/Desktop/course-ai-agents/.venv/lib/python3.12/site-packages/firecrawl/v2/client_async.py\", line 65, in scrape\n",
      "    return await async_scrape.scrape(self.async_http_client, url, options)\n",
      "                 â”‚            â”‚      â”‚    â”‚                  â”‚    â”” ScrapeOptions(formats=['markdown'], headers=None, include_tags=None, exclude_tags=None, only_main_content=None, timeout=12000...\n",
      "                 â”‚            â”‚      â”‚    â”‚                  â”” 'https://x.com/lenadroid/status/1943685060785524824'\n",
      "                 â”‚            â”‚      â”‚    â”” <firecrawl.v2.utils.http_client_async.AsyncHttpClient object at 0x132bac650>\n",
      "                 â”‚            â”‚      â”” <firecrawl.v2.client_async.AsyncFirecrawlClient object at 0x132b85640>\n",
      "                 â”‚            â”” <function scrape at 0x126babf60>\n",
      "                 â”” <module 'firecrawl.v2.methods.aio.scrape' from '/Users/fabio/Desktop/course-ai-agents/.venv/lib/python3.12/site-packages/fire...\n",
      "  File \"/Users/fabio/Desktop/course-ai-agents/.venv/lib/python3.12/site-packages/firecrawl/v2/methods/aio/scrape.py\", line 26, in scrape\n",
      "    handle_response_error(response, \"scrape\")\n",
      "    â”‚                     â”” <Response [403 Forbidden]>\n",
      "    â”” <function handle_response_error at 0x126c03b00>\n",
      "  File \"/Users/fabio/Desktop/course-ai-agents/.venv/lib/python3.12/site-packages/firecrawl/v2/utils/error_handler.py\", line 95, in handle_response_error\n",
      "    raise WebsiteNotSupportedError(message, response.status_code, response)\n",
      "          â”‚                        â”‚        â”‚        â”‚            â”” <Response [403 Forbidden]>\n",
      "          â”‚                        â”‚        â”‚        â”” 403\n",
      "          â”‚                        â”‚        â”” <Response [403 Forbidden]>\n",
      "          â”‚                        â”” 'Website Not Supported: Failed to scrape. This website is no longer supported, please reach out to help@firecrawl.com for mor...\n",
      "          â”” <class 'firecrawl.v2.utils.error_handler.WebsiteNotSupportedError'>\n",
      "\n",
      "\u001b[31m\u001b[1mfirecrawl.v2.utils.error_handler.WebsiteNotSupportedError\u001b[0m:\u001b[1m Website Not Supported: Failed to scrape. This website is no longer supported, please reach out to help@firecrawl.com for more info on how to activate it on your account. - No additional error details provided.\u001b[0m\n",
      "\u001b[32m2025-09-23 12:53:51.651\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mlogging\u001b[0m:\u001b[36mcallHandlers\u001b[0m:\u001b[36m1762\u001b[0m | âš ï¸ Error scraping https://x.com/lenadroid/status/1943685060785524824 (attempt 1/3). Retrying in 5s...\n",
      "\u001b[32m2025-09-23 12:53:56.797\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36mlogging\u001b[0m:\u001b[36mcallHandlers\u001b[0m:\u001b[36m1762\u001b[0m | Error scraping https://x.com/lenadroid/status/1943685060785524824: Website Not Supported: Failed to scrape. This website is no longer supported, please reach out to help@firecrawl.com for more info on how to activate it on your account. - No additional error details provided.\n",
      "\u001b[33m\u001b[1mTraceback (most recent call last):\u001b[0m\n",
      "\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/Users/fabio/Desktop/course-ai-agents/.venv/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "    â”‚   â”” <bound method Application.launch_instance of <class 'ipykernel.kernelapp.IPKernelApp'>>\n",
      "    â”” <module 'ipykernel.kernelapp' from '/Users/fabio/Desktop/course-ai-agents/.venv/lib/python3.12/site-packages/ipykernel/kernel...\n",
      "  File \"/Users/fabio/Desktop/course-ai-agents/.venv/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "    â”‚   â”” <function IPKernelApp.start at 0x105b92a20>\n",
      "    â”” <ipykernel.kernelapp.IPKernelApp object at 0x102b1be30>\n",
      "  File \"/Users/fabio/Desktop/course-ai-agents/.venv/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "    â”‚    â”‚       â”” <function BaseAsyncIOLoop.start at 0x105b937e0>\n",
      "    â”‚    â”” <tornado.platform.asyncio.AsyncIOMainLoop object at 0x105ba65a0>\n",
      "    â”” <ipykernel.kernelapp.IPKernelApp object at 0x102b1be30>\n",
      "  File \"/Users/fabio/Desktop/course-ai-agents/.venv/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "    â”‚    â”‚            â”” <function _patch_loop.<locals>.run_forever at 0x121e667a0>\n",
      "    â”‚    â”” <_UnixSelectorEventLoop running=True closed=False debug=False>\n",
      "    â”” <tornado.platform.asyncio.AsyncIOMainLoop object at 0x105ba65a0>\n",
      "  File \"/Users/fabio/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/asyncio/base_events.py\", line 645, in run_forever\n",
      "    self._run_once()\n",
      "    â”‚    â”” <function _patch_loop.<locals>._run_once at 0x121e668e0>\n",
      "    â”” <_UnixSelectorEventLoop running=True closed=False debug=False>\n",
      "  File \"/Users/fabio/Desktop/course-ai-agents/.venv/lib/python3.12/site-packages/nest_asyncio.py\", line 133, in _run_once\n",
      "    handle._run()\n",
      "    â”‚      â”” <function Handle._run at 0x1031b8680>\n",
      "    â”” <Handle Task.__step()>\n",
      "  File \"/Users/fabio/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "    â”‚    â”‚            â”‚    â”‚           â”‚    â”” <member '_args' of 'Handle' objects>\n",
      "    â”‚    â”‚            â”‚    â”‚           â”” <Handle Task.__step()>\n",
      "    â”‚    â”‚            â”‚    â”” <member '_callback' of 'Handle' objects>\n",
      "    â”‚    â”‚            â”” <Handle Task.__step()>\n",
      "    â”‚    â”” <member '_context' of 'Handle' objects>\n",
      "    â”” <Handle Task.__step()>\n",
      "  File \"/Users/fabio/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/asyncio/tasks.py\", line 303, in __step\n",
      "    self.__step_run_and_handle_result(exc)\n",
      "    â”‚                                 â”” None\n",
      "    â”” <Task pending name='Task-82' coro=<scrape_urls_concurrently.<locals>.scrape_with_semaphore() running at /Users/fabio/Desktop/...\n",
      "  File \"/Users/fabio/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             â”‚    â”” <method 'send' of 'coroutine' objects>\n",
      "             â”” <coroutine object scrape_urls_concurrently.<locals>.scrape_with_semaphore at 0x132aee440>\n",
      "\n",
      "  File \"\u001b[32m/Users/fabio/Desktop/course-ai-agents/lessons/research_agent_part_2/mcp_server/src/app/\u001b[0m\u001b[32m\u001b[1mscraping_handler.py\u001b[0m\", line \u001b[33m189\u001b[0m, in \u001b[35mscrape_with_semaphore\u001b[0m\n",
      "    \u001b[35m\u001b[1mreturn\u001b[0m \u001b[35m\u001b[1mawait\u001b[0m \u001b[1mscrape_and_clean\u001b[0m\u001b[1m(\u001b[0m\u001b[1murl\u001b[0m\u001b[1m,\u001b[0m \u001b[1mguidelines\u001b[0m\u001b[1m,\u001b[0m \u001b[1mfirecrawl_app\u001b[0m\u001b[1m,\u001b[0m \u001b[1mchat_model\u001b[0m\u001b[1m)\u001b[0m\n",
      "    \u001b[36m             â”‚                â”‚    â”‚           â”‚              â”” \u001b[0m\u001b[36m\u001b[1mChatGoogleGenerativeAI(model='models/gemini-2.5-flash', google_api_key=SecretStr('**********'), temperature=1.0, max_retries=...\u001b[0m\n",
      "    \u001b[36m             â”‚                â”‚    â”‚           â”” \u001b[0m\u001b[36m\u001b[1m<firecrawl.client.AsyncFirecrawl object at 0x132b84a10>\u001b[0m\n",
      "    \u001b[36m             â”‚                â”‚    â”” \u001b[0m\u001b[36m\u001b[1m'## Global Context of the Lesson\\n\\n### What We Are Planning to Share\\n\\nWe will write a lesson on context engineering, the c...\u001b[0m\n",
      "    \u001b[36m             â”‚                â”” \u001b[0m\u001b[36m\u001b[1m'https://x.com/lenadroid/status/1943685060785524824'\u001b[0m\n",
      "    \u001b[36m             â”” \u001b[0m\u001b[36m\u001b[1m<function scrape_and_clean at 0x132ac7600>\u001b[0m\n",
      "\n",
      "  File \"\u001b[32m/Users/fabio/Desktop/course-ai-agents/lessons/research_agent_part_2/mcp_server/src/app/\u001b[0m\u001b[32m\u001b[1mscraping_handler.py\u001b[0m\", line \u001b[33m152\u001b[0m, in \u001b[35mscrape_and_clean\u001b[0m\n",
      "    \u001b[1mscraped\u001b[0m \u001b[35m\u001b[1m=\u001b[0m \u001b[35m\u001b[1mawait\u001b[0m \u001b[1mscrape_url\u001b[0m\u001b[1m(\u001b[0m\u001b[1murl\u001b[0m\u001b[1m,\u001b[0m \u001b[1mfirecrawl_app\u001b[0m\u001b[1m)\u001b[0m\n",
      "    \u001b[36m                â”‚          â”‚    â”” \u001b[0m\u001b[36m\u001b[1m<firecrawl.client.AsyncFirecrawl object at 0x132b84a10>\u001b[0m\n",
      "    \u001b[36m                â”‚          â”” \u001b[0m\u001b[36m\u001b[1m'https://x.com/lenadroid/status/1943685060785524824'\u001b[0m\n",
      "    \u001b[36m                â”” \u001b[0m\u001b[36m\u001b[1m<function scrape_url at 0x132ac7420>\u001b[0m\n",
      "\n",
      "> File \"\u001b[32m/Users/fabio/Desktop/course-ai-agents/lessons/research_agent_part_2/mcp_server/src/app/\u001b[0m\u001b[32m\u001b[1mscraping_handler.py\u001b[0m\", line \u001b[33m61\u001b[0m, in \u001b[35mscrape_url\u001b[0m\n",
      "    \u001b[1mres\u001b[0m \u001b[35m\u001b[1m=\u001b[0m \u001b[35m\u001b[1mawait\u001b[0m \u001b[1mfirecrawl_app\u001b[0m\u001b[35m\u001b[1m.\u001b[0m\u001b[1mscrape\u001b[0m\u001b[1m(\u001b[0m\n",
      "    \u001b[36m            â”‚             â”” \u001b[0m\u001b[36m\u001b[1m<bound method AsyncFirecrawlClient.scrape of <firecrawl.v2.client_async.AsyncFirecrawlClient object at 0x132b85640>>\u001b[0m\n",
      "    \u001b[36m            â”” \u001b[0m\u001b[36m\u001b[1m<firecrawl.client.AsyncFirecrawl object at 0x132b84a10>\u001b[0m\n",
      "\n",
      "  File \"/Users/fabio/Desktop/course-ai-agents/.venv/lib/python3.12/site-packages/firecrawl/v2/client_async.py\", line 65, in scrape\n",
      "    return await async_scrape.scrape(self.async_http_client, url, options)\n",
      "                 â”‚            â”‚      â”‚    â”‚                  â”‚    â”” ScrapeOptions(formats=['markdown'], headers=None, include_tags=None, exclude_tags=None, only_main_content=None, timeout=12000...\n",
      "                 â”‚            â”‚      â”‚    â”‚                  â”” 'https://x.com/lenadroid/status/1943685060785524824'\n",
      "                 â”‚            â”‚      â”‚    â”” <firecrawl.v2.utils.http_client_async.AsyncHttpClient object at 0x132bac650>\n",
      "                 â”‚            â”‚      â”” <firecrawl.v2.client_async.AsyncFirecrawlClient object at 0x132b85640>\n",
      "                 â”‚            â”” <function scrape at 0x126babf60>\n",
      "                 â”” <module 'firecrawl.v2.methods.aio.scrape' from '/Users/fabio/Desktop/course-ai-agents/.venv/lib/python3.12/site-packages/fire...\n",
      "  File \"/Users/fabio/Desktop/course-ai-agents/.venv/lib/python3.12/site-packages/firecrawl/v2/methods/aio/scrape.py\", line 26, in scrape\n",
      "    handle_response_error(response, \"scrape\")\n",
      "    â”‚                     â”” <Response [403 Forbidden]>\n",
      "    â”” <function handle_response_error at 0x126c03b00>\n",
      "  File \"/Users/fabio/Desktop/course-ai-agents/.venv/lib/python3.12/site-packages/firecrawl/v2/utils/error_handler.py\", line 95, in handle_response_error\n",
      "    raise WebsiteNotSupportedError(message, response.status_code, response)\n",
      "          â”‚                        â”‚        â”‚        â”‚            â”” <Response [403 Forbidden]>\n",
      "          â”‚                        â”‚        â”‚        â”” 403\n",
      "          â”‚                        â”‚        â”” <Response [403 Forbidden]>\n",
      "          â”‚                        â”” 'Website Not Supported: Failed to scrape. This website is no longer supported, please reach out to help@firecrawl.com for mor...\n",
      "          â”” <class 'firecrawl.v2.utils.error_handler.WebsiteNotSupportedError'>\n",
      "\n",
      "\u001b[31m\u001b[1mfirecrawl.v2.utils.error_handler.WebsiteNotSupportedError\u001b[0m:\u001b[1m Website Not Supported: Failed to scrape. This website is no longer supported, please reach out to help@firecrawl.com for more info on how to activate it on your account. - No additional error details provided.\u001b[0m\n",
      "\u001b[32m2025-09-23 12:53:56.801\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mlogging\u001b[0m:\u001b[36mcallHandlers\u001b[0m:\u001b[36m1762\u001b[0m | âš ï¸ Error scraping https://x.com/lenadroid/status/1943685060785524824 (attempt 2/3). Retrying in 10s...\n",
      "\u001b[32m2025-09-23 12:54:00.080\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36mlogging\u001b[0m:\u001b[36mcallHandlers\u001b[0m:\u001b[36m1762\u001b[0m | Error scraping https://x.com/karpathy/status/1937902205765607626: Website Not Supported: Failed to scrape. This website is no longer supported, please reach out to help@firecrawl.com for more info on how to activate it on your account. - No additional error details provided.\n",
      "\u001b[33m\u001b[1mTraceback (most recent call last):\u001b[0m\n",
      "\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/Users/fabio/Desktop/course-ai-agents/.venv/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "    â”‚   â”” <bound method Application.launch_instance of <class 'ipykernel.kernelapp.IPKernelApp'>>\n",
      "    â”” <module 'ipykernel.kernelapp' from '/Users/fabio/Desktop/course-ai-agents/.venv/lib/python3.12/site-packages/ipykernel/kernel...\n",
      "  File \"/Users/fabio/Desktop/course-ai-agents/.venv/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "    â”‚   â”” <function IPKernelApp.start at 0x105b92a20>\n",
      "    â”” <ipykernel.kernelapp.IPKernelApp object at 0x102b1be30>\n",
      "  File \"/Users/fabio/Desktop/course-ai-agents/.venv/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "    â”‚    â”‚       â”” <function BaseAsyncIOLoop.start at 0x105b937e0>\n",
      "    â”‚    â”” <tornado.platform.asyncio.AsyncIOMainLoop object at 0x105ba65a0>\n",
      "    â”” <ipykernel.kernelapp.IPKernelApp object at 0x102b1be30>\n",
      "  File \"/Users/fabio/Desktop/course-ai-agents/.venv/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "    â”‚    â”‚            â”” <function _patch_loop.<locals>.run_forever at 0x121e667a0>\n",
      "    â”‚    â”” <_UnixSelectorEventLoop running=True closed=False debug=False>\n",
      "    â”” <tornado.platform.asyncio.AsyncIOMainLoop object at 0x105ba65a0>\n",
      "  File \"/Users/fabio/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/asyncio/base_events.py\", line 645, in run_forever\n",
      "    self._run_once()\n",
      "    â”‚    â”” <function _patch_loop.<locals>._run_once at 0x121e668e0>\n",
      "    â”” <_UnixSelectorEventLoop running=True closed=False debug=False>\n",
      "  File \"/Users/fabio/Desktop/course-ai-agents/.venv/lib/python3.12/site-packages/nest_asyncio.py\", line 133, in _run_once\n",
      "    handle._run()\n",
      "    â”‚      â”” <function Handle._run at 0x1031b8680>\n",
      "    â”” <Handle Task.__step()>\n",
      "  File \"/Users/fabio/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "    â”‚    â”‚            â”‚    â”‚           â”‚    â”” <member '_args' of 'Handle' objects>\n",
      "    â”‚    â”‚            â”‚    â”‚           â”” <Handle Task.__step()>\n",
      "    â”‚    â”‚            â”‚    â”” <member '_callback' of 'Handle' objects>\n",
      "    â”‚    â”‚            â”” <Handle Task.__step()>\n",
      "    â”‚    â”” <member '_context' of 'Handle' objects>\n",
      "    â”” <Handle Task.__step()>\n",
      "  File \"/Users/fabio/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/asyncio/tasks.py\", line 303, in __step\n",
      "    self.__step_run_and_handle_result(exc)\n",
      "    â”‚                                 â”” None\n",
      "    â”” <Task pending name='Task-81' coro=<scrape_urls_concurrently.<locals>.scrape_with_semaphore() running at /Users/fabio/Desktop/...\n",
      "  File \"/Users/fabio/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             â”‚    â”” <method 'send' of 'coroutine' objects>\n",
      "             â”” <coroutine object scrape_urls_concurrently.<locals>.scrape_with_semaphore at 0x132aee040>\n",
      "\n",
      "  File \"\u001b[32m/Users/fabio/Desktop/course-ai-agents/lessons/research_agent_part_2/mcp_server/src/app/\u001b[0m\u001b[32m\u001b[1mscraping_handler.py\u001b[0m\", line \u001b[33m189\u001b[0m, in \u001b[35mscrape_with_semaphore\u001b[0m\n",
      "    \u001b[35m\u001b[1mreturn\u001b[0m \u001b[35m\u001b[1mawait\u001b[0m \u001b[1mscrape_and_clean\u001b[0m\u001b[1m(\u001b[0m\u001b[1murl\u001b[0m\u001b[1m,\u001b[0m \u001b[1mguidelines\u001b[0m\u001b[1m,\u001b[0m \u001b[1mfirecrawl_app\u001b[0m\u001b[1m,\u001b[0m \u001b[1mchat_model\u001b[0m\u001b[1m)\u001b[0m\n",
      "    \u001b[36m             â”‚                â”‚    â”‚           â”‚              â”” \u001b[0m\u001b[36m\u001b[1mChatGoogleGenerativeAI(model='models/gemini-2.5-flash', google_api_key=SecretStr('**********'), temperature=1.0, max_retries=...\u001b[0m\n",
      "    \u001b[36m             â”‚                â”‚    â”‚           â”” \u001b[0m\u001b[36m\u001b[1m<firecrawl.client.AsyncFirecrawl object at 0x132b84a10>\u001b[0m\n",
      "    \u001b[36m             â”‚                â”‚    â”” \u001b[0m\u001b[36m\u001b[1m'## Global Context of the Lesson\\n\\n### What We Are Planning to Share\\n\\nWe will write a lesson on context engineering, the c...\u001b[0m\n",
      "    \u001b[36m             â”‚                â”” \u001b[0m\u001b[36m\u001b[1m'https://x.com/karpathy/status/1937902205765607626'\u001b[0m\n",
      "    \u001b[36m             â”” \u001b[0m\u001b[36m\u001b[1m<function scrape_and_clean at 0x132ac7600>\u001b[0m\n",
      "\n",
      "  File \"\u001b[32m/Users/fabio/Desktop/course-ai-agents/lessons/research_agent_part_2/mcp_server/src/app/\u001b[0m\u001b[32m\u001b[1mscraping_handler.py\u001b[0m\", line \u001b[33m152\u001b[0m, in \u001b[35mscrape_and_clean\u001b[0m\n",
      "    \u001b[1mscraped\u001b[0m \u001b[35m\u001b[1m=\u001b[0m \u001b[35m\u001b[1mawait\u001b[0m \u001b[1mscrape_url\u001b[0m\u001b[1m(\u001b[0m\u001b[1murl\u001b[0m\u001b[1m,\u001b[0m \u001b[1mfirecrawl_app\u001b[0m\u001b[1m)\u001b[0m\n",
      "    \u001b[36m                â”‚          â”‚    â”” \u001b[0m\u001b[36m\u001b[1m<firecrawl.client.AsyncFirecrawl object at 0x132b84a10>\u001b[0m\n",
      "    \u001b[36m                â”‚          â”” \u001b[0m\u001b[36m\u001b[1m'https://x.com/karpathy/status/1937902205765607626'\u001b[0m\n",
      "    \u001b[36m                â”” \u001b[0m\u001b[36m\u001b[1m<function scrape_url at 0x132ac7420>\u001b[0m\n",
      "\n",
      "> File \"\u001b[32m/Users/fabio/Desktop/course-ai-agents/lessons/research_agent_part_2/mcp_server/src/app/\u001b[0m\u001b[32m\u001b[1mscraping_handler.py\u001b[0m\", line \u001b[33m61\u001b[0m, in \u001b[35mscrape_url\u001b[0m\n",
      "    \u001b[1mres\u001b[0m \u001b[35m\u001b[1m=\u001b[0m \u001b[35m\u001b[1mawait\u001b[0m \u001b[1mfirecrawl_app\u001b[0m\u001b[35m\u001b[1m.\u001b[0m\u001b[1mscrape\u001b[0m\u001b[1m(\u001b[0m\n",
      "    \u001b[36m            â”‚             â”” \u001b[0m\u001b[36m\u001b[1m<bound method AsyncFirecrawlClient.scrape of <firecrawl.v2.client_async.AsyncFirecrawlClient object at 0x132b85640>>\u001b[0m\n",
      "    \u001b[36m            â”” \u001b[0m\u001b[36m\u001b[1m<firecrawl.client.AsyncFirecrawl object at 0x132b84a10>\u001b[0m\n",
      "\n",
      "  File \"/Users/fabio/Desktop/course-ai-agents/.venv/lib/python3.12/site-packages/firecrawl/v2/client_async.py\", line 65, in scrape\n",
      "    return await async_scrape.scrape(self.async_http_client, url, options)\n",
      "                 â”‚            â”‚      â”‚    â”‚                  â”‚    â”” ScrapeOptions(formats=['markdown'], headers=None, include_tags=None, exclude_tags=None, only_main_content=None, timeout=12000...\n",
      "                 â”‚            â”‚      â”‚    â”‚                  â”” 'https://x.com/karpathy/status/1937902205765607626'\n",
      "                 â”‚            â”‚      â”‚    â”” <firecrawl.v2.utils.http_client_async.AsyncHttpClient object at 0x132bac650>\n",
      "                 â”‚            â”‚      â”” <firecrawl.v2.client_async.AsyncFirecrawlClient object at 0x132b85640>\n",
      "                 â”‚            â”” <function scrape at 0x126babf60>\n",
      "                 â”” <module 'firecrawl.v2.methods.aio.scrape' from '/Users/fabio/Desktop/course-ai-agents/.venv/lib/python3.12/site-packages/fire...\n",
      "  File \"/Users/fabio/Desktop/course-ai-agents/.venv/lib/python3.12/site-packages/firecrawl/v2/methods/aio/scrape.py\", line 26, in scrape\n",
      "    handle_response_error(response, \"scrape\")\n",
      "    â”‚                     â”” <Response [403 Forbidden]>\n",
      "    â”” <function handle_response_error at 0x126c03b00>\n",
      "  File \"/Users/fabio/Desktop/course-ai-agents/.venv/lib/python3.12/site-packages/firecrawl/v2/utils/error_handler.py\", line 95, in handle_response_error\n",
      "    raise WebsiteNotSupportedError(message, response.status_code, response)\n",
      "          â”‚                        â”‚        â”‚        â”‚            â”” <Response [403 Forbidden]>\n",
      "          â”‚                        â”‚        â”‚        â”” 403\n",
      "          â”‚                        â”‚        â”” <Response [403 Forbidden]>\n",
      "          â”‚                        â”” 'Website Not Supported: Failed to scrape. This website is no longer supported, please reach out to help@firecrawl.com for mor...\n",
      "          â”” <class 'firecrawl.v2.utils.error_handler.WebsiteNotSupportedError'>\n",
      "\n",
      "\u001b[31m\u001b[1mfirecrawl.v2.utils.error_handler.WebsiteNotSupportedError\u001b[0m:\u001b[1m Website Not Supported: Failed to scrape. This website is no longer supported, please reach out to help@firecrawl.com for more info on how to activate it on your account. - No additional error details provided.\u001b[0m\n",
      "\u001b[32m2025-09-23 12:54:00.083\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36mlogging\u001b[0m:\u001b[36mcallHandlers\u001b[0m:\u001b[36m1762\u001b[0m | âš ï¸ Error scraping https://x.com/karpathy/status/1937902205765607626 after 3 attempts: Website Not Supported: Failed to scrape. This website is no longer supported, please reach out to help@firecrawl.com for more info on how to activate it on your account. - No additional error details provided.\n",
      "\u001b[33m\u001b[1mTraceback (most recent call last):\u001b[0m\n",
      "\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/Users/fabio/Desktop/course-ai-agents/.venv/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "    â”‚   â”” <bound method Application.launch_instance of <class 'ipykernel.kernelapp.IPKernelApp'>>\n",
      "    â”” <module 'ipykernel.kernelapp' from '/Users/fabio/Desktop/course-ai-agents/.venv/lib/python3.12/site-packages/ipykernel/kernel...\n",
      "  File \"/Users/fabio/Desktop/course-ai-agents/.venv/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "    â”‚   â”” <function IPKernelApp.start at 0x105b92a20>\n",
      "    â”” <ipykernel.kernelapp.IPKernelApp object at 0x102b1be30>\n",
      "  File \"/Users/fabio/Desktop/course-ai-agents/.venv/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "    â”‚    â”‚       â”” <function BaseAsyncIOLoop.start at 0x105b937e0>\n",
      "    â”‚    â”” <tornado.platform.asyncio.AsyncIOMainLoop object at 0x105ba65a0>\n",
      "    â”” <ipykernel.kernelapp.IPKernelApp object at 0x102b1be30>\n",
      "  File \"/Users/fabio/Desktop/course-ai-agents/.venv/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "    â”‚    â”‚            â”” <function _patch_loop.<locals>.run_forever at 0x121e667a0>\n",
      "    â”‚    â”” <_UnixSelectorEventLoop running=True closed=False debug=False>\n",
      "    â”” <tornado.platform.asyncio.AsyncIOMainLoop object at 0x105ba65a0>\n",
      "  File \"/Users/fabio/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/asyncio/base_events.py\", line 645, in run_forever\n",
      "    self._run_once()\n",
      "    â”‚    â”” <function _patch_loop.<locals>._run_once at 0x121e668e0>\n",
      "    â”” <_UnixSelectorEventLoop running=True closed=False debug=False>\n",
      "  File \"/Users/fabio/Desktop/course-ai-agents/.venv/lib/python3.12/site-packages/nest_asyncio.py\", line 133, in _run_once\n",
      "    handle._run()\n",
      "    â”‚      â”” <function Handle._run at 0x1031b8680>\n",
      "    â”” <Handle Task.__step()>\n",
      "  File \"/Users/fabio/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "    â”‚    â”‚            â”‚    â”‚           â”‚    â”” <member '_args' of 'Handle' objects>\n",
      "    â”‚    â”‚            â”‚    â”‚           â”” <Handle Task.__step()>\n",
      "    â”‚    â”‚            â”‚    â”” <member '_callback' of 'Handle' objects>\n",
      "    â”‚    â”‚            â”” <Handle Task.__step()>\n",
      "    â”‚    â”” <member '_context' of 'Handle' objects>\n",
      "    â”” <Handle Task.__step()>\n",
      "  File \"/Users/fabio/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/asyncio/tasks.py\", line 303, in __step\n",
      "    self.__step_run_and_handle_result(exc)\n",
      "    â”‚                                 â”” None\n",
      "    â”” <Task pending name='Task-81' coro=<scrape_urls_concurrently.<locals>.scrape_with_semaphore() running at /Users/fabio/Desktop/...\n",
      "  File \"/Users/fabio/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             â”‚    â”” <method 'send' of 'coroutine' objects>\n",
      "             â”” <coroutine object scrape_urls_concurrently.<locals>.scrape_with_semaphore at 0x132aee040>\n",
      "\n",
      "  File \"\u001b[32m/Users/fabio/Desktop/course-ai-agents/lessons/research_agent_part_2/mcp_server/src/app/\u001b[0m\u001b[32m\u001b[1mscraping_handler.py\u001b[0m\", line \u001b[33m189\u001b[0m, in \u001b[35mscrape_with_semaphore\u001b[0m\n",
      "    \u001b[35m\u001b[1mreturn\u001b[0m \u001b[35m\u001b[1mawait\u001b[0m \u001b[1mscrape_and_clean\u001b[0m\u001b[1m(\u001b[0m\u001b[1murl\u001b[0m\u001b[1m,\u001b[0m \u001b[1mguidelines\u001b[0m\u001b[1m,\u001b[0m \u001b[1mfirecrawl_app\u001b[0m\u001b[1m,\u001b[0m \u001b[1mchat_model\u001b[0m\u001b[1m)\u001b[0m\n",
      "    \u001b[36m             â”‚                â”‚    â”‚           â”‚              â”” \u001b[0m\u001b[36m\u001b[1mChatGoogleGenerativeAI(model='models/gemini-2.5-flash', google_api_key=SecretStr('**********'), temperature=1.0, max_retries=...\u001b[0m\n",
      "    \u001b[36m             â”‚                â”‚    â”‚           â”” \u001b[0m\u001b[36m\u001b[1m<firecrawl.client.AsyncFirecrawl object at 0x132b84a10>\u001b[0m\n",
      "    \u001b[36m             â”‚                â”‚    â”” \u001b[0m\u001b[36m\u001b[1m'## Global Context of the Lesson\\n\\n### What We Are Planning to Share\\n\\nWe will write a lesson on context engineering, the c...\u001b[0m\n",
      "    \u001b[36m             â”‚                â”” \u001b[0m\u001b[36m\u001b[1m'https://x.com/karpathy/status/1937902205765607626'\u001b[0m\n",
      "    \u001b[36m             â”” \u001b[0m\u001b[36m\u001b[1m<function scrape_and_clean at 0x132ac7600>\u001b[0m\n",
      "\n",
      "  File \"\u001b[32m/Users/fabio/Desktop/course-ai-agents/lessons/research_agent_part_2/mcp_server/src/app/\u001b[0m\u001b[32m\u001b[1mscraping_handler.py\u001b[0m\", line \u001b[33m152\u001b[0m, in \u001b[35mscrape_and_clean\u001b[0m\n",
      "    \u001b[1mscraped\u001b[0m \u001b[35m\u001b[1m=\u001b[0m \u001b[35m\u001b[1mawait\u001b[0m \u001b[1mscrape_url\u001b[0m\u001b[1m(\u001b[0m\u001b[1murl\u001b[0m\u001b[1m,\u001b[0m \u001b[1mfirecrawl_app\u001b[0m\u001b[1m)\u001b[0m\n",
      "    \u001b[36m                â”‚          â”‚    â”” \u001b[0m\u001b[36m\u001b[1m<firecrawl.client.AsyncFirecrawl object at 0x132b84a10>\u001b[0m\n",
      "    \u001b[36m                â”‚          â”” \u001b[0m\u001b[36m\u001b[1m'https://x.com/karpathy/status/1937902205765607626'\u001b[0m\n",
      "    \u001b[36m                â”” \u001b[0m\u001b[36m\u001b[1m<function scrape_url at 0x132ac7420>\u001b[0m\n",
      "\n",
      "> File \"\u001b[32m/Users/fabio/Desktop/course-ai-agents/lessons/research_agent_part_2/mcp_server/src/app/\u001b[0m\u001b[32m\u001b[1mscraping_handler.py\u001b[0m\", line \u001b[33m61\u001b[0m, in \u001b[35mscrape_url\u001b[0m\n",
      "    \u001b[1mres\u001b[0m \u001b[35m\u001b[1m=\u001b[0m \u001b[35m\u001b[1mawait\u001b[0m \u001b[1mfirecrawl_app\u001b[0m\u001b[35m\u001b[1m.\u001b[0m\u001b[1mscrape\u001b[0m\u001b[1m(\u001b[0m\n",
      "    \u001b[36m            â”‚             â”” \u001b[0m\u001b[36m\u001b[1m<bound method AsyncFirecrawlClient.scrape of <firecrawl.v2.client_async.AsyncFirecrawlClient object at 0x132b85640>>\u001b[0m\n",
      "    \u001b[36m            â”” \u001b[0m\u001b[36m\u001b[1m<firecrawl.client.AsyncFirecrawl object at 0x132b84a10>\u001b[0m\n",
      "\n",
      "  File \"/Users/fabio/Desktop/course-ai-agents/.venv/lib/python3.12/site-packages/firecrawl/v2/client_async.py\", line 65, in scrape\n",
      "    return await async_scrape.scrape(self.async_http_client, url, options)\n",
      "                 â”‚            â”‚      â”‚    â”‚                  â”‚    â”” ScrapeOptions(formats=['markdown'], headers=None, include_tags=None, exclude_tags=None, only_main_content=None, timeout=12000...\n",
      "                 â”‚            â”‚      â”‚    â”‚                  â”” 'https://x.com/karpathy/status/1937902205765607626'\n",
      "                 â”‚            â”‚      â”‚    â”” <firecrawl.v2.utils.http_client_async.AsyncHttpClient object at 0x132bac650>\n",
      "                 â”‚            â”‚      â”” <firecrawl.v2.client_async.AsyncFirecrawlClient object at 0x132b85640>\n",
      "                 â”‚            â”” <function scrape at 0x126babf60>\n",
      "                 â”” <module 'firecrawl.v2.methods.aio.scrape' from '/Users/fabio/Desktop/course-ai-agents/.venv/lib/python3.12/site-packages/fire...\n",
      "  File \"/Users/fabio/Desktop/course-ai-agents/.venv/lib/python3.12/site-packages/firecrawl/v2/methods/aio/scrape.py\", line 26, in scrape\n",
      "    handle_response_error(response, \"scrape\")\n",
      "    â”‚                     â”” <Response [403 Forbidden]>\n",
      "    â”” <function handle_response_error at 0x126c03b00>\n",
      "  File \"/Users/fabio/Desktop/course-ai-agents/.venv/lib/python3.12/site-packages/firecrawl/v2/utils/error_handler.py\", line 95, in handle_response_error\n",
      "    raise WebsiteNotSupportedError(message, response.status_code, response)\n",
      "          â”‚                        â”‚        â”‚        â”‚            â”” <Response [403 Forbidden]>\n",
      "          â”‚                        â”‚        â”‚        â”” 403\n",
      "          â”‚                        â”‚        â”” <Response [403 Forbidden]>\n",
      "          â”‚                        â”” 'Website Not Supported: Failed to scrape. This website is no longer supported, please reach out to help@firecrawl.com for mor...\n",
      "          â”” <class 'firecrawl.v2.utils.error_handler.WebsiteNotSupportedError'>\n",
      "\n",
      "\u001b[31m\u001b[1mfirecrawl.v2.utils.error_handler.WebsiteNotSupportedError\u001b[0m:\u001b[1m Website Not Supported: Failed to scrape. This website is no longer supported, please reach out to help@firecrawl.com for more info on how to activate it on your account. - No additional error details provided.\u001b[0m\n",
      "\u001b[32m2025-09-23 12:54:06.949\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36mlogging\u001b[0m:\u001b[36mcallHandlers\u001b[0m:\u001b[36m1762\u001b[0m | Error scraping https://x.com/lenadroid/status/1943685060785524824: Website Not Supported: Failed to scrape. This website is no longer supported, please reach out to help@firecrawl.com for more info on how to activate it on your account. - No additional error details provided.\n",
      "\u001b[33m\u001b[1mTraceback (most recent call last):\u001b[0m\n",
      "\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/Users/fabio/Desktop/course-ai-agents/.venv/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "    â”‚   â”” <bound method Application.launch_instance of <class 'ipykernel.kernelapp.IPKernelApp'>>\n",
      "    â”” <module 'ipykernel.kernelapp' from '/Users/fabio/Desktop/course-ai-agents/.venv/lib/python3.12/site-packages/ipykernel/kernel...\n",
      "  File \"/Users/fabio/Desktop/course-ai-agents/.venv/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "    â”‚   â”” <function IPKernelApp.start at 0x105b92a20>\n",
      "    â”” <ipykernel.kernelapp.IPKernelApp object at 0x102b1be30>\n",
      "  File \"/Users/fabio/Desktop/course-ai-agents/.venv/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "    â”‚    â”‚       â”” <function BaseAsyncIOLoop.start at 0x105b937e0>\n",
      "    â”‚    â”” <tornado.platform.asyncio.AsyncIOMainLoop object at 0x105ba65a0>\n",
      "    â”” <ipykernel.kernelapp.IPKernelApp object at 0x102b1be30>\n",
      "  File \"/Users/fabio/Desktop/course-ai-agents/.venv/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "    â”‚    â”‚            â”” <function _patch_loop.<locals>.run_forever at 0x121e667a0>\n",
      "    â”‚    â”” <_UnixSelectorEventLoop running=True closed=False debug=False>\n",
      "    â”” <tornado.platform.asyncio.AsyncIOMainLoop object at 0x105ba65a0>\n",
      "  File \"/Users/fabio/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/asyncio/base_events.py\", line 645, in run_forever\n",
      "    self._run_once()\n",
      "    â”‚    â”” <function _patch_loop.<locals>._run_once at 0x121e668e0>\n",
      "    â”” <_UnixSelectorEventLoop running=True closed=False debug=False>\n",
      "  File \"/Users/fabio/Desktop/course-ai-agents/.venv/lib/python3.12/site-packages/nest_asyncio.py\", line 133, in _run_once\n",
      "    handle._run()\n",
      "    â”‚      â”” <function Handle._run at 0x1031b8680>\n",
      "    â”” <Handle Task.__step()>\n",
      "  File \"/Users/fabio/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "    â”‚    â”‚            â”‚    â”‚           â”‚    â”” <member '_args' of 'Handle' objects>\n",
      "    â”‚    â”‚            â”‚    â”‚           â”” <Handle Task.__step()>\n",
      "    â”‚    â”‚            â”‚    â”” <member '_callback' of 'Handle' objects>\n",
      "    â”‚    â”‚            â”” <Handle Task.__step()>\n",
      "    â”‚    â”” <member '_context' of 'Handle' objects>\n",
      "    â”” <Handle Task.__step()>\n",
      "  File \"/Users/fabio/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/asyncio/tasks.py\", line 303, in __step\n",
      "    self.__step_run_and_handle_result(exc)\n",
      "    â”‚                                 â”” None\n",
      "    â”” <Task pending name='Task-82' coro=<scrape_urls_concurrently.<locals>.scrape_with_semaphore() running at /Users/fabio/Desktop/...\n",
      "  File \"/Users/fabio/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             â”‚    â”” <method 'send' of 'coroutine' objects>\n",
      "             â”” <coroutine object scrape_urls_concurrently.<locals>.scrape_with_semaphore at 0x132aee440>\n",
      "\n",
      "  File \"\u001b[32m/Users/fabio/Desktop/course-ai-agents/lessons/research_agent_part_2/mcp_server/src/app/\u001b[0m\u001b[32m\u001b[1mscraping_handler.py\u001b[0m\", line \u001b[33m189\u001b[0m, in \u001b[35mscrape_with_semaphore\u001b[0m\n",
      "    \u001b[35m\u001b[1mreturn\u001b[0m \u001b[35m\u001b[1mawait\u001b[0m \u001b[1mscrape_and_clean\u001b[0m\u001b[1m(\u001b[0m\u001b[1murl\u001b[0m\u001b[1m,\u001b[0m \u001b[1mguidelines\u001b[0m\u001b[1m,\u001b[0m \u001b[1mfirecrawl_app\u001b[0m\u001b[1m,\u001b[0m \u001b[1mchat_model\u001b[0m\u001b[1m)\u001b[0m\n",
      "    \u001b[36m             â”‚                â”‚    â”‚           â”‚              â”” \u001b[0m\u001b[36m\u001b[1mChatGoogleGenerativeAI(model='models/gemini-2.5-flash', google_api_key=SecretStr('**********'), temperature=1.0, max_retries=...\u001b[0m\n",
      "    \u001b[36m             â”‚                â”‚    â”‚           â”” \u001b[0m\u001b[36m\u001b[1m<firecrawl.client.AsyncFirecrawl object at 0x132b84a10>\u001b[0m\n",
      "    \u001b[36m             â”‚                â”‚    â”” \u001b[0m\u001b[36m\u001b[1m'## Global Context of the Lesson\\n\\n### What We Are Planning to Share\\n\\nWe will write a lesson on context engineering, the c...\u001b[0m\n",
      "    \u001b[36m             â”‚                â”” \u001b[0m\u001b[36m\u001b[1m'https://x.com/lenadroid/status/1943685060785524824'\u001b[0m\n",
      "    \u001b[36m             â”” \u001b[0m\u001b[36m\u001b[1m<function scrape_and_clean at 0x132ac7600>\u001b[0m\n",
      "\n",
      "  File \"\u001b[32m/Users/fabio/Desktop/course-ai-agents/lessons/research_agent_part_2/mcp_server/src/app/\u001b[0m\u001b[32m\u001b[1mscraping_handler.py\u001b[0m\", line \u001b[33m152\u001b[0m, in \u001b[35mscrape_and_clean\u001b[0m\n",
      "    \u001b[1mscraped\u001b[0m \u001b[35m\u001b[1m=\u001b[0m \u001b[35m\u001b[1mawait\u001b[0m \u001b[1mscrape_url\u001b[0m\u001b[1m(\u001b[0m\u001b[1murl\u001b[0m\u001b[1m,\u001b[0m \u001b[1mfirecrawl_app\u001b[0m\u001b[1m)\u001b[0m\n",
      "    \u001b[36m                â”‚          â”‚    â”” \u001b[0m\u001b[36m\u001b[1m<firecrawl.client.AsyncFirecrawl object at 0x132b84a10>\u001b[0m\n",
      "    \u001b[36m                â”‚          â”” \u001b[0m\u001b[36m\u001b[1m'https://x.com/lenadroid/status/1943685060785524824'\u001b[0m\n",
      "    \u001b[36m                â”” \u001b[0m\u001b[36m\u001b[1m<function scrape_url at 0x132ac7420>\u001b[0m\n",
      "\n",
      "> File \"\u001b[32m/Users/fabio/Desktop/course-ai-agents/lessons/research_agent_part_2/mcp_server/src/app/\u001b[0m\u001b[32m\u001b[1mscraping_handler.py\u001b[0m\", line \u001b[33m61\u001b[0m, in \u001b[35mscrape_url\u001b[0m\n",
      "    \u001b[1mres\u001b[0m \u001b[35m\u001b[1m=\u001b[0m \u001b[35m\u001b[1mawait\u001b[0m \u001b[1mfirecrawl_app\u001b[0m\u001b[35m\u001b[1m.\u001b[0m\u001b[1mscrape\u001b[0m\u001b[1m(\u001b[0m\n",
      "    \u001b[36m            â”‚             â”” \u001b[0m\u001b[36m\u001b[1m<bound method AsyncFirecrawlClient.scrape of <firecrawl.v2.client_async.AsyncFirecrawlClient object at 0x132b85640>>\u001b[0m\n",
      "    \u001b[36m            â”” \u001b[0m\u001b[36m\u001b[1m<firecrawl.client.AsyncFirecrawl object at 0x132b84a10>\u001b[0m\n",
      "\n",
      "  File \"/Users/fabio/Desktop/course-ai-agents/.venv/lib/python3.12/site-packages/firecrawl/v2/client_async.py\", line 65, in scrape\n",
      "    return await async_scrape.scrape(self.async_http_client, url, options)\n",
      "                 â”‚            â”‚      â”‚    â”‚                  â”‚    â”” ScrapeOptions(formats=['markdown'], headers=None, include_tags=None, exclude_tags=None, only_main_content=None, timeout=12000...\n",
      "                 â”‚            â”‚      â”‚    â”‚                  â”” 'https://x.com/lenadroid/status/1943685060785524824'\n",
      "                 â”‚            â”‚      â”‚    â”” <firecrawl.v2.utils.http_client_async.AsyncHttpClient object at 0x132bac650>\n",
      "                 â”‚            â”‚      â”” <firecrawl.v2.client_async.AsyncFirecrawlClient object at 0x132b85640>\n",
      "                 â”‚            â”” <function scrape at 0x126babf60>\n",
      "                 â”” <module 'firecrawl.v2.methods.aio.scrape' from '/Users/fabio/Desktop/course-ai-agents/.venv/lib/python3.12/site-packages/fire...\n",
      "  File \"/Users/fabio/Desktop/course-ai-agents/.venv/lib/python3.12/site-packages/firecrawl/v2/methods/aio/scrape.py\", line 26, in scrape\n",
      "    handle_response_error(response, \"scrape\")\n",
      "    â”‚                     â”” <Response [403 Forbidden]>\n",
      "    â”” <function handle_response_error at 0x126c03b00>\n",
      "  File \"/Users/fabio/Desktop/course-ai-agents/.venv/lib/python3.12/site-packages/firecrawl/v2/utils/error_handler.py\", line 95, in handle_response_error\n",
      "    raise WebsiteNotSupportedError(message, response.status_code, response)\n",
      "          â”‚                        â”‚        â”‚        â”‚            â”” <Response [403 Forbidden]>\n",
      "          â”‚                        â”‚        â”‚        â”” 403\n",
      "          â”‚                        â”‚        â”” <Response [403 Forbidden]>\n",
      "          â”‚                        â”” 'Website Not Supported: Failed to scrape. This website is no longer supported, please reach out to help@firecrawl.com for mor...\n",
      "          â”” <class 'firecrawl.v2.utils.error_handler.WebsiteNotSupportedError'>\n",
      "\n",
      "\u001b[31m\u001b[1mfirecrawl.v2.utils.error_handler.WebsiteNotSupportedError\u001b[0m:\u001b[1m Website Not Supported: Failed to scrape. This website is no longer supported, please reach out to help@firecrawl.com for more info on how to activate it on your account. - No additional error details provided.\u001b[0m\n",
      "\u001b[32m2025-09-23 12:54:06.952\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36mlogging\u001b[0m:\u001b[36mcallHandlers\u001b[0m:\u001b[36m1762\u001b[0m | âš ï¸ Error scraping https://x.com/lenadroid/status/1943685060785524824 after 3 attempts: Website Not Supported: Failed to scrape. This website is no longer supported, please reach out to help@firecrawl.com for more info on how to activate it on your account. - No additional error details provided.\n",
      "\u001b[33m\u001b[1mTraceback (most recent call last):\u001b[0m\n",
      "\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/Users/fabio/Desktop/course-ai-agents/.venv/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "    â”‚   â”” <bound method Application.launch_instance of <class 'ipykernel.kernelapp.IPKernelApp'>>\n",
      "    â”” <module 'ipykernel.kernelapp' from '/Users/fabio/Desktop/course-ai-agents/.venv/lib/python3.12/site-packages/ipykernel/kernel...\n",
      "  File \"/Users/fabio/Desktop/course-ai-agents/.venv/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "    â”‚   â”” <function IPKernelApp.start at 0x105b92a20>\n",
      "    â”” <ipykernel.kernelapp.IPKernelApp object at 0x102b1be30>\n",
      "  File \"/Users/fabio/Desktop/course-ai-agents/.venv/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "    â”‚    â”‚       â”” <function BaseAsyncIOLoop.start at 0x105b937e0>\n",
      "    â”‚    â”” <tornado.platform.asyncio.AsyncIOMainLoop object at 0x105ba65a0>\n",
      "    â”” <ipykernel.kernelapp.IPKernelApp object at 0x102b1be30>\n",
      "  File \"/Users/fabio/Desktop/course-ai-agents/.venv/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "    â”‚    â”‚            â”” <function _patch_loop.<locals>.run_forever at 0x121e667a0>\n",
      "    â”‚    â”” <_UnixSelectorEventLoop running=True closed=False debug=False>\n",
      "    â”” <tornado.platform.asyncio.AsyncIOMainLoop object at 0x105ba65a0>\n",
      "  File \"/Users/fabio/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/asyncio/base_events.py\", line 645, in run_forever\n",
      "    self._run_once()\n",
      "    â”‚    â”” <function _patch_loop.<locals>._run_once at 0x121e668e0>\n",
      "    â”” <_UnixSelectorEventLoop running=True closed=False debug=False>\n",
      "  File \"/Users/fabio/Desktop/course-ai-agents/.venv/lib/python3.12/site-packages/nest_asyncio.py\", line 133, in _run_once\n",
      "    handle._run()\n",
      "    â”‚      â”” <function Handle._run at 0x1031b8680>\n",
      "    â”” <Handle Task.__step()>\n",
      "  File \"/Users/fabio/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "    â”‚    â”‚            â”‚    â”‚           â”‚    â”” <member '_args' of 'Handle' objects>\n",
      "    â”‚    â”‚            â”‚    â”‚           â”” <Handle Task.__step()>\n",
      "    â”‚    â”‚            â”‚    â”” <member '_callback' of 'Handle' objects>\n",
      "    â”‚    â”‚            â”” <Handle Task.__step()>\n",
      "    â”‚    â”” <member '_context' of 'Handle' objects>\n",
      "    â”” <Handle Task.__step()>\n",
      "  File \"/Users/fabio/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/asyncio/tasks.py\", line 303, in __step\n",
      "    self.__step_run_and_handle_result(exc)\n",
      "    â”‚                                 â”” None\n",
      "    â”” <Task pending name='Task-82' coro=<scrape_urls_concurrently.<locals>.scrape_with_semaphore() running at /Users/fabio/Desktop/...\n",
      "  File \"/Users/fabio/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             â”‚    â”” <method 'send' of 'coroutine' objects>\n",
      "             â”” <coroutine object scrape_urls_concurrently.<locals>.scrape_with_semaphore at 0x132aee440>\n",
      "\n",
      "  File \"\u001b[32m/Users/fabio/Desktop/course-ai-agents/lessons/research_agent_part_2/mcp_server/src/app/\u001b[0m\u001b[32m\u001b[1mscraping_handler.py\u001b[0m\", line \u001b[33m189\u001b[0m, in \u001b[35mscrape_with_semaphore\u001b[0m\n",
      "    \u001b[35m\u001b[1mreturn\u001b[0m \u001b[35m\u001b[1mawait\u001b[0m \u001b[1mscrape_and_clean\u001b[0m\u001b[1m(\u001b[0m\u001b[1murl\u001b[0m\u001b[1m,\u001b[0m \u001b[1mguidelines\u001b[0m\u001b[1m,\u001b[0m \u001b[1mfirecrawl_app\u001b[0m\u001b[1m,\u001b[0m \u001b[1mchat_model\u001b[0m\u001b[1m)\u001b[0m\n",
      "    \u001b[36m             â”‚                â”‚    â”‚           â”‚              â”” \u001b[0m\u001b[36m\u001b[1mChatGoogleGenerativeAI(model='models/gemini-2.5-flash', google_api_key=SecretStr('**********'), temperature=1.0, max_retries=...\u001b[0m\n",
      "    \u001b[36m             â”‚                â”‚    â”‚           â”” \u001b[0m\u001b[36m\u001b[1m<firecrawl.client.AsyncFirecrawl object at 0x132b84a10>\u001b[0m\n",
      "    \u001b[36m             â”‚                â”‚    â”” \u001b[0m\u001b[36m\u001b[1m'## Global Context of the Lesson\\n\\n### What We Are Planning to Share\\n\\nWe will write a lesson on context engineering, the c...\u001b[0m\n",
      "    \u001b[36m             â”‚                â”” \u001b[0m\u001b[36m\u001b[1m'https://x.com/lenadroid/status/1943685060785524824'\u001b[0m\n",
      "    \u001b[36m             â”” \u001b[0m\u001b[36m\u001b[1m<function scrape_and_clean at 0x132ac7600>\u001b[0m\n",
      "\n",
      "  File \"\u001b[32m/Users/fabio/Desktop/course-ai-agents/lessons/research_agent_part_2/mcp_server/src/app/\u001b[0m\u001b[32m\u001b[1mscraping_handler.py\u001b[0m\", line \u001b[33m152\u001b[0m, in \u001b[35mscrape_and_clean\u001b[0m\n",
      "    \u001b[1mscraped\u001b[0m \u001b[35m\u001b[1m=\u001b[0m \u001b[35m\u001b[1mawait\u001b[0m \u001b[1mscrape_url\u001b[0m\u001b[1m(\u001b[0m\u001b[1murl\u001b[0m\u001b[1m,\u001b[0m \u001b[1mfirecrawl_app\u001b[0m\u001b[1m)\u001b[0m\n",
      "    \u001b[36m                â”‚          â”‚    â”” \u001b[0m\u001b[36m\u001b[1m<firecrawl.client.AsyncFirecrawl object at 0x132b84a10>\u001b[0m\n",
      "    \u001b[36m                â”‚          â”” \u001b[0m\u001b[36m\u001b[1m'https://x.com/lenadroid/status/1943685060785524824'\u001b[0m\n",
      "    \u001b[36m                â”” \u001b[0m\u001b[36m\u001b[1m<function scrape_url at 0x132ac7420>\u001b[0m\n",
      "\n",
      "> File \"\u001b[32m/Users/fabio/Desktop/course-ai-agents/lessons/research_agent_part_2/mcp_server/src/app/\u001b[0m\u001b[32m\u001b[1mscraping_handler.py\u001b[0m\", line \u001b[33m61\u001b[0m, in \u001b[35mscrape_url\u001b[0m\n",
      "    \u001b[1mres\u001b[0m \u001b[35m\u001b[1m=\u001b[0m \u001b[35m\u001b[1mawait\u001b[0m \u001b[1mfirecrawl_app\u001b[0m\u001b[35m\u001b[1m.\u001b[0m\u001b[1mscrape\u001b[0m\u001b[1m(\u001b[0m\n",
      "    \u001b[36m            â”‚             â”” \u001b[0m\u001b[36m\u001b[1m<bound method AsyncFirecrawlClient.scrape of <firecrawl.v2.client_async.AsyncFirecrawlClient object at 0x132b85640>>\u001b[0m\n",
      "    \u001b[36m            â”” \u001b[0m\u001b[36m\u001b[1m<firecrawl.client.AsyncFirecrawl object at 0x132b84a10>\u001b[0m\n",
      "\n",
      "  File \"/Users/fabio/Desktop/course-ai-agents/.venv/lib/python3.12/site-packages/firecrawl/v2/client_async.py\", line 65, in scrape\n",
      "    return await async_scrape.scrape(self.async_http_client, url, options)\n",
      "                 â”‚            â”‚      â”‚    â”‚                  â”‚    â”” ScrapeOptions(formats=['markdown'], headers=None, include_tags=None, exclude_tags=None, only_main_content=None, timeout=12000...\n",
      "                 â”‚            â”‚      â”‚    â”‚                  â”” 'https://x.com/lenadroid/status/1943685060785524824'\n",
      "                 â”‚            â”‚      â”‚    â”” <firecrawl.v2.utils.http_client_async.AsyncHttpClient object at 0x132bac650>\n",
      "                 â”‚            â”‚      â”” <firecrawl.v2.client_async.AsyncFirecrawlClient object at 0x132b85640>\n",
      "                 â”‚            â”” <function scrape at 0x126babf60>\n",
      "                 â”” <module 'firecrawl.v2.methods.aio.scrape' from '/Users/fabio/Desktop/course-ai-agents/.venv/lib/python3.12/site-packages/fire...\n",
      "  File \"/Users/fabio/Desktop/course-ai-agents/.venv/lib/python3.12/site-packages/firecrawl/v2/methods/aio/scrape.py\", line 26, in scrape\n",
      "    handle_response_error(response, \"scrape\")\n",
      "    â”‚                     â”” <Response [403 Forbidden]>\n",
      "    â”” <function handle_response_error at 0x126c03b00>\n",
      "  File \"/Users/fabio/Desktop/course-ai-agents/.venv/lib/python3.12/site-packages/firecrawl/v2/utils/error_handler.py\", line 95, in handle_response_error\n",
      "    raise WebsiteNotSupportedError(message, response.status_code, response)\n",
      "          â”‚                        â”‚        â”‚        â”‚            â”” <Response [403 Forbidden]>\n",
      "          â”‚                        â”‚        â”‚        â”” 403\n",
      "          â”‚                        â”‚        â”” <Response [403 Forbidden]>\n",
      "          â”‚                        â”” 'Website Not Supported: Failed to scrape. This website is no longer supported, please reach out to help@firecrawl.com for mor...\n",
      "          â”” <class 'firecrawl.v2.utils.error_handler.WebsiteNotSupportedError'>\n",
      "\n",
      "\u001b[31m\u001b[1mfirecrawl.v2.utils.error_handler.WebsiteNotSupportedError\u001b[0m:\u001b[1m Website Not Supported: Failed to scrape. This website is no longer supported, please reach out to help@firecrawl.com for more info on how to activate it on your account. - No additional error details provided.\u001b[0m\n",
      "\u001b[32m2025-09-23 12:55:09.187\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlogging\u001b[0m:\u001b[36mcallHandlers\u001b[0m:\u001b[36m1762\u001b[0m | Processing request of type CallToolRequest\n",
      "\u001b[32m2025-09-23 12:55:09.192\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mgitingest.entrypoint\u001b[0m:\u001b[36mingest_async\u001b[0m:\u001b[36m89\u001b[0m | Starting ingestion process | {\"source\":\"https://github.com/humanlayer/12-factor-agents/blob/main/content/factor-03-own-your-context-window.md\"}\n",
      "\u001b[32m2025-09-23 12:55:09.193\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mgitingest.entrypoint\u001b[0m:\u001b[36mingest_async\u001b[0m:\u001b[36m98\u001b[0m | Parsing remote repository | {\"source\":\"https://github.com/humanlayer/12-factor-agents/blob/main/content/factor-03-own-your-context-window.md\"}\n",
      "\u001b[32m2025-09-23 12:55:10.683\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mgitingest.clone\u001b[0m:\u001b[36mclone_repo\u001b[0m:\u001b[36m56\u001b[0m | Starting git clone operation | {\"url\":\"https://github.com/humanlayer/12-factor-agents\",\"local_path\":\"/var/folders/6w/zxb4hbqn3p722sx3s_r_dms40000gn/T/gitingest/a033a83e-3a6b-4bc8-ac5d-4cba750577d3/humanlayer-12-factor-agents\",\"partial_clone\":true,\"subpath\":\"/content/factor-03-own-your-context-window.md\",\"branch\":\"main\",\"tag\":null,\"commit\":\"d20c728368bf9c189d6d7aab704744decb6ec0cc\",\"include_submodules\":false}\n",
      "\u001b[32m2025-09-23 12:55:11.335\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mgitingest.clone\u001b[0m:\u001b[36mclone_repo\u001b[0m:\u001b[36m97\u001b[0m | Executing git clone command | {\"command\":\"git -c http.https://github.com/.extraheader=Authorization: Basic eC1vYXV0aC1iYXNpYzpnaHBfcGFqZHA1S2E0ZVRnOGg3bjM5ZFhvQWtCTFJQaWlPMmYyNk1G clone --single-branch --no-checkout --depth=1 --filter=blob:none --sparse https://github.com/humanlayer/12-factor-agents <url> /var/folders/6w/zxb4hbqn3p722sx3s_r_dms40000gn/T/gitingest/a033a83e-3a6b-4bc8-ac5d-4cba750577d3/humanlayer-12-factor-agents\"}\n",
      "\u001b[32m2025-09-23 12:55:12.024\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mgitingest.clone\u001b[0m:\u001b[36mclone_repo\u001b[0m:\u001b[36m99\u001b[0m | Git clone completed successfully\n",
      "\u001b[32m2025-09-23 12:55:12.025\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mgitingest.clone\u001b[0m:\u001b[36mclone_repo\u001b[0m:\u001b[36m103\u001b[0m | Setting up partial clone for subpath | {\"subpath\":\"/content/factor-03-own-your-context-window.md\"}\n",
      "\u001b[32m2025-09-23 12:55:12.665\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mgitingest.clone\u001b[0m:\u001b[36mclone_repo\u001b[0m:\u001b[36m114\u001b[0m | Checking out commit | {\"commit\":\"d20c728368bf9c189d6d7aab704744decb6ec0cc\"}\n",
      "\u001b[32m2025-09-23 12:55:13.154\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mgitingest.clone\u001b[0m:\u001b[36mclone_repo\u001b[0m:\u001b[36m123\u001b[0m | Git clone operation completed successfully | {\"local_path\":\"/var/folders/6w/zxb4hbqn3p722sx3s_r_dms40000gn/T/gitingest/a033a83e-3a6b-4bc8-ac5d-4cba750577d3/humanlayer-12-factor-agents\"}\n",
      "\u001b[32m2025-09-23 12:55:13.154\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mgitingest.entrypoint\u001b[0m:\u001b[36mingest_async\u001b[0m:\u001b[36m132\u001b[0m | Repository cloned, starting file processing\n",
      "\u001b[32m2025-09-23 12:55:13.158\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mgitingest.entrypoint\u001b[0m:\u001b[36mingest_async\u001b[0m:\u001b[36m140\u001b[0m | Processing files and generating output\n",
      "\u001b[32m2025-09-23 12:55:13.158\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mgitingest.ingestion\u001b[0m:\u001b[36mingest_query\u001b[0m:\u001b[36m44\u001b[0m | Starting file ingestion | {\"slug\":\"humanlayer-12-factor-agents\",\"subpath\":\"/content/factor-03-own-your-context-window.md\",\"local_path\":\"/var/folders/6w/zxb4hbqn3p722sx3s_r_dms40000gn/T/gitingest/a033a83e-3a6b-4bc8-ac5d-4cba750577d3/humanlayer-12-factor-agents\",\"max_file_size\":10485760}\n",
      "\u001b[32m2025-09-23 12:55:13.159\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mgitingest.ingestion\u001b[0m:\u001b[36mingest_query\u001b[0m:\u001b[36m64\u001b[0m | Processing single file | {\"file_path\":\"/var/folders/6w/zxb4hbqn3p722sx3s_r_dms40000gn/T/gitingest/a033a83e-3a6b-4bc8-ac5d-4cba750577d3/humanlayer-12-factor-agents/content/factor-03-own-your-context-window.md\"}\n",
      "\u001b[32m2025-09-23 12:55:13.160\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mgitingest.ingestion\u001b[0m:\u001b[36mingest_query\u001b[0m:\u001b[36m87\u001b[0m | Single file processing completed | {\"file_name\":\"factor-03-own-your-context-window.md\",\"file_size\":9458}\n",
      "\u001b[32m2025-09-23 12:55:13.337\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mgitingest.entrypoint\u001b[0m:\u001b[36mingest_async\u001b[0m:\u001b[36m147\u001b[0m | Ingestion completed successfully\n",
      "\u001b[32m2025-09-23 12:55:17.892\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlogging\u001b[0m:\u001b[36mcallHandlers\u001b[0m:\u001b[36m1762\u001b[0m | Processing request of type CallToolRequest\n",
      "\u001b[32m2025-09-23 12:55:17.896\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mlogging\u001b[0m:\u001b[36mcallHandlers\u001b[0m:\u001b[36m1762\u001b[0m | File not found: /Users/fabio/Desktop/course-ai-agents/lessons/research_agent_part_2/data/sample_context_engineering/.nova/perplexity_results.md\n",
      "\u001b[32m2025-09-23 12:55:50.783\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlogging\u001b[0m:\u001b[36mcallHandlers\u001b[0m:\u001b[36m1762\u001b[0m | Processing request of type CallToolRequest\n",
      "\u001b[32m2025-09-23 12:55:56.006\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlogging\u001b[0m:\u001b[36mcallHandlers\u001b[0m:\u001b[36m1762\u001b[0m | Processing request of type CallToolRequest\n",
      "\u001b[32m2025-09-23 12:56:31.607\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlogging\u001b[0m:\u001b[36mcallHandlers\u001b[0m:\u001b[36m1762\u001b[0m | Processing request of type CallToolRequest\n",
      "\u001b[32m2025-09-23 12:56:36.174\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlogging\u001b[0m:\u001b[36mcallHandlers\u001b[0m:\u001b[36m1762\u001b[0m | Processing request of type CallToolRequest\n",
      "\u001b[32m2025-09-23 12:57:06.283\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlogging\u001b[0m:\u001b[36mcallHandlers\u001b[0m:\u001b[36m1762\u001b[0m | Processing request of type CallToolRequest\n",
      "\u001b[32m2025-09-23 12:57:36.207\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlogging\u001b[0m:\u001b[36mcallHandlers\u001b[0m:\u001b[36m1762\u001b[0m | Processing request of type CallToolRequest\n",
      "\u001b[32m2025-09-23 12:58:05.469\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlogging\u001b[0m:\u001b[36mcallHandlers\u001b[0m:\u001b[36m1762\u001b[0m | Processing request of type CallToolRequest\n",
      "\u001b[32m2025-09-23 12:58:34.316\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlogging\u001b[0m:\u001b[36mcallHandlers\u001b[0m:\u001b[36m1762\u001b[0m | Processing request of type CallToolRequest\n",
      "\u001b[32m2025-09-23 12:59:33.134\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlogging\u001b[0m:\u001b[36mcallHandlers\u001b[0m:\u001b[36m1762\u001b[0m | ğŸ‘ 52 sources accepted.\n",
      "\u001b[32m2025-09-23 12:59:38.952\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlogging\u001b[0m:\u001b[36mcallHandlers\u001b[0m:\u001b[36m1762\u001b[0m | Processing request of type CallToolRequest\n",
      "\u001b[32m2025-09-23 13:00:01.644\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlogging\u001b[0m:\u001b[36mcallHandlers\u001b[0m:\u001b[36m1762\u001b[0m | ğŸ‘ 5 sources selected to scrape.\n",
      "\u001b[32m2025-09-23 13:00:07.830\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlogging\u001b[0m:\u001b[36mcallHandlers\u001b[0m:\u001b[36m1762\u001b[0m | Processing request of type CallToolRequest\n",
      "\u001b[32m2025-09-23 13:01:46.402\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlogging\u001b[0m:\u001b[36mcallHandlers\u001b[0m:\u001b[36m1762\u001b[0m | Processing request of type CallToolRequest\n",
      "\u001b[32m2025-09-23 14:26:54.431\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlogging\u001b[0m:\u001b[36mcallHandlers\u001b[0m:\u001b[36m1762\u001b[0m | ğŸ‘‹ Terminating application...\n"
     ]
    }
   ],
   "source": [
    "from research_agent_part_2.mcp_server.src.tools import select_research_sources_to_keep_tool\n",
    "\n",
    "# Test the source selection tool\n",
    "research_folder = \"/path/to/research_folder\"\n",
    "result = await select_research_sources_to_keep_tool(research_directory=research_folder)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb299bd",
   "metadata": {},
   "source": [
    "The tool provides feedback on which sources were selected, allowing users to review the decisions if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51fd4b9",
   "metadata": {},
   "source": [
    "## 4. Selecting Sources for Full Content Scraping\n",
    "\n",
    "After filtering for quality, we need to identify which sources deserve a full scrape. While Perplexity provides summaries and excerpts, some high-quality sources contain much more valuable content in their full form. The `select_research_sources_to_scrape` tool analyzes the filtered results and strategically chooses the most valuable sources for comprehensive content extraction. This full content will provide the writing agent with richer context, detailed examples, and comprehensive coverage that brief excerpts cannot capture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d57804",
   "metadata": {},
   "source": [
    "### 4.1 Understanding the Selection Logic\n",
    "\n",
    "This tool takes filtered Perplexity results and selects the most valuable sources for full scraping. It analyzes the article guidelines, accepted sources, and already-scraped guideline content to avoid duplication. The tool uses an LLM to evaluate sources based on relevance, authority, quality, and uniqueness, then outputs a prioritized list of URLs. The default limit of 5 sources balances comprehensive coverage with processing efficiency and API costs.\n",
    "\n",
    "Source: _mcp_server/src/tools/select_research_sources_to_scrape_tool.py_\n",
    "\n",
    "```python\n",
    "async def select_research_sources_to_scrape_tool(research_directory: str, max_sources: int = 5) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Select up to max_sources priority research sources to scrape in full.\n",
    "    \n",
    "    Analyzes the filtered Perplexity results together with the article guidelines and\n",
    "    the material already scraped from guideline URLs, then chooses up to max_sources diverse,\n",
    "    authoritative sources whose full content will add most value. The chosen URLs are\n",
    "    written (one per line) to urls_to_scrape_from_research.md.\n",
    "    \"\"\"\n",
    "    # Gather context from the research folder\n",
    "    guidelines_path = research_path / ARTICLE_GUIDELINE_FILE\n",
    "    results_selected_path = nova_path / PERPLEXITY_RESULTS_SELECTED_FILE\n",
    "    \n",
    "    article_guidelines = read_file_safe(guidelines_path)\n",
    "    accepted_sources_data = read_file_safe(results_selected_path)\n",
    "    scraped_guideline_ctx = load_scraped_guideline_context(nova_path)\n",
    "    \n",
    "    # Use LLM to select top sources for scraping\n",
    "    selected_urls, reasoning = await select_top_sources(\n",
    "        article_guidelines, accepted_sources_data, scraped_guideline_ctx, max_sources\n",
    "    )\n",
    "    \n",
    "    # Write selected URLs to file\n",
    "    urls_out_path = nova_path / URLS_TO_SCRAPE_FROM_RESEARCH_FILE\n",
    "    urls_out_path.write_text(\"\\n\".join(selected_urls) + \"\\n\", encoding=\"utf-8\")\n",
    "    \n",
    "    return {\n",
    "        \"status\": \"success\",\n",
    "        \"sources_selected\": selected_urls,\n",
    "        \"sources_selected_count\": len(selected_urls),\n",
    "        \"output_path\": str(urls_out_path.resolve()),\n",
    "        \"reasoning\": reasoning,\n",
    "        \"message\": f\"Successfully selected {len(selected_urls)} sources for full scraping...\"\n",
    "    }\n",
    "```\n",
    "\n",
    "The core of this tool is the `select_top_sources` function, which uses the `PROMPT_SELECT_TOP_SOURCES` prompt to evaluate each source and select the best ones to scrape."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be4aaee",
   "metadata": {},
   "source": [
    "### 4.2 The Source Selection Prompt\n",
    "\n",
    "The tool uses a prompt to choose the most valuable sources:\n",
    "\n",
    "```python\n",
    "PROMPT_SELECT_TOP_SOURCES = \"\"\"\n",
    "You are assisting with research for an upcoming article.\n",
    "\n",
    "Your task is to select the most relevant and trustworthy sources from the web search results.\n",
    "You should consider:\n",
    "1. **Relevance**: How well each source addresses the article guidelines\n",
    "2. **Authority**: The credibility and reputation of the source\n",
    "3. **Quality**: The depth and accuracy of the information provided\n",
    "4. **Uniqueness**: Sources that provide unique insights not covered by the scraped guideline URLs\n",
    "\n",
    "Please select the top {top_n} sources that would be most valuable for the article research.\n",
    "\n",
    "Return your selection with the following structure:\n",
    "- **selected_urls**: A list of the most valuable URLs to scrape in full, ordered by priority\n",
    "- **reasoning**: A short explanation summarizing why these specific URLs were chosen\n",
    "\"\"\".strip()\n",
    "```\n",
    "\n",
    "This prompt optimizes resource allocation by strategically selecting sources for expensive full-content scraping. The four-dimensional evaluation framework (relevance, authority, quality, uniqueness) ensures maximum research value while avoiding duplication with already-scraped guideline content. The uniqueness criterion is particularly important as it prevents redundant scraping of similar information. The reasoning requirement provides transparency for human oversight and helps identify potential gaps in the selection logic, making the process auditable and improvable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f402e0e",
   "metadata": {},
   "source": [
    "### 4.3 Testing the Source Selection Tool\n",
    "\n",
    "Now let's test the source selection tool to see which URLs it chooses for full content scraping. The tool will analyze our filtered sources and select the most valuable ones based on their potential contribution to the final research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0c671fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected sources:\n",
      "{'status': 'success', 'urls_selected_count': 3, 'selected_urls': ['https://hatchworks.com/blog/gen-ai/large-language-models-guide/', 'https://dev.to/gmo-flatt-security-inc/llm-external-access-security-risks-mcp-and-ai-agent-38ee', 'https://fireworks.ai/blog/function-calling'], 'selection_reasoning': \"These sources were chosen for their direct relevance to the article's core themes, offering unique insights not covered in the already scraped guideline URLs. The Hatchworks article provides a clear overview of the fundamental limitations of LLMs that function calling aims to solve, which is crucial for the 'Understanding why agents need tools' section. The article from dev.to offers a detailed analysis of security risks and mitigation strategies specific to LLM agents with external tool-calling capabilities, adding a critical dimension to the research. Finally, the Fireworks.ai blog post provides an excellent conceptual understanding of how function calling contributes to the development of more autonomous AI agents and 'action models', framing the broader impact of this technology.\", 'urls_output_path': '/Users/fabio/Desktop/course-ai-agents/lessons/research_agent_part_2/data/sample_research_folder/.nova/urls_to_scrape_from_research.md', 'message': \"âœ… Saved 3 URL(s) to scrape to /Users/fabio/Desktop/course-ai-agents/lessons/research_agent_part_2/data/sample_research_folder/.nova/urls_to_scrape_from_research.md.\\nReasoning: These sources were chosen for their direct relevance to the article's core themes, offering unique insights not covered in the already scraped guideline URLs. The Hatchworks article provides a clear overview of the fundamental limitations of LLMs that function calling aims to solve, which is crucial for the 'Understanding why agents need tools' section. The article from dev.to offers a detailed analysis of security risks and mitigation strategies specific to LLM agents with external tool-calling capabilities, adding a critical dimension to the research. Finally, the Fireworks.ai blog post provides an excellent conceptual understanding of how function calling contributes to the development of more autonomous AI agents and 'action models', framing the broader impact of this technology.\"}\n"
     ]
    }
   ],
   "source": [
    "from research_agent_part_2.mcp_server.src.tools import select_research_sources_to_scrape_tool\n",
    "\n",
    "# Test selecting sources to scrape\n",
    "result = await select_research_sources_to_scrape_tool(research_directory=research_folder, max_sources=3)\n",
    "print(\"Selected sources:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ce812b",
   "metadata": {},
   "source": [
    "## 5. Scraping Selected Research URLs\n",
    "\n",
    "The `scrape_research_urls_tool` handles the full content extraction from our selected sources. It works very similarly to the guideline URL scraping tool we saw in lesson 16, using Firecrawl for robust web scraping and an LLM for content cleaning. For this reason, we won't go into the details of the scraping process here, and we only provide the code for testing the tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9e5b5a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping results:\n",
      "{'status': 'success', 'urls_processed': 3, 'urls_total': 3, 'original_urls_count': 3, 'deduplicated_count': 0, 'files_saved': 3, 'output_directory': '/Users/fabio/Desktop/course-ai-agents/lessons/research_agent_part_2/data/sample_research_folder/.nova/urls_from_research', 'saved_files': ['large-language-models-what-you-need-to-know-in-2025-hatchwor.md', 'securing-llm-function-calling-risks-mitigations-for-ai-agent.md', 'fireworks-ai.md'], 'message': \"Processed 3 new URLs from urls_to_scrape_from_research.md in '/Users/fabio/Desktop/course-ai-agents/lessons/research_agent_part_2/data/sample_research_folder'. Scraped 3/3 web pages.\"}\n"
     ]
    }
   ],
   "source": [
    "from research_agent_part_2.mcp_server.src.tools import scrape_research_urls_tool\n",
    "\n",
    "# Test scraping the selected research URLs\n",
    "result = await scrape_research_urls_tool(research_directory=research_folder, concurrency_limit=2)\n",
    "print(\"Scraping results:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22f413d",
   "metadata": {},
   "source": [
    "## 6. Creating the Final Research File\n",
    "\n",
    "The `create_research_file_tool` is the final step of our entire workflow. It takes all the accumulated research data and formats it into a comprehensive, well-organized markdown file that serves as input for the writing agent we'll build in the next part of the course."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbcd045",
   "metadata": {},
   "source": [
    "### 6.1 Understanding the Compilation Process\n",
    "\n",
    "This tool serves as the final orchestrator, combining all research data into a comprehensive markdown file. It takes the research directory as input and collects content from multiple sources: filtered Perplexity results, scraped guideline sources, code repositories, YouTube transcripts, and additional research sources. The tool organizes everything into collapsible sections and outputs a structured `research.md` file with detailed statistics about the compilation process.\n",
    "\n",
    "Source: _mcp_server/src/tools/create_research_file_tool.py_\n",
    "\n",
    "```python\n",
    "def create_research_file_tool(research_directory: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Generate comprehensive research.md file from all research data.\n",
    "    \n",
    "    Combines all research data including filtered Perplexity results, scraped guideline\n",
    "    sources, and full research sources into a comprehensive research.md file. The file\n",
    "    is organized into sections with collapsible blocks for easy navigation.\n",
    "    \"\"\"\n",
    "    # Convert to Path object\n",
    "    article_dir = Path(research_directory)\n",
    "    nova_dir = article_dir / NOVA_FOLDER\n",
    "    \n",
    "    # Collect all research data\n",
    "    perplexity_results = read_file_safe(nova_dir / PERPLEXITY_RESULTS_SELECTED_FILE)\n",
    "    \n",
    "    # Collect scraped sources from guidelines\n",
    "    scraped_sources = collect_directory_markdowns_with_titles(nova_dir / URLS_FROM_GUIDELINES_FOLDER)\n",
    "    code_sources = collect_directory_markdowns_with_titles(nova_dir / URLS_FROM_GUIDELINES_CODE_FOLDER)\n",
    "    youtube_transcripts = collect_directory_markdowns_with_titles(nova_dir / URLS_FROM_GUIDELINES_YOUTUBE_FOLDER)\n",
    "    \n",
    "    # Collect full research sources\n",
    "    additional_sources = collect_directory_markdowns_with_titles(nova_dir / URLS_FROM_RESEARCH_FOLDER)\n",
    "    \n",
    "    # Build comprehensive research sections\n",
    "    research_results_section = build_research_results_section(perplexity_results)\n",
    "    scraped_sources_section = build_sources_section(\"Scraped Sources from Guidelines\", scraped_sources)\n",
    "    code_sources_section = build_sources_section(\"Code Sources from Guidelines\", code_sources)\n",
    "    youtube_section = build_sources_section(\"YouTube Transcripts from Guidelines\", youtube_transcripts)\n",
    "    additional_section = build_sources_section(\"Additional Research Sources\", additional_sources)\n",
    "    \n",
    "    # Combine all sections into final research file\n",
    "    research_content = combine_research_sections([\n",
    "        research_results_section,\n",
    "        scraped_sources_section,\n",
    "        code_sources_section,\n",
    "        youtube_section,\n",
    "        additional_section\n",
    "    ])\n",
    "    \n",
    "    # Write final research file\n",
    "    research_file_path = article_dir / RESEARCH_MD_FILE\n",
    "    research_file_path.write_text(research_content, encoding=\"utf-8\")\n",
    "    \n",
    "    return {\n",
    "        \"status\": \"success\",\n",
    "        \"markdown_file\": str(research_file_path.resolve()),\n",
    "        \"research_results_count\": len(extract_perplexity_chunks(perplexity_results)),\n",
    "        \"scraped_sources_count\": len(scraped_sources),\n",
    "        \"code_sources_count\": len(code_sources),\n",
    "        \"youtube_transcripts_count\": len(youtube_transcripts),\n",
    "        \"additional_sources_count\": len(additional_sources),\n",
    "        \"message\": f\"Successfully created comprehensive research file: {research_file_path.name}\"\n",
    "    }\n",
    "```\n",
    "\n",
    "No need to go into the details of the code here, as it's standard Python code for file I/O and string manipulation. We'll simply test the tool and see the final output to better understand what the input of the writing agent will be."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ac2c54",
   "metadata": {},
   "source": [
    "### 6.2 The Research File Structure\n",
    "\n",
    "The final research file `research.md` is organized into collapsible sections for easy navigation. It will look like the following:\n",
    "\n",
    "```markdown\n",
    "# Research Results\n",
    "\n",
    "## Research Results from Web Search\n",
    "<details>\n",
    "<summary>Query: [Original Query]</summary>\n",
    "\n",
    "### Source [1]: [URL]\n",
    "[Content from source]\n",
    "\n",
    "### Source [2]: [URL]\n",
    "[Content from source]\n",
    "</details>\n",
    "\n",
    "## Scraped Sources from Guidelines\n",
    "<details>\n",
    "<summary>Source: [Filename]</summary>\n",
    "[Full scraped content]\n",
    "</details>\n",
    "\n",
    "## Code Sources from Guidelines\n",
    "<details>\n",
    "<summary>Repository: [Repository Name]</summary>\n",
    "[Repository analysis and code content]\n",
    "</details>\n",
    "\n",
    "## YouTube Transcripts from Guidelines\n",
    "<details>\n",
    "<summary>Video: [Video Title]</summary>\n",
    "[Full video transcript]\n",
    "</details>\n",
    "\n",
    "## Additional Research Sources\n",
    "<details>\n",
    "<summary>Source: [URL]</summary>\n",
    "[Full scraped research content]\n",
    "</details>\n",
    "```\n",
    "\n",
    "This structure provides comprehensive coverage while remaining navigable for both humans and AI writing agents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3274a55e",
   "metadata": {},
   "source": [
    "### 6.3 Testing the Research File Creation\n",
    "\n",
    "Now let's test the final compilation tool to see how it brings together all our research data into a comprehensive, well-structured file. This represents the final step of our entire research workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "330624e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Research file creation results:\n",
      "{'status': 'success', 'markdown_file': '/Users/fabio/Desktop/course-ai-agents/lessons/research_agent_part_2/data/sample_research_folder/research.md', 'research_results_count': 3, 'scraped_sources_count': 3, 'code_sources_count': 2, 'youtube_transcripts_count': 1, 'additional_sources_count': 2, 'message': 'âœ… Generated research markdown file:\\n  - research.md'}\n",
      "\n",
      "First 1000 characters of the research file:\n",
      "# Research\n",
      "\n",
      "## Research Results\n",
      "\n",
      "<details>\n",
      "<summary>What are the fundamental limitations of large language models that tool use and function calling aim to solve?</summary>\n",
      "\n",
      "### Source [1]: https://hatchworks.com/blog/gen-ai/large-language-models-guide/\n",
      "\n",
      "Query: What are the fundamental limitations of large language models that tool use and function calling aim to solve?\n",
      "\n",
      "Answer: Large Language Models (LLMs) have several fundamental technical limitations that tool use and function calling aim to solve:\n",
      "\n",
      "- **Domain Mismatch**: LLMs trained on general datasets struggle with providing accurate or detailed responses in specialized or niche domains, leading to generic or incorrect outputs when specific expertise is required.\n",
      "\n",
      "- **Word Prediction Issues**: LLMs often fail with less common words or phrases, affecting their ability to generate or translate technical or domain-specific content accurately.\n",
      "\n",
      "- **Hallucinations**: LLMs sometimes produce highly original but fabricated information. F...\n"
     ]
    }
   ],
   "source": [
    "from research_agent_part_2.mcp_server.src.tools import create_research_file_tool\n",
    "\n",
    "# Test creating the final research file\n",
    "result = create_research_file_tool(research_directory=research_folder)\n",
    "print(\"Research file creation results:\")\n",
    "print(result)\n",
    "\n",
    "# Read and display a sample of the generated research file\n",
    "with open(result[\"markdown_file\"], \"r\") as f:\n",
    "    content = f.read()\n",
    "    print(\"\\nFirst 1000 characters of the research file:\")\n",
    "    print(content[:1000] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b448dd51",
   "metadata": {},
   "source": [
    "## 7. Human-in-the-Loop Feedback Integration\n",
    "\n",
    "As we saw in the previous lesson, it's possible to integrate human feedback into the research agent workflow by simply providing instructions after invoking the MCP prompt. For example, when running the workflow, users can instruct the agent to:\n",
    "\n",
    "- Show the source IDs selected by `select_research_sources_to_keep` and ask for approval before proceeding.\n",
    "- Display the URLs chosen by `select_research_sources_to_scrape` and allow modifications.\n",
    "- Pause after any step for human review and guidance.\n",
    "\n",
    "This flexibility allows users to maintain control over the research quality while benefiting from the agent's analytical capabilities. Notice that this is possible because of how the MCP prompt is designed and how the inputs/outputs of the MCP tools are structured!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab1d1d7",
   "metadata": {},
   "source": [
    "## 8. Testing the Complete Agent Workflow\n",
    "\n",
    "Now let's test the complete end-to-end research agent workflow. We'll use the MCP client to run the full workflow and examine the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "776abad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ› ï¸  Available tools: 11\n",
      "ğŸ“š Available resources: 2\n",
      "ğŸ’¬ Available prompts: 1\n",
      "\n",
      "Available Commands: /tools, /resources, /prompts, /prompt/<name>, /resource/<uri>, /model-thinking-switch, /quit\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95mğŸ¤” LLM's Thoughts:\u001b[0m\n",
      "\u001b[35m**Let's Get This Research Started!**\n",
      "\n",
      "Okay, so I'm ready to roll. My first step is to lay out this research workflow â€“ gotta make sure we're on the same page and, critically, that I have the right starting point. I need to explain the steps, and then I'll politely ask for that all-important `research_directory`. I'm also going to check if there are any tweaks or modifications you'd like to make to the process.\n",
      "\n",
      "Once that's clear, I'll kick things off by calling `extract_guidelines_urls` to grab the URLs and local files listed in `ARTICLE_GUIDELINE_FILE`. That gives me a solid initial pool to work with.\n",
      "\n",
      "Then, the heavy lifting begins. I'll launch several parallel processes to handle different types of resources efficiently. I'll call `process_local_files` for local content, `scrape_and_clean_other_urls` for general web pages, `process_github_urls` for those GitHub repositories (always a good source!), and `transcribe_youtube_urls` for any relevant YouTube videos. Parallel processing is key here for speed.\n",
      "\n",
      "Next comes the core research loop. We'll run this three times, each time generating new search queries with `generate_next_queries`. Those queries will then be fed into `run_perplexity_research` to pull in the initial research results.\n",
      "\n",
      "Now, we filter. I'll use `select_research_sources_to_keep` to identify and keep only the highest-quality sources from the Perplexity results. This is where expertise comes into play â€“ I need to make sure we're focusing on the good stuff.\n",
      "\n",
      "Once the good stuff is identified, I'll identify sources for a complete scrape using `select_research_sources_to_scrape`. Then I'll call `scrape_research_urls` to scrape the selected sources.\n",
      "\n",
      "Finally, I'll synthesize everything. I'll call `create_research_file` to compile all the research, cleaned content, scraped pages, and initial sources into a comprehensive `research.md` file.\n",
      "\n",
      "Now, a critical point: I need to be prepared for potential hiccups. If *any* of the tools run into trouble and successfully process zero items, I'm going to halt the entire process. I'll state which tool failed and the associated error message, and then I'll ask for your guidance on how to proceed. Better safe than sorry â€“ a failed start could compromise the integrity of the entire process!\u001b[0m\n",
      "\n",
      "\u001b[37mğŸ’¬ LLM Response: \u001b[0mHello! I will help you execute the Nova research agent workflow. Here are the steps involved:\n",
      "\n",
      "1.  **Setup**: I will begin by extracting URLs and local file references from your `ARTICLE_GUIDELINE_FILE`.\n",
      "2.  **Process Extracted Resources**: I will then process these extracted resources in parallel:\n",
      "    *   Copying referenced local files to a dedicated folder.\n",
      "    *   Scraping and cleaning content from other web URLs.\n",
      "    *   Processing GitHub URLs using gitingest to create summaries.\n",
      "    *   Transcribing YouTube video URLs.\n",
      "3.  **Research Loop**: I will run a research loop for 3 rounds. In each round:\n",
      "    *   I will generate new web-search queries based on existing research and guidelines.\n",
      "    *   I will execute these queries using Perplexity and save the results.\n",
      "4.  **Filter Perplexity Results**: After the research rounds, I will automatically select high-quality sources from the Perplexity results based on trustworthiness, authority, and relevance.\n",
      "5.  **Scrape Selected Research Sources**: I will identify up to 5 priority research sources from the filtered Perplexity results that deserve a full scrape, and then proceed to scrape their full content.\n",
      "6.  **Write Final Research File**: Finally, I will combine all the collected and processed research data into a comprehensive `research.md` file, organized into sections for easy navigation.\n",
      "\n",
      "Please provide the path to your research directory. Also, let me know if you need any modifications to this workflow, such as starting from a specific step or adding user feedback points.\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95mğŸ¤” LLM's Thoughts:\u001b[0m\n",
      "\u001b[35m**Ready to Get Started!**\n",
      "\n",
      "Okay, so I've been given the `research_directory` and it's time to kick off the whole process. Seems straightforward enough. The first thing on the agenda, as per that workflow I've been following, is to run the `extract_guidelines_urls` tool.  Gotta start with the URLs, right? That's step 1.3, so let's get those urls extracted and move on to the next phase.  Let's see what we can find!\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[36mğŸ”§ Function Call (Tool):\u001b[0m\n",
      "\u001b[36m  Tool: \u001b[0m\u001b[1m\u001b[36mextract_guidelines_urls\u001b[0m\n",
      "\u001b[36m  Arguments: \u001b[0m\u001b[36m{\n",
      "  \"research_directory\": \"/Users/fabio/Desktop/course-ai-agents/lessons/research_agent_part_2/data/sample_context_engineering\"\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[36mâš¡ Executing tool 'extract_guidelines_urls' via MCP server...\u001b[0m\n",
      "\u001b[36mâœ… Tool execution successful!\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[95mğŸ¤” LLM's Thoughts:\u001b[0m\n",
      "\u001b[35m**Moving on to Parallel Processing**\n",
      "\n",
      "Okay, so I've received the research directory and confirmation to run the full workflow. Step 1.3, `extract_guidelines_urls`, is already done. Looking at the results, it's pretty straightforward. We have one GitHub source, no YouTube sources, nine web sources, and no local files.\n",
      "\n",
      "Now, I'm ready for Step 2: \"Process the extracted resources in parallel\". Let's break this down.\n",
      "\n",
      "*   2.1: `process_local_files` -  `local_files_count` is zero, so I can skip this one. No need to waste time there.\n",
      "*   2.2: `scrape_and_clean_other_urls` - The `web_sources_count` is nine. This is definitely something to process, so I'll call this tool.\n",
      "*   2.3: `process_github_urls` - With one GitHub source, this is also a clear yes. Call this tool.\n",
      "*   2.4: `transcribe_youtube_urls` - Since `youtube_sources_count` is zero, I can skip this step.\n",
      "\n",
      "The efficient approach here is to launch `scrape_and_clean_other_urls` and `process_github_urls` in parallel. That should maximize throughput and get things moving.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[36mğŸ”§ Function Call (Tool):\u001b[0m\n",
      "\u001b[36m  Tool: \u001b[0m\u001b[1m\u001b[36mscrape_and_clean_other_urls\u001b[0m\n",
      "\u001b[36m  Arguments: \u001b[0m\u001b[36m{\n",
      "  \"research_directory\": \"/Users/fabio/Desktop/course-ai-agents/lessons/research_agent_part_2/data/sample_context_engineering\"\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[36mâš¡ Executing tool 'scrape_and_clean_other_urls' via MCP server...\u001b[0m\n",
      "\u001b[36mâœ… Tool execution successful!\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1758624909.196635 3235692 fork_posix.cc:71] Other threads are currently calling into gRPC, skipping fork() handlers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95mğŸ¤” LLM's Thoughts:\u001b[0m\n",
      "\u001b[35m**Ready to Process Extracted Resources**\n",
      "\n",
      "Okay, so the user has given me the research directory and wants to execute the full workflow. Good, Step 1.3 (`extract_guidelines_urls`) has already finished without a hitch! Now, I need to move on to Step 2, which involves processing the resources I just extracted â€“ and it needs to be done in parallel.\n",
      "\n",
      "According to the workflow instructions, I've got to run `process_local_files`, `scrape_and_clean_other_urls`, `process_github_urls`, and `transcribe_youtube_urls` simultaneously.\n",
      "\n",
      "I've got the output from `extract_guidelines_urls`, which tells me a few key things: there's 1 GitHub source, no YouTube sources, 9 web sources, and no local files. And the `scrape_and_clean_other_urls` step has already completed successfully, processing 7 out of those 9 web sources.\n",
      "\n",
      "Since there are no local files and no YouTube sources, I can skip `process_local_files` and `transcribe_youtube_urls`. That simplifies things. It looks like the only thing left to do in parallel is to call `process_github_urls`. That seems like a reasonable and efficient execution plan. The research directory I'm working with is `/Users/fabio/Desktop/course-ai-agents/lessons/research_agent_part_2/data/sample_context_engineering`. Time to get started!\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[36mğŸ”§ Function Call (Tool):\u001b[0m\n",
      "\u001b[36m  Tool: \u001b[0m\u001b[1m\u001b[36mprocess_github_urls\u001b[0m\n",
      "\u001b[36m  Arguments: \u001b[0m\u001b[36m{\n",
      "  \"research_directory\": \"/Users/fabio/Desktop/course-ai-agents/lessons/research_agent_part_2/data/sample_context_engineering\"\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[36mâš¡ Executing tool 'process_github_urls' via MCP server...\u001b[0m\n",
      "\u001b[36mâœ… Tool execution successful!\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[95mğŸ¤” LLM's Thoughts:\u001b[0m\n",
      "\u001b[35m**Moving to the Core Research Loop**\n",
      "\n",
      "Okay, so the user's ready for the full workflow run. I've already handled the preliminary steps: extracting the guideline URLs and getting started on scraping and processing the web resources. Based on the outputs, I've got one GitHub URL done, seven out of nine \"other\" URLs processed.\n",
      "\n",
      "My immediate focus now is completing step 2: fully processing those extracted resources. Luckily, it looks pretty straightforward. The `extract_guidelines_urls` tool confirmed no local files or YouTube URLs, so I can skip `process_local_files` and `transcribe_youtube_urls` entirely.\n",
      "\n",
      "That means I'm free to wrap up step 2 and then kick off step 3: the critical research loop. Three rounds of `generate_next_queries` and `run_perplexity_research` are up next. It's time to dig into the main insights. Let's make this happen!\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[36mğŸ”§ Function Call (Tool):\u001b[0m\n",
      "\u001b[36m  Tool: \u001b[0m\u001b[1m\u001b[36mgenerate_next_queries\u001b[0m\n",
      "\u001b[36m  Arguments: \u001b[0m\u001b[36m{\n",
      "  \"research_directory\": \"/Users/fabio/Desktop/course-ai-agents/lessons/research_agent_part_2/data/sample_context_engineering\"\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[36mâš¡ Executing tool 'generate_next_queries' via MCP server...\u001b[0m\n",
      "\u001b[36mâœ… Tool execution successful!\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[95mğŸ¤” LLM's Thoughts:\u001b[0m\n",
      "\u001b[35m**Next Steps in the Research Workflow**\n",
      "\n",
      "Okay, so the user wants to run the full workflow, and I've already completed the initial steps: `extract_guidelines_urls`, `scrape_and_clean_other_urls`, `process_github_urls`, and `generate_next_queries`. Based on what I know about the overall process, here's what I need to do next.\n",
      "\n",
      "First, I should call `process_local_files`. The results from the initial URL extraction showed a count of zero local files, so it's likely this step won't do much, but it's part of the designated workflow. Then, I need to transcribe YouTube URLs, which is called via `transcribe_youtube_urls`.  Again, the initial analysis identified zero YouTube sources, so this might not generate any new data, but I need to execute it nonetheless.\n",
      "\n",
      "Finally, I'm at the first round of the iterative research loop. I've already run the initial query generation. Now, I need to execute `run_perplexity_research` using the queries I generated in the previous step. The output of `generate_next_queries` will provide the required input for this tool. After I call all three, the research cycle will be complete.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[36mğŸ”§ Function Call (Tool):\u001b[0m\n",
      "\u001b[36m  Tool: \u001b[0m\u001b[1m\u001b[36mprocess_local_files\u001b[0m\n",
      "\u001b[36m  Arguments: \u001b[0m\u001b[36m{\n",
      "  \"research_directory\": \"/Users/fabio/Desktop/course-ai-agents/lessons/research_agent_part_2/data/sample_context_engineering\"\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[36mâš¡ Executing tool 'process_local_files' via MCP server...\u001b[0m\n",
      "\u001b[36mâœ… Tool execution successful!\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[95mğŸ¤” LLM's Thoughts:\u001b[0m\n",
      "\u001b[35m**Continuing the Research Workflow**\n",
      "\n",
      "Okay, so the user's provided the research directory, and I'm ready to keep this process moving. Let's see... I've already handled steps 1.3 (extracting guideline URLs), 2.1 (processing local files - even though it reported no files), 2.2 (scraping and cleaning other URLs), 2.3 (processing GitHub URLs), and the initial round of 3.1 (generating those next research queries).\n",
      "\n",
      "Since the output from the URL extraction step shows no YouTube sources, I can safely skip the YouTube transcription step (2.4). That saves some time.\n",
      "\n",
      "Now, the next logical move is 3.2: executing `run_perplexity_research`. I'll feed it the queries I just generated in the first iteration of step 3.1. After that's done, I need to loop back and run 3.1 and 3.2 twice more to complete the three research rounds. Then it's on to 4.1, selecting the research sources to keep. Following that will be 5.1, selecting which sources to scrape, and then the actual scraping in 5.2. Finally, the big finish: creating the research file in 6.1.\n",
      "\n",
      "Alright, let's get started on the first round of `run_perplexity_research` using those freshly minted `queries`. Let's get this research rolling!\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[36mğŸ”§ Function Call (Tool):\u001b[0m\n",
      "\u001b[36m  Tool: \u001b[0m\u001b[1m\u001b[36mrun_perplexity_research\u001b[0m\n",
      "\u001b[36m  Arguments: \u001b[0m\u001b[36m{\n",
      "  \"research_directory\": \"/Users/fabio/Desktop/course-ai-agents/lessons/research_agent_part_2/data/sample_context_engineering\",\n",
      "  \"queries\": [\n",
      "    \"When to use context engineering versus fine-tuning for LLM applications?\",\n",
      "    \"What is a formal taxonomy of context components for large language models?\",\n",
      "    \"What are best practices and code examples for using XML to structure context in LLM prompts?\",\n",
      "    \"What is the historical evolution of LLM applications from simple chatbots to complex, memory-enabled agents?\",\n",
      "    \"How does the token efficiency of YAML compare to JSON when providing structured data to LLMs?\"\n",
      "  ]\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[36mâš¡ Executing tool 'run_perplexity_research' via MCP server...\u001b[0m\n",
      "\u001b[36mâœ… Tool execution successful!\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[95mğŸ¤” LLM's Thoughts:\u001b[0m\n",
      "\u001b[35m**Ready for the Next Phase of Research**\n",
      "\n",
      "Okay, so we're making good progress. I've already set the stage â€“ explained the workflow, got the research directory (thanks for that!), and extracted those guidelines URLs. My tools for processing the extracted resources have done their work. `process_local_files` found nothing locally, `scrape_and_clean_other_urls` and `process_github_urls` completed successfully.  I see I skipped `transcribe_youtube_urls` earlier, but given that `youtube_sources_count` was zero, I'm expecting it'll also be a quick run. Let's get that done, just to be thorough.\n",
      "\n",
      "Now, we're at the heart of the matter: the research loop. We've just finished the first round, where I successfully generated five queries and ran them through Perplexity. That's a good start. I can see what I need to do:  I need to go through two more rounds of `generate_next_queries` and `run_perplexity_research` to complete Step 3. After that, I'll be ready to move on to refining the sources (Steps 4 and 5) and finally generating the research file (Step 6).\n",
      "\n",
      "Here's my immediate plan:\n",
      "\n",
      "*   Call `transcribe_youtube_urls`.\n",
      "*   Start Round 2 of the research loop: Call `generate_next_queries` and then `run_perplexity_research`.\n",
      "*   Repeat for Round 3: Call `generate_next_queries` and then `run_perplexity_research`.\n",
      "\n",
      "After that, I'm ready to move onto source selection and scraping, and then I'll generate the final research file.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[36mğŸ”§ Function Call (Tool):\u001b[0m\n",
      "\u001b[36m  Tool: \u001b[0m\u001b[1m\u001b[36mtranscribe_youtube_urls\u001b[0m\n",
      "\u001b[36m  Arguments: \u001b[0m\u001b[36m{\n",
      "  \"research_directory\": \"/Users/fabio/Desktop/course-ai-agents/lessons/research_agent_part_2/data/sample_context_engineering\"\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[36mâš¡ Executing tool 'transcribe_youtube_urls' via MCP server...\u001b[0m\n",
      "\u001b[36mâœ… Tool execution successful!\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[95mğŸ¤” LLM's Thoughts:\u001b[0m\n",
      "\u001b[35m**Continuing the Research Workflow**\n",
      "\n",
      "Okay, so the user's given me the go-ahead to run the entire workflow on this research directory.  I've already handled the initial steps: extracted guidelines URLs, processed the local files (though there weren't any), scraped and cleaned the other URLs, dealt with the GitHub links, and transcribed what YouTube content there was (again, nothing to find).  I've also completed the first round of the research loop, running both `generate_next_queries` and `run_perplexity_research`.\n",
      "\n",
      "Now it's time to keep the momentum going. I need to complete two more iterations of that research loop, so that means another two calls to `generate_next_queries` and two more calls to `run_perplexity_research`. Let's get these next queries generated and see what insights Perplexity uncovers.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[36mğŸ”§ Function Call (Tool):\u001b[0m\n",
      "\u001b[36m  Tool: \u001b[0m\u001b[1m\u001b[36mgenerate_next_queries\u001b[0m\n",
      "\u001b[36m  Arguments: \u001b[0m\u001b[36m{\n",
      "  \"research_directory\": \"/Users/fabio/Desktop/course-ai-agents/lessons/research_agent_part_2/data/sample_context_engineering\"\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[36mâš¡ Executing tool 'generate_next_queries' via MCP server...\u001b[0m\n",
      "\u001b[36mâœ… Tool execution successful!\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[95mğŸ¤” LLM's Thoughts:\u001b[0m\n",
      "\u001b[35m**Current Research Loop Status & Next Steps**\n",
      "\n",
      "Okay, so I've been given the go-ahead and the research directory is ready. I've already plowed through the initial stages - got the guidelines URLs, scraped and cleaned other URLs, processed the GitHub and local files (nothing there), and transcribed YouTube URLs (also nothing). I've even completed the first round of the research loop, including generating the next queries and running the Perplexity research.\n",
      "\n",
      "Now, my focus is the research loop itself. I need to run through two more rounds - so, generate the next queries for round 2, then run the research, rinse and repeat for round 3. Then, I'll be moving on to steps 4, 5, and 6. I'm keen to get the deeper dives started in each round.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[36mğŸ”§ Function Call (Tool):\u001b[0m\n",
      "\u001b[36m  Tool: \u001b[0m\u001b[1m\u001b[36mrun_perplexity_research\u001b[0m\n",
      "\u001b[36m  Arguments: \u001b[0m\u001b[36m{\n",
      "  \"research_directory\": \"/Users/fabio/Desktop/course-ai-agents/lessons/research_agent_part_2/data/sample_context_engineering\",\n",
      "  \"queries\": [\n",
      "    \"What is the 'lost in the middle' problem in large language models and what are the best techniques to mitigate it?\",\n",
      "    \"How does the number and design of tools impact LLM agent performance and lead to 'tool confusion'?\",\n",
      "    \"What are practical examples of assembling short-term and long-term memory components into a system prompt for a stateful AI agent?\",\n",
      "    \"How can the orchestrator-worker pattern be used for context isolation in complex multi-agent systems?\",\n",
      "    \"What are the causes and mitigation strategies for 'context drift' in long-running AI agents?\"\n",
      "  ]\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[36mâš¡ Executing tool 'run_perplexity_research' via MCP server...\u001b[0m\n",
      "\u001b[36mâœ… Tool execution successful!\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[95mğŸ¤” LLM's Thoughts:\u001b[0m\n",
      "\u001b[35m**My Current State and Next Steps**\n",
      "\n",
      "Okay, so the user has given me the green light to run the entire workflow, and I've already handled the initial steps. I've explained the process, obtained the research directory (which is now set), and the `extract_guidelines_urls` step has finished successfully.\n",
      "\n",
      "Now, it's time to move on to step 2, which is all about parallel processing of the extracted resources.  Looking at the tool outputs,  `scrape_and_clean_other_urls`, `process_github_urls`, `process_local_files`, and `transcribe_youtube_urls` all ran successfully.  I see that `extract_guidelines_urls` found 1 GitHub URL, 9 other URLs, and no local or YouTube URLs. Everything seems to have processed correctly. `process_local_files` reported no local files, as expected, and `transcribe_youtube_urls` reported no YouTube URLs, which is also correct based on the initial output. So, step 2 is effectively complete.\n",
      "\n",
      "Next up, it's time for the research loop, which will take us through three rounds. I've already done two rounds of `generate_next_queries` and `run_perplexity_research`. The first round generated 5 queries and added 18 sources, and the second round generated another 5 queries and added 19 sources.  So, I just need to complete the third and final round of `generate_next_queries` and `run_perplexity_research`.\n",
      "\n",
      "Once that's done, I'll proceed to step 4, which is selecting the research sources to keep.  Following that, step 5 will involve selecting the sources to scrape and actually scraping them. Finally, I will wrap everything up in step 6 by creating the research file. I am ready to get that final round started!\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[36mğŸ”§ Function Call (Tool):\u001b[0m\n",
      "\u001b[36m  Tool: \u001b[0m\u001b[1m\u001b[36mgenerate_next_queries\u001b[0m\n",
      "\u001b[36m  Arguments: \u001b[0m\u001b[36m{\n",
      "  \"research_directory\": \"/Users/fabio/Desktop/course-ai-agents/lessons/research_agent_part_2/data/sample_context_engineering\"\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[36mâš¡ Executing tool 'generate_next_queries' via MCP server...\u001b[0m\n",
      "\u001b[36mâœ… Tool execution successful!\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[95mğŸ¤” LLM's Thoughts:\u001b[0m\n",
      "\u001b[35m**Finalizing the Research Workflow**\n",
      "\n",
      "Okay, so I've been given the go-ahead to run the entire workflow on this research directory.  Let me see where I stand... I've already powered through a good chunk of it.  Specifically, I've got Step 1.3 (extracting URLs) and all of Step 2 (processing files, both local and web-based) completed.  I know there weren't any local files, so 2.1 was a no-op, but everything else in Step 2 went off without a hitch.  Step 3 is where the heavy lifting happens, and I've already done three rounds of query generation and research with Perplexity.\n",
      "\n",
      "Now, I'm at the very end of Round 3 of `run_perplexity_research`.  Once that completes, I can finally move on to Steps 4, 5, and 6.\n",
      "\n",
      "Here's the plan I need to execute:\n",
      "1.  **Finish up this final round of research with Perplexity.**\n",
      "2.  **Select the key sources I want to keep from the gathered research.**\n",
      "3.  **Choose the remaining research sources that actually need to be scraped.**\n",
      "4.  **Go ahead and scrape the remaining relevant URLs.**\n",
      "5.  **Finally, I'll create the master research file.**\n",
      "\n",
      "Alright, let's get this finished!\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[36mğŸ”§ Function Call (Tool):\u001b[0m\n",
      "\u001b[36m  Tool: \u001b[0m\u001b[1m\u001b[36mrun_perplexity_research\u001b[0m\n",
      "\u001b[36m  Arguments: \u001b[0m\u001b[36m{\n",
      "  \"research_directory\": \"/Users/fabio/Desktop/course-ai-agents/lessons/research_agent_part_2/data/sample_context_engineering\",\n",
      "  \"queries\": [\n",
      "    \"What are advanced context compression techniques for LLM agents, such as deduplication and memory offloading?\",\n",
      "    \"What are practical strategies to mitigate context drift in long-running, stateful AI agents?\",\n",
      "    \"Python code examples for a complete LLM system prompt integrating short-term history, episodic memory, and semantic knowledge using XML tags.\",\n",
      "    \"What is the timeline of enterprise AI application development from simple chatbots to RAG, tool-using agents, and memory-enabled agents?\",\n",
      "    \"How does context engineering intersect with software engineering, data engineering, and MLOps for production AI systems?\"\n",
      "  ]\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[36mâš¡ Executing tool 'run_perplexity_research' via MCP server...\u001b[0m\n",
      "\u001b[36mâœ… Tool execution successful!\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[95mğŸ¤” LLM's Thoughts:\u001b[0m\n",
      "\u001b[35m**My Next Steps in the Research Workflow**\n",
      "\n",
      "Alright, I've got the research directory ready, and it looks like the user wants to push through and complete the entire process. Good, good.  Let's see where we are. Okay, looks like I've already plowed through all the initial steps: extracted guideline URLs, processed local files (though there weren't any), scraped and cleaned other URLs, handled GitHub and YouTube links (again, effectively a no-op on the YouTube side). And I've even burned through three rounds of query generation and Perplexity research â€“ good progress.\n",
      "\n",
      "Now, according to my internal logic and the process the user has requested, I need to pick up at Step 4. That means filtering the Perplexity results for quality.  So, first up will be running the `select_research_sources_to_keep` tool. Then, once I've got a solid set of retained sources, I'll need to move on to Step 5, identifying which of them deserve a full scrape. I'll execute the `select_research_sources_to_scrape` tool, followed by `scrape_research_urls` to actually get the full content. Finally, Step 6, which is to finalize the research by executing `create_research_file`.  Pretty straightforward - keep moving forward!\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[36mğŸ”§ Function Call (Tool):\u001b[0m\n",
      "\u001b[36m  Tool: \u001b[0m\u001b[1m\u001b[36mselect_research_sources_to_keep\u001b[0m\n",
      "\u001b[36m  Arguments: \u001b[0m\u001b[36m{\n",
      "  \"research_directory\": \"/Users/fabio/Desktop/course-ai-agents/lessons/research_agent_part_2/data/sample_context_engineering\"\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[36mâš¡ Executing tool 'select_research_sources_to_keep' via MCP server...\u001b[0m\n",
      "\u001b[36mâœ… Tool execution successful!\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[95mğŸ¤” LLM's Thoughts:\u001b[0m\n",
      "\u001b[35m**My Current Workflow & Next Steps**\n",
      "\n",
      "Okay, so the user has given me the research directory and wants me to complete the entire workflow. I've already plowed through a significant chunk of it, as I've already successfully:\n",
      "\n",
      "*   **Extracted Guidelines URLs** and then **Scraped, Cleaned, and Processed various URL Types**: Local Files, GitHub Repositories, Other URLs and YouTube URLs.\n",
      "*   **Run through three rounds of the Research Loop**: I generated new queries and hit Perplexity with them each time, filtering the results based on their quality as I went.\n",
      "\n",
      "Right now, I'm at the crucial junction of deciding which sources merit a *deep dive*. That's where Step 5 comes in. I need to:\n",
      "\n",
      "*   **Step 5.1:** Use the \"select\\_research\\_sources\\_to\\_scrape\" tool to identify which of those accepted sources from my Perplexity results really deserve a full-blown scraping.\n",
      "*   **Step 5.2:** Then I need to execute the \"scrape\\_research\\_urls\" tool on those selected URLs.\n",
      "\n",
      "Once I've done that, I'm practically at the finish line! All that will remain is Step 6: \"Write final research file.\" That will involve:\n",
      "\n",
      "*   **Step 6.1:** Running the \"create\\_research\\_file\" tool to compile everything into a usable, final output.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[36mğŸ”§ Function Call (Tool):\u001b[0m\n",
      "\u001b[36m  Tool: \u001b[0m\u001b[1m\u001b[36mselect_research_sources_to_scrape\u001b[0m\n",
      "\u001b[36m  Arguments: \u001b[0m\u001b[36m{\n",
      "  \"research_directory\": \"/Users/fabio/Desktop/course-ai-agents/lessons/research_agent_part_2/data/sample_context_engineering\"\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[36mâš¡ Executing tool 'select_research_sources_to_scrape' via MCP server...\u001b[0m\n",
      "\u001b[36mâœ… Tool execution successful!\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[95mğŸ¤” LLM's Thoughts:\u001b[0m\n",
      "\u001b[35m**Workflow Progress & Next Steps**\n",
      "\n",
      "Okay, so the user has given me the research directory and wants to run the complete workflow. Let's see where we're at. Based on the last input, I've already done quite a bit. I explained the workflow, got the research directory, and called `extract_guidelines_urls`. Then, `process_local_files`, `scrape_and_clean_other_urls`, `process_github_urls`, and `transcribe_youtube_urls` have all been executed. The Perplexity research rounds have also completed - three rounds of query generation and subsequent research execution. Following that, I called the `select_research_sources_to_keep` and then the `select_research_sources_to_scrape` functions.\n",
      "\n",
      "Now, the next logical steps are to *actually* scrape those selected research URLs. So, I need to call `scrape_research_urls`. After that, I'll need to wrap things up with a `create_research_file` function to consolidate the results.\n",
      "\n",
      "**Reflecting on the Workflow's Progress**\n",
      "\n",
      "Okay, so the user has given me the research directory and wants me to keep moving through the workflow.  Let's see where we stand... I've already handled the preliminary explanations and gathering of the directory. Excellent. I've successfully extracted the guidelines URLs, and processed the local files (though there were none to process) and the other URLs and one GitHub URL. We've done three rounds of Perplexity research, generating new queries and running the searches. I've also selected the research sources to keep and the sources that need scraping.\n",
      "\n",
      "So, where am I now? Aha! My next tasks are clear. I need to call the `scrape_research_urls` function to gather more data and then `create_research_file ` to finalize the research file, which marks a significant completion point. Let's get these last two parts done.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[36mğŸ”§ Function Call (Tool):\u001b[0m\n",
      "\u001b[36m  Tool: \u001b[0m\u001b[1m\u001b[36mscrape_research_urls\u001b[0m\n",
      "\u001b[36m  Arguments: \u001b[0m\u001b[36m{\n",
      "  \"research_directory\": \"/Users/fabio/Desktop/course-ai-agents/lessons/research_agent_part_2/data/sample_context_engineering\"\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[36mâš¡ Executing tool 'scrape_research_urls' via MCP server...\u001b[0m\n",
      "\u001b[36mâœ… Tool execution successful!\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[95mğŸ¤” LLM's Thoughts:\u001b[0m\n",
      "\u001b[35m**Final Research Synthesis: Ready to Conclude**\n",
      "\n",
      "Okay, so I've been running through this workflow, and it looks like we're in good shape. I've already set up the explanation and gotten the directory information, which is a good start. I can see that the initial URL extraction went well, and all the resource processing steps (local files, other URLs, GitHub, and YouTube) were executed successfully, with no critical failures. We might have had no local files and no YouTube videos, but that's perfectly fineâ€”the tools handled the absence gracefully.\n",
      "\n",
      "The iterative research loop is looking solid too. Each of the three rounds went smoothly, with the query generation and Perplexity searches executing as expected. I like that we're generating new queries each time; that's crucial for exploring the topic more broadly. And the source selection at the end of each round is giving us some good material to work with.\n",
      "\n",
      "The final filtering step worked as intended, refining the collection of sources down to a manageable number. It's good to see that the scraping of the selected sources also executed without issues, bringing us closer to having all the primary data we need.\n",
      "\n",
      "So, where are we now? The next and final step is to generate the research file itself. I've done all the preparation, the selection, and the scraping. I think I'm ready to write up the final report and deliver it, considering the successful previous execution. Let's get that final file created.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[36mğŸ”§ Function Call (Tool):\u001b[0m\n",
      "\u001b[36m  Tool: \u001b[0m\u001b[1m\u001b[36mcreate_research_file\u001b[0m\n",
      "\u001b[36m  Arguments: \u001b[0m\u001b[36m{\n",
      "  \"research_directory\": \"/Users/fabio/Desktop/course-ai-agents/lessons/research_agent_part_2/data/sample_context_engineering\"\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[36mâš¡ Executing tool 'create_research_file' via MCP server...\u001b[0m\n",
      "\u001b[36mâœ… Tool execution successful!\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[95mğŸ¤” LLM's Thoughts:\u001b[0m\n",
      "\u001b[35m**Workflow Completion Assessment**\n",
      "\n",
      "Okay, so the user's provided the research directory and wants me to run the whole shebang. I've already plowed through step 1.3, which was `extract_guidelines_urls`. Looking at the outputs from that, it was a pretty standard mix: one GitHub source, no YouTube, a good nine web sources, and zilch local files. The follow-up steps, 2.1 through 2.4, can run in parallel, and I see that I've already done most of the heavy lifting there. The `scrape_and_clean_other_urls` tool got most of the web sources, the GitHub tool grabbed the one from there, and, as expected, nothing from YouTube or local files.\n",
      "\n",
      "The next part of the workflow is the research loop â€“ steps 3.1 and 3.2. I've already done that twice, and the last Perplexity run was successful, so I just need to hit that loop one more time.\n",
      "\n",
      "Beyond that, the rest seems to be in the bag. I've already executed the filtering step (4.1), the selection and scraping (5.1, 5.2), and I even created the final research file (6.1). Everything looks complete. I should probably tell the user that the workflow's done and give them a summary of the outcomes.\u001b[0m\n",
      "\n",
      "\u001b[37mğŸ’¬ LLM Response: \u001b[0mI have completed the entire workflow. All the steps, including extracting guidelines, processing various URLs (other web, GitHub, YouTube), handling local files, conducting three rounds of Perplexity research, filtering sources, scraping selected research URLs, and finally creating the comprehensive research file, have been executed successfully.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from research_agent_part_2.mcp_client.src.client import main as client_main\n",
    "import sys\n",
    "\n",
    "async def run_client():\n",
    "    _argv_backup = sys.argv[:]\n",
    "    sys.argv = [\"client\"]\n",
    "    try:\n",
    "        await client_main()\n",
    "    finally:\n",
    "        sys.argv = _argv_backup\n",
    "\n",
    "# Start client with in-memory server \n",
    "await run_client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2deeba4",
   "metadata": {},
   "source": [
    "Once the client is running, you can:\n",
    "\n",
    "1. **Start the complete workflow**: Type `/prompt/full_research_instructions_prompt` to load the complete research workflow\n",
    "2. **Provide the research directory**: Write something like `The research folder is /path/to/research_folder. Run the complete workflow from start to finish.`\n",
    "4. **Examine the final output**: Check the generated `research.md` file\n",
    "\n",
    "Try these commands in sequence:\n",
    "- `/prompt/full_research_instructions_prompt`\n",
    "- `The research folder is /path/to/research_folder. Run the complete workflow from start to finish.`\n",
    "- `/quit` after the agent completes all steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525c4c7a",
   "metadata": {},
   "source": [
    "## 9. Using Cursor with the MCP Server\n",
    "\n",
    "Our research agent can also be used directly within Cursor IDE through the MCP protocol. The `mcp_server` folder contains a `.mcp.json.sample` file that shows how to configure Cursor to use our research agent.\n",
    "\n",
    "Here's the content of the `.mcp.json.sample` file:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"mcpServers\": {\n",
    "    \"nova-research\": {\n",
    "      \"command\": \"/path/to/uv\",\n",
    "      \"args\": [\n",
    "        \"--directory\",\n",
    "        \"/path/to/mcp_server\",\n",
    "        \"run\",\n",
    "        \"-m\",\n",
    "        \"src.server\",\n",
    "        \"--transport\",\n",
    "        \"stdio\"\n",
    "      ],\n",
    "      \"env\": {\n",
    "        \"GOOGLE_API_KEY\": \"...\",\n",
    "        \"OPENAI_API_KEY\": \"...\",\n",
    "        \"PPLX_API_KEY\": \"...\",\n",
    "        \"FIRECRAWL_API_KEY\": \"...\",\n",
    "        \"OPIK_API_KEY\": \"...\",\n",
    "        \"OPIK_WORKSPACE\": \"...\",\n",
    "        \"OPIK_PROJECT_NAME\": \"...\",\n",
    "        \"GITHUB_TOKEN\": \"...\"\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "To set this up:\n",
    "1. Copy `.mcp.json.sample` to `.mcp.json` in your Cursor workspace\n",
    "2. Update the paths to point to your research agent installation\n",
    "3. Fill in your actual API keys in the `env` section\n",
    "4. Restart Cursor to load the MCP server\n",
    "5. Use the research tools directly in Cursor's chat interface\n",
    "\n",
    "This integration allows you to conduct research directly within your development environment, making it easy to incorporate findings into your coding projects.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
