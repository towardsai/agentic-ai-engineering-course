{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 22: Implementing the Foundations of the Writing Workflow\n",
    "\n",
    "In this lesson, we'll dive deep into the Brown writing agent, a sophisticated system designed to generate high-quality technical articles about AI.\n",
    "\n",
    "**Learning Objectives:**\n",
    "\n",
    "- Understand context engineering techniques for manipulating multiple inputs, structure and reason accordingly\n",
    "- Learn the orchestrator-worker pattern for generating media assets like Mermaid diagrams\n",
    "- Entity modeling using Pydantic to structure guidelines, research, profiles, and media\n",
    "- Writing professional documents such as articles or lessons\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [!NOTE]\n",
    "> üí° Remember that you can also run `brown` as a standalone Python package by going to `lessons/writing_workflow/` and following the instructions from there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "First, we define some standard Magic Python commands to autoreload Python packages whenever they change:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up Python Environment\n",
    "\n",
    "To set up your Python virtual environment using `uv` and load it into the Notebook, follow the step-by-step instructions from the `Course Admin` lesson from the beginning of the course.\n",
    "\n",
    "**TL/DR:** Be sure the correct kernel pointing to your `uv` virtual environment is selected.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Gemini API\n",
    "\n",
    "To configure the Gemini API, follow the step-by-step instructions in the `Course Admin` lesson.\n",
    "\n",
    "Here is a quick checklist of what you need to run this notebook:\n",
    "\n",
    "1.  Get your key from [Google AI Studio](https://aistudio.google.com/app/api-keys).\n",
    "2.  From the root of your project, run: `cp .env.example .env` \n",
    "3.  Within the `.env` file, fill in the `GOOGLE_API_KEY` variable:\n",
    "\n",
    "Now, the code below will load the key from the `.env` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment variables loaded from `/Users/pauliusztin/Documents/01_projects/TAI/agentic-ai-engineering-course/.env`\n",
      "Environment variables loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "from utils import env\n",
    "\n",
    "env.load(required_env_vars=[\"GOOGLE_API_KEY\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Key Packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "from utils import pretty_print\n",
    "\n",
    "nest_asyncio.apply()  # Allow nested async usage in notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Required Files\n",
    "\n",
    "We need to download the configuration files and input data that Brown uses for article generation and editing.\n",
    "\n",
    "First, let's download the configs folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "!rm -rf configs\n",
    "!curl -L -o configs.zip https://raw.githubusercontent.com/iusztinpaul/agentic-ai-engineering-course-data/main/data/configs.zip\n",
    "!unzip configs.zip\n",
    "!rm -rf configs.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's download the inputs folder containing profiles, examples, and test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "!rm -rf inputs\n",
    "!curl -L -o inputs.zip https://raw.githubusercontent.com/iusztinpaul/agentic-ai-engineering-course-data/main/data/inputs.zip\n",
    "!unzip inputs.zip\n",
    "!rm -rf inputs.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify what we downloaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "article_guideline.md   \u001b[1m\u001b[36minputs\u001b[m\u001b[m/                notebook_guideline.md\n",
      "\u001b[1m\u001b[36mconfigs\u001b[m\u001b[m/               notebook.ipynb\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up Directory Constants\n",
    "\n",
    "Now let's define constants to reference these directories throughout the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configs directory exists: True\n",
      "Inputs directory exists: True\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "CONFIGS_DIR = Path(\"configs\")\n",
    "INPUTS_DIR = Path(\"inputs\")\n",
    "\n",
    "print(f\"Configs directory exists: {CONFIGS_DIR.exists()}\")\n",
    "print(f\"Inputs directory exists: {INPUTS_DIR.exists()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples directory exists: True\n",
      "Profiles directory exists: True\n"
     ]
    }
   ],
   "source": [
    "EXAMPLES_DIR = Path(\"inputs/examples/course_lessons\")\n",
    "PROFILES_DIR = Path(\"inputs/profiles\")\n",
    "\n",
    "print(f\"Examples directory exists: {EXAMPLES_DIR.exists()}\")\n",
    "print(f\"Profiles directory exists: {PROFILES_DIR.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will load a simpler example that runs faster and is easier to understand. At the end, we will load a larger sample that is closer to what we do on our end to generate professional articles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples directory exists: True\n"
     ]
    }
   ],
   "source": [
    "SAMPLE_DIR = Path(\"inputs/tests/01_sample_small\")\n",
    "\n",
    "print(f\"Samples directory exists: {SAMPLE_DIR.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Brown Package\n",
    "\n",
    "Throughout this notebook, we'll be importing code from the `brown` package. This package contains all the code for the writing workflow which is located in the `lessons/writing_workflow/`. \n",
    "\n",
    "The Brown package is installed as a local Python package using `uv` and contains all the core functionality we'll explore:\n",
    "\n",
    "- **Entities** (`brown.entities`): Pydantic models for articles, guidelines, research, profiles, and media\n",
    "- **Nodes** (`brown.nodes`): Workflow components like ArticleWriter and MediaGeneratorOrchestrator\n",
    "- **Models** (`brown.models`): LLM configuration and initialization utilities\n",
    "- **Loaders** (`brown.loaders`): Classes to load markdown content into entities\n",
    "- **Builders** (`brown.builders`): Factory patterns for creating loaders, renderers, and models\n",
    "- **Renderers** (`brown.renderers`): Classes to save entities back to markdown files\n",
    "- **Config** (`brown.config`): Settings management using Pydantic\n",
    "\n",
    "Let's start by understanding how we load environment variables and configure the system.\n",
    "\n",
    "> [!NOTE]\n",
    "> üí° You can also run `brown` as a standalone Python package by going to `lessons/writing_workflow/` and following the instructions from there.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implementing Our Settings Class\n",
    "\n",
    "The Brown agent uses a centralized settings system built with Pydantic's `BaseSettings` to load all sensitive credentials, such as API Keys, from a centralized class. This ensures that all your credentials are stored within a local `.env` file or in memory, while it leverages all the type safety features that come with Pydantic.\n",
    "\n",
    "Let's examine the `Settings` class from `brown.config`:\n",
    "\n",
    "```python\n",
    "import os\n",
    "from functools import lru_cache\n",
    "from typing import Annotated\n",
    "\n",
    "from loguru import logger\n",
    "from pydantic import Field, FilePath, SecretStr\n",
    "from pydantic_settings import BaseSettings, SettingsConfigDict\n",
    "\n",
    "ENV_FILE_PATH = os.getenv(\"ENV_FILE_PATH\", \".env\")\n",
    "logger.info(f\"Loading environment file from `{ENV_FILE_PATH}`\")\n",
    "\n",
    "\n",
    "class Settings(BaseSettings):\n",
    "    model_config = SettingsConfigDict(env_file=ENV_FILE_PATH, extra=\"ignore\", env_file_encoding=\"utf-8\")\n",
    "\n",
    "    # --- Gemini ---\n",
    "    GOOGLE_API_KEY: SecretStr | None = Field(default=None, description=\"The API key for the Gemini API.\")\n",
    "\n",
    "    # --- Opik ---\n",
    "    OPIK_ENABLED: bool = Field(default=False, description=\"Whether to use Opik for monitoring and logging.\")\n",
    "    OPIK_WORKSPACE: str | None = Field(default=None, description=\"Name of the Opik workspace containing the project.\")\n",
    "    OPIK_PROJECT_NAME: str = Field(default=\"brown\", description=\"Name of the Opik project.\")\n",
    "    OPIK_API_KEY: SecretStr | None = Field(default=None, description=\"The API key for the Opik API.\")\n",
    "\n",
    "    # --- App Config ---\n",
    "    CONFIG_FILE: Annotated[FilePath, Field(default=\"configs/course.yaml\", description=\"Path to the application configuration YAML file.\")]\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=1)\n",
    "def get_settings() -> Settings:\n",
    "    return Settings()\n",
    "```\n",
    "\n",
    "When the `Settings` class is instantiated, it first looks for environment variables in memory, then in the `.env` file. Thus, while working with these Notebooks, it's enough to have them only inside the memory, which is taken care of by the `env.load` function from the setup section.\n",
    "\n",
    "But if you run the Notebook locally, you can also keep them inside a local `.env` file. The choice is yours. \n",
    "\n",
    "**Key Design Patterns:**\n",
    "\n",
    "1. **Singleton Pattern**: The `@lru_cache(maxsize=1)` decorator ensures we only instantiate `Settings` once, making it behave like a singleton throughout the application.\n",
    "\n",
    "2. **Type Safety**: Using `SecretStr` for sensitive data like API keys ensures they're handled securely and not accidentally logged.\n",
    "\n",
    "3. **Environment Integration**: `SettingsConfigDict` automatically loads values from the `.env` file, with sensible defaults for missing values.\n",
    "\n",
    "4. **Flexible Configuration**: The `CONFIG_FILE` field points to a YAML configuration that controls workflow behavior, model selection, and more. More on this in Lesson 23."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Scoping the Writing Workflow Architecture\n",
    "\n",
    "Before diving into the implementation details, let's understand the three-step workflow that powers the Brown writing agent.\n",
    "\n",
    "### The Three-Step Process\n",
    "\n",
    "**Step 1: Load Context into Memory**\n",
    "\n",
    "The first step involves gathering all the necessary context that will guide the article generation process:\n",
    "\n",
    "- **Article Guideline**: The user's input describing what the article should contain, its outline, and any specific requirements\n",
    "- **Research**: Factual data, references, and information that will support the article's claims\n",
    "- **Few-Shot Examples**: Sample articles demonstrating the desired writing style and structure (your training set)\n",
    "- **Content Generation Profiles**: Specialized profiles that control different aspects of the writing:\n",
    "  - Character profile (voice and perspective)\n",
    "  - Article profile (article-specific rules)\n",
    "  - Structure profile (formatting rules)\n",
    "  - Mechanics profile (writing mechanics)\n",
    "  - Terminology profile (word choice)\n",
    "  - Tonality profile (tone and style)\n",
    "\n",
    "**Step 2: Generate Media Items (Orchestrator-Worker Pattern)**\n",
    "\n",
    "Once we have the context, based on the article guideline, we need to generate any required media assets like diagrams or charts. This step uses the orchestrator-worker pattern:\n",
    "\n",
    "- The **MediaGeneratorOrchestrator** analyzes the article guideline and research to identify what media items need to be generated\n",
    "- For each identified requirement, the orchestrator delegates to specialized **worker tools** (e.g., `MermaidDiagramGenerator`)\n",
    "- Workers generate their specialized content type in parallel\n",
    "- All generated media items are collected and prepared for integration into the article\n",
    "\n",
    "**Step 3: Write the Article**\n",
    "\n",
    "With all context and media items ready, the **ArticleWriter** node generates the final article:\n",
    "\n",
    "- Takes all the context from Step 1\n",
    "- Integrates the media items from Step 2\n",
    "- Follows all profile rules to generate content matching the desired style\n",
    "- Produces a complete, high-quality article\n",
    "\n",
    "Let's visualize this workflow with a diagram:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/iusztinpaul/agentic-ai-engineering-course-data/main/images/l22_writing_workflow.png\" alt=\"Article Generation Workflow\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we understand the high-level workflow, let's dive into the implementation details of each component, starting with how we structure and load the context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Modeling our Domain Entities as Context for LLMs\n",
    "\n",
    "Now that we understand the high-level workflow, let's explore how we model and load the various context components. \n",
    "\n",
    "The Brown writing workflow uses Pydantic models (entities) to represent different types of content, and specialized loaders to read this content from disk. \n",
    "\n",
    "Everything within the domain layer, such as the article guideline, research, writing profiles or few-shot examples, is modeled as an entity. Like this, we can very easily attach functionality to each type, structure it and add data validation. As we presented in the project structure lesson, these entities will be passed all over the AI application. They will be used as inputs and outputs to all our business logic. They will be the only way to pass information between functions, classes and between LLMs (software 3.0) and our Python code (software 1.0).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The ContextMixin: Foundation for Context Engineering\n",
    "\n",
    "Before we dive into specific entities, let's understand the `ContextMixin` - a crucial abstraction that enables powerful context engineering throughout the system.\n",
    "\n",
    "The `ContextMixin` provides a standardized way to convert any entity into a context representation surrounded by XML tags. This is essential for:\n",
    "\n",
    "1. **Clear Boundaries**: XML tags clearly delineate different pieces of context in prompts\n",
    "2. **Structured Prompts**: LLMs can easily parse and understand structured XML context\n",
    "3. **Consistent Interface**: All entities follow the same pattern for converting Python objects to LLM input context\n",
    "4. **Easy Composition**: Different context elements can be seamlessly combined\n",
    "\n",
    "Here's the implementation from `brown.entities.mixins`:\n",
    "\n",
    "```python\n",
    "from abc import abstractmethod\n",
    "from brown.utils.s import camel_to_snake\n",
    "\n",
    "\n",
    "class ContextMixin:\n",
    "    @property\n",
    "    def xml_tag(self) -> str:\n",
    "        return camel_to_snake(self.__class__.__name__)\n",
    "\n",
    "    @abstractmethod\n",
    "    def to_context(self) -> str:\n",
    "        \"\"\"Context representation of the object.\"\"\"\n",
    "        pass\n",
    "```\n",
    "\n",
    "**Key Features:**\n",
    "\n",
    "- **Automatic Tag Generation**: The `xml_tag` property automatically converts the class name to snake_case (e.g., `ArticleGuideline` ‚Üí `article_guideline`)\n",
    "- **Abstract Method**: `to_context()` must be implemented by each entity, ensuring consistency\n",
    "- **Simplicity**: The `ContextMixin` is a simple interface, that standardizes how we map Python objects to LLM context inputs\n",
    "\n",
    "For example, an `ArticleGuideline` entity will be wrapped in `<article_guideline>...</article_guideline>` tags when passed to the LLM.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Article Guideline Entity\n",
    "\n",
    "The `ArticleGuideline` represents the user's input - what they want the article to contain, how it should be structured, and any specific requirements. It's the primary driver of content generation that is different for each article.\n",
    "\n",
    "From `brown.entities.guidelines`:\n",
    "```python\n",
    "from pydantic import BaseModel\n",
    "from brown.entities.mixins import ContextMixin\n",
    "\n",
    "\n",
    "class ArticleGuideline(BaseModel, ContextMixin):\n",
    "    content: str\n",
    "\n",
    "    def to_context(self) -> str:\n",
    "        return f\"\"\"\n",
    "<{self.xml_tag}>\n",
    "    <content>{self.content}</content>\n",
    "</{self.xml_tag}>\n",
    "\"\"\"\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f\"ArticleGuideline(len_content={len(self.content)})\"\n",
    "```\n",
    "\n",
    "‚ö†Ô∏è Note how we implemented the `to_context()` method.\n",
    "\n",
    "The guideline typically contains:\n",
    "- Article outline and section structure\n",
    "- Specific instructions for each section\n",
    "- Length constraints or requirements\n",
    "- Important references to prioritize\n",
    "- Any special formatting needs\n",
    "\n",
    "Let's load the sample article guideline:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-11-29 17:55:09.155\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mbrown.config\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m10\u001b[0m - \u001b[1mLoading environment file from `.env`\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from brown.loaders import MarkdownArticleGuidelineLoader\n",
    "\n",
    "guideline_loader = MarkdownArticleGuidelineLoader(uri=Path(\"article_guideline.md\"))\n",
    "article_guideline = guideline_loader.load(working_uri=SAMPLE_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will show only the first 1200 characters to see how the enclosing XML tags logic works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m-------------------------------------------------------- Article Guideline (First 1200 Chars) --------------------------------------------------------\u001b[0m\n",
      "  ## Outline\n",
      "\n",
      "1. Introduction: The Critical Decision Every AI Engineer Faces\n",
      "2. Understanding the Spectrum: From Workflows to Agents\n",
      "3. Choosing Your Path\n",
      "4. Conclusion: The Challenges of Every AI Engineer\n",
      "\n",
      "## Section 1 - Introduction: The Critical Decision Every AI Engineer Faces\n",
      "\n",
      "- **The Problem:** When building AI applications, engineers face a critical architectural decision early in their development process. Should they create a predictable, step-by-step workflow where they control every action, or should they build an autonomous agent that can think and decide for itself? This is one of the key decisions that will impact everything from the product such as development time and costs to reliability and user experience.\n",
      "- Quick walkthrough of what we'll learn by the end of this lesson\n",
      "\n",
      "- **Section length:** 100 words\n",
      "\n",
      "## Section 2 - Understanding the Spectrum: From Workflows to Agents\n",
      "\n",
      "- In this section we want to take a brief look at what LLM workflows and AI agents are. At this point we don't focus on the technical specifics of each, but rather on their properties and how they are used.\n",
      "- On **LLM workflows** we care about:\n",
      "\t- Definition: A sequence of tasks involving LLM call...\n",
      "\u001b[93m------------------------------------------------------------------------------------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "cropped_article_guideline = article_guideline.model_copy()\n",
    "cropped_article_guideline.content = f\"{cropped_article_guideline.content[:1200]}...\"\n",
    "\n",
    "pretty_print.wrapped(f\"{cropped_article_guideline.content}\", title=\"Article Guideline (First 1200 Chars)\", width=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also call the `to_context()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m----------------------------------- Article Guideline as Context -----------------------------------\u001b[0m\n",
      "  \n",
      "<article_guideline>\n",
      "    <content>## Outline\n",
      "\n",
      "1. Introduction: The Critical Decision Every AI Engineer Faces\n",
      "2. Understanding the Spectrum: From Workflows to Agents\n",
      "3. Choosing Your Path\n",
      "4. Conclusion: The Challenges of Every AI Engineer\n",
      "\n",
      "## Section 1 - Introduction: The Critical Decision Every AI Engineer Faces\n",
      "\n",
      "- **The Problem:** When building AI applications, engineers face a critical architectural decision early in their development process. Should they create a predictable, step-by-step workflow where they control every action, or should they build an autonomous agent that can think and decide for itself? This is one of the key decisions that will impact everything from the product such as development time and costs to reliability and user experience.\n",
      "- Quick walkthrough of what we'll learn by the end of this lesson\n",
      "\n",
      "- **Section length:** 100 words\n",
      "\n",
      "## Section 2 - Understanding the Spectrum: From Workflows to Agents\n",
      "\n",
      "- In this section we want to take a brief look at what LLM workflows and AI agents are. At this point we don't focus on the technical specifics of each, but rather on their properties and how they are used.\n",
      "- On **LLM workflows** we care about:\n",
      "\t- Definition: A sequence of tasks involving LLM call...</content>\n",
      "</article_guideline>\n",
      "\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "pretty_print.wrapped(f\"{cropped_article_guideline.to_context()}\", title=\"Article Guideline as Context\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the whole content is surrounded by the `<article_gudeline>...</article_guideline>` XML tags, while the content attribute is surrounded by the `<content>...</content>` XML tag.\n",
    "\n",
    "We could always pass the `content` field directly as a plain string. If you want to add only one or two entities within the context, that would have worked. But in our use case, we will model up to 10 different context elements, each with its own attributes. Thus, having a clear way to tell the LLM how to differentiate between them, and how to reference them when writing instructions it's an essential skill for context engineering. \n",
    "\n",
    "The `to_context()` method maps the Pydantic model to its XML representation, where the root XML tag is inferred from the entity class name, and the fields use the attribute's name directly. Like this, when we pass these entities to an LLM, it can clearly reason what is what, such as separating between different entities and different attributes within an entity.\n",
    "\n",
    "Now, whenever we write reasoning instructions to the LLM, we can just say \"Do X based on <article_gudeline> ...\" and it will know to reference the information from these particular XML tags.\n",
    "\n",
    "We chose XML over JSON because it's easier to read and less verbose, which, on average, translates to fewer input tokens while still being easy for humans to read."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Research Entity\n",
    "\n",
    "The `Research` entity loads the research file generated by the Nova deep research agent that contains factual data, references, and information that supports the article's claims. It also extracts and validates image URLs from the research file.\n",
    "\n",
    "From `brown.entities.research`:\n",
    "\n",
    "```python\n",
    "import re\n",
    "from functools import cached_property\n",
    "\n",
    "from loguru import logger\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from brown.entities.mixins import ContextMixin\n",
    "from brown.utils.a import asyncio_run, run_jobs\n",
    "from brown.utils.network import is_image_url_valid\n",
    "\n",
    "\n",
    "class Research(BaseModel, ContextMixin):\n",
    "    content: str\n",
    "    max_image_urls: int = 30\n",
    "\n",
    "    @cached_property\n",
    "    def image_urls(self) -> list[str]:\n",
    "        # Extract image URLs using regex\n",
    "        image_urls = re.findall(\n",
    "            r\"(?!data:image/)https?://[^\\s]+\\.(?:jpg|jpeg|png|bmp|webp)\",\n",
    "            self.content,\n",
    "            re.IGNORECASE,\n",
    "        )\n",
    "        # Validate URLs asynchronously by pinging them.\n",
    "        jobs = [is_image_url_valid(url) for url in image_urls]\n",
    "        results = asyncio_run(run_jobs(jobs))\n",
    "\n",
    "        urls = [url for url, valid in zip(image_urls, results) if valid]\n",
    "        if len(urls) > self.max_image_urls:\n",
    "            logger.warning(f\"Found `{len(urls)} > {self.max_image_urls}` image URLs. Trimming to first {self.max_image_urls}.\")\n",
    "            urls = urls[: self.max_image_urls]\n",
    "\n",
    "        return urls\n",
    "\n",
    "    def to_context(self) -> str:\n",
    "        return f\"\"\"\n",
    "<{self.xml_tag}>\n",
    "    {self.content}\n",
    "</{self.xml_tag}>\n",
    "\"\"\"\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f\"Research(len_content={len(self.content)}, len_image_urls={len(self.image_urls)})\"\n",
    "```\n",
    "\n",
    "**Advanced Features:**\n",
    "\n",
    "1. **Image URL Extraction**: Automatically finds image URLs in the research content to manipulate them within the LLM context\n",
    "2. **URL Validation**: Asynchronously validates that image URLs are accessible by pinging them\n",
    "3. **Caching**: Uses `@cached_property` to avoid re-extracting URLs\n",
    "4. **Safety Limits**: Caps the number of images to prevent context overflow\n",
    "\n",
    "The extracted image URLs can be passed to multimodal models (like Gemini) along with text prompts for richer context.\n",
    "\n",
    "Let's load the research data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from brown.loaders import MarkdownResearchLoader\n",
    "\n",
    "research_loader = MarkdownResearchLoader(uri=Path(\"research.md\"))\n",
    "research = research_loader.load(working_uri=SAMPLE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before let's load just the first 1500 characters to see everything within the Notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m----------------------------------- Research (First 1200 Chars) -----------------------------------\u001b[0m\n",
      "  # Research\n",
      "\n",
      "## Code Sources\n",
      "\n",
      "<details>\n",
      "<summary>Repository analysis for https://github.com/google-gemini/gemini-cli/blob/main/README.md</summary>\n",
      "\n",
      "# Repository analysis for https://github.com/google-gemini/gemini-cli/blob/main/README.md\n",
      "\n",
      "## Summary\n",
      "Repository: google-gemini/gemini-cli\n",
      "File: README.md\n",
      "Lines: 211\n",
      "\n",
      "Estimated tokens: 1.6k\n",
      "\n",
      "## File tree\n",
      "```Directory structure:\n",
      "‚îî‚îÄ‚îÄ README.md\n",
      "\n",
      "```\n",
      "\n",
      "## Extracted content\n",
      "================================================\n",
      "FILE: README.md\n",
      "================================================\n",
      "# Gemini CLI\n",
      "\n",
      " https://github.com/google-gemini/gemini-cli/actions/workflows/ci.yml/badge.svg ](https://github.com/google-gemini/gemini-cli/actions/workflows/ci.yml)\n",
      "\n",
      " ./docs/assets/gemini-screenshot.png \n",
      "\n",
      "This repository contains the Gemini CLI, a command-line AI workflow tool that connects to your\n",
      "tools, understands your code and accelerates your workflows.\n",
      "\n",
      "With the Gemini CLI you can:\n",
      "\n",
      "- Query and edit large codebases in and beyond Gemini's 1M token context window.\n",
      "- Generate new apps from PDFs or sketches, using Gemini's multimodal capabilities.\n",
      "- Automate operational tasks, like querying pull requests or handling complex rebases.\n",
      "- Use tools and MCP server...\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "cropped_research = research.model_copy()\n",
    "cropped_research.content = f\"{cropped_research.content[:1200]}...\"\n",
    "\n",
    "pretty_print.wrapped(f\"{cropped_research.content}\", title=\"Research (First 1200 Chars)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m--------------------------------------------- Research ---------------------------------------------\u001b[0m\n",
      "  \n",
      "<research>\n",
      "    # Research\n",
      "\n",
      "## Code Sources\n",
      "\n",
      "<details>\n",
      "<summary>Repository analysis for https://github.com/google-gemini/gemini-cli/blob/main/README.md</summary>\n",
      "\n",
      "# Repository analysis for https://github.com/google-gemini/gemini-cli/blob/main/README.md\n",
      "\n",
      "## Summary\n",
      "Repository: google-gemini/gemini-cli\n",
      "File: README.md\n",
      "Lines: 211\n",
      "\n",
      "Estimated tokens: 1.6k\n",
      "\n",
      "## File tree\n",
      "```Directory structure:\n",
      "‚îî‚îÄ‚îÄ README.md\n",
      "\n",
      "```\n",
      "\n",
      "## Extracted content\n",
      "================================================\n",
      "FILE: README.md\n",
      "================================================\n",
      "# Gemini CLI\n",
      "\n",
      " https://github.com/google-gemini/gemini-cli/actions/workflows/ci.yml/badge.svg ](https://github.com/google-gemini/gemini-cli/actions/workflows/ci.yml)\n",
      "\n",
      " ./docs/assets/gemini-screenshot.png \n",
      "\n",
      "This repository contains the Gemini CLI, a command-line AI workflow tool that connects to your\n",
      "tools, understands your code and accelerates your workflows.\n",
      "\n",
      "With the Gemini CLI you can:\n",
      "\n",
      "- Query and edit large codebases in and beyond Gemini's 1M token context window.\n",
      "- Generate new apps from PDFs or sketches, using Gemini's multimodal capabilities.\n",
      "- Automate operational tasks, like querying pull requests or handling complex rebases.\n",
      "- Use tools and MCP server...\n",
      "</research>\n",
      "\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "pretty_print.wrapped(f\"{cropped_research.to_context()}\", title=\"Research\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we can see how the Research object it's mapped to it's content surrounded by the `<research>...</research>` XML tags making sure that whenever we want to pass this object to an LLM we respect this rule.\n",
    "\n",
    "As an important gotcha, we don't have to be too rigid about how we translate these objects to XML. For example, here we completely omitted the `<content>` field. As there is just one field, encoding the attribute name is redundant. The most important element is the root `<research>...</research>` XML tag. \n",
    "\n",
    "We intentionally implemented both options to highlight this method's flexibility. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Article Examples Entity\n",
    "\n",
    "The `ArticleExamples` entity contains few-shot examples that demonstrate the desired writing style, structure, and quality. These serve as templates for the LLM to understand what kind of output is expected. Intuitively, these can be seen as the on-demand training set that specializes the LLM on our concrete task.\n",
    "\n",
    "From `brown.entities.articles`:\n",
    "\n",
    "```python\n",
    "from pydantic import BaseModel\n",
    "from brown.entities.mixins import ContextMixin\n",
    "\n",
    "\n",
    "class ArticleExample(BaseModel, ContextMixin):\n",
    "    content: str\n",
    "\n",
    "    def to_context(self) -> str:\n",
    "        return f\"\"\"\n",
    "<{self.xml_tag}>\n",
    "    {self.content}\n",
    "</{self.xml_tag}>\n",
    "\"\"\"\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f\"ArticleExample(len_content={len(self.content)})\"\n",
    "\n",
    "\n",
    "class ArticleExamples(BaseModel, ContextMixin):\n",
    "    examples: list[ArticleExample]\n",
    "\n",
    "    def to_context(self) -> str:\n",
    "        return f\"\"\"\n",
    "<{self.xml_tag}>\n",
    "        {\"\\n\".join([example.to_context() for example in self.examples])}\n",
    "</{self.xml_tag}>\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "**Composition Pattern:**\n",
    "\n",
    "- `ArticleExample`: Represents a single example article\n",
    "- `ArticleExamples`: Contains multiple examples and composes their context representations\n",
    "- When converted to context, all examples are nested within the parent XML tags\n",
    "\n",
    "This pattern allows us to provide multiple examples to the LLM, showing consistency across different articles.\n",
    "\n",
    "Let's load the few-shot examples:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m--------------------------------------- First Article Example (First 500 Chars) ---------------------------------------\u001b[0m\n",
      "  ArticleExample(len_content=503)\n",
      "\u001b[93m------------------------------------------------------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from brown.loaders import MarkdownArticleExampleLoader\n",
    "\n",
    "examples_loader = MarkdownArticleExampleLoader(uri=EXAMPLES_DIR)\n",
    "article_examples = examples_loader.load()\n",
    "\n",
    "cropped_article_examples = article_examples.model_copy()\n",
    "for example in cropped_article_examples.examples:\n",
    "    example.content = f\"{example.content[:500]}...\"\n",
    "\n",
    "pretty_print.wrapped(f\"{article_examples.examples[0]}\", title=\"First Article Example (First 500 Chars)\", width=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also call the `to_context()` method on this nested structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m------------------------------------ Article Examples as Context (First 500 Chars) ------------------------------------\u001b[0m\n",
      "  \n",
      "<article_examples>\n",
      "        \n",
      "<article_example>\n",
      "    # Lesson 3: Context Engineering\n",
      "\n",
      "AI applications have evolved rapidly. In 2022, we had simple chatbots for question-answering. By 2023, Retrieval-Augmented Generation (RAG) systems connected LLMs to domain-specific knowledge. 2024 brought us tool-using agents that could perform actions. Now, we are building memory-enabled agents that remember past interactions and build relationships over time.\n",
      "\n",
      "In our last lesson, we explored how to choose between AI agents and LLM workflows when designing a sy...\n",
      "</article_example>\n",
      "\n",
      "\n",
      "<article_example>\n",
      "    # Lesson 4: Structured Outputs\n",
      "\n",
      "In our previous lessons, we laid the groundwork for AI Engineering. We explored the AI agent landscape, looked at the difference between rule-based LLM workflows and autonomous AI agents, and covered context engineering: the art of feeding the right information to an LLM. In this lesson, we will tackle a fundamental challenge: getting structured and reliable information¬†*out*¬†of an LLM.\n",
      "\n",
      "```mermaid\n",
      "flowchart LR\n",
      "    subgraph \"Software 3.0\"\n",
      "        A((\"LLMs\"))\n",
      "    e...\n",
      "</article_example>\n",
      "\n",
      "</article_examples>\n",
      "\n",
      "\u001b[93m------------------------------------------------------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "pretty_print.wrapped(\n",
    "    f\"{cropped_article_examples.to_context()}\", title=\"Article Examples as Context (First 500 Chars)\", width=120\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how we represented a list of objects with XML tags, using the following nested structured:\n",
    "```\n",
    "<article_examples>\n",
    "    <article_example>\n",
    "    ...\n",
    "    </article_example>\n",
    "    <article_example>\n",
    "    ...\n",
    "    </article_example>\n",
    "</article_examples>\n",
    "```\n",
    "Like this we can represent any list of objects when transforming them from Pydantic entities to LLM context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Profiles Entity\n",
    "\n",
    "Profiles are the secret sauce of the Brown agent. They provide detailed instructions that control different aspects of how we expect the article to look and sound like.\n",
    "\n",
    "First, let's look at how we structured them as Pydantic classes, and then we will explain in more detail the role of each one.\n",
    "\n",
    "From `brown.entities.profiles`:\n",
    "\n",
    "```python\n",
    "from pydantic import BaseModel\n",
    "from brown.entities.mixins import ContextMixin\n",
    "\n",
    "\n",
    "class Profile(BaseModel, ContextMixin):\n",
    "    name: str\n",
    "    content: str\n",
    "\n",
    "    def to_context(self) -> str:\n",
    "        return f\"\"\"\n",
    "<{self.xml_tag}>\n",
    "    <name>{self.name}</name>\n",
    "    <content>{self.content}</content>\n",
    "</{self.xml_tag}>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class CharacterProfile(Profile):\n",
    "    pass\n",
    "\n",
    "\n",
    "class ArticleProfile(Profile):\n",
    "    pass\n",
    "\n",
    "\n",
    "class StructureProfile(Profile):\n",
    "    pass\n",
    "\n",
    "\n",
    "class MechanicsProfile(Profile):\n",
    "    pass\n",
    "\n",
    "\n",
    "class TerminologyProfile(Profile):\n",
    "    pass\n",
    "\n",
    "\n",
    "class TonalityProfile(Profile):\n",
    "    pass\n",
    "\n",
    "\n",
    "class ArticleProfiles(BaseModel):\n",
    "    character: CharacterProfile\n",
    "    article: ArticleProfile\n",
    "    structure: StructureProfile\n",
    "    mechanics: MechanicsProfile\n",
    "    terminology: TerminologyProfile\n",
    "    tonality: TonalityProfile\n",
    "```\n",
    "\n",
    "**Profile Hierarchy:**\n",
    "\n",
    "1. **Base `Profile` Class**: Provides common structure (name + content) and context conversion\n",
    "2. **Specialized Profile Classes**: Inherit from `Profile` for specialized context elements\n",
    "3. **`ArticleProfiles` Container**: Holds all six profile types together\n",
    "\n",
    "**Why Separate Profile Classes?**\n",
    "\n",
    "Even though they have the same structure, separate classes provide:\n",
    "- Type safety (can't accidentally swap profile types)\n",
    "- Context clarity (each profile will have its own XML tag when added as context)\n",
    "\n",
    "Let's load all the profiles:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m---------------------------------- Profile Sizes (in characters) ----------------------------------\u001b[0m\n",
      "  {\n",
      "  \"Character Profile\": 3033,\n",
      "  \"Article Profile\": 13074,\n",
      "  \"Structure Profile\": 22660,\n",
      "  \"Mechanics Profile\": 4747,\n",
      "  \"Terminology Profile\": 10730,\n",
      "  \"Tonality Profile\": 4192,\n",
      "  \"Total\": 58436\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from brown.loaders import MarkdownArticleProfilesLoader\n",
    "\n",
    "profiles_input = {\n",
    "    \"article\": PROFILES_DIR / \"article_profile.md\",\n",
    "    \"character\": PROFILES_DIR / \"character_profiles\" / \"paul_iusztin.md\",\n",
    "    \"mechanics\": PROFILES_DIR / \"mechanics_profile.md\",\n",
    "    \"structure\": PROFILES_DIR / \"structure_profile.md\",\n",
    "    \"terminology\": PROFILES_DIR / \"terminology_profile.md\",\n",
    "    \"tonality\": PROFILES_DIR / \"tonality_profile.md\",\n",
    "}\n",
    "\n",
    "profiles_loader = MarkdownArticleProfilesLoader(uri=profiles_input)\n",
    "article_profiles = profiles_loader.load()\n",
    "\n",
    "profile_sizes = {\n",
    "    \"Character Profile\": len(article_profiles.character.content),\n",
    "    \"Article Profile\": len(article_profiles.article.content),\n",
    "    \"Structure Profile\": len(article_profiles.structure.content),\n",
    "    \"Mechanics Profile\": len(article_profiles.mechanics.content),\n",
    "    \"Terminology Profile\": len(article_profiles.terminology.content),\n",
    "    \"Tonality Profile\": len(article_profiles.tonality.content),\n",
    "}\n",
    "profile_sizes[\"Total\"] = sum(profile_sizes.values())\n",
    "pretty_print.wrapped(\n",
    "    profile_sizes,\n",
    "    title=\"Profile Sizes (in characters)\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's take a look at the article character profile:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m------------------------------------------------------------------- Article Profile (First 400 Chars) -------------------------------------------------------------------\u001b[0m\n",
      "  ## Tonality\n",
      "\n",
      "You should write in a humanized way as writing a blog article or book.\n",
      "\n",
      "Write the description of ideas as fluid as possible. Remember that you are writing a book or blog article. Thus, everything should flow naturally, without too many bullet points or subheaders. Use them only when it really makes sense. Otherwise, stick to normal paragraphs.\n",
      "\n",
      "## General Article Structure\n",
      "\n",
      "The articl...\n",
      "\u001b[93m--------------------------------------------------------------------------------------------------------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "article_profile_copy = article_profiles.article.model_copy()\n",
    "article_profile_copy.content = f\"{article_profile_copy.content[:400]}...\"\n",
    "\n",
    "pretty_print.wrapped(f\"{article_profile_copy.content}\", title=\"Article Profile (First 400 Chars)\", width=170)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's call the `to_context()` method. We will show only the first 500 characters to see how the enclosing XML tags logic works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m--------------------------- Article Profile as Context (First 400 Chars) ---------------------------\u001b[0m\n",
      "  \n",
      "<article_profile>\n",
      "    <name>article</name>\n",
      "    <content>## Tonality\n",
      "\n",
      "You should write in a humanized way as writing a blog article or book.\n",
      "\n",
      "Write the description of ideas as fluid as possible. Remember that you are writing a book or blog article. Thus, everything should flow naturally, without too many bullet points or subheaders. Use them only when it really makes sense. Otherwise, stick to normal paragraphs.\n",
      "\n",
      "## General Article Structure\n",
      "\n",
      "The articl...</content>\n",
      "</article_profile>\n",
      "\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "pretty_print.wrapped(f\"{article_profile_copy.to_context()}\", title=\"Article Profile as Context (First 400 Chars)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if we call the `to_context()` method on a different profile we will see it picked on its own XML tag, reason why it was important to create different classes for each:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m-------------------------- Structure Profile as Context (First 400 Chars) --------------------------\u001b[0m\n",
      "  \n",
      "<structure_profile>\n",
      "    <name>structure</name>\n",
      "    <content>## Sentence and paragraph length patterns\n",
      "\n",
      "Write sentences 5‚Äì25 words; allow occasional 30-word 'story' sentences. Keep paragraphs ‚â§ 80 words; allow an occasional 1-sentence paragraph to emphasize a point.\n",
      "\n",
      "- Good examples:\n",
      "  - four 18-word sentence, as a paragraph of 72 words.\n",
      "  - Ocassional 1-sentece paragraph.\n",
      "- Bad examples:\n",
      "  - Frequent 40-word run-ons.\n",
      "  - five 18-word sentence, as a paragra...</content>\n",
      "</structure_profile>\n",
      "\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "structure_profile_copy = article_profiles.structure.model_copy()\n",
    "structure_profile_copy.content = f\"{structure_profile_copy.content[:400]}...\"\n",
    "\n",
    "pretty_print.wrapped(f\"{structure_profile_copy.to_context()}\", title=\"Structure Profile as Context (First 400 Chars)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how we represent the `name` and `content` attributes under the `<name>...</name>` and `<content>...</content>` child XML tags, under the `<structure_profile>...</structure_profile>` parent XML tag."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Understanding the Role of the Profiles\n",
    "\n",
    "Now that we've loaded all the profiles, let's understand what each one does and why it's essential for generating high-quality content. Profiles are the key to making the Brown writing workflow produce consistent, high-quality output that matches your desired style and voice.\n",
    "\n",
    "We have four general profiles: mechanics, structure, terminology, and tonality. Then, we have two specific profiles that cover: article and character\n",
    "\n",
    "The general ones are used to instruct the LLM on general rules of thumb for what a professional piece of content should look like, such as defining the voice, setting sentence length, and avoiding AI slop. At the same time, the specific ones are used to configure the LLM to produce our ideal article and provide a unique kick to the writing process.\n",
    "\n",
    "As shown in image below, you can replace the examples, article, or character profile with your own to begin generating other article formats and different content types, such as video transcripts or social media posts, and add your own personality to the writing process. \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/iusztinpaul/agentic-ai-engineering-course-data/main/images/l22_writing_workflow_profiles.png\" alt=\"Profiles\"/>\n",
    "\n",
    "Keeping things modular in separate Markdown files, even at the context-engineering level, makes it much easier to grow this into a full-fledged, scalable product. For example, if you introduce the concept of a user, each user can configure their own character profile, which is dynamically added to the workflow at the beginning without altering any other aspect of the implementation.\n",
    "\n",
    "> [!NOTE]\n",
    "> üí° As each profile has hundreds of lines, we won‚Äôt be able to show them one-on-one within the lesson. We will only show relevant pieces from each, but we encourage you to open each file and skim through it.\n",
    "\n",
    "\n",
    "Let's go over every 6 profiles:\n",
    "\n",
    "1. **Mechanics Profile (General)**\n",
    "    \n",
    "    This profile governs the technical aspects of writing, encompassing word choice and paragraph structure. It ensures the text is readable, consistent, and grammatically aligned with our standards. \n",
    "    \n",
    "    Some concrete examples:\n",
    "    \n",
    "    - **Active vs. passive voice usage:**¬†‚ÄúAlways strive for an active voice.‚Äù\n",
    "    - **Point of View:**¬†‚ÄúThe piece is created by a team writing for a single reader... always use ‚Äòwe,‚Äô ‚Äòour,‚Äô and ‚Äòus‚Äô to refer to the team... and ‚Äòyou‚Äô or ‚Äòyour‚Äô to address the reader.‚Äù\n",
    "    - **Punctuation preferences:**¬†‚ÄúAvoid semicolons ‚Äò;‚Äô or em-dashes ‚Äò‚Äî‚Äô to add phauses... Instead, split it into two sentences.‚Äù\n",
    "    \n",
    "    See the complete profile at¬†`inputs/profiles/mechanics_profile.md`\n",
    "    \n",
    "2. **Structure Profile (General)**\n",
    "    \n",
    "    This profile defines how content is visually and logically organized to ensure readability. It governs the hierarchy of headers, the formatting of lists and code, and the placement of media.\n",
    "    \n",
    "    Some concrete examples:\n",
    "    \n",
    "    - **Sentence and paragraph length patterns:**¬†‚ÄúWrite sentences 5‚Äì25 words... Keep paragraphs ‚â§ 80 words.‚Äù\n",
    "    - **Sections Sub-Heading Formatting:**¬†‚ÄúUse at maximum H3... sub-headers... avoid using H4/H5/H6 sub-headings at all costs.‚Äù\n",
    "    - **Formatting Media:**¬†‚ÄúWe handle three types of media: Tables... Diagrams... Images... All the media items have a unique identifier... and a small description.‚Äù\n",
    "    - **Formatting Code:**¬†‚ÄúAvoid describing big chunks of code that go over 35 lines... split the code into logical groups.‚Äù\n",
    "    \n",
    "    See the complete profile at¬†`inputs/profiles/structure_profile.md`\n",
    "    \n",
    "3. **Terminology Profile (General)**\n",
    "    \n",
    "    This profile guides word choice to ensure we speak the reader‚Äôs language without sounding robotic. It guides the choice of words when transitioning between sentences or paragraphs. Also, it strictly enforces a ban on ‚ÄúAI Slop‚Äù, the fluffy, distinctive style often generated by default LLM outputs.\n",
    "    \n",
    "    Some concrete examples:\n",
    "    \n",
    "    - **Word Choice Patterns:**¬†‚ÄúAvoid using complex words... Use a casual and direct vocabulary... Use a concrete, hands-on language.‚Äù\n",
    "    - **Descriptive Language Patterns:**¬†‚ÄúBe excited and personal about positive outcomes... Be realistic, pragmatic, and resilient about negative outcomes.‚Äù\n",
    "    - **AI Slop Banned Expressions List:**¬†Avoid words like ‚Äúdelve‚Äù, ‚Äúparamount‚Äù, ‚Äúthrive‚Äù, ‚Äúrealm‚Äù, ‚Äúdive deep into‚Äù, ‚Äúunlock‚Äù, ‚Äúunleash‚Äù, ‚Äúgame-changer‚Äù.\n",
    "    \n",
    "    See the complete profile at¬†`inputs/profiles/terminology_profile.md`\n",
    "    \n",
    "4. **Tonality Profile (General)**\n",
    "    \n",
    "    This profile sets the emotional resonance and personality of the text. It ensures the writing feels both human and approachable, rather than corporate or purely academic. It guides the level of difficulty and emotion of each word.\n",
    "    \n",
    "    Some concrete examples:\n",
    "    \n",
    "    - **Primary voice characteristics:**¬†‚Äúhuman, technical, informative, casual, friendly, confident, direct, professional, concise.‚Äù\n",
    "    - **Formality level:**¬†‚Äú7/10 - As we write technical professional content, keep it somewhat formal, but NOT too formal.‚Äù\n",
    "    - **On-Brand Tones (Desired):**¬†‚ÄúJoyful, Excited... Friendly, Sincere... Diplomatic, Empathetic... Direct, Assertive.‚Äù\n",
    "    - **Off-Brand Tones (Undesired):**¬†‚ÄúDissatisfied, Dismissive... Disapproving, Accusatory... Overconfident... Informal.‚Äù\n",
    "    \n",
    "    See the complete profile at `inputs/profiles/tonality_profile.md`\n",
    "    \n",
    "5. **Article Profile (Specific)**\n",
    "    \n",
    "    This profile contains rules specific to the article format, ensuring a consistent narrative flow. It dictates how to structure the article, transition between sections, and handle citations.\n",
    "    \n",
    "    Some concrete examples:\n",
    "    \n",
    "    - **General Article Structure:**¬†‚ÄúThe article is a collection of blocks that flow naturally... It starts with one introduction, continues with multiple sections... and wraps up with a conclusion.‚Äù\n",
    "    - **Introduction, Section, Conclusion Guidelines:**¬†‚ÄúIntroduction: short summary... presenting the ‚Äòwhy‚Äô and ‚Äòwhat‚Äô... Sections: present the ‚Äòhow‚Äô... Conclusion: very short wrap-up.‚Äù\n",
    "    - **Narrative Flow of the Article:**¬†‚ÄúProblem -> Why other solutions fail -> Theoretical solution -> Examples -> Advanced theory -> Complex example -> Connection to field.‚Äù\n",
    "    - **Referencing Ideas Between Sections:**¬†‚ÄúAvoid repeating the same idea twice... You may, however, revisit a prior point from a different perspective.‚Äù\n",
    "    - **References:**¬†‚ÄúReferences written in APA 7th edition format.‚Äù\n",
    "    \n",
    "    You can swap this profile with a LinkedIn post or video transcript profile to create different types of outputs.\n",
    "    \n",
    "    See the full profile at¬†`inputs/profiles/article_profile.md`\n",
    "    \n",
    "6. **Character Profile (Specific)**\n",
    "    \n",
    "    This injects a specific persona into the writing, making it feel authentic and authoritative. For this course, we use our ‚ÄúPaul Iusztin‚Äù profile because it felt the most natural for us, as one of the lead instructors. Here, we can add a description of the character, its style, experience, or any other aspect that makes it unique. \n",
    "    \n",
    "    Some concrete examples:\n",
    "    \n",
    "    - **About Paul Iusztin:**¬†‚ÄúSenior AI Engineer... Author of the bestseller LLM Engineer‚Äôs Handbook... Founder of the Decoding AI Magazine.‚Äù\n",
    "    - **Similar Personas:**¬†‚ÄúAndrew Ng, Chip Huyen, Sebastian Raschka, Louis-Fran√ßois Bouchard, Maxime Labonne, Jason Liu, Lex Fridman, Aleksa Gordic.‚Äù\n",
    "    - **Style:**¬†‚ÄúReal... Trust... Minimalist... Simple... Controversy.‚Äù\n",
    "    \n",
    "    You can switch this from the Paul Iusztin voice to your own voice, or to another popular character such as Richard Feynman, to inject different personalities and backgrounds.\n",
    "    \n",
    "    See the full profile at¬†`inputs/profiles/character_profiles/paul_iusztin.md`\n",
    "    \n",
    "\n",
    "In practice, the line between some profiles, such as terminology and tonality, or mechanics and structure, can get blurry. We could merge them into one or keep them as is. We haven‚Äôt spent much time optimizing which instruction goes where, and we‚Äôve mostly added them intuitively.\n",
    "\n",
    "Therefore, we recommend not overthinking the boundaries between profiles, but instead focusing on the specific instructions within them. The most crucial distinction to preserve is between the generic, article, and character profiles. Within the generic profiles, you have the freedom to change this categorization in future iterations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Unifying LLM Calls\n",
    "\n",
    "Before we dive into the workflow nodes, let's understand how the Brown agent manages different LLM APIs and configurations.\n",
    "### The `get_model` Function\n",
    "\n",
    "The central function for model initialization is `get_model` from `brown.models.get_model`. It provides a unified interface for creating LLM instances across the codebase.\n",
    "\n",
    "From `brown.models.get_model`:\n",
    "\n",
    "```python\n",
    "import json\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.language_models import BaseChatModel\n",
    "\n",
    "from brown.config import get_settings\n",
    "from brown.models.config import DEFAULT_MODEL_CONFIGS, ModelConfig, SupportedModels\n",
    "from brown.models.fake_model import FakeModel\n",
    "\n",
    "MODEL_TO_REQUIRED_API_KEY = {\n",
    "    SupportedModels.GOOGLE_GEMINI_30_PRO: \"GOOGLE_API_KEY\",\n",
    "    SupportedModels.GOOGLE_GEMINI_25_PRO: \"GOOGLE_API_KEY\",\n",
    "    SupportedModels.GOOGLE_GEMINI_25_FLASH: \"GOOGLE_API_KEY\",\n",
    "    SupportedModels.GOOGLE_GEMINI_25_FLASH_LITE: \"GOOGLE_API_KEY\",\n",
    "}\n",
    "\n",
    "\n",
    "def get_model(model: SupportedModels, config: ModelConfig | None = None) -> BaseChatModel:\n",
    "    if model == SupportedModels.FAKE_MODEL:\n",
    "        if config and config.mocked_response is not None:\n",
    "            if hasattr(config.mocked_response, \"model_dump\"):\n",
    "                mocked_response_json = config.mocked_response.model_dump(mode=\"json\")\n",
    "            else:\n",
    "                mocked_response_json = json.dumps(config.mocked_response)\n",
    "            return FakeModel(responses=[mocked_response_json])\n",
    "        else:\n",
    "            return FakeModel(responses=[])\n",
    "\n",
    "    config = config or DEFAULT_MODEL_CONFIGS.get(model) or ModelConfig()\n",
    "    model_kwargs = {\n",
    "        \"model\": model.value,\n",
    "        **config.model_dump(),\n",
    "    }\n",
    "\n",
    "    required_api_key = MODEL_TO_REQUIRED_API_KEY.get(model)\n",
    "    if required_api_key:\n",
    "        settings = get_settings()\n",
    "        if not getattr(settings, required_api_key):\n",
    "            raise ValueError(f\"Required environment variable `{required_api_key}` is not set\")\n",
    "        else:\n",
    "            model_kwargs[\"api_key\"] = getattr(settings, required_api_key)\n",
    "\n",
    "    return init_chat_model(**model_kwargs)\n",
    "```\n",
    "\n",
    "**Key Features:**\n",
    "\n",
    "1. **API Key Management**: Automatically pulls credentials from settings\n",
    "2. **Default Configurations**: Falls back to sensible defaults\n",
    "3. **LangChain Integration**: Uses `init_chat_model` for consistent interface between different LLMs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Configuration Structures\n",
    "\n",
    "The system uses three key structures for model configuration:\n",
    "\n",
    "From `brown.models.config`:\n",
    "\n",
    "```python\n",
    "from enum import StrEnum\n",
    "from typing import Any\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class SupportedModels(StrEnum):\n",
    "    GOOGLE_GEMINI_30_PRO = \"google_genai:gemini-3-pro-preview\"\n",
    "    GOOGLE_GEMINI_25_PRO = \"google_genai:gemini-2.5-pro\"\n",
    "    GOOGLE_GEMINI_25_FLASH = \"google_genai:gemini-2.5-flash\"\n",
    "    GOOGLE_GEMINI_25_FLASH_LITE = \"google_genai:gemini-2.5-flash-lite\"\n",
    "    FAKE_MODEL = \"fake\"\n",
    "\n",
    "\n",
    "class ModelConfig(BaseModel):\n",
    "    temperature: float = 0.7\n",
    "    top_k: int | None = None\n",
    "    n: int = 1\n",
    "    response_modalities: list[str] | None = None\n",
    "    include_thoughts: bool = False\n",
    "    thinking_budget: int | None = Field(\n",
    "        default=None,\n",
    "        ge=0,\n",
    "        description=\"If reasoning is available, the maximum number of tokens the model can use for thinking.\",\n",
    "    )\n",
    "    max_output_tokens: int | None = None\n",
    "    max_retries: int = 6\n",
    "\n",
    "    mocked_response: Any | None = None\n",
    "\n",
    "\n",
    "DEFAULT_MODEL_CONFIGS = {\n",
    "    \"google_genai:gemini-2.5-pro\": ModelConfig(\n",
    "        temperature=0.7,\n",
    "        include_thoughts=False,\n",
    "        thinking_budget=1000,\n",
    "        max_retries=3,\n",
    "    ),\n",
    "    \"google_genai:gemini-2.5-flash\": ModelConfig(\n",
    "        temperature=1,\n",
    "        thinking_budget=1000,\n",
    "        include_thoughts=False,\n",
    "        max_retries=3,\n",
    "    ),\n",
    "    \"google_genai:gemini-2.0-flash-exp\": ModelConfig(\n",
    "        temperature=0.7,\n",
    "        thinking_budget=1000,\n",
    "        include_thoughts=False,\n",
    "        max_retries=3,\n",
    "    ),\n",
    "}\n",
    "```\n",
    "\n",
    "**Design Insights:**\n",
    "\n",
    "1. **`SupportedModels` Enum**: Type-safe model selection\n",
    "2. **`ModelConfig` Pydantic Model**: Validates configuration parameters\n",
    "3. **`DEFAULT_MODEL_CONFIGS` Dict**: Pre-configured settings for each model\n",
    "\n",
    "This allows different nodes in the workflow to use different models with different configurations, optimizing for specific tasks (e.g., using faster models for media generation, more powerful models for article writing).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage Throughout the Codebase\n",
    "\n",
    "Every node in the workflow uses `get_model` to instantiate its LLM. This provides:\n",
    "\n",
    "- **Consistency**: Same interface everywhere\n",
    "- **Flexibility**: Easy to swap models for different nodes\n",
    "- **Configuration**: Centralized model settings\n",
    "- **Testing**: Can use FakeModel for tests\n",
    "\n",
    "Let's try it out with a simple example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model response: \n"
     ]
    }
   ],
   "source": [
    "from brown.models import ModelConfig, SupportedModels, get_model\n",
    "\n",
    "# Create a model instance with custom configuration\n",
    "model_config = ModelConfig(temperature=0.5, max_output_tokens=100)\n",
    "model = get_model(SupportedModels.GOOGLE_GEMINI_25_FLASH, config=model_config)\n",
    "\n",
    "# Test it with a simple prompt\n",
    "response = await model.ainvoke([{\"role\": \"user\", \"content\": \"Say hello in one sentence!\"}])\n",
    "print(f\"Model response: {response.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the fake model works as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model response: \"This is a fake response!\"\n"
     ]
    }
   ],
   "source": [
    "model_config = ModelConfig(temperature=0.5, max_output_tokens=100, mocked_response=\"This is a fake response!\")\n",
    "model = get_model(SupportedModels.FAKE_MODEL, config=model_config)\n",
    "\n",
    "# Test it with a simple prompt\n",
    "response = await model.ainvoke([{\"role\": \"user\", \"content\": \"Say hello in one sentence!\"}])\n",
    "print(f\"Model response: {response.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Generating Media Items Using the Orchestrator-Worker Pattern\n",
    "\n",
    "Now let's explore one of the most interesting architectural patterns in the Brown agent: the orchestrator-worker pattern for media generation. This pattern efficiently delegates specialized tasks to expert workers.\n",
    "\n",
    "### Understanding Node Abstractions\n",
    "\n",
    "First, let's understand the base abstractions that all workflow nodes inherit from.\n",
    "\n",
    "From `brown.nodes.base`:\n",
    "\n",
    "```python\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Any, Iterable, Literal, TypedDict\n",
    "\n",
    "from langchain_core.runnables import Runnable\n",
    "from langchain_core.tools import BaseTool\n",
    "\n",
    "\n",
    "class ToolCall(TypedDict):\n",
    "    name: str\n",
    "    args: dict[str, Any]\n",
    "    id: str\n",
    "    type: Literal[\"tool_call\"]\n",
    "\n",
    "\n",
    "class Toolkit(ABC):\n",
    "    \"\"\"Base class for toolkits following LangChain's toolkit pattern.\"\"\"\n",
    "\n",
    "    def __init__(self, tools: list[BaseTool]) -> None:\n",
    "        self._tools: list[BaseTool] = tools\n",
    "        self._tools_mapping: dict[str, BaseTool] = {tool.name: tool for tool in self._tools}\n",
    "\n",
    "    def get_tools(self) -> list[BaseTool]:\n",
    "        \"\"\"Get all registered media item generation tools.\"\"\"\n",
    "        return self._tools.copy()\n",
    "\n",
    "    def get_tools_mapping(self) -> dict[str, BaseTool]:\n",
    "        \"\"\"Get a mapping of tool names to tool instances.\"\"\"\n",
    "        return self._tools_mapping\n",
    "\n",
    "    def get_tool_by_name(self, name: str) -> BaseTool | None:\n",
    "        \"\"\"Get a specific tool by name.\"\"\"\n",
    "        return self._tools_mapping.get(name)\n",
    "\n",
    "\n",
    "class Node(ABC):\n",
    "    def __init__(self, model: Runnable, toolkit: Toolkit) -> None:\n",
    "        self.toolkit = toolkit\n",
    "        self.model = self._extend_model(model)\n",
    "\n",
    "    def _extend_model(self, model: Runnable) -> Runnable:\n",
    "        # Can be overridden to bind tools, structured output, etc.\n",
    "        return model\n",
    "\n",
    "    def build_user_input_content(self, inputs: Iterable[str], image_urls: list[str] | None = None) -> list[dict[str, Any]]:\n",
    "        \"\"\"Build multimodal input content with optional images.\"\"\"\n",
    "        messages: list[dict[str, Any]] = []\n",
    "        if image_urls:\n",
    "            for image_url in image_urls:\n",
    "                messages.append({\"type\": \"image_url\", \"image_url\": {\"url\": image_url}})\n",
    "        \n",
    "        for input_ in inputs:\n",
    "            messages.append({\"type\": \"text\", \"text\": input_})\n",
    "\n",
    "        return messages\n",
    "\n",
    "    @abstractmethod\n",
    "    async def ainvoke(self) -> Any:\n",
    "        pass\n",
    "```\n",
    "\n",
    "**Key Abstractions:**\n",
    "\n",
    "1. **`ToolCall`**: TypedDict representing a tool call (name, args, id)\n",
    "2. **`Toolkit`**: Manages a collection of tools that can be passed to a node\n",
    "3. **`Node`**: Base class for all workflow nodes\n",
    "   - Takes a model and toolkit\n",
    "   - Can extend the model (bind tools, structured output)\n",
    "   - Supports multimodal input (text + images)\n",
    "   - Abstract `ainvoke` method for execution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The MediaGeneratorOrchestrator Node\n",
    "\n",
    "The orchestrator analyzes the article guideline and research to identify what media items need generation, then delegates to specialized worker tools.\n",
    "\n",
    "Key implementation details from `brown.nodes.media_generator.MediaGeneratorOrchestrator`:\n",
    "\n",
    "**1. Class Initialization:**\n",
    "```python\n",
    "class MediaGeneratorOrchestrator(Node):\n",
    "    system_prompt_template = \"...\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        article_guideline: ArticleGuideline,\n",
    "        research: Research,\n",
    "        model: Runnable,\n",
    "        toolkit: Toolkit,\n",
    "    ) -> None:\n",
    "        self.article_guideline = article_guideline\n",
    "        self.research = research\n",
    "        super().__init__(model, toolkit)\n",
    "```\n",
    "\n",
    "**2. Model Extension (Tool Binding):**\n",
    "```python\n",
    "def _extend_model(self, model: Runnable) -> Runnable:\n",
    "    model = cast(BaseChatModel, super()._extend_model(model))\n",
    "    model = model.bind_tools(self.toolkit.get_tools(), tool_choice=\"any\")\n",
    "    return model\n",
    "```\n",
    "\n",
    "The orchestrator binds all available worker tools to the model, allowing it to call multiple tools.\n",
    "\n",
    "üí° Through tools we can easily extend the orchestrator with multiple worker types without even touching the orchestrator, making this a fully modular implementation.\n",
    "\n",
    "**3. Async Invocation:**\n",
    "```python\n",
    "async def ainvoke(self) -> list[ToolCall]:\n",
    "    system_prompt = self.system_prompt_template.format(\n",
    "        article_guideline=self.article_guideline.to_context(),\n",
    "        research=self.research.to_context(),\n",
    "    )\n",
    "    user_input_content = self.build_user_input_content(\n",
    "        inputs=[system_prompt], \n",
    "        image_urls=self.research.image_urls\n",
    "    )\n",
    "    inputs = [{\"role\": \"user\", \"content\": user_input_content}]\n",
    "    response = await self.model.ainvoke(inputs)\n",
    "    \n",
    "    if isinstance(response, AIMessage) and response.tool_calls:\n",
    "        jobs = cast(list[ToolCall], response.tool_calls)\n",
    "    else:\n",
    "        jobs = []\n",
    "    \n",
    "    return jobs\n",
    "```\n",
    "\n",
    "Returns a list of `ToolCall` objects describing what media items to generate.\n",
    "\n",
    "**4. System Prompt:**\n",
    "\n",
    "The orchestrator uses a detailed system prompt that instructs it to:\n",
    "- Analyze the article guideline for media requirements\n",
    "- Look for explicit indicators like \"Render a Mermaid diagram\"\n",
    "- Call appropriate worker tools with detailed descriptions\n",
    "- Handle cases where no media is needed\n",
    "\n",
    "Here is the full system prompt attached to the node:\n",
    "```python\n",
    "class MediaGeneratorOrchestrator(Node):\n",
    "    system_prompt_template = \"\"\"You are an Media Generation Orchestrator responsible for analyzing article \n",
    "guidelines and research to identify what media items need to be generated for the article.\n",
    "\n",
    "Your task is to:\n",
    "1. Carefully analyze the article guideline and research content provided\n",
    "2. Identify ALL explicit requests for media items (diagrams, charts, visual illustrations, etc.)\n",
    "3. For each identified media requirement, call the appropriate tool to generate the media item\n",
    "4. Provide clear, detailed descriptions for each media item based on the guideline requirements and research context\n",
    "\n",
    "## Analysis Guidelines\n",
    "\n",
    "**Look for these explicit indicators in the article guideline:**\n",
    "- Direct mentions: \"Render a Mermaid diagram\", \"Draw a diagram\", \"Create a visual\", \"Illustrate with\", etc.\n",
    "- Visual requirements: \"diagram visually explaining\", \"chart showing\", \"figure depicting\", \"visual representation\"\n",
    "- Process flows: descriptions of workflows, architectures, data flows, or system interactions\n",
    "- Structural elements: hierarchies, relationships, comparisons, or step-by-step processes\n",
    "\n",
    "**Key places to look:**\n",
    "- Section requirements and descriptions  \n",
    "- Specific instructions mentioning visual elements\n",
    "- Complex concepts that would benefit from visual explanation\n",
    "- Architecture or system descriptions\n",
    "- Process flows or workflows\n",
    "\n",
    "## Tool Usage Instructions\n",
    "\n",
    "You will call multiple tools to generate the media items. You will use the tool that is most appropriate for the media item you are \n",
    "generating.\n",
    "\n",
    "For each identified media requirement:\n",
    "\n",
    "**When to use MermaidDiagramGenerator:**\n",
    "- Explicit requests for Mermaid diagrams\n",
    "- System architectures and workflows\n",
    "- Process flows and data pipelines  \n",
    "- Organizational structures or hierarchies\n",
    "- Flowcharts for decision-making processes\n",
    "- Sequence diagrams for interactions\n",
    "- Entity-relationship diagrams\n",
    "- Class diagrams for software structures\n",
    "- State diagrams for system states\n",
    "- Mind maps for concept relationships\n",
    "\n",
    "**Description Requirements:**\n",
    "When calling tools, provide detailed descriptions that include:\n",
    "- The specific purpose and context from the article guideline\n",
    "- Key components that should be included based on the research\n",
    "- The type of diagram most appropriate (flowchart, sequence, architecture, etc.)\n",
    "- Specific elements, relationships, or flows to highlight\n",
    "- Any terminology or technical details from the research\n",
    "\n",
    "## Example Analysis Process\n",
    "\n",
    "1. **Scan the guideline** for phrases like:\n",
    "   - \"Render a Mermaid diagram of...\"\n",
    "   - \"Draw a diagram showing...\"\n",
    "   - \"Illustrate the architecture...\"\n",
    "   - \"Visual representation of...\"\n",
    "\n",
    "2. **For each found requirement:**\n",
    "   - Extract the specific context and purpose\n",
    "   - Identify what should be visualized\n",
    "   - Determine the most appropriate diagram type\n",
    "   - Craft a detailed description incorporating research insights\n",
    "\n",
    "3. **Call the appropriate tool** with the comprehensive description\n",
    "\n",
    "## Input Context\n",
    "\n",
    "Here is the article guideline:\n",
    "{article_guideline}\n",
    "\n",
    "Here is the research:\n",
    "{research}\n",
    "\n",
    "## Your Response\n",
    "\n",
    "Analyze the provided article guideline and research, then call the appropriate tools for each \n",
    "identified media item requirement. Each tool call should include a detailed description that ensures \n",
    "the generated media item will be relevant, accurate, and valuable for the article's educational goals.\n",
    "\n",
    "If no explicit media requirements are found in the guideline, respond with: \n",
    "\"No explicit media item requirements found in the article guideline.\"\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "üí° The only element from the orchestrator that is aware about its tools, such as the `MermaidDiagramGenerator`, is the system prompt. Here we did this to ensure it always picks up on what we want, but you can further experiment to make the system prompt more general and choose the right tools solely based on their tool description and interface. Ultimately, doing so, you make the orchestrator fully modular. But sometimes hardcoding stuff in the system prompt to get the job done is totally fine. You just have to be aware about the rules to know when to break them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The ToolNode Abstraction\n",
    "\n",
    "Worker tools inherit from `ToolNode`, a special type of `Node` that can be converted into a LangChain tool.\n",
    "\n",
    "From `brown.nodes.base`:\n",
    "\n",
    "```python\n",
    "class ToolNode(Node):\n",
    "    def __init__(self, model: Runnable) -> None:\n",
    "        super().__init__(model, toolkit=Toolkit(tools=[]))\n",
    "\n",
    "    def as_tool(self) -> StructuredTool:\n",
    "        return StructuredTool.from_function(\n",
    "            coroutine=self.ainvoke,\n",
    "            name=f\"{camel_to_snake(self.__class__.__name__)}_tool\",\n",
    "        )\n",
    "\n",
    "    @abstractmethod\n",
    "    async def ainvoke(self) -> Any:\n",
    "        pass\n",
    "```\n",
    "\n",
    "**Key Features:**\n",
    "\n",
    "- Inherits from `Node` but has no tools itself (empty toolkit)\n",
    "- `as_tool()` method converts the node into a LangChain `StructuredTool`\n",
    "- The tool name is automatically derived from the class name\n",
    "- The tool's coroutine is the node's `ainvoke` method\n",
    "\n",
    "This allows any `ToolNode` to be easily integrated as a tool into any node's toolkit. Like this we can easily transform each node into a tool that can be passed to another node, providing full composability between nodes.\n",
    "\n",
    "In our use case we will create a `MediaGeneratorOrchestrator` node that we will transform into a tool that will be passed as a worker to the orchestrator. Let's see how that works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The MermaidDiagramGenerator Worker\n",
    "\n",
    "Before running the `MediaGeneratorOrchestrator`, let's examine a concrete worker implementation from `brown.nodes.tool_nodes.MermaidDiagramGenerator`.\n",
    "\n",
    "üí≠ Even if an LLM is capable of generating both the Mermaid diagram and the article at the same time, having a specialized worker that is carefully prompted for generating Mermaid diagrams gives us more control as we can customize how the Mermaid diagrams should look like, carefully prompt engineering special use cases such as treating special characters and guiding the LLM what we expect from the diagram. Ultimately ending up with richer, more beautiful, and working Mermaid diagrams.\n",
    "\n",
    "**1. Structured Output Model:**\n",
    "```python\n",
    "class GeneratedMermaidDiagram(BaseModel):\n",
    "    content: str = Field(description=\"The Mermaid diagram code formatted in Markdown format as: ```mermaid\\n[diagram content here]\\n```\")\n",
    "    caption: str = Field(description=\"The caption, as a short description of the diagram.\")\n",
    "```\n",
    "\n",
    "**2. Class Structure:**\n",
    "```python\n",
    "class MermaidDiagramGenerator(ToolNode):\n",
    "    prompt_template = \"...\"\n",
    "\n",
    "    def _extend_model(self, model: Runnable) -> Runnable:\n",
    "        model = cast(BaseChatModel, super()._extend_model(model))\n",
    "        model = model.with_structured_output(GeneratedMermaidDiagram, include_raw=False)\n",
    "        return model\n",
    "\n",
    "    async def ainvoke(self, description_of_the_diagram: str, section_title: str) -> MermaidDiagram:\n",
    "        \"\"\"Specialized tool to generate a mermaid diagram from a text description. This tool uses a specialized LLM to\n",
    "        convert a natural language description into a mermaid diagram.\n",
    "\n",
    "        Use this tool when you need to generate a mermaid diagram to explain a concept. Don't confuse mermaid diagrams,\n",
    "        or diagrams in general, with media data, such as images, videos, audio, etc. Diagrams are rendered dynamically\n",
    "        customized for each article, while media data are static data added as URLs from external sources.\n",
    "        This tool is used explicitly to dynamically generate diagrams, not to add media data.\n",
    "\n",
    "        Args:\n",
    "            description_of_the_diagram: Natural language description of the diagram to generate.\n",
    "            section_title: Title of the section that the diagram is for.\n",
    "\n",
    "        Returns:\n",
    "            The generated mermaid diagram code in Markdown format.\n",
    "\n",
    "        Raises:\n",
    "            Exception: If diagram generation fails.\n",
    "\n",
    "        Examples:\n",
    "        >>> description = \"A flowchart showing data flowing from user input to database\"\n",
    "        >>> diagram = await generate_mermaid_diagram(description)\n",
    "        >>> print(diagram)\n",
    "        ```mermaid\n",
    "        graph LR\n",
    "            A[User Input] --> B[Processing]\n",
    "            B --> C[(Database)]\n",
    "        ```\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            response = await self.model.ainvoke(\n",
    "                [\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": self.prompt_template.format(\n",
    "                            description_of_the_diagram=description_of_the_diagram,\n",
    "                        ),\n",
    "                    }\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.exception(f\"Failed to generate Mermaid diagram: {e}\")\n",
    "\n",
    "            return MermaidDiagram(\n",
    "                location=section_title,\n",
    "                content=f'```mermaid\\ngraph TD\\n    A[\"Error: Failed to generate diagram\"]\\n    A --> B[\"{str(e)}\"]\\n```',\n",
    "                caption=f\"Error: Failed to generate diagram: {str(e)}\",\n",
    "            )\n",
    "\n",
    "        if not isinstance(response, GeneratedMermaidDiagram):\n",
    "            raise InvalidOutputTypeException(GeneratedMermaidDiagram, type(response))\n",
    "\n",
    "        return MermaidDiagram(\n",
    "            location=section_title,\n",
    "            content=response.content,\n",
    "            caption=response.caption,\n",
    "        )\n",
    "```\n",
    "\n",
    "**Key Features:**\n",
    "\n",
    "- Uses structured output to ensure valid diagram generation\n",
    "- The ainvoke() method has a carefully designed pydoc and signature that will be used when the node is transformed into a tool used by the orchestrator.\n",
    "- In case of error, we return a placeholder diagram to avoid failing the whole workflow because of a tool failure. Remember that we can run dozens of tools in parallel, thus the change of failure due to things such as rate limits is high.\n",
    "\n",
    "The prompt template contains detailed instructions for generating valid Mermaid diagrams, including:\n",
    "- Syntax rules (always use double quotes, never use semicolons)\n",
    "- Examples of different diagram types (flowcharts, process flows)\n",
    "- Common mistakes to avoid\n",
    "\n",
    "Here is the full prompt template:\n",
    "```python\n",
    "class MermaidDiagramGenerator(ToolNode):\n",
    "    prompt_template = \"\"\"\n",
    "You are a specialized agent that creates clean, readable Mermaid diagrams from text descriptions.\n",
    "\n",
    "## Task\n",
    "Generate a valid Mermaid diagram based on this description:\n",
    "<description_of_the_diagram>\n",
    "{description_of_the_diagram}\n",
    "</description_of_the_diagram>\n",
    "\n",
    "## Output Format\n",
    "Return ONLY the Mermaid code block in this exact format:\n",
    "``mermaid\n",
    "[diagram content here]\n",
    "``\n",
    "\n",
    "## Diagram Types & Examples\n",
    "\n",
    "### Process Flow\n",
    "``mermaid\n",
    "graph LR\n",
    "    A[\"Input\"] --> B[\"Process\"] --> C[\"Output\"]\n",
    "    B --> D[\"Validation\"]\n",
    "    D -->|\"Valid\"| C\n",
    "    D -->|\"Invalid\"| A\n",
    "``mermaid\n",
    "\n",
    "\n",
    "### Flowcharts (Most Common)\n",
    "``mermaid\n",
    "graph TD\n",
    "    A[\"Start\"] --> B{{\"Decision\"}}\n",
    "    B -->|\"Yes\"| C[\"Action 1\"]\n",
    "    B -->|\"No\"| D[\"Action 2\"]\n",
    "    C --> E[\"End\"]\n",
    "    D --> E\n",
    "``mermaid\n",
    "\n",
    "... # More examples\n",
    "\n",
    "## Syntax Rules\n",
    "1. **Node Labels**: Use square brackets `[Label]` for rectangular nodes\n",
    "2. **Decisions**: Use curly braces `{{Decision}}` for diamond shapes  \n",
    "3. **Databases**: Use `[(Label)]` for cylindrical database shapes\n",
    "4. **Circles**: Use `((Label))` for circular nodes\n",
    "5. **Arrows**: Use `-->` for solid arrows, `-.->` for dotted arrows\n",
    "6. **Labels on Arrows**: Use `-->|Label|` for labeled connections\n",
    "7. **Subgraphs**: Use `subgraph \"Title\"` and `end` to group elements\n",
    "8. **Comments**: Use `%%` for comments\n",
    "9. **ERD Entities**: Use `ENTITY_NAME {{ field_type field_name }}` format\n",
    "10. **ERD Relationships**: Use `||--o{{`, `||--||`, `}}o--||` for different cardinalities\n",
    "11. **Class Definitions**: Use `class ClassName {{ +type attribute +method() }}` format\n",
    "12. **Class Relationships**: Use `<|--` (inheritance), `-->` (association), `--*` (composition)\n",
    "\n",
    "## Formatting Rules\n",
    "**ALWAYS wrap node labels in double quotes `\"...\"` to prevent parsing errors:**\n",
    "\n",
    "**WRONG** (causes parse errors):\n",
    "h\n",
    "\n",
    "**Key formatting requirements:**\n",
    "- **Always use double quotes** around ALL node labels: `A[\"Label\"]`\n",
    "- **Never use semicolons** at the end of lines (they're optional and can cause issues)\n",
    "- **Quote any label** containing: parentheses `()`, commas `,`, periods `.`, colons `:`, or spaces\n",
    "- **Quote subgraph titles** as well: `subgraph \"Title\"`\n",
    "\n",
    "## Styling Guidelines\n",
    "- **Use default colors only** - do not add color specifications or custom styling\n",
    "- Do not use `fill:`, `stroke:`, `color:` or any CSS styling properties\n",
    "- Keep diagrams clean and professional with standard Mermaid appearance\n",
    "- Focus on structure and clarity, not visual customization\n",
    "\n",
    "## Key Guidelines\n",
    "- Keep node labels concise (avoid parentheses and special characters in labels)\n",
    "- Use clear, logical flow from top to bottom or left to right\n",
    "- Keep the diagram simple and easy to understand\n",
    "- Group related elements with subgraphs when helpful\n",
    "- Maintain consistent spacing and formatting\n",
    "- Choose the appropriate diagram type for the concept being illustrated\n",
    "\n",
    "## Common Mistakes to Avoid\n",
    "- **NEVER use unquoted labels** - always wrap in double quotes: `A[\"Label\"]`\n",
    "- **NEVER use semicolons** at the end of lines (causes parsing issues)\n",
    "- **NEVER put parentheses `()` in unquoted labels** - parser treats them as shape tokens\n",
    "- Don't create overly complex diagrams with too many connections\n",
    "- Avoid extremely long labels that break the visual flow\n",
    "- **Never use custom colors or styling** - stick to Mermaid's default appearance\n",
    "\n",
    "Generate a clean, professional diagram that clearly illustrates the described concept using only default Mermaid \n",
    "styling. Remember: ALWAYS use double quotes around ALL labels and NEVER use semicolons.\n",
    "\"\"\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Generating Media Items with the Orchestrator\n",
    "\n",
    "Let's see the orchestrator-worker pattern in action by generating multiple Mermaid diagrams based on our article guideline:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "  Analyzing article guideline for media requirements...\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "  Found {len(media_jobs)} media items to generate\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m---------------------------------------------- Job 1 ----------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"Tool\": \"mermaid_diagram_generator_tool\",\n",
      "  \"Description\": \"A flowchart illustrating a typical LLM workflow. It should start with an input, show an LLM call or ...\",\n",
      "  \"Section\": \"Understanding the Spectrum: From Workflows to Agents\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m---------------------------------------------- Job 2 ----------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"Tool\": \"mermaid_diagram_generator_tool\",\n",
      "  \"Description\": \"A flowchart illustrating the core architecture and dynamic decision-making process of an AI agent. I...\",\n",
      "  \"Section\": \"Understanding the Spectrum: From Workflows to Agents\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m---------------------------------------------- Job 3 ----------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"Tool\": \"mermaid_diagram_generator_tool\",\n",
      "  \"Description\": \"A flowchart illustrating the AI generation and human verification loop. This diagram should show a c...\",\n",
      "  \"Section\": \"Choosing Your Path\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "  Generating media items in parallel...\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "  Generated 3 media items successfully!\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "from brown.entities.media_items import MediaItem\n",
    "from brown.models import SupportedModels, get_model\n",
    "from brown.nodes import MediaGeneratorOrchestrator, MermaidDiagramGenerator, Toolkit\n",
    "\n",
    "# Create worker tool\n",
    "diagram_model = get_model(SupportedModels.GOOGLE_GEMINI_25_FLASH)\n",
    "mermaid_generator = MermaidDiagramGenerator(model=diagram_model)\n",
    "toolkit = Toolkit(tools=[mermaid_generator.as_tool()])\n",
    "\n",
    "# Create orchestrator\n",
    "orchestrator_model = get_model(SupportedModels.GOOGLE_GEMINI_25_FLASH)\n",
    "orchestrator = MediaGeneratorOrchestrator(\n",
    "    article_guideline=article_guideline,\n",
    "    research=research,\n",
    "    model=orchestrator_model,\n",
    "    toolkit=toolkit,\n",
    ")\n",
    "\n",
    "# Get media generation jobs\n",
    "pretty_print.wrapped(\"Analyzing article guideline for media requirements...\")\n",
    "media_jobs = await orchestrator.ainvoke()\n",
    "\n",
    "\n",
    "pretty_print.wrapped(\"Found {len(media_jobs)} media items to generate\")\n",
    "media_jobs_dict = {}\n",
    "for i, job in enumerate(media_jobs):\n",
    "    pretty_print.wrapped(\n",
    "        {\n",
    "            \"Tool\": job[\"name\"],\n",
    "            \"Description\": job[\"args\"].get(\"description_of_the_diagram\", \"N/A\")[:100] + \"...\",\n",
    "            \"Section\": job[\"args\"].get(\"section_title\", \"N/A\"),\n",
    "        },\n",
    "        title=f\"Job {i + 1}\",\n",
    "    )\n",
    "\n",
    "pretty_print.wrapped(\"Generating media items in parallel...\")\n",
    "coroutines = []\n",
    "for job in media_jobs:\n",
    "    tool = orchestrator.toolkit.get_tool_by_name(job[\"name\"])\n",
    "    if tool:\n",
    "        coroutines.append(tool.ainvoke(job[\"args\"]))\n",
    "\n",
    "media_items: list[MediaItem] = await asyncio.gather(*coroutines)\n",
    "\n",
    "pretty_print.wrapped(f\"Generated {len(media_items)} media items successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the generated diagrams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m----------------------- Understanding the Spectrum: From Workflows to Agents -----------------------\u001b[0m\n",
      "  {\n",
      "  \"Caption\": \"A flowchart illustrating a deterministic LLM workflow with tool-calling capabilities.\",\n",
      "  \"Diagram Code\": \"```mermaid\\ngraph TD\\n    A[\\\"_Start_\\\"] --> B[\\\"User Input\\\"]\\n    B --> C[\\\"Tool-Calling LLM (Deterministic Tool Selection)\\\"]\\n    C --> D[\\\"Tool 1: Data Retrieval (Predefined Step)\\\"]\\n    D --> E[\\\"Tool 2: API Call (Predefined Step)\\\"]\\n    E --> F[\\\"Tool 3: Data Processing (Predefined Step)\\\"]\\n    F --> G[\\\"Final Output\\\"]\\n    G --> H[\\\"_End_\\\"]\\n```\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m----------------------- Understanding the Spectrum: From Workflows to Agents -----------------------\u001b[0m\n",
      "  {\n",
      "  \"Caption\": \"A flowchart illustrating the core architecture and dynamic decision-making process of an AI agent, showing its interaction with memory, planning, tools, and internal loops to achieve a goal.\",\n",
      "  \"Diagram Code\": \"```mermaid\\ngraph TD\\n    A[\\\"Start: Goal/Task Received\\\"] --> B[\\\"LLM Agent (Core Decision Engine)\\\"]\\n\\n    subgraph \\\"Context & Memory\\\"\\n        C[\\\"Short-Term Memory (Working Context)\\\"]\\n        D[\\\"Long-Term Memory (Knowledge Base/Experience)\\\"]\\n    end\\n\\n    B --> C\\n    B --> D\\n    C --\\\"Provides Context\\\"--> B\\n    D --\\\"Provides Knowledge\\\"--> B\\n\\n    B --> E{\\\"Reason & Plan: Reflect & Self-Critique?\\\"}\\n    E --\\\"Refine Plan\\\"--> B\\n\\n    E --\\\"Proceed with Plan\\\"--> F[\\\"Generate Action/Tool Use Strategy\\\"]\\n\\n    F --> G{\\\"Requires Tool Use?\\\"}\\n\\n    subgraph \\\"Available Tools\\\"\\n        H[\\\"Vector Search Engine\\\"]\\n        I[\\\"Web Search\\\"]\\n        J[\\\"Calculator\\\"]\\n        K[\\\"Email Provider\\\"]\\n        L[\\\"Messaging App\\\"]\\n    end\\n\\n    G --\\\"Yes\\\"--> M[\\\"Select & Execute Tool\\\"]\\n    M --> H\\n    M --> I\\n    M --> J\\n    M --> K\\n    M --> L\\n    H --> N[\\\"Tool Output\\\"]\\n    I --> N\\n    J --> N\\n    K --> N\\n    L --> N\\n\\n    N --> O[\\\"Update Memory & Re-evaluate\\\"]\\n    G --\\\"No\\\"--> P[\\\"Formulate Response/Direct Action\\\"]\\n\\n    O --> B\\n    P --> Q{\\\"Goal Achieved?\\\"}\\n    Q --\\\"No, Continue\\\"--> B\\n    Q --\\\"Yes\\\"--> R[\\\"End: Goal Accomplished / Output\\\"]\\n```\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m---------------------------------------- Choosing Your Path ----------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"Caption\": \"A flowchart illustrating the AI generation and human verification loop.\",\n",
      "  \"Diagram Code\": \"```mermaid\\ngraph TD\\n    A[\\\"Start\\\"] --> B[\\\"AI Generates Content/Action\\\"]\\n    B --> C{\\\"Human Verification Passed?\\\"}\\n    C -- \\\"Yes\\\" --> D[\\\"Process Complete/Proceed\\\"]\\n    C -- \\\"No\\\" --> E[\\\"Human Provides Feedback/Correction\\\"]\\n    E --> B\\n```\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "for media_item in media_items:\n",
    "    pretty_print.wrapped(\n",
    "        {\n",
    "            \"Caption\": media_item.caption,\n",
    "            \"Diagram Code\": media_item.content,\n",
    "        },\n",
    "        title=media_item.location,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, let's wrap up the media items into a Pydantic entity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from brown.entities.media_items import MediaItems\n",
    "\n",
    "media_items = MediaItems.build(media_items=media_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° Now, with these classes in place, we can easily plug in new media generators as tools, such as a tool that generates images, one that generates videos, or any other super specialized tool that generates brand assets as desired."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do we model multimodal inputs?\n",
    "\n",
    "Another interesting thing to look at before we dig into the `ArticleWriter` node is how we input multimodal prompts, such as all the images from the research.\n",
    "\n",
    "We do that through the `build_user_input_content()` method, which is part of the `Node` base class. We usually combine it with the `image_url` extracted from the research as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'text', 'text': 'Use the following images as <research> context:'},\n",
       " {'type': 'image_url',\n",
       "  'image_url': {'url': 'https://towardsdatascience.com/wp-content/uploads/2025/06/agent-vs-workflow.jpeg'}},\n",
       " {'type': 'text', 'text': 'Use the following images as <research> context:'},\n",
       " {'type': 'image_url',\n",
       "  'image_url': {'url': 'https://contributor.insightmediagroup.io/wp-content/uploads/2025/06/when-agents-win-683x1024.jpeg'}},\n",
       " {'type': 'text', 'text': 'Use the following images as <research> context:'},\n",
       " {'type': 'image_url',\n",
       "  'image_url': {'url': 'https://contributor.insightmediagroup.io/wp-content/uploads/2025/06/when-workflows-win-683x1024.jpeg'}},\n",
       " {'type': 'text', 'text': 'Use the following images as <research> context:'},\n",
       " {'type': 'image_url',\n",
       "  'image_url': {'url': 'https://contributor.insightmediagroup.io/wp-content/uploads/2025/06/image-116.png'}},\n",
       " {'type': 'text', 'text': 'Use the following images as <research> context:'},\n",
       " {'type': 'image_url',\n",
       "  'image_url': {'url': 'https://substackcdn.com/image/fetch/$s_!gLNT!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F67ffe267-55f2-4af7-9910-7410c7605550_1220x754.png'}},\n",
       " {'type': 'text', 'text': 'some random inputs'},\n",
       " {'type': 'text', 'text': 'some other random inputs'}]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orchestrator.build_user_input_content(\n",
    "    inputs=[\"some random inputs\", \"some other random inputs\"],\n",
    "    image_urls=orchestrator.research.image_urls[:5]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pack everything into a dictionary, passing the images as URLs extracted from the research, and add the remaining inputs at the end. Also, note that each URL includes a piece of text that explicitly marks it as <research>. Because we model the context as XML tags, it's easy to use them as unique reference IDs throughout the context window.\n",
    "\n",
    "We explicitly added the input messages at the bottom. As we don't know how many images we will have as input, we avoid the needle-in-the-haystack problem by always keeping the text at the bottom, where the LLM can focus.\n",
    "\n",
    "This list of dictionaries will then be passed as a single user input as follows:\n",
    "```python\n",
    "user_input_content = self.build_user_input_content(\n",
    " inputs=[system_prompt], \n",
    " image_urls=self.research.image_urls\n",
    " )\n",
    " inputs = [{\"role\": \"user\", \"content\": user_input_content}]\n",
    " response = await self.model.ainvoke(inputs)\n",
    "```\n",
    "\n",
    "Using this strategy, we can easily extend the multimodal input to include documents, videos, or audio data, as long as they can be encoded as base64, URLs, or binary (as we've seen in the multimodal lesson)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Generating the Article: Bringing It All Together\n",
    "\n",
    "The `ArticleWriter` is the heart of the content generation system. It takes all the context we've prepared and generates a high-quality article following all the profile rules.\n",
    "\n",
    "### Class Structure\n",
    "\n",
    "From `brown.nodes.article_writer.ArticleWriter`:\n",
    "\n",
    "**1. Initialization:**\n",
    "```python\n",
    "class ArticleWriter(Node):\n",
    "    system_prompt_template = \"\"\"...\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        article_guideline: ArticleGuideline,\n",
    "        research: Research,\n",
    "        article_profiles: ArticleProfiles,\n",
    "        media_items: MediaItems,\n",
    "        article_examples: ArticleExamples,\n",
    "        model: Runnable,\n",
    "    ) -> None:\n",
    "        super().__init__(model, toolkit=Toolkit(tools=[]))\n",
    "        \n",
    "        self.article_guideline = article_guideline\n",
    "        self.research = research\n",
    "        self.article_profiles = article_profiles\n",
    "        self.media_items = media_items\n",
    "        self.article_examples = article_examples\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The ainvoke Method\n",
    "\n",
    "**2. Article Generation Logic:**\n",
    "```python\n",
    "async def ainvoke(self) -> Article | SelectedText:\n",
    "    # Format the system prompt with all context\n",
    "    system_prompt = self.system_prompt_template.format(\n",
    "        article_guideline=self.article_guideline.to_context(),\n",
    "        research=self.research.to_context(),\n",
    "        article_profile=self.article_profiles.article.to_context(),\n",
    "        character_profile=self.article_profiles.character.to_context(),\n",
    "        mechanics_profile=self.article_profiles.mechanics.to_context(),\n",
    "        structure_profile=self.article_profiles.structure.to_context(),\n",
    "        terminology_profile=self.article_profiles.terminology.to_context(),\n",
    "        tonality_profile=self.article_profiles.tonality.to_context(),\n",
    "        media_items=self.media_items.to_context(),\n",
    "        article_examples=self.article_examples.to_context(),\n",
    "    )\n",
    "    \n",
    "    # Build multimodal input (text + images from research)\n",
    "    user_input_content = self.build_user_input_content(\n",
    "        inputs=[system_prompt], \n",
    "        image_urls=self.research.image_urls\n",
    "    )\n",
    "    \n",
    "    inputs = [{\"role\": \"user\", \"content\": user_input_content}]\n",
    "    \n",
    "    # Generate the article\n",
    "    written_output = await self.model.ainvoke(inputs)\n",
    "    if not isinstance(written_output, AIMessage):\n",
    "        raise InvalidOutputTypeException(AIMessage, type(written_output))\n",
    "    written_output = cast(str, written_output.text)\n",
    "    \n",
    "    return Article(content=written_output)\n",
    "```\n",
    "\n",
    "The method:\n",
    "1. Combines all context using `to_context()` methods\n",
    "2. Supports multimodal input with research images\n",
    "3. Check if we get the expected output. As the article generation is a critical step within our workflow, we want to fail the whole workflow if this node fails.\n",
    "4. Returns an `Article` entity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The System Prompt (Key Sections)\n",
    "\n",
    "The `ArticleWriter` uses an extensive system prompt that includes all the context. Here are the key sections (simplified for clarity):\n",
    "\n",
    "**3. System Prompt Structure:**\n",
    "```python\n",
    "class ArticleWriter(Node):\n",
    "    system_prompt_template = \"\"\"\n",
    "You are Brown, a professional human writer specialized in writing technical, educative and informational articles\n",
    "about AI. \n",
    "\n",
    "Your task is to write a high-quality article, while providing you the following context:\n",
    "- **article guideline:** the user intent describing how the article should look like. Specific to this particular article.\n",
    "- **research:** the factual data used to support the ideas from the article guideline. Specific to this particular article.\n",
    "- **article profile:** rules specific to writing articles. Generic for all articles.\n",
    "- **character profile:** the character you will impersonate while writing. Generic for all content.\n",
    "- **structure profile:** Structure rules guiding the final output format. Generic for all content.\n",
    "- **mechanics profile:** Mechanics rules guiding the writing process. Generic for all content.\n",
    "- **terminology profile:** Terminology rules guiding word choice and phrasing. Generic for all content.\n",
    "- **tonality profile:** Tonality rules guiding the writing style. Generic for all content.\n",
    "\n",
    "Each of these will be carefully considered to guide your writing process. You will never ignore or deviate from these rules. These\n",
    "rules are your north star, your bible, the only reality you know and operate on. They are the only truth you have.\n",
    "\n",
    "## Character Profile\n",
    "\n",
    "To make the writing more personable, you will impersonate the following character profile. The character profile \n",
    "will anchor your identity and specify things such as your:\n",
    "- **personal details:** name, age, location, etc.\n",
    "- **working details:** company, job title, etc.\n",
    "- **artistic preferences:** it's niche, core content pillars, style, tone, voice, etc.\n",
    "\n",
    "What to avoid using the character profile for:\n",
    "- explicitly mentioning the character profile in the article, such as \"I'm Paul Iusztin, founder of Decoding AI.\" Use\n",
    "it only to impersonate the character and make the writing more personable. For example if you are \"Paul Iusztin\",\n",
    "you will never say all the time \"I'm Paul Iusztin, founder of Decoding AI.\" as people already know who you are.\n",
    "- using the character profile to generate article sections, such as \"Okay, I'm Paul Iusztin, founder of Decoding AI. \n",
    "Let's cut through the hype and talk real engineering for AI agents.\" Use the character profile only to adapt the\n",
    "writing style and introduce references to the character. Nothing more.\n",
    "\n",
    "Here is the character profile:\n",
    "{character_profile}\n",
    "\n",
    "## Research\n",
    "\n",
    "When using factual data to write the article, anchor your results exclusively in information from the given \n",
    "<research> or <article_guideline> tags. Avoid, at all costs, using factual information from your internal knowledge.\n",
    "\n",
    "The <research> will contain most of the factual data to write the article. But the user might add additional information\n",
    "within the <article_guideline>. \n",
    "\n",
    "Thus, always prioritize the factual data from the <article_guideline> over the <research>.\n",
    "\n",
    "Here is the research you will use as factual data for writing the article:\n",
    "{research}\n",
    "\n",
    "## Article Examples\n",
    "\n",
    "Here is a set of article examples you will use to understand how to write the article:\n",
    "{article_examples}\n",
    "\n",
    "## Tonality Profile\n",
    "\n",
    "Here is the tonality profile, describing the tone, voice and style of the writing:\n",
    "{tonality_profile}\n",
    "\n",
    "## Terminology Profile\n",
    "\n",
    "Here is the terminology profile, describing how to choose the right words and phrases\n",
    "to the target audience:\n",
    "{terminology_profile}\n",
    "\n",
    "## Mechanics Profile\n",
    "\n",
    "Here is the mechanics profile, describing how the sentences and words should be written:\n",
    "{mechanics_profile}\n",
    "\n",
    "## Structure Profile\n",
    "\n",
    "Here is the structure profile, describing general rules on how to structure text, such as the sections, paragraphs, lists,\n",
    "code blocks, or media items:\n",
    "{structure_profile}\n",
    "\n",
    "## Media Items\n",
    "\n",
    "Within the <article_guideline>, the user requested to include all types of media items, such as tables, diagrams, images, etc. Some of the \n",
    "media items will be present inside the <research> or <article_guideline> tags as links. But often, we will have to generate the \n",
    "media items ourselves.\n",
    "\n",
    "Thus, here is the list of media items that we already generated before writing the article that should be included as they are:\n",
    "{media_items}\n",
    "\n",
    "The list contains the <location> of each media item to know where to place it within the article. The location is the section title, \n",
    "inferred from the <article_guideline> outline. Based on the <location>, locate the generated media item within the <article_guideline>, \n",
    "and use it as is when writing the article.\n",
    "\n",
    "Replace the media item requirements from the <article_guideline> with the generated media item and its caption. We always\n",
    "want to group a media item with its caption.\n",
    "\n",
    "## Article Profile\n",
    "\n",
    "Here is the article profile, describing particularities on how the end-to-end article should look like:\n",
    "{article_profile}\n",
    "\n",
    "## Article Guideline: \n",
    "\n",
    "Here is the article guideline, representing the user intent, describing how the actual article should look like:\n",
    "{article_guideline}\n",
    "\n",
    "You will always start understand what to write by reading the <article_guideline>.\n",
    "\n",
    "As the <article_guideline> represents the user intent, it will always have priority over anything else. If any information\n",
    "contradicts between the <article_guideline> and other rules, you will always pick the one from the <article_guideline>.\n",
    "\n",
    "Avoid using the whole <research> when writing the article. Extract from the <research> only what is useful to respect the \n",
    "user intent from the <article_guideline>. Still, always anchor your content based on the facts from the <research> or <article_guideline>.\n",
    "\n",
    "Always prioritize the facts directly passed by the user in the <article_guideline> over the facts from the <research>. Avoid at all costs \n",
    "to use your internal knowledge when writing the article.\n",
    "\n",
    "The <article_guideline> will ALWAYS contain:\n",
    "- all the sections of the article expected to be written, in the correct order\n",
    "- a level of detail for each section, describing what each section should contain. Depending on how much detail you have in a\n",
    "particular section of the <article_guideline>, you will use more or less information from the <research> tags to write the section.\n",
    "\n",
    "The <article_guideline> can ALSO contain:\n",
    "- length constraints for each section, such as the number of characters, words or reading time. If present, you will respect them.\n",
    "- important (golden) references as URLs or titles present in the <research> tags. If present, always prioritize them over anything else \n",
    "from the <research>.\n",
    "- information about anchoring the article into a series such as a course or a book. Extremely important when the article is part of \n",
    "something bigger and we have to anchor the article into the learning journey of the reader. For example, when introducing concepts\n",
    "in previous articles that we don't want to reintroduce into the current one.\n",
    "- concrete information about writing the article. If present, you will ALWAYS prioritize the instructions from the <article_guideline> \n",
    "over any other instructions.\n",
    "\n",
    "## Article Outline\n",
    "\n",
    "Internally, based on the <article_guideline>, before starting to write the article, you will plan an article outline, \n",
    "as a short summary of the article, describing what each section contains and in what order.\n",
    "\n",
    "Here are the rules you will use to generate the article outline:\n",
    "- The user's <article_guideline> always has priority! If the user already provides an article outline or a list of sections, \n",
    "you will use them instead of generating a new one.\n",
    "- If the section titles are already provided in the <article_guideline>, you will use them as is, with 0 modifications.\n",
    "- Extract the core ideas from the <article_guideline> and lay them down into sections.\n",
    "- Your internal description of each section will be verbose enough for you to understand what each section contains.\n",
    "- Ultimately, the CORE scope of the article outline is to have an internal process that verifies that each section is anchored into the\n",
    "<article_guideline>, <research> and all the other profiles.\n",
    "- Before starting writing the final article, verify that the flow of ideas between the sections, from top to bottom, \n",
    "is coherent and natural.\n",
    "\n",
    "## Chain of Thought\n",
    "\n",
    "1. Plan the article outline\n",
    "2. Write the article following the article outline and all the other constraints.\n",
    "3. Check if all the constraints are respected. Edit the article if not.\n",
    "4. Return ONLY the final version of the article.\n",
    "\n",
    "With that in mind, based on the <article_guideline>, you will write an in-depth and high-quality article following all \n",
    "the <research>, guidelines and profiles.\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "**Key Prompt Engineering Techniques:**\n",
    "\n",
    "1. **Clear Role Definition**: \"You are Brown, a professional human writer...\"\n",
    "2. **Structured Context**: Each piece of context has its own section that is automatically enclosed by XML tags due to the `ContextMixin.to_context()` interface. \n",
    "3. **Priority Guidance**: The \"Article guideline\" which is the user input, always has priority over everything else.\n",
    "4. **Chain of Thought**: Explicit reasoning steps\n",
    "5. **Constraint Adherence**: Multiple reminders to follow all profiles to ensure the LLM picks up on our core instructions.\n",
    "6. **Grounding in Facts**: \"Anchor exclusively in research, avoid internal knowledge\" to avoid hallucinations\n",
    "\n",
    "This comprehensive prompt ensures the LLM has all the information and instructions it needs to generate high-quality, consistent content.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Running the Writing Workflow End-To-End\n",
    "\n",
    "Now let's put everything together and run the complete workflow to generate an article using our test data. \n",
    "\n",
    "This time, we will use a more comprehensive test sample containing a more detailed `article_guideline.md` and more facts within the `research.md` file to show what a professional input would look like. \n",
    "\n",
    "We recommend opening the new `article_guideline.md` from `02_sample_medium` and comparing it to the one from `01_sample_small`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SAMPLE_DIR = Path(\"inputs/tests/02_sample_medium\")\n",
    "SAMPLE_DIR.exists()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's get to it. We will start everything from scratch for complete clarity.\n",
    "\n",
    "### Step 1: Load all necessary context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "  STEP 1: Loading Context\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "‚úì Guideline: 22,854 characters\n",
      "‚úì Research: 211,790 characters, 14 images\n",
      "‚úì Profiles: 6 profiles loaded\n",
      "‚úì Examples: 2 article examples\n"
     ]
    }
   ],
   "source": [
    "from brown.loaders import (\n",
    "    MarkdownArticleExampleLoader,\n",
    "    MarkdownArticleGuidelineLoader,\n",
    "    MarkdownArticleProfilesLoader,\n",
    "    MarkdownResearchLoader,\n",
    ")\n",
    "\n",
    "pretty_print.wrapped(\"STEP 1: Loading Context\", width=100)\n",
    "\n",
    "# Load guideline\n",
    "guideline_loader = MarkdownArticleGuidelineLoader(uri=Path(\"article_guideline.md\"))\n",
    "article_guideline = guideline_loader.load(working_uri=SAMPLE_DIR)\n",
    "\n",
    "# Load research\n",
    "research_loader = MarkdownResearchLoader(uri=Path(\"research.md\"))\n",
    "research = research_loader.load(working_uri=SAMPLE_DIR)\n",
    "\n",
    "# Load profiles\n",
    "profiles_input = {\n",
    "    \"article\": PROFILES_DIR / \"article_profile.md\",\n",
    "    \"character\": PROFILES_DIR / \"character_profiles\" / \"paul_iusztin.md\",\n",
    "    \"mechanics\": PROFILES_DIR / \"mechanics_profile.md\",\n",
    "    \"structure\": PROFILES_DIR / \"structure_profile.md\",\n",
    "    \"terminology\": PROFILES_DIR / \"terminology_profile.md\",\n",
    "    \"tonality\": PROFILES_DIR / \"tonality_profile.md\",\n",
    "}\n",
    "profiles_loader = MarkdownArticleProfilesLoader(uri=profiles_input)\n",
    "article_profiles = profiles_loader.load()\n",
    "\n",
    "# Load examples\n",
    "examples_loader = MarkdownArticleExampleLoader(uri=EXAMPLES_DIR)\n",
    "article_examples = examples_loader.load()\n",
    "\n",
    "print(f\"‚úì Guideline: {len(article_guideline.content):,} characters\")\n",
    "print(f\"‚úì Research: {len(research.content):,} characters, {len(research.image_urls)} images\")\n",
    "print(f\"‚úì Profiles: {len(profiles_input)} profiles loaded\")\n",
    "print(f\"‚úì Examples: {len(article_examples.examples)} article examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Generate media items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "  Analyzing article guideline for media requirements...\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "  Found {len(media_jobs)} media items to generate\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m---------------------------------------------- Job 1 ----------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"Tool\": \"mermaid_diagram_generator_tool\",\n",
      "  \"Description\": \"A flowchart illustrating a simple LLM workflow. The workflow starts, an LLM call or other operation ...\",\n",
      "  \"Section\": \"Understanding the Spectrum: From Workflows to Agents\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m---------------------------------------------- Job 2 ----------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"Tool\": \"mermaid_diagram_generator_tool\",\n",
      "  \"Description\": \"A flowchart illustrating the AI generation and human verification loop. It should show an AI compone...\",\n",
      "  \"Section\": \"Choosing Your Path\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m---------------------------------------------- Job 3 ----------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"Tool\": \"mermaid_diagram_generator_tool\",\n",
      "  \"Description\": \"A flowchart showing an LLM workflow for chaining and routing. The workflow should start, then an LLM...\",\n",
      "  \"Section\": \"Exploring Common Patterns\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m---------------------------------------------- Job 4 ----------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"Tool\": \"mermaid_diagram_generator_tool\",\n",
      "  \"Description\": \"A flowchart illustrating the Orchestrator-Worker pattern for LLM workflows. It should show a central...\",\n",
      "  \"Section\": \"Exploring Common Patterns\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m---------------------------------------------- Job 5 ----------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"Tool\": \"mermaid_diagram_generator_tool\",\n",
      "  \"Description\": \"A flowchart illustrating the Evaluator-Optimizer loop pattern for LLM workflows. It should show an i...\",\n",
      "  \"Section\": \"Exploring Common Patterns\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m---------------------------------------------- Job 6 ----------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"Tool\": \"mermaid_diagram_generator_tool\",\n",
      "  \"Description\": \"A high-level flowchart illustrating the core components and dynamics of an AI agent using the ReAct ...\",\n",
      "  \"Section\": \"Exploring Common Patterns\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m---------------------------------------------- Job 7 ----------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"Tool\": \"mermaid_diagram_generator_tool\",\n",
      "  \"Description\": \"A simple workflow diagram for document summarization and analysis by Gemini in Google Workspace. It ...\",\n",
      "  \"Section\": \"Zooming In on Our Favorite Examples\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m---------------------------------------------- Job 8 ----------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"Tool\": \"mermaid_diagram_generator_tool\",\n",
      "  \"Description\": \"A flowchart illustrating the operational loop of the Gemini CLI coding assistant based on the ReAct ...\",\n",
      "  \"Section\": \"Zooming In on Our Favorite Examples\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m---------------------------------------------- Job 9 ----------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"Tool\": \"mermaid_diagram_generator_tool\",\n",
      "  \"Description\": \"A flowchart illustrating the iterative multi-step process of Perplexity's Deep Research agent, highl...\",\n",
      "  \"Section\": \"Zooming In on Our Favorite Examples\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "  Generating media items in parallel...\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "  Generated 9 media items successfully!\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "from brown.entities.media_items import MediaItems\n",
    "from brown.models import SupportedModels, get_model\n",
    "from brown.nodes import MediaGeneratorOrchestrator, MermaidDiagramGenerator, Toolkit\n",
    "\n",
    "# Create worker tool\n",
    "diagram_model = get_model(SupportedModels.GOOGLE_GEMINI_25_FLASH)\n",
    "mermaid_generator = MermaidDiagramGenerator(model=diagram_model)\n",
    "toolkit = Toolkit(tools=[mermaid_generator.as_tool()])\n",
    "\n",
    "# Create orchestrator\n",
    "orchestrator_model = get_model(SupportedModels.GOOGLE_GEMINI_25_FLASH)\n",
    "orchestrator = MediaGeneratorOrchestrator(\n",
    "    article_guideline=article_guideline,\n",
    "    research=research,\n",
    "    model=orchestrator_model,\n",
    "    toolkit=toolkit,\n",
    ")\n",
    "\n",
    "# Get media generation jobs\n",
    "pretty_print.wrapped(\"Analyzing article guideline for media requirements...\")\n",
    "media_jobs = await orchestrator.ainvoke()\n",
    "\n",
    "\n",
    "pretty_print.wrapped(\"Found {len(media_jobs)} media items to generate\")\n",
    "media_jobs_dict = {}\n",
    "for i, job in enumerate(media_jobs):\n",
    "    pretty_print.wrapped(\n",
    "        {\n",
    "            \"Tool\": job[\"name\"],\n",
    "            \"Description\": job[\"args\"].get(\"description_of_the_diagram\", \"N/A\")[:100] + \"...\",\n",
    "            \"Section\": job[\"args\"].get(\"section_title\", \"N/A\"),\n",
    "        },\n",
    "        title=f\"Job {i + 1}\",\n",
    "    )\n",
    "\n",
    "pretty_print.wrapped(\"Generating media items in parallel...\")\n",
    "coroutines = []\n",
    "for job in media_jobs:\n",
    "    tool = orchestrator.toolkit.get_tool_by_name(job[\"name\"])\n",
    "    if tool:\n",
    "        coroutines.append(tool.ainvoke(job[\"args\"]))\n",
    "\n",
    "media_items = await asyncio.gather(*coroutines)\n",
    "media_items = MediaItems.build(media_items=media_items)\n",
    "\n",
    "pretty_print.wrapped(f\"Generated {len(media_items.media_items)} media items successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As context objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m-------------------------------------- Media Items As Context --------------------------------------\u001b[0m\n",
      "  \n",
      "<media_items>\n",
      "\n",
      "<mermaid_diagram>\n",
      "    <location>Understanding the Spectrum: From Workflows to Agents</location>\n",
      "    <content>```mermaid\n",
      "graph TD\n",
      "    A[\"Start\"] --> B[\"LLM Call / Data Operation\"]\n",
      "    B --> C[\"End\"]\n",
      "```</content>\n",
      "    <caption>A simple LLM workflow flowchart.</caption>\n",
      "</mermaid_diagram>\n",
      "\n",
      "\n",
      "<mermaid_diagram>\n",
      "    <location>Choosing Your Path</location>\n",
      "    <content>```mermaid\n",
      "graph TD\n",
      "    A[\"AI Generates Content/Solution\"]\n",
      "    B[\"Human Reviews and Verifies Output\"]\n",
      "    A --> B\n",
      "    B --> A: \"Refinement/Correction Feedback\"\n",
      "```</content>\n",
      "    <caption>A flowchart illustrating the AI generation and human verification loop with a feedback mechanism.</caption>\n",
      "</mermaid_diagram>\n",
      "\n",
      "\n",
      "<mermaid_diagram>\n",
      "    <location>Exploring Common Patterns</location>\n",
      "    <content>```mermaid\n",
      "graph TD\n",
      "    A[\"Start\"] --> B{\"LLM Router Decision\"}\n",
      "    B -->|\"Route to Chain 1\"| C[\"LLM Chain 1\"]\n",
      "    B -->|\"Route to Chain 2\"| D[\"Tool Chain A\"]\n",
      "    B -->|\"Route to Chain 3\"| E[\"LLM Chain 2\"]\n",
      "    C --> F[\"End\"]\n",
      "    D --> F\n",
      "    E --> F\n",
      "```</content>\n",
      "    <caption>An LLM workflow for chaining and routing decisions.</caption>\n",
      "</mermaid_diagram>\n",
      "\n",
      "\n",
      "<mermaid_diagram>\n",
      "    <location>Exploring Common Patterns</location>\n",
      "    <content>```mermaid\n",
      "graph TD\n",
      "    A[\"User Intent\"] --> B[\"Orchestrator LLM\"]\n",
      "    B --> C[\"Dynamically Plans Actions\"]\n",
      "    C --> D[\"Delegates Tasks\"]\n",
      "\n",
      "    subgraph \"Execution Layer\"\n",
      "        D --> E[\"Worker LLM 1\"]\n",
      "        D --> F[\"Worker LLM 2\"]\n",
      "        D --> G[\"Specialized Tool\"]\n",
      "    end\n",
      "\n",
      "    E --> H[\"Worker Results\"]\n",
      "    F --> H\n",
      "    G --> H\n",
      "\n",
      "    H --> B\n",
      "    B --> I[\"Synthesizes Results\"]\n",
      "    I --> J[\"Final Answer\"]\n",
      "```</content>\n",
      "    <caption>A flowchart illustrating the Orchestrator-Worker pattern for LLM workflows, showing how an Orchestrator LLM plans actions, delegates tasks to various workers, and synthesizes their results into a final answer.</caption>\n",
      "</mermaid_diagram>\n",
      "\n",
      "\n",
      "<mermaid_diagram>\n",
      "    <location>Exploring Common Patterns</location>\n",
      "    <content>```mermaid\n",
      "graph TD\n",
      "    A[\"Generator LLM\"] --> B[\"Output\"]\n",
      "    B --> C[\"Evaluator LLM (Reviewer)\"]\n",
      "    C --> D[\"Feedback / Error Report (Reflection)\"]\n",
      "    D --> A\n",
      "```</content>\n",
      "    <caption>Flowchart illustrating the Evaluator-Optimizer loop for LLM workflows.</caption>\n",
      "</mermaid_diagram>\n",
      "\n",
      "\n",
      "<mermaid_diagram>\n",
      "    <location>Exploring Common Patterns</location>\n",
      "    <content>```mermaid\n",
      "graph TD\n",
      "    A[\"Agent (LLM)\"]\n",
      "    B[\"Tools (Environment Actions)\"]\n",
      "    C[\"Short-term Memory (Working Context)\"]\n",
      "    D[\"Long-term Memory (Factual Data & Preferences)\"]\n",
      "\n",
      "    A -- \"1. Reason (Thought)\" --> C\n",
      "    A -- \"1. Reason (Thought)\" --> D\n",
      "\n",
      "    C -- \"Provide Context\" --> A\n",
      "    D -- \"Provide Knowledge\" --> A\n",
      "\n",
      "    A -- \"2. Act (Tool Call)\" --> B\n",
      "    B -- \"3. Observe (Tool Output)\" --> A\n",
      "\n",
      "    A -- \"4. Update Short-term Memory\" --> C\n",
      "    A -- \"5. Update Long-term Memory (Optional)\" --> D\n",
      "\n",
      "    C -- \"Loop for Next Iteration\" --> A\n",
      "```</content>\n",
      "    <caption>High-level flowchart illustrating the core components and dynamics of an AI agent using the ReAct pattern.</caption>\n",
      "</mermaid_diagram>\n",
      "\n",
      "\n",
      "<mermaid_diagram>\n",
      "    <location>Zooming In on Our Favorite Examples</location>\n",
      "    <content>```mermaid\n",
      "graph TD\n",
      "    A[\"Read Document\"] --> B[\"Summarize (LLM Call)\"]\n",
      "    B --> C[\"Extract Key Points (LLM Call)\"]\n",
      "    C --> D[\"Save Results to Database\"]\n",
      "    D --> E[\"Show Results to User\"]\n",
      "```</content>\n",
      "    <caption>A simple workflow diagram for document summarization and analysis by Gemini in Google Workspace.</caption>\n",
      "</mermaid_diagram>\n",
      "\n",
      "\n",
      "<mermaid_diagram>\n",
      "    <location>Zooming In on Our Favorite Examples</location>\n",
      "    <content>```mermaid\n",
      "graph TD\n",
      "    A[\"Context Gathering (Directory, Tools, Conversation)\"] --> B[\"LLM Reasoning (Analyze Input, Plan Actions)\"]\n",
      "    B --> C{\"Human in the Loop (Validate Plan)?\"}\n",
      "    C -->|\"Plan Valid\"| D[\"Tool Execution (File ops, Web, Code Gen)\"]\n",
      "    C -->|\"Plan Invalid\"| B\n",
      "    D --> E[\"Evaluation (Run/Compile Code)\"]\n",
      "    E --> F{\"Loop Decision (Task Complete?)\"}\n",
      "    F -->|\"Yes\"| G[\"Task Completed\"]\n",
      "    F -->|\"No (Repeat)\"| B\n",
      "```</content>\n",
      "    <caption>Flowchart illustrating the operational loop of the Gemini CLI coding assistant based on the ReAct agent architecture.</caption>\n",
      "</mermaid_diagram>\n",
      "\n",
      "\n",
      "<mermaid_diagram>\n",
      "    <location>Zooming In on Our Favorite Examples</location>\n",
      "    <content>```mermaid\n",
      "graph TD\n",
      "    A[\"Research Question\"]\n",
      "    B[\"Orchestrator\"]\n",
      "    C[\"Breakdown into Sub-questions\"]\n",
      "    D[\"Parallel Specialized Search Agents (Web Search, Document Retrieval)\"]\n",
      "    E[\"Analysis & Synthesis\"]\n",
      "    F{\"Gaps Filled?\"}\n",
      "    G[\"Final Report\"]\n",
      "\n",
      "    A --> B\n",
      "    B --> C\n",
      "    C --> D\n",
      "    D --> E\n",
      "    E --> F\n",
      "\n",
      "    F -->|\"No Gaps\"| G\n",
      "    F -->|\"Gaps Remain / Follow-up Query\"| C\n",
      "```</content>\n",
      "    <caption>A flowchart illustrating Perplexity's Deep Research agent's iterative multi-step process.</caption>\n",
      "</mermaid_diagram>\n",
      "\n",
      "</media_items>\n",
      "\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "pretty_print.wrapped(f\"{media_items.to_context()}\", title=\"Media Items As Context\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Generate article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "  STEP 3: Writing Article (First Draft)\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "This may take 1-2 minutes...\n",
      "‚úì Article generated: 26,658 characters\n",
      "\u001b[93m-------------------------------------------- First Draft (First 1000 chars) --------------------------------------------\u001b[0m\n",
      "  # Workflows vs. Agents: The AI Engineering Decision That Shapes Success\n",
      "### Choose wisely to ship reliable, cost-effective AI applications\n",
      "\n",
      "When building AI applications, we often face a critical architectural decision early on: do we create a predictable, step-by-step workflow where we control every action, or do we build an autonomous agent that can think and decide for itself? This is one of the key choices that impacts everything from development time and costs to reliability and user experience. Making the wrong move can lead to an overly rigid system that breaks with unexpected user input or new features. It can also result in an unpredictable agent that shines in demos but fails catastrophically when it matters most.\n",
      "\n",
      "We have seen, especially in 2024 and 2025, how this architectural decision can make or break billion-dollar AI startups. Successful companies, teams, and AI engineers know when to use workflows versus agents, and more importantly, how to combine both approaches eff\n",
      "\u001b[93m------------------------------------------------------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from brown.nodes import ArticleWriter\n",
    "\n",
    "pretty_print.wrapped(\"STEP 3: Writing Article (First Draft)\", width=100)\n",
    "print(\"This may take 1-2 minutes...\")\n",
    "\n",
    "writer_model = get_model(SupportedModels.GOOGLE_GEMINI_25_FLASH)\n",
    "article_writer = ArticleWriter(\n",
    "    article_guideline=article_guideline,\n",
    "    research=research,\n",
    "    article_profiles=article_profiles,\n",
    "    media_items=media_items,\n",
    "    article_examples=article_examples,\n",
    "    model=writer_model,\n",
    ")\n",
    "\n",
    "article = await article_writer.ainvoke()\n",
    "\n",
    "print(f\"‚úì Article generated: {len(article.content):,} characters\")\n",
    "pretty_print.wrapped(article.content[:1000], title=\"First Draft (First 1000 chars)\", width=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even the article is a context object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m----------------------------------- Generated Article As Context -----------------------------------\u001b[0m\n",
      "  \n",
      "<article>\n",
      "    # Workflows vs. Agents: The AI Engineering Decision That Shapes Success\n",
      "### Choose wisely to ship reliable, cost-effective AI applications\n",
      "\n",
      "When building AI applications, we often face a critical architectural decision early on: do we create a predictable, step-by-step workflow where we control every action, or do we build an autonomous agent that can think and decide for itself? This is one of the key choices that impacts everything from development time and costs to reliability and user experience. Making the wrong move can lead to an overly rigid system that breaks with unexpected user input or new features. It can also result in an unpredictable agent that shines in demos but fails catastrophically when it matters most.\n",
      "\n",
      "We have seen, especially in 2024 and 2025, how this architectural decision can make or break billion-dollar AI startups. Successful companies, teams, and AI engineers know when to use workflows versus agents, and more importantly, how to combine both approaches effectively. Choosing wisely can save months of development time, prevent frustrated users, and keep executives from struggling with skyrocketing operational costs.\n",
      "\n",
      "By the end of this lesson, you will understand the spectrum of AI systems. You will learn how to make informed architectural decisions that lead to robust, efficient, and safe AI applications.\n",
      "\n",
      "## Understanding the Spectrum: From Workflows to Agents\n",
      "\n",
      "Let's briefly look at what LLM workflows and AI agents are. At this poi...\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "pretty_print.wrapped(f\"{article.to_context()[:1500]}...\", title=\"Generated Article As Context\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Save the articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from brown.renderers import MarkdownArticleRenderer\n",
    "\n",
    "output_path = SAMPLE_DIR / \"article.md\"\n",
    "renderer = MarkdownArticleRenderer()\n",
    "renderer.render(article, output_uri=output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open it at the following path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "  Article saved to inputs/tests/02_sample_medium/article.md\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "pretty_print.wrapped(f\"Article saved to {output_path}\", width=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now check out the article and let us know what you think!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Conclusion\n",
    "\n",
    "Congratulations! In this lesson, we built the core engine of Brown, the writing workflow. We learned:\n",
    "\n",
    "1. **Context Engineering:**¬†How to map domain entities to XML-structured context using¬†`ContextMixin` , plug them into a massive system prompt and help the LLM reason through them.\n",
    "2. **Writing Profiles + Few-shot examples:**¬†How to enforce style and voice using granular profile entities and few-shot examples.\n",
    "3. **Orchestrator-Worker:**¬†How to write the orchestrator-worker pattern from scratch and dynamically generate media items by delegating to specialized tools.\n",
    "\n",
    "We now have a system that can write a good first draft. Still, because the system prompt is extremely complex, the LLM cannot reason through everything at once. In our opinion, that‚Äôs natural. Ultimately, even we, as humans, need a few iterations to refine our work.\n",
    "\n",
    "That‚Äôs why, in the next lesson (Lesson 23), we will implement the¬†**Evaluator-Optimizer**¬†pattern to iteratively review and edit the article. Additionally, we will integrate everything into a robust LangGraph workflow. Then, in Lesson 24, we will expose these workflows via MCP to bring humans into the loop.\n",
    "\n",
    "### Practicing Ideas\n",
    "\n",
    "As a practical exercise for part 4 of the course, where you have to implement your own project, here are some ideas on how you can further extend the code:  \n",
    "\n",
    "- **Add support for image and video generation** within the Orchestrator-Worker layer to create richer media assets.\n",
    "- **Reduce costs and latency**¬†by caching constant inputs (like research and profiles) between LLM calls to avoid recomputing them, and by compressing the research relative to the article guideline.\n",
    "- **Extend the writer**¬†to other media formats such as social media posts, email newsletter articles, technical documentation, or video transcripts.\n",
    "- **Add different character profiles**¬†alongside our Paul Iusztin one to support multiple voices and personas.\n",
    "\n",
    "### Useful Resources\n",
    "\n",
    "- **Brown Package**: Explore `lessons/writing_workflow`\n",
    "- **Writing Profiles**: Check `inputs/profiles/` for more profile templates\n",
    "- **Test Data**: Use `inputs/tests/` for additional examples\n",
    "\n",
    "### üí° Run Brown as a Standalone Python Project\n",
    "\n",
    "Remember that you can also run `brown` as a standalone Python project by going to `lessons/writing_workflow/` and following the instructions from there.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
