{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lesson 22: Foundations of the Writing Workflow — Brown Agent\n",
        "\n",
        "In this lesson, we'll dive deep into the Brown writing agent, a sophisticated system designed to generate high-quality technical articles about AI. This lesson focuses on the foundational components that make the writing workflow effective:\n",
        "\n",
        "- **The orchestrator-worker pattern** for generating media assets like diagrams\n",
        "- **Context engineering** techniques for providing rich context to the article writer\n",
        "- **Entity modeling** using Pydantic to structure guidelines, research, profiles, and media\n",
        "- **Custom node abstractions** for building workflow components\n",
        "- **Markdown manipulation** utilities for handling content\n",
        "\n",
        "Learning Objectives:\n",
        "- Understand how to engineer context for high-quality content generation\n",
        "- Learn the orchestrator-worker pattern for delegating specialized tasks\n",
        "- Master entity modeling with mixins for flexible context representation\n",
        "- Build custom workflow nodes with tool integration\n",
        "- Work with Markdown content programmatically\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup\n",
        "\n",
        "First, we define some standard Magic Python commands to autoreload Python packages whenever they change:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Set Up Python Environment\n",
        "\n",
        "To set up your Python virtual environment using `uv` and load it into the Notebook, follow the step-by-step instructions from the `Course Admin` lesson from the beginning of the course.\n",
        "\n",
        "**TL/DR:** Be sure the correct kernel pointing to your `uv` virtual environment is selected.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Configure Gemini API\n",
        "\n",
        "To configure the Gemini API, follow the step-by-step instructions from the `Course Admin` lesson.\n",
        "\n",
        "But here is a quick check on what you need to run this Notebook:\n",
        "\n",
        "1.  Get your key from [Google AI Studio](https://aistudio.google.com/app/apikey).\n",
        "2.  From the root of your project, run: `cp .env.example .env` \n",
        "3.  Within the `.env` file, fill in the `GOOGLE_API_KEY` variable:\n",
        "\n",
        "Now, the code below will load the key from the `.env` file:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Environment variables loaded from `/Users/pauliusztin/Documents/01_projects/TAI/course-ai-agents/.env`\n",
            "Environment variables loaded successfully.\n"
          ]
        }
      ],
      "source": [
        "from utils import env\n",
        "\n",
        "env.load(required_env_vars=[\"GOOGLE_API_KEY\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Import Key Packages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()  # Allow nested async usage in notebooks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Download Required Resources\n",
        "\n",
        "Before we start, we need to download the necessary configuration files, profiles, examples, and test data. These resources contain the content generation profiles, few-shot examples, and test inputs we'll use throughout this lesson.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Download Configs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1763830787.372942  916673 fork_posix.cc:71] Other threads are currently calling into gRPC, skipping fork() handlers\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1763830787.624038  916673 fork_posix.cc:71] Other threads are currently calling into gRPC, skipping fork() handlers\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100  1239  100  1239    0     0   7244      0 --:--:-- --:--:-- --:--:--  7288\n",
            "Archive:  configs.zip\n",
            "   creating: configs\n",
            "  inflating: configs/debug.yaml      \n",
            "  inflating: configs/course.yaml     \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1763830788.049297  916673 fork_posix.cc:71] Other threads are currently calling into gRPC, skipping fork() handlers\n",
            "I0000 00:00:1763830788.299086  916673 fork_posix.cc:71] Other threads are currently calling into gRPC, skipping fork() handlers\n"
          ]
        }
      ],
      "source": [
        "!rm -rf configs\n",
        "!curl -L -o configs.zip https://raw.githubusercontent.com/iusztinpaul/agentic-ai-engineering-course-data/main/data/configs.zip\n",
        "!unzip configs.zip\n",
        "!rm -rf configs.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Download Inputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1763829889.765284  916673 fork_posix.cc:71] Other threads are currently calling into gRPC, skipping fork() handlers\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1763829890.012438  916673 fork_posix.cc:71] Other threads are currently calling into gRPC, skipping fork() handlers\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100 1410k  100 1410k    0     0  2918k      0 --:--:-- --:--:-- --:--:-- 2919k\n",
            "Archive:  inputs.zip\n",
            "   creating: inputs\n",
            "   creating: inputs/evals\n",
            "   creating: inputs/evals/dataset\n",
            "  inflating: inputs/evals/dataset/metadata.json  \n",
            "   creating: inputs/evals/dataset/data\n",
            "   creating: inputs/evals/dataset/data/08_react_practice\n",
            "  inflating: inputs/evals/dataset/data/08_react_practice/research.md  \n",
            "  inflating: inputs/evals/dataset/data/08_react_practice/article_guideline.md  \n",
            "  inflating: inputs/evals/dataset/data/08_react_practice/article_ground_truth.md  \n",
            "   creating: inputs/evals/dataset/data/04_structured_outputs\n",
            "  inflating: inputs/evals/dataset/data/04_structured_outputs/article_generated.md  \n",
            "  inflating: inputs/evals/dataset/data/04_structured_outputs/research.md  \n",
            "  inflating: inputs/evals/dataset/data/04_structured_outputs/article_guideline.md  \n",
            "  inflating: inputs/evals/dataset/data/04_structured_outputs/article_ground_truth.md  \n",
            "   creating: inputs/evals/dataset/data/05_workflow_patterns\n",
            "  inflating: inputs/evals/dataset/data/05_workflow_patterns/research.md  \n",
            "  inflating: inputs/evals/dataset/data/05_workflow_patterns/article_guideline.md  \n",
            "  inflating: inputs/evals/dataset/data/05_workflow_patterns/article_ground_truth.md  \n",
            "   creating: inputs/evals/dataset/data/10_memory_knowledge_access\n",
            "  inflating: inputs/evals/dataset/data/10_memory_knowledge_access/research.md  \n",
            "  inflating: inputs/evals/dataset/data/10_memory_knowledge_access/article_guideline.md  \n",
            "  inflating: inputs/evals/dataset/data/10_memory_knowledge_access/article_ground_truth.md  \n",
            "   creating: inputs/evals/dataset/data/02_workflows_vs_agents\n",
            "  inflating: inputs/evals/dataset/data/02_workflows_vs_agents/research.md  \n",
            "  inflating: inputs/evals/dataset/data/02_workflows_vs_agents/article_guideline.md  \n",
            "  inflating: inputs/evals/dataset/data/02_workflows_vs_agents/article_ground_truth.md  \n",
            "   creating: inputs/evals/dataset/data/06_tools\n",
            "  inflating: inputs/evals/dataset/data/06_tools/research.md  \n",
            "  inflating: inputs/evals/dataset/data/06_tools/article_guideline.md  \n",
            "  inflating: inputs/evals/dataset/data/06_tools/article_ground_truth.md  \n",
            "   creating: inputs/evals/dataset/data/09_RAG\n",
            "  inflating: inputs/evals/dataset/data/09_RAG/research.md  \n",
            "  inflating: inputs/evals/dataset/data/09_RAG/article_guideline.md  \n",
            "  inflating: inputs/evals/dataset/data/09_RAG/article_ground_truth.md  \n",
            "   creating: inputs/evals/dataset/data/03_context_engineering\n",
            "  inflating: inputs/evals/dataset/data/03_context_engineering/research.md  \n",
            "  inflating: inputs/evals/dataset/data/03_context_engineering/article_guideline.md  \n",
            "  inflating: inputs/evals/dataset/data/03_context_engineering/article_ground_truth.md  \n",
            "   creating: inputs/evals/dataset/data/07_reasoning_planning\n",
            "  inflating: inputs/evals/dataset/data/07_reasoning_planning/article_generated.md  \n",
            "  inflating: inputs/evals/dataset/data/07_reasoning_planning/research.md  \n",
            "  inflating: inputs/evals/dataset/data/07_reasoning_planning/article_guideline.md  \n",
            "  inflating: inputs/evals/dataset/data/07_reasoning_planning/article_ground_truth.md  \n",
            "   creating: inputs/evals/dataset/data/11_multimodal\n",
            "  inflating: inputs/evals/dataset/data/11_multimodal/research.md  \n",
            "  inflating: inputs/evals/dataset/data/11_multimodal/article_guideline.md  \n",
            "  inflating: inputs/evals/dataset/data/11_multimodal/article_ground_truth.md  \n",
            "   creating: inputs/tests\n",
            "   creating: inputs/tests/01_sample\n",
            "  inflating: inputs/tests/01_sample/research.md  \n",
            "  inflating: inputs/tests/01_sample/article_guideline.md  \n",
            "   creating: inputs/tests/00_debug\n",
            "  inflating: inputs/tests/00_debug/research.md  \n",
            "  inflating: inputs/tests/00_debug/article_guideline.md  \n",
            "   creating: inputs/examples\n",
            "   creating: inputs/examples/course_lessons\n",
            "  inflating: inputs/examples/course_lessons/agentic_ai_engineering_03_context_engineering.md  \n",
            "  inflating: inputs/examples/course_lessons/agentic_ai_engineering_04_structured_outputs.md  \n",
            "   creating: inputs/examples/debug\n",
            "  inflating: inputs/examples/debug/ai_agents_foundations_02_structured_outputs.md  \n",
            "   creating: inputs/profiles\n",
            "   creating: inputs/profiles/character_profiles\n",
            "  inflating: inputs/profiles/character_profiles/paul_iusztin.md  \n",
            "  inflating: inputs/profiles/article_profile.md  \n",
            "  inflating: inputs/profiles/terminology_profile.md  \n",
            "  inflating: inputs/profiles/mechanics_profile.md  \n",
            "  inflating: inputs/profiles/tonality_profile.md  \n",
            "  inflating: inputs/profiles/structure_profile.md  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1763829890.997430  916673 fork_posix.cc:71] Other threads are currently calling into gRPC, skipping fork() handlers\n",
            "I0000 00:00:1763829891.283910  916673 fork_posix.cc:71] Other threads are currently calling into gRPC, skipping fork() handlers\n"
          ]
        }
      ],
      "source": [
        "!rm -rf inputs\n",
        "!curl -L -o inputs.zip https://raw.githubusercontent.com/iusztinpaul/agentic-ai-engineering-course-data/main/data/inputs.zip\n",
        "!unzip inputs.zip\n",
        "!rm -rf inputs.zip\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Verify Downloaded Resources\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "article_guideline.md       \u001b[1m\u001b[36minputs\u001b[m\u001b[m/\n",
            "article_guideline_text.md  notebook.ipynb\n",
            "\u001b[1m\u001b[36mconfigs\u001b[m\u001b[m/                   notebook_old.ipynb\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1763829891.566757  916673 fork_posix.cc:71] Other threads are currently calling into gRPC, skipping fork() handlers\n"
          ]
        }
      ],
      "source": [
        "%ls\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Understanding the Brown Package\n",
        "\n",
        "Throughout this notebook, we'll be importing code from the `brown` package. This package is the production version of the writing workflow located in `../writing_workflow/`. \n",
        "\n",
        "The Brown package is installed as a local Python package using `uv` and contains all the core functionality we'll explore:\n",
        "\n",
        "- **Entities** (`brown.entities`): Pydantic models for articles, guidelines, research, profiles, and media\n",
        "- **Nodes** (`brown.nodes`): Workflow components like ArticleWriter and MediaGeneratorOrchestrator\n",
        "- **Models** (`brown.models`): LLM configuration and initialization utilities\n",
        "- **Loaders** (`brown.loaders`): Classes to load markdown content into entities\n",
        "- **Builders** (`brown.builders`): Factory patterns for creating loaders, renderers, and models\n",
        "- **Renderers** (`brown.renderers`): Classes to save entities back to markdown files\n",
        "- **Config** (`brown.config`): Settings management using Pydantic\n",
        "\n",
        "Let's start by understanding how we load environment variables and configure the system.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Configuration and Settings\n",
        "\n",
        "The Brown agent uses a centralized settings system built with Pydantic's `BaseSettings`. This ensures type-safe configuration management and seamless integration with environment variables.\n",
        "\n",
        "Let's examine the `Settings` class from `brown.config`:\n",
        "\n",
        "```python\n",
        "import os\n",
        "from functools import lru_cache\n",
        "from typing import Annotated\n",
        "\n",
        "from loguru import logger\n",
        "from pydantic import Field, FilePath, SecretStr\n",
        "from pydantic_settings import BaseSettings, SettingsConfigDict\n",
        "\n",
        "ENV_FILE_PATH = os.getenv(\"ENV_FILE_PATH\", \".env\")\n",
        "logger.info(f\"Loading environment file from `{ENV_FILE_PATH}`\")\n",
        "\n",
        "\n",
        "class Settings(BaseSettings):\n",
        "    model_config = SettingsConfigDict(env_file=ENV_FILE_PATH, extra=\"ignore\", env_file_encoding=\"utf-8\")\n",
        "\n",
        "    # --- Gemini ---\n",
        "    GOOGLE_API_KEY: SecretStr | None = Field(default=None, description=\"The API key for the Gemini API.\")\n",
        "\n",
        "    # --- Opik ---\n",
        "    OPIK_WORKSPACE: str | None = Field(default=None, description=\"Name of the Opik workspace containing the project.\")\n",
        "    OPIK_PROJECT_NAME: str = Field(default=\"brown\", description=\"Name of the Opik project.\")\n",
        "    OPIK_API_KEY: SecretStr | None = Field(default=None, description=\"The API key for the Opik API.\")\n",
        "\n",
        "    # --- App Config ---\n",
        "    CONFIG_FILE: Annotated[FilePath, Field(default=\"configs/course.yaml\", description=\"Path to the application configuration YAML file.\")]\n",
        "\n",
        "\n",
        "@lru_cache(maxsize=1)\n",
        "def get_settings() -> Settings:\n",
        "    return Settings()\n",
        "```\n",
        "\n",
        "**Key Design Patterns:**\n",
        "\n",
        "1. **Singleton Pattern**: The `@lru_cache(maxsize=1)` decorator ensures we only instantiate `Settings` once, making it behave like a singleton throughout the application.\n",
        "\n",
        "2. **Type Safety**: Using `SecretStr` for sensitive data like API keys ensures they're handled securely and not accidentally logged.\n",
        "\n",
        "3. **Environment Integration**: `SettingsConfigDict` automatically loads values from the `.env` file, with sensible defaults for missing values.\n",
        "\n",
        "4. **Flexible Configuration**: The `CONFIG_FILE` field points to a YAML configuration that controls workflow behavior, model selection, and more.\n",
        "\n",
        "This pattern ensures consistent, type-safe access to configuration throughout the codebase.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. How the Writing Agent Works: High-Level Overview\n",
        "\n",
        "Before diving into the implementation details, let's understand the three-step workflow that powers the Brown writing agent. This systematic approach ensures high-quality article generation by properly managing context and delegating specialized tasks.\n",
        "\n",
        "### The Three-Step Process\n",
        "\n",
        "**Step 1: Load Context into Memory**\n",
        "\n",
        "The first step involves gathering all the necessary context that will guide the article generation process:\n",
        "\n",
        "- **Article Guideline**: The user's intent describing what the article should contain, its structure, and any specific requirements\n",
        "- **Research**: Factual data, references, and information that will support the article's claims\n",
        "- **Few-Shot Examples**: Sample articles demonstrating the desired writing style and structure\n",
        "- **Content Generation Profiles**: Specialized profiles that control different aspects of the writing:\n",
        "  - Character profile (voice and perspective)\n",
        "  - Article profile (article-specific rules)\n",
        "  - Structure profile (formatting rules)\n",
        "  - Mechanics profile (writing mechanics)\n",
        "  - Terminology profile (word choice)\n",
        "  - Tonality profile (tone and style)\n",
        "\n",
        "**Step 2: Generate Media Items (Orchestrator-Worker Pattern)**\n",
        "\n",
        "Once we have the context, we need to generate any required media assets like diagrams or charts. This step uses the orchestrator-worker pattern:\n",
        "\n",
        "- The **MediaGeneratorOrchestrator** analyzes the article guideline and research to identify what media items need to be generated\n",
        "- For each identified requirement, the orchestrator delegates to specialized **worker tools** (e.g., `MermaidDiagramGenerator`)\n",
        "- Workers generate their specialized content type in parallel\n",
        "- All generated media items are collected and prepared for integration into the article\n",
        "\n",
        "**Step 3: Write the Article**\n",
        "\n",
        "With all context and media items ready, the **ArticleWriter** node generates the final article:\n",
        "\n",
        "- Takes all the context from Step 1\n",
        "- Integrates the media items from Step 2\n",
        "- Follows all profile rules to generate content matching the desired style\n",
        "- Produces a complete, high-quality article\n",
        "\n",
        "Let's visualize this workflow with a diagram:\n",
        "```mermaid\n",
        "graph TD\n",
        "    Step1[\"Step 1: Load Context\"]\n",
        "    \n",
        "    Step1 --> LoadGuideline[\"Load Article Guideline\"]\n",
        "    Step1 --> LoadResearch[\"Load Research\"]\n",
        "    Step1 --> LoadExamples[\"Load Few-Shot Examples\"]\n",
        "    Step1 --> LoadProfiles[\"Load Profiles\"]\n",
        "    \n",
        "    LoadGuideline --> Step2[\"Step 2: Media Generator Orchestrator Node\"]\n",
        "    LoadResearch --> Step2\n",
        "    LoadExamples --> Step2\n",
        "    LoadProfiles --> Step2\n",
        "    \n",
        "    Step2 --> DelegateWorkers[\"Delegate to Worker Tools\"]\n",
        "    \n",
        "    DelegateWorkers --> Worker1\n",
        "    DelegateWorkers --> Worker2\n",
        "    DelegateWorkers --> Worker3\n",
        "    \n",
        "    Worker1 --> CollectMedia[\"Collect Generated Media\"]\n",
        "    Worker2 --> CollectMedia\n",
        "    Worker3 --> CollectMedia\n",
        "    \n",
        "    CollectMedia --> Step3[\"Step 3: Article Writer Node\"]\n",
        "    \n",
        "    Step3 --> GeneratedArticle[\"Generated Article\"]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<img src=\"https://raw.githubusercontent.com/iusztinpaul/agentic-ai-engineering-course-data/main/images/l22_3_steps_writing_workflow.png\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from IPython.display import Image\n",
        "\n",
        "Image(\n",
        "    url=\"https://raw.githubusercontent.com/iusztinpaul/agentic-ai-engineering-course-data/main/images/l22_3_steps_writing_workflow.png\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Key Design Principles\n",
        "\n",
        "This workflow embodies several important design principles:\n",
        "\n",
        "1. **Separation of Concerns**: Each step has a clear, focused responsibility\n",
        "2. **Context Engineering**: All relevant information is properly structured before generation\n",
        "3. **Specialization**: Media generation is delegated to specialized workers\n",
        "4. **Parallelization**: Media items are generated concurrently for efficiency\n",
        "5. **Composability**: Each component can be tested and improved independently\n",
        "\n",
        "Now that we understand the high-level workflow, let's dive into the implementation details of each component, starting with how we structure and load the context.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Context Components: Understanding Entities and Loaders\n",
        "\n",
        "Now that we understand the high-level workflow, let's explore how we model and load the various context components. The Brown agent uses Pydantic models (entities) to represent different types of content, and specialized loaders to read this content from disk.\n",
        "\n",
        "### Setting Up Directory Constants\n",
        "\n",
        "First, let's define the directories where our context files are located:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Directory containing our test data\n",
        "TEST_DIR = Path(\"inputs/tests/01_sample\")\n",
        "\n",
        "# Directory containing few-shot examples\n",
        "EXAMPLES_DIR = Path(\"inputs/examples/course_lessons\")\n",
        "\n",
        "# Directory containing content generation profiles\n",
        "PROFILES_DIR = Path(\"inputs/profiles\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### The ContextMixin: Foundation for Context Engineering\n",
        "\n",
        "Before we dive into specific entities, let's understand the `ContextMixin` - a crucial abstraction that enables powerful context engineering throughout the system.\n",
        "\n",
        "The `ContextMixin` provides a standardized way to convert any entity into a context representation surrounded by XML tags. This is essential for:\n",
        "\n",
        "1. **Clear Boundaries**: XML tags clearly delineate different pieces of context in prompts\n",
        "2. **Structured Prompts**: LLMs can easily parse and understand structured XML context\n",
        "3. **Consistent Interface**: All entities follow the same pattern for context conversion\n",
        "4. **Easy Composition**: Different context elements can be seamlessly combined\n",
        "\n",
        "Here's the implementation from `brown.entities.mixins`:\n",
        "\n",
        "```python\n",
        "from abc import abstractmethod\n",
        "from brown.utils.s import camel_to_snake\n",
        "\n",
        "\n",
        "class ContextMixin:\n",
        "    @property\n",
        "    def xml_tag(self) -> str:\n",
        "        return camel_to_snake(self.__class__.__name__)\n",
        "\n",
        "    @abstractmethod\n",
        "    def to_context(self) -> str:\n",
        "        \"\"\"Context representation of the object.\"\"\"\n",
        "        pass\n",
        "```\n",
        "\n",
        "**Key Features:**\n",
        "\n",
        "- **Automatic Tag Generation**: The `xml_tag` property automatically converts the class name to snake_case (e.g., `ArticleGuideline` → `article_guideline`)\n",
        "- **Abstract Method**: `to_context()` must be implemented by each entity, ensuring consistency\n",
        "- **Flexible Implementation**: Each entity can customize how it represents itself as context\n",
        "\n",
        "For example, an `ArticleGuideline` entity will be wrapped in `<article_guideline>...</article_guideline>` tags when passed to the LLM.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Article Guideline Entity\n",
        "\n",
        "The `ArticleGuideline` represents the user's intent - what they want the article to contain, how it should be structured, and any specific requirements. It's the primary driver of content generation.\n",
        "\n",
        "From `brown.entities.guidelines`:\n",
        "\n",
        "```python\n",
        "from pydantic import BaseModel\n",
        "from brown.entities.mixins import ContextMixin\n",
        "\n",
        "\n",
        "class ArticleGuideline(BaseModel, ContextMixin):\n",
        "    content: str\n",
        "\n",
        "    def to_context(self) -> str:\n",
        "        return f\"\"\"\n",
        "<{self.xml_tag}>\n",
        "    <content>{self.content}</content>\n",
        "</{self.xml_tag}>\n",
        "\"\"\"\n",
        "\n",
        "    def __str__(self) -> str:\n",
        "        return f\"ArticleGuideline(len_content={len(self.content)})\"\n",
        "```\n",
        "\n",
        "**Design Insights:**\n",
        "\n",
        "1. **Simplicity**: Just a single `content` field containing the full guideline text\n",
        "2. **ContextMixin Integration**: Implements `to_context()` to wrap content in XML tags\n",
        "3. **Helpful String Representation**: Shows content length for debugging\n",
        "\n",
        "The guideline typically contains:\n",
        "- Article outline and section structure\n",
        "- Specific instructions for each section\n",
        "- Length constraints or requirements\n",
        "- Important references to prioritize\n",
        "- Any special formatting needs\n",
        "\n",
        "Let's load an example article guideline:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded: ArticleGuideline(len_content=23127)\n",
            "\u001b[93m----------------------------------------- Article Guideline (First 1500 Chars) -----------------------------------------\u001b[0m\n",
            "  ## Outline\n",
            "\n",
            "1. Introduction: The Critical Decision Every AI Engineer Faces\n",
            "2. Understanding the Spectrum: From Workflows to Agents\n",
            "3. Choosing Your Path\n",
            "4. Exploring Common Patterns\n",
            "5. Zooming In on Our Favorite Examples\n",
            "6. The Challenges of Every AI Engineer\n",
            "\n",
            "## Section 1 - Introduction: The Critical Decision Every AI Engineer Faces\n",
            "\n",
            "- **The Problem:** When building AI applications, engineers face a critical architectural decision early in their development process. Should they create a predictable, step-by-step workflow where they control every action, or should they build an autonomous agent that can think and decide for itself? This is one of the key decisions that will impact everything from the product such as development time and costs to reliability and user experience.\n",
            "- **Why This Decision Matters:** Choose the wrong approach and you might end up with:\n",
            "  - An overly rigid system that breaks when users deviate from expected patterns or developers try to add new features\n",
            "  - An unpredictable agent that works brilliantly 80% of the time but fails catastrophically when it matters most\n",
            "  - Months of development time wasted rebuilding the entire architecture\n",
            "  - Frustrated users who can't rely on the AI application\n",
            "  - Frustrated executives who cannot affort to keep the AI agent running as the costs are too high relative to the profits\n",
            "- Make a quick reference to the real-world where in 2024-2025 billion-dollar AI startups succeed or fail based primarily on this architect\n",
            "\u001b[93m------------------------------------------------------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "from brown.loaders import MarkdownArticleGuidelineLoader\n",
        "from utils import pretty_print\n",
        "\n",
        "# Load the article guideline\n",
        "guideline_loader = MarkdownArticleGuidelineLoader(uri=Path(\"article_guideline.md\"))\n",
        "article_guideline = guideline_loader.load(working_uri=TEST_DIR)\n",
        "\n",
        "print(f\"Loaded: {article_guideline}\")\n",
        "pretty_print.wrapped(article_guideline.content[:1500], title=\"Article Guideline (First 1500 Chars)\", width=120)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Research Entity\n",
        "\n",
        "The `Research` entity contains factual data, references, and information that supports the article's claims. It can also extract and validate image URLs from the content.\n",
        "\n",
        "From `brown.entities.research`:\n",
        "\n",
        "```python\n",
        "import re\n",
        "from functools import cached_property\n",
        "\n",
        "from loguru import logger\n",
        "from pydantic import BaseModel\n",
        "\n",
        "from brown.entities.mixins import ContextMixin\n",
        "from brown.utils.a import asyncio_run, run_jobs\n",
        "from brown.utils.network import is_image_url_valid\n",
        "\n",
        "\n",
        "class Research(BaseModel, ContextMixin):\n",
        "    content: str\n",
        "    max_image_urls: int = 30\n",
        "\n",
        "    @cached_property\n",
        "    def image_urls(self) -> list[str]:\n",
        "        # Extract image URLs using regex\n",
        "        image_urls = re.findall(\n",
        "            r\"(?!data:image/)https?://[^\\s]+\\.(?:jpg|jpeg|png|bmp|webp)\",\n",
        "            self.content,\n",
        "            re.IGNORECASE,\n",
        "        )\n",
        "        # Validate URLs asynchronously\n",
        "        jobs = [is_image_url_valid(url) for url in image_urls]\n",
        "        results = asyncio_run(run_jobs(jobs))\n",
        "\n",
        "        urls = [url for url, valid in zip(image_urls, results) if valid]\n",
        "        if len(urls) > self.max_image_urls:\n",
        "            logger.warning(f\"Found `{len(urls)} > {self.max_image_urls}` image URLs. Trimming to first {self.max_image_urls}.\")\n",
        "            urls = urls[: self.max_image_urls]\n",
        "\n",
        "        return urls\n",
        "\n",
        "    def to_context(self) -> str:\n",
        "        return f\"\"\"\n",
        "<{self.xml_tag}>\n",
        "    {self.content}\n",
        "</{self.xml_tag}>\n",
        "\"\"\"\n",
        "\n",
        "    def __str__(self) -> str:\n",
        "        return f\"Research(len_content={len(self.content)}, len_image_urls={len(self.image_urls)})\"\n",
        "```\n",
        "\n",
        "**Advanced Features:**\n",
        "\n",
        "1. **Image URL Extraction**: Automatically finds image URLs in the research content\n",
        "2. **URL Validation**: Asynchronously validates that image URLs are accessible\n",
        "3. **Caching**: Uses `@cached_property` to avoid re-extracting URLs\n",
        "4. **Safety Limits**: Caps the number of images to prevent context overflow\n",
        "\n",
        "The extracted image URLs can be passed to multimodal models (like Gemini) along with text prompts for richer context.\n",
        "\n",
        "Let's load the research data:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded: Research(len_content=211792, len_image_urls=13)\n",
            "\u001b[93m--------------------------------------------- Research (First 1500 Chars) ---------------------------------------------\u001b[0m\n",
            "  # Research\n",
            "\n",
            "## Code Sources\n",
            "\n",
            "<details>\n",
            "<summary>Repository analysis for https://github.com/google-gemini/gemini-cli/blob/main/README.md</summary>\n",
            "\n",
            "# Repository analysis for https://github.com/google-gemini/gemini-cli/blob/main/README.md\n",
            "\n",
            "## Summary\n",
            "Repository: google-gemini/gemini-cli\n",
            "File: README.md\n",
            "Lines: 211\n",
            "\n",
            "Estimated tokens: 1.6k\n",
            "\n",
            "## File tree\n",
            "```Directory structure:\n",
            "└── README.md\n",
            "\n",
            "```\n",
            "\n",
            "## Extracted content\n",
            "================================================\n",
            "FILE: README.md\n",
            "================================================\n",
            "# Gemini CLI\n",
            "\n",
            " https://github.com/google-gemini/gemini-cli/actions/workflows/ci.yml/badge.svg ](https://github.com/google-gemini/gemini-cli/actions/workflows/ci.yml)\n",
            "\n",
            " ./docs/assets/gemini-screenshot.png \n",
            "\n",
            "This repository contains the Gemini CLI, a command-line AI workflow tool that connects to your\n",
            "tools, understands your code and accelerates your workflows.\n",
            "\n",
            "With the Gemini CLI you can:\n",
            "\n",
            "- Query and edit large codebases in and beyond Gemini's 1M token context window.\n",
            "- Generate new apps from PDFs or sketches, using Gemini's multimodal capabilities.\n",
            "- Automate operational tasks, like querying pull requests or handling complex rebases.\n",
            "- Use tools and MCP servers to connect new capabilities, including  https://github.com/GoogleCloudPlatform/vertex-ai-creative-studio/tree/main/experiments/mcp-genmedia \n",
            "- Ground your queries with the  https://ai.google.dev/gemini-api/docs/grounding \n",
            "  tool, built into Gemini.\n",
            "\n",
            "## Quickstart\n",
            "\n",
            "You have two options to install G\n",
            "\u001b[93m------------------------------------------------------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "from brown.loaders import MarkdownResearchLoader\n",
        "\n",
        "research_loader = MarkdownResearchLoader(uri=Path(\"research.md\"))\n",
        "research = research_loader.load(working_uri=TEST_DIR)\n",
        "\n",
        "print(f\"Loaded: {research}\")\n",
        "pretty_print.wrapped(research.content[:1500], title=\"Research (First 1500 Chars)\", width=120)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Article Examples Entity\n",
        "\n",
        "The `ArticleExamples` entity contains few-shot examples that demonstrate the desired writing style, structure, and quality. These serve as templates for the LLM to understand what kind of output is expected.\n",
        "\n",
        "From `brown.entities.articles`:\n",
        "\n",
        "```python\n",
        "from pydantic import BaseModel\n",
        "from brown.entities.mixins import ContextMixin\n",
        "\n",
        "\n",
        "class ArticleExample(BaseModel, ContextMixin):\n",
        "    content: str\n",
        "\n",
        "    def to_context(self) -> str:\n",
        "        return f\"\"\"\n",
        "<{self.xml_tag}>\n",
        "    {self.content}\n",
        "</{self.xml_tag}>\n",
        "\"\"\"\n",
        "\n",
        "    def __str__(self) -> str:\n",
        "        return f\"ArticleExample(len_content={len(self.content)})\"\n",
        "\n",
        "\n",
        "class ArticleExamples(BaseModel, ContextMixin):\n",
        "    examples: list[ArticleExample]\n",
        "\n",
        "    def to_context(self) -> str:\n",
        "        return f\"\"\"\n",
        "<{self.xml_tag}>\n",
        "        {\"\\n\".join([example.to_context() for example in self.examples])}\n",
        "</{self.xml_tag}>\n",
        "\"\"\"\n",
        "```\n",
        "\n",
        "**Composition Pattern:**\n",
        "\n",
        "- `ArticleExample`: Represents a single example article\n",
        "- `ArticleExamples`: Contains multiple examples and composes their context representations\n",
        "- When converted to context, all examples are nested within the parent XML tags\n",
        "\n",
        "This pattern allows us to provide multiple examples to the LLM, showing consistency across different articles.\n",
        "\n",
        "Let's load the few-shot examples:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 2 article examples\n",
            "\u001b[93m--------------------------------------- First Article Example (First 1500 Chars) ---------------------------------------\u001b[0m\n",
            "  # Lesson 3: Context Engineering\n",
            "\n",
            "AI applications have evolved rapidly. In 2022, we had simple chatbots for question-answering. By 2023, Retrieval-Augmented Generation (RAG) systems connected LLMs to domain-specific knowledge. 2024 brought us tool-using agents that could perform actions. Now, we are building memory-enabled agents that remember past interactions and build relationships over time.\n",
            "\n",
            "In our last lesson, we explored how to choose between AI agents and LLM workflows when designing a system. As these applications grow more complex, prompt engineering, a practice that once served us well, is showing its limits. It optimizes single LLM calls but fails when managing systems with memory, actions, and long interaction histories. The sheer volume of information an agent might need, past conversations, user data, documents, and action descriptions, has grown exponentially. Simply stuffing all this into a prompt is not a viable strategy. This is where context engineering comes in. It is the discipline of orchestrating this entire information ecosystem to ensure the LLM gets exactly what it needs, when it needs it. This skill is becoming a core foundation for AI engineering.\n",
            "\n",
            "## From Prompt to Context Engineering\n",
            "\n",
            "Prompt engineering, while effective for simple tasks, is designed for single, stateless interactions. It treats each call to an LLM as a new, isolated event. This approach breaks down in stateful applications where context must be preserved and managed across multip\n",
            "\u001b[93m------------------------------------------------------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "from brown.loaders import MarkdownArticleExampleLoader\n",
        "\n",
        "examples_loader = MarkdownArticleExampleLoader(uri=EXAMPLES_DIR)\n",
        "article_examples = examples_loader.load()\n",
        "\n",
        "print(f\"Loaded {len(article_examples.examples)} article examples\")\n",
        "if article_examples.examples:\n",
        "    pretty_print.wrapped(\n",
        "        article_examples.examples[0].content[:1500], title=\"First Article Example (First 1500 Chars)\", width=120\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Profiles Entity\n",
        "\n",
        "Profiles are the secret sauce of the Brown agent. They provide detailed instructions that control different aspects of content generation. The system uses multiple specialized profiles that work together.\n",
        "\n",
        "From `brown.entities.profiles`:\n",
        "\n",
        "```python\n",
        "from pydantic import BaseModel\n",
        "from brown.entities.mixins import ContextMixin\n",
        "\n",
        "\n",
        "class Profile(BaseModel, ContextMixin):\n",
        "    name: str\n",
        "    content: str\n",
        "\n",
        "    def to_context(self) -> str:\n",
        "        return f\"\"\"\n",
        "<{self.xml_tag}>\n",
        "    <name>{self.name}</name>\n",
        "    <content>{self.content}</content>\n",
        "</{self.xml_tag}>\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class CharacterProfile(Profile):\n",
        "    pass\n",
        "\n",
        "\n",
        "class ArticleProfile(Profile):\n",
        "    pass\n",
        "\n",
        "\n",
        "class StructureProfile(Profile):\n",
        "    pass\n",
        "\n",
        "\n",
        "class MechanicsProfile(Profile):\n",
        "    pass\n",
        "\n",
        "\n",
        "class TerminologyProfile(Profile):\n",
        "    pass\n",
        "\n",
        "\n",
        "class TonalityProfile(Profile):\n",
        "    pass\n",
        "\n",
        "\n",
        "class ArticleProfiles(BaseModel):\n",
        "    character: CharacterProfile\n",
        "    article: ArticleProfile\n",
        "    structure: StructureProfile\n",
        "    mechanics: MechanicsProfile\n",
        "    terminology: TerminologyProfile\n",
        "    tonality: TonalityProfile\n",
        "```\n",
        "\n",
        "**Profile Hierarchy:**\n",
        "\n",
        "1. **Base `Profile` Class**: Provides common structure (name + content) and context conversion\n",
        "2. **Specialized Profile Classes**: Inherit from `Profile` to provide semantic meaning\n",
        "3. **`ArticleProfiles` Container**: Holds all six profile types together\n",
        "\n",
        "**Why Separate Profile Classes?**\n",
        "\n",
        "Even though they have the same structure, separate classes provide:\n",
        "- Type safety (can't accidentally swap profile types)\n",
        "- Context clarity (each profile will have it's own XML tag when added as context)\n",
        "\n",
        "Let's load all the profiles:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Profile Sizes (in characters):\n",
            "\n",
            "Character Profile: 3,033 characters\n",
            "Article Profile: 13,074 characters\n",
            "Structure Profile: 22,660 characters\n",
            "Mechanics Profile: 4,747 characters\n",
            "Terminology Profile: 10,730 characters\n",
            "Tonality Profile: 4,192 characters\n",
            "\n",
            "Total Profile Content: 58,436 characters\n",
            "\n",
            "\u001b[93m------------------------------------------ Article Profile (First 1500 Chars) ------------------------------------------\u001b[0m\n",
            "  ## Tonality\n",
            "\n",
            "You should write in a humanized way as writing a blog article or book.\n",
            "\n",
            "Write the description of ideas as fluid as possible. Remember that you are writing a book or blog article. Thus, everything should flow naturally, without too many bullet points or subheaders. Use them only when it really makes sense. Otherwise, stick to normal paragraphs.\n",
            "\n",
            "## General Article Structure\n",
            "\n",
            "The article is a collection of blocks that flow naturally one after the other. It starts with one introduction, continues with multiple sections in between and wrap-ups with a conclusion. The information flows naturally from the introduction, to the sections, to the conclusion.\n",
            "\n",
            "## Introduction Guidelines\n",
            "\n",
            "The introduction is a short summary of the article, quickly presenting the `why` (problem), `what` (solution), and captivating the reader to continue reading.\n",
            "\n",
            "- Make it short and concise, it should be a summary of the article transformed into a light, personal and engaging story that makes the reader wanna dig into the whole article\n",
            "- Make it engaging and interesting, it should be a hook to raise the curiosity of the reader\n",
            "- Induce curiosity, urgency, or emotional response\n",
            "- Bring out emotions (Surprise, Shock, Intrigue, Skepticism, fear of missing out, curiosity) (in the hooks mainly)\n",
            "- Make it memorable and catchy\n",
            "- The introduction is super highlevel, present a summary of the article in a way that is engaging and increases the \n",
            "curiosity of the reader. It should respond to question such\n",
            "\u001b[93m------------------------------------------------------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "from brown.loaders import MarkdownArticleProfilesLoader\n",
        "\n",
        "profiles_input = {\n",
        "    \"article\": PROFILES_DIR / \"article_profile.md\",\n",
        "    \"character\": PROFILES_DIR / \"character_profiles\" / \"paul_iusztin.md\",\n",
        "    \"mechanics\": PROFILES_DIR / \"mechanics_profile.md\",\n",
        "    \"structure\": PROFILES_DIR / \"structure_profile.md\",\n",
        "    \"terminology\": PROFILES_DIR / \"terminology_profile.md\",\n",
        "    \"tonality\": PROFILES_DIR / \"tonality_profile.md\",\n",
        "}\n",
        "\n",
        "profiles_loader = MarkdownArticleProfilesLoader(uri=profiles_input)\n",
        "article_profiles = profiles_loader.load()\n",
        "\n",
        "print(\"Profile Sizes (in characters):\\n\")\n",
        "print(f\"Character Profile: {len(article_profiles.character.content):,} characters\")\n",
        "print(f\"Article Profile: {len(article_profiles.article.content):,} characters\")\n",
        "print(f\"Structure Profile: {len(article_profiles.structure.content):,} characters\")\n",
        "print(f\"Mechanics Profile: {len(article_profiles.mechanics.content):,} characters\")\n",
        "print(f\"Terminology Profile: {len(article_profiles.terminology.content):,} characters\")\n",
        "print(f\"Tonality Profile: {len(article_profiles.tonality.content):,} characters\")\n",
        "print(\n",
        "    f\"\\nTotal Profile Content: {\n",
        "        sum(\n",
        "            [\n",
        "                len(article_profiles.character.content),\n",
        "                len(article_profiles.article.content),\n",
        "                len(article_profiles.structure.content),\n",
        "                len(article_profiles.mechanics.content),\n",
        "                len(article_profiles.terminology.content),\n",
        "                len(article_profiles.tonality.content),\n",
        "            ]\n",
        "        ):,} characters\"\n",
        ")\n",
        "\n",
        "print()\n",
        "pretty_print.wrapped(article_profiles.article.content[:1500], title=\"Article Profile (First 1500 Chars)\", width=120)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Deep Dive: Understanding Content Generation Profiles\n",
        "\n",
        "Now that we've loaded all the profiles, let's understand what each one does and why it's essential for generating high-quality content. Profiles are the key to making the Brown agent produce consistent, high-quality output that matches your desired style and voice.\n",
        "\n",
        "### Profile Categories\n",
        "\n",
        "The profiles fall into two categories:\n",
        "\n",
        "**1. General Profiles** (applicable to any content type):\n",
        "- Mechanics Profile\n",
        "- Structure Profile  \n",
        "- Terminology Profile\n",
        "- Tonality Profile\n",
        "\n",
        "**2. Specialized Profiles** (specific to particular use cases):\n",
        "- Article Profile (content-type specific)\n",
        "- Character Profile (voice-specific)\n",
        "\n",
        "Let's explore each profile and understand its role.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Mechanics Profile\n",
        "\n",
        "**Purpose**: Controls the technical mechanics of writing words, sentences up to paragraphs.\n",
        "\n",
        "**Location**: `inputs/profiles/mechanics_profile.md`\n",
        "\n",
        "**What it covers**:\n",
        "- Sentence length and complexity\n",
        "- Paragraph structure and transitions\n",
        "- Active vs passive voice usage\n",
        "- Punctuation rules\n",
        "- Readability guidelines\n",
        "\n",
        "**Example rules**:\n",
        "- \"Vary sentence length for rhythm (mix short and long sentences)\"\n",
        "- \"Use active voice unless passive is more appropriate\"\n",
        "- \"Limit sentences to 20-25 words for clarity\"\n",
        "\n",
        "This profile ensures the writing is clean and easy to read.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Structure Profile\n",
        "\n",
        "**Purpose**: Defines how the whole content should be organized and formatted. Instead of looking at the paragraph level, here we are interested at how the whole piece is structured.\n",
        "\n",
        "**Location**: `inputs/profiles/structure_profile.md`\n",
        "\n",
        "**What it covers**:\n",
        "- Document structure (headings, sections)\n",
        "- List formatting (when to use bullets vs numbers)\n",
        "- Code block formatting\n",
        "- Media placement rules\n",
        "- Table of contents guidelines\n",
        "\n",
        "**Example rules**:\n",
        "- \"Use ## for main sections, ### for subsections\"\n",
        "- \"Code blocks must include language specification\"\n",
        "- \"Use numbered lists for sequential steps, bullets for items\"\n",
        "\n",
        "This profile ensures consistent formatting across all generated content.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Terminology Profile\n",
        "\n",
        "**Purpose**: Guides word choice and phrasing to match the target audience. Tries to avoid AI slop.\n",
        "\n",
        "**Location**: `inputs/profiles/terminology_profile.md`\n",
        "\n",
        "**What it covers**:\n",
        "- Technical vs simple language decisions\n",
        "- Jargon usage guidelines\n",
        "- Acronym handling\n",
        "- Preferred terminology\n",
        "- Words to avoid\n",
        "\n",
        "**Example rules**:\n",
        "- \"Define technical terms on first use\"\n",
        "- \"Spell out acronyms on first mention: Machine Learning (ML)\"\n",
        "- \"Avoid AI slop jargon like 'synergy' or 'leverage'\"\n",
        "\n",
        "This profile ensures the content speaks the right language for its audience.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. Tonality Profile\n",
        "\n",
        "**Purpose**: Sets the overall tone, voice, and style of the writing.\n",
        "\n",
        "**Location**: `inputs/profiles/tonality_profile.md`\n",
        "\n",
        "**What it covers**:\n",
        "- Formality level\n",
        "- Use of humor and personality\n",
        "- Emotional tone\n",
        "- Perspective (first/second/third person)\n",
        "- Energy and enthusiasm level\n",
        "\n",
        "**Example rules**:\n",
        "- \"Maintain a conversational but professional tone\"\n",
        "- \"Use second person ('you') to engage readers\"\n",
        "- \"Include occasional light humor when appropriate\"\n",
        "\n",
        "This profile gives the content its personality and makes it engaging.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5. Article Profile\n",
        "\n",
        "**Purpose**: Defines rules specific to article generation (content-type specific).\n",
        "\n",
        "**Location**: `inputs/profiles/article_profile.md`\n",
        "\n",
        "**What it covers**:\n",
        "- Article-specific structure requirements\n",
        "- Introduction and conclusion patterns\n",
        "- How to handle references and citations\n",
        "- Examples and case study integration\n",
        "- Call-to-action guidelines\n",
        "\n",
        "**Example rules**:\n",
        "- \"Start with a hook that captures attention\"\n",
        "- \"Include learning objectives near the beginning\"\n",
        "- \"End with key takeaways and next steps\"\n",
        "- \"Cite sources using footnotes or inline links\"\n",
        "\n",
        "This profile contains rules that only make sense for articles. For example, we could easily extend the agent to other formats such as video scripts or social media posts by adding `video_script_profile.md and `social_media_posts_profile.md`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6. Character Profile\n",
        "\n",
        "**Purpose**: Injects a specific voice or persona into the writing.\n",
        "\n",
        "**Location**: `inputs/profiles/character_profiles/paul_iusztin.md` (or other character files)\n",
        "\n",
        "**What it covers**:\n",
        "- Personal details (name, background)\n",
        "- Professional expertise and experience\n",
        "- Writing style preferences\n",
        "- Content focus areas\n",
        "- Unique phrases or expressions\n",
        "\n",
        "**Example rules**:\n",
        "- \"Write from Paul's perspective as an AI engineer and educator\"\n",
        "- \"Emphasize hands-on implementation over theory\"\n",
        "- \"Use examples from real projects when possible\"\n",
        "\n",
        "**Why Character Profiles Matter:**\n",
        "\n",
        "Character profiles allow the agent to write in a consistent voice:\n",
        "- **Personal Brand**: Content feels like it comes from a real person\n",
        "- **Authenticity**: References experiences and perspectives naturally\n",
        "- **Flexibility**: Can switch between different voices (e.g., Paul Iusztin vs.Louis-Francois vs. Richard Feynman)\n",
        "- **Consistency**: Maintains the same voice across multiple articles\n",
        "\n",
        "You can create profiles for yourself or emulate famous figures' writing styles.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### How Profiles Work Together\n",
        "\n",
        "The magic happens when all six profiles work together:\n",
        "\n",
        "1. **Character Profile** → Establishes whose voice we're writing in\n",
        "2. **Tonality Profile** → Sets the emotional tone and energy\n",
        "3. **Terminology Profile** → Chooses the right words for the audience\n",
        "4. **Mechanics Profile** → Ensures technical writing quality\n",
        "5. **Structure Profile** → Organizes content logically\n",
        "6. **Article Profile** → Applies article-specific best practices\n",
        "\n",
        "**Example: How Profiles Shape a Single Paragraph**\n",
        "\n",
        "Without profiles, an LLM might write:\n",
        "> \"Machine learning models require data. You need to preprocess this data. Then you train the model.\"\n",
        "\n",
        "With all profiles applied:\n",
        "> \"Here's the thing about ML models - they're hungry for data, but not just any data. Before you can train your model, you'll need to clean and transform your raw data into a format the model can actually learn from. Think of it like preparing ingredients before cooking: the better your prep work, the better your final dish.\"\n",
        "\n",
        "The profiles transform bland, generic text into engaging, personable, and technically accurate content that sounds like a real expert talking to a friend.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Model Configuration: Flexible LLM Management\n",
        "\n",
        "Before we dive into the workflow nodes, let's understand how the Brown agent manages LLM configuration. The system provides a flexible way to configure different models for different nodes, allowing you to optimize cost and performance.\n",
        "\n",
        "### The `get_model` Function\n",
        "\n",
        "The central function for model initialization is `get_model` from `brown.models.get_model`. It provides a unified interface for creating LLM instances across the codebase.\n",
        "\n",
        "From `brown.models.get_model`:\n",
        "\n",
        "```python\n",
        "import json\n",
        "\n",
        "from langchain.chat_models import init_chat_model\n",
        "from langchain_core.language_models import BaseChatModel\n",
        "\n",
        "from brown.config import get_settings\n",
        "from brown.models.config import DEFAULT_MODEL_CONFIGS, ModelConfig, SupportedModels\n",
        "from brown.models.fake_model import FakeModel\n",
        "\n",
        "MODEL_TO_REQUIRED_API_KEY = {\n",
        "    SupportedModels.GOOGLE_GEMINI_30_PRO: \"GOOGLE_API_KEY\",\n",
        "    SupportedModels.GOOGLE_GEMINI_25_PRO: \"GOOGLE_API_KEY\",\n",
        "    SupportedModels.GOOGLE_GEMINI_25_FLASH: \"GOOGLE_API_KEY\",\n",
        "    SupportedModels.GOOGLE_GEMINI_25_FLASH_LITE: \"GOOGLE_API_KEY\",\n",
        "}\n",
        "\n",
        "\n",
        "def get_model(model: SupportedModels, config: ModelConfig | None = None) -> BaseChatModel:\n",
        "    if model == SupportedModels.FAKE_MODEL:\n",
        "        if config and config.mocked_response is not None:\n",
        "            if hasattr(config.mocked_response, \"model_dump\"):\n",
        "                mocked_response_json = config.mocked_response.model_dump(mode=\"json\")\n",
        "            else:\n",
        "                mocked_response_json = json.dumps(config.mocked_response)\n",
        "            return FakeModel(responses=[mocked_response_json])\n",
        "        else:\n",
        "            return FakeModel(responses=[])\n",
        "\n",
        "    config = config or DEFAULT_MODEL_CONFIGS.get(model) or ModelConfig()\n",
        "    model_kwargs = {\n",
        "        \"model\": model.value,\n",
        "        **config.model_dump(),\n",
        "    }\n",
        "\n",
        "    required_api_key = MODEL_TO_REQUIRED_API_KEY.get(model)\n",
        "    if required_api_key:\n",
        "        settings = get_settings()\n",
        "        if not getattr(settings, required_api_key):\n",
        "            raise ValueError(f\"Required environment variable `{required_api_key}` is not set\")\n",
        "        else:\n",
        "            model_kwargs[\"api_key\"] = getattr(settings, required_api_key)\n",
        "\n",
        "    return init_chat_model(**model_kwargs)\n",
        "```\n",
        "\n",
        "**Key Features:**\n",
        "\n",
        "1. **Fake Model Support**: For testing without API calls\n",
        "2. **API Key Management**: Automatically pulls credentials from settings\n",
        "3. **Default Configurations**: Falls back to sensible defaults\n",
        "4. **LangChain Integration**: Uses `init_chat_model` for consistent interface\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Configuration Structures\n",
        "\n",
        "The system uses three key structures for model configuration:\n",
        "\n",
        "From `brown.models.config`:\n",
        "\n",
        "```python\n",
        "from enum import StrEnum\n",
        "from typing import Any\n",
        "\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "\n",
        "class SupportedModels(StrEnum):\n",
        "    GOOGLE_GEMINI_30_PRO = \"google_genai:gemini-3-pro-preview\"\n",
        "    GOOGLE_GEMINI_25_PRO = \"google_genai:gemini-2.5-pro\"\n",
        "    GOOGLE_GEMINI_25_FLASH = \"google_genai:gemini-2.5-flash\"\n",
        "    GOOGLE_GEMINI_25_FLASH_LITE = \"google_genai:gemini-2.5-flash-lite\"\n",
        "    FAKE_MODEL = \"fake\"\n",
        "\n",
        "\n",
        "class ModelConfig(BaseModel):\n",
        "    temperature: float = 0.7\n",
        "    top_k: int | None = None\n",
        "    n: int = 1\n",
        "    response_modalities: list[str] | None = None\n",
        "    include_thoughts: bool = False\n",
        "    thinking_budget: int | None = Field(\n",
        "        default=None,\n",
        "        ge=0,\n",
        "        description=\"If reasoning is available, the maximum number of tokens the model can use for thinking.\",\n",
        "    )\n",
        "    max_output_tokens: int | None = None\n",
        "    max_retries: int = 6\n",
        "\n",
        "    mocked_response: Any | None = None\n",
        "\n",
        "\n",
        "DEFAULT_MODEL_CONFIGS = {\n",
        "    \"google_genai:gemini-2.5-pro\": ModelConfig(\n",
        "        temperature=0.7,\n",
        "        include_thoughts=False,\n",
        "        thinking_budget=1000,\n",
        "        max_retries=3,\n",
        "    ),\n",
        "    \"google_genai:gemini-2.5-flash\": ModelConfig(\n",
        "        temperature=1,\n",
        "        thinking_budget=1000,\n",
        "        include_thoughts=False,\n",
        "        max_retries=3,\n",
        "    ),\n",
        "    \"google_genai:gemini-2.0-flash-exp\": ModelConfig(\n",
        "        temperature=0.7,\n",
        "        thinking_budget=1000,\n",
        "        include_thoughts=False,\n",
        "        max_retries=3,\n",
        "    ),\n",
        "}\n",
        "```\n",
        "\n",
        "**Design Insights:**\n",
        "\n",
        "1. **`SupportedModels` Enum**: Type-safe model selection\n",
        "2. **`ModelConfig` Pydantic Model**: Validates configuration parameters\n",
        "3. **`DEFAULT_MODEL_CONFIGS` Dict**: Pre-configured settings for each model\n",
        "\n",
        "This allows different nodes in the workflow to use different models with different configurations, optimizing for specific tasks (e.g., using faster models for media generation, more powerful models for article writing).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Usage Throughout the Codebase\n",
        "\n",
        "Every node in the workflow uses `get_model` to instantiate its LLM. This provides:\n",
        "\n",
        "- **Consistency**: Same interface everywhere\n",
        "- **Flexibility**: Easy to swap models for different nodes\n",
        "- **Configuration**: Centralized model settings\n",
        "- **Testing**: Can use FakeModel for tests\n",
        "\n",
        "Let's try it out with a simple example:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model response: Hello there!\n"
          ]
        }
      ],
      "source": [
        "from brown.models import get_model, ModelConfig, SupportedModels\n",
        "\n",
        "# Create a model instance with custom configuration\n",
        "model_config = ModelConfig(temperature=0.5, max_output_tokens=100)\n",
        "model = get_model(SupportedModels.GOOGLE_GEMINI_25_FLASH, config=model_config)\n",
        "\n",
        "# Test it with a simple prompt\n",
        "response = await model.ainvoke([{\"role\": \"user\", \"content\": \"Say hello in one sentence!\"}])\n",
        "print(f\"Model response: {response.content}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Media Generation: The Orchestrator-Worker Pattern\n",
        "\n",
        "Now let's explore one of the most interesting architectural patterns in the Brown agent: the orchestrator-worker pattern for media generation. This pattern efficiently delegates specialized tasks to expert workers.\n",
        "\n",
        "### Understanding Node Abstractions\n",
        "\n",
        "First, let's understand the base abstractions that all workflow nodes inherit from.\n",
        "\n",
        "From `brown.nodes.base`:\n",
        "\n",
        "```python\n",
        "from abc import ABC, abstractmethod\n",
        "from typing import Any, Iterable, Literal, TypedDict\n",
        "\n",
        "from langchain_core.runnables import Runnable\n",
        "from langchain_core.tools import BaseTool\n",
        "\n",
        "\n",
        "class ToolCall(TypedDict):\n",
        "    name: str\n",
        "    args: dict[str, Any]\n",
        "    id: str\n",
        "    type: Literal[\"tool_call\"]\n",
        "\n",
        "\n",
        "class Toolkit(ABC):\n",
        "    \"\"\"Base class for toolkits following LangChain's toolkit pattern.\"\"\"\n",
        "\n",
        "    def __init__(self, tools: list[BaseTool]) -> None:\n",
        "        self._tools: list[BaseTool] = tools\n",
        "        self._tools_mapping: dict[str, BaseTool] = {tool.name: tool for tool in self._tools}\n",
        "\n",
        "    def get_tools(self) -> list[BaseTool]:\n",
        "        \"\"\"Get all registered media item generation tools.\"\"\"\n",
        "        return self._tools.copy()\n",
        "\n",
        "    def get_tools_mapping(self) -> dict[str, BaseTool]:\n",
        "        \"\"\"Get a mapping of tool names to tool instances.\"\"\"\n",
        "        return self._tools_mapping\n",
        "\n",
        "    def get_tool_by_name(self, name: str) -> BaseTool | None:\n",
        "        \"\"\"Get a specific tool by name.\"\"\"\n",
        "        return self._tools_mapping.get(name)\n",
        "\n",
        "\n",
        "class Node(ABC):\n",
        "    def __init__(self, model: Runnable, toolkit: Toolkit) -> None:\n",
        "        self.toolkit = toolkit\n",
        "        self.model = self._extend_model(model)\n",
        "\n",
        "    def _extend_model(self, model: Runnable) -> Runnable:\n",
        "        # Can be overridden to bind tools, structured output, etc.\n",
        "        return model\n",
        "\n",
        "    def build_user_input_content(self, inputs: Iterable[str], image_urls: list[str] | None = None) -> list[dict[str, Any]]:\n",
        "        \"\"\"Build multimodal input content with optional images.\"\"\"\n",
        "        messages: list[dict[str, Any]] = []\n",
        "        if image_urls:\n",
        "            for image_url in image_urls:\n",
        "                messages.append({\"type\": \"image_url\", \"image_url\": {\"url\": image_url}})\n",
        "        \n",
        "        for input_ in inputs:\n",
        "            messages.append({\"type\": \"text\", \"text\": input_})\n",
        "\n",
        "        return messages\n",
        "\n",
        "    @abstractmethod\n",
        "    async def ainvoke(self) -> Any:\n",
        "        pass\n",
        "```\n",
        "\n",
        "**Key Abstractions:**\n",
        "\n",
        "1. **`ToolCall`**: TypedDict representing a tool call (name, args, id)\n",
        "2. **`Toolkit`**: Manages a collection of tools that can be passed to a node\n",
        "3. **`Node`**: Base class for all workflow nodes\n",
        "   - Takes a model and toolkit\n",
        "   - Can extend the model (bind tools, structured output)\n",
        "   - Supports multimodal input (text + images)\n",
        "   - Abstract `ainvoke` method for execution\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### The MediaGeneratorOrchestrator Node\n",
        "\n",
        "The orchestrator analyzes the article guideline and research to identify what media items need generation, then delegates to specialized worker tools.\n",
        "\n",
        "Key implementation details from `brown.nodes.media_generator.MediaGeneratorOrchestrator`:\n",
        "\n",
        "**1. Class Initialization:**\n",
        "```python\n",
        "class MediaGeneratorOrchestrator(Node):\n",
        "    system_prompt_template = \"...\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        article_guideline: ArticleGuideline,\n",
        "        research: Research,\n",
        "        model: Runnable,\n",
        "        toolkit: Toolkit,\n",
        "    ) -> None:\n",
        "        self.article_guideline = article_guideline\n",
        "        self.research = research\n",
        "        super().__init__(model, toolkit)\n",
        "```\n",
        "\n",
        "**2. Model Extension (Tool Binding):**\n",
        "```python\n",
        "def _extend_model(self, model: Runnable) -> Runnable:\n",
        "    model = cast(BaseChatModel, super()._extend_model(model))\n",
        "    model = model.bind_tools(self.toolkit.get_tools(), tool_choice=\"any\")\n",
        "    return model\n",
        "```\n",
        "\n",
        "The orchestrator binds all available worker tools to the model, allowing it to call multiple tools.\n",
        "\n",
        "**3. Async Invocation:**\n",
        "```python\n",
        "async def ainvoke(self) -> list[ToolCall]:\n",
        "    system_prompt = self.system_prompt_template.format(\n",
        "        article_guideline=self.article_guideline.to_context(),\n",
        "        research=self.research.to_context(),\n",
        "    )\n",
        "    user_input_content = self.build_user_input_content(\n",
        "        inputs=[system_prompt], \n",
        "        image_urls=self.research.image_urls\n",
        "    )\n",
        "    inputs = [{\"role\": \"user\", \"content\": user_input_content}]\n",
        "    response = await self.model.ainvoke(inputs)\n",
        "    \n",
        "    if isinstance(response, AIMessage) and response.tool_calls:\n",
        "        jobs = cast(list[ToolCall], response.tool_calls)\n",
        "    else:\n",
        "        jobs = []\n",
        "    \n",
        "    return jobs\n",
        "```\n",
        "\n",
        "Returns a list of `ToolCall` objects describing what media items to generate.\n",
        "\n",
        "**4. System Prompt (excerpt):**\n",
        "\n",
        "The orchestrator uses a detailed system prompt that instructs it to:\n",
        "- Analyze the article guideline for media requirements\n",
        "- Look for explicit indicators like \"Render a Mermaid diagram\"\n",
        "- Call appropriate worker tools with detailed descriptions\n",
        "- Handle cases where no media is needed\n",
        "\n",
        "Here is the full system prompt attached to the node:\n",
        "```python\n",
        "class MediaGeneratorOrchestrator(Node):\n",
        "    system_prompt_template = \"\"\"You are an Media Generation Orchestrator responsible for analyzing article \n",
        "guidelines and research to identify what media items need to be generated for the article.\n",
        "\n",
        "Your task is to:\n",
        "1. Carefully analyze the article guideline and research content provided\n",
        "2. Identify ALL explicit requests for media items (diagrams, charts, visual illustrations, etc.)\n",
        "3. For each identified media requirement, call the appropriate tool to generate the media item\n",
        "4. Provide clear, detailed descriptions for each media item based on the guideline requirements and research context\n",
        "\n",
        "## Analysis Guidelines\n",
        "\n",
        "**Look for these explicit indicators in the article guideline:**\n",
        "- Direct mentions: \"Render a Mermaid diagram\", \"Draw a diagram\", \"Create a visual\", \"Illustrate with\", etc.\n",
        "- Visual requirements: \"diagram visually explaining\", \"chart showing\", \"figure depicting\", \"visual representation\"\n",
        "- Process flows: descriptions of workflows, architectures, data flows, or system interactions\n",
        "- Structural elements: hierarchies, relationships, comparisons, or step-by-step processes\n",
        "\n",
        "**Key places to look:**\n",
        "- Section requirements and descriptions  \n",
        "- Specific instructions mentioning visual elements\n",
        "- Complex concepts that would benefit from visual explanation\n",
        "- Architecture or system descriptions\n",
        "- Process flows or workflows\n",
        "\n",
        "## Tool Usage Instructions\n",
        "\n",
        "You will call multiple tools to generate the media items. You will use the tool that is most appropriate for the media item you are \n",
        "generating.\n",
        "\n",
        "For each identified media requirement:\n",
        "\n",
        "**When to use MermaidDiagramGenerator:**\n",
        "- Explicit requests for Mermaid diagrams\n",
        "- System architectures and workflows\n",
        "- Process flows and data pipelines  \n",
        "- Organizational structures or hierarchies\n",
        "- Flowcharts for decision-making processes\n",
        "- Sequence diagrams for interactions\n",
        "- Entity-relationship diagrams\n",
        "- Class diagrams for software structures\n",
        "- State diagrams for system states\n",
        "- Mind maps for concept relationships\n",
        "\n",
        "**Description Requirements:**\n",
        "When calling tools, provide detailed descriptions that include:\n",
        "- The specific purpose and context from the article guideline\n",
        "- Key components that should be included based on the research\n",
        "- The type of diagram most appropriate (flowchart, sequence, architecture, etc.)\n",
        "- Specific elements, relationships, or flows to highlight\n",
        "- Any terminology or technical details from the research\n",
        "\n",
        "## Example Analysis Process\n",
        "\n",
        "1. **Scan the guideline** for phrases like:\n",
        "   - \"Render a Mermaid diagram of...\"\n",
        "   - \"Draw a diagram showing...\"\n",
        "   - \"Illustrate the architecture...\"\n",
        "   - \"Visual representation of...\"\n",
        "\n",
        "2. **For each found requirement:**\n",
        "   - Extract the specific context and purpose\n",
        "   - Identify what should be visualized\n",
        "   - Determine the most appropriate diagram type\n",
        "   - Craft a detailed description incorporating research insights\n",
        "\n",
        "3. **Call the appropriate tool** with the comprehensive description\n",
        "\n",
        "## Input Context\n",
        "\n",
        "Here is the article guideline:\n",
        "{article_guideline}\n",
        "\n",
        "Here is the research:\n",
        "{research}\n",
        "\n",
        "## Your Response\n",
        "\n",
        "Analyze the provided article guideline and research, then call the appropriate tools for each \n",
        "identified media item requirement. Each tool call should include a detailed description that ensures \n",
        "the generated media item will be relevant, accurate, and valuable for the article's educational goals.\n",
        "\n",
        "If no explicit media requirements are found in the guideline, respond with: \n",
        "\"No explicit media item requirements found in the article guideline.\"\n",
        "\"\"\"\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### The ToolNode Abstraction\n",
        "\n",
        "Worker tools inherit from `ToolNode`, a special type of `Node` that can be converted into a LangChain tool.\n",
        "\n",
        "From `brown.nodes.base`:\n",
        "\n",
        "```python\n",
        "class ToolNode(Node):\n",
        "    def __init__(self, model: Runnable) -> None:\n",
        "        super().__init__(model, toolkit=Toolkit(tools=[]))\n",
        "\n",
        "    def as_tool(self) -> StructuredTool:\n",
        "        return StructuredTool.from_function(\n",
        "            coroutine=self.ainvoke,\n",
        "            name=f\"{camel_to_snake(self.__class__.__name__)}_tool\",\n",
        "        )\n",
        "\n",
        "    @abstractmethod\n",
        "    async def ainvoke(self) -> Any:\n",
        "        pass\n",
        "```\n",
        "\n",
        "**Key Features:**\n",
        "\n",
        "- Inherits from `Node` but has no tools itself (empty toolkit)\n",
        "- `as_tool()` method converts the node into a LangChain `StructuredTool`\n",
        "- The tool name is automatically derived from the class name\n",
        "- The tool's coroutine is the node's `ainvoke` method\n",
        "\n",
        "This allows any `ToolNode` to be easily integrated as a worker tool in an orchestrator's toolkit.\n",
        "\n",
        "Still, this pattern can easily be extended with the posability of making each node a tool for another node, providing full composability between nodes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### The MermaidDiagramGenerator Worker\n",
        "\n",
        "Before running the `MediaGeneratorOrchestrator`, let's examine a concrete worker implementation from `brown.nodes.tool_nodes.MermaidDiagramGenerator`.\n",
        "\n",
        "**1. Structured Output Model:**\n",
        "```python\n",
        "class GeneratedMermaidDiagram(BaseModel):\n",
        "    content: str = Field(description=\"The Mermaid diagram code formatted in Markdown format as: ```mermaid\\n[diagram content here]\\n```\")\n",
        "    caption: str = Field(description=\"The caption, as a short description of the diagram.\")\n",
        "```\n",
        "\n",
        "**2. Class Structure:**\n",
        "```python\n",
        "class MermaidDiagramGenerator(ToolNode):\n",
        "    prompt_template = \"...\"\n",
        "\n",
        "    def _extend_model(self, model: Runnable) -> Runnable:\n",
        "        model = cast(BaseChatModel, super()._extend_model(model))\n",
        "        model = model.with_structured_output(GeneratedMermaidDiagram, include_raw=False)\n",
        "        return model\n",
        "\n",
        "    async def ainvoke(self, description_of_the_diagram: str, section_title: str) -> MermaidDiagram:\n",
        "        \"\"\"Specialized tool to generate a mermaid diagram from a text description. This tool uses a specialized LLM to\n",
        "        convert a natural language description into a mermaid diagram.\n",
        "\n",
        "        Use this tool when you need to generate a mermaid diagram to explain a concept. Don't confuse mermaid diagrams,\n",
        "        or diagrams in general, with media data, such as images, videos, audio, etc. Diagrams are rendered dynamically\n",
        "        customized for each article, while media data are static data added as URLs from external sources.\n",
        "        This tool is used explicitly to dynamically generate diagrams, not to add media data.\n",
        "\n",
        "        Args:\n",
        "            description_of_the_diagram: Natural language description of the diagram to generate.\n",
        "            section_title: Title of the section that the diagram is for.\n",
        "\n",
        "        Returns:\n",
        "            The generated mermaid diagram code in Markdown format.\n",
        "\n",
        "        Raises:\n",
        "            Exception: If diagram generation fails.\n",
        "\n",
        "        Examples:\n",
        "        >>> description = \"A flowchart showing data flowing from user input to database\"\n",
        "        >>> diagram = await generate_mermaid_diagram(description)\n",
        "        >>> print(diagram)\n",
        "        ```mermaid\n",
        "        graph LR\n",
        "            A[User Input] --> B[Processing]\n",
        "            B --> C[(Database)]\n",
        "        ```\n",
        "        \"\"\"\n",
        "\n",
        "        try:\n",
        "            response = await self.model.ainvoke(\n",
        "                [\n",
        "                    {\n",
        "                        \"role\": \"user\",\n",
        "                        \"content\": self.prompt_template.format(\n",
        "                            description_of_the_diagram=description_of_the_diagram,\n",
        "                        ),\n",
        "                    }\n",
        "                ]\n",
        "            )\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.exception(f\"Failed to generate Mermaid diagram: {e}\")\n",
        "\n",
        "            return MermaidDiagram(\n",
        "                location=section_title,\n",
        "                content=f'```mermaid\\ngraph TD\\n    A[\"Error: Failed to generate diagram\"]\\n    A --> B[\"{str(e)}\"]\\n```',\n",
        "                caption=f\"Error: Failed to generate diagram: {str(e)}\",\n",
        "            )\n",
        "\n",
        "        if not isinstance(response, GeneratedMermaidDiagram):\n",
        "            raise InvalidOutputTypeException(GeneratedMermaidDiagram, type(response))\n",
        "\n",
        "        return MermaidDiagram(\n",
        "            location=section_title,\n",
        "            content=response.content,\n",
        "            caption=response.caption,\n",
        "        )\n",
        "```\n",
        "\n",
        "**Key Features:**\n",
        "\n",
        "- Uses structured output to ensure valid diagram generation\n",
        "- The ainvoke() method has a carefully designed pydoc and signature that will be used when the node it's transformed into a tool used by the orchestrator.\n",
        "- In case of error, we return a placeholder diagram to avoid failing the whole workflow because of a tool failure. Remember that we can run dozens of tools in parallel, thus the change of failure due to things such as rate limits is high.\n",
        "\n",
        "The prompt template contains detailed instructions for generating valid Mermaid diagrams, including:\n",
        "- Syntax rules (always use double quotes, never use semicolons)\n",
        "- Examples of different diagram types (flowcharts, process flows)\n",
        "- Common mistakes to avoid\n",
        "\n",
        "Here is the full prompt template:\n",
        "```python\n",
        "class MermaidDiagramGenerator(ToolNode):\n",
        "    prompt_template = \"\"\"\n",
        "You are a specialized agent that creates clean, readable Mermaid diagrams from text descriptions.\n",
        "\n",
        "## Task\n",
        "Generate a valid Mermaid diagram based on this description:\n",
        "<description_of_the_diagram>\n",
        "{description_of_the_diagram}\n",
        "</description_of_the_diagram>\n",
        "\n",
        "## Output Format\n",
        "Return ONLY the Mermaid code block in this exact format:\n",
        "``mermaid\n",
        "[diagram content here]\n",
        "``\n",
        "\n",
        "## Diagram Types & Examples\n",
        "\n",
        "### Process Flow\n",
        "``mermaid\n",
        "graph LR\n",
        "    A[\"Input\"] --> B[\"Process\"] --> C[\"Output\"]\n",
        "    B --> D[\"Validation\"]\n",
        "    D -->|\"Valid\"| C\n",
        "    D -->|\"Invalid\"| A\n",
        "``mermaid\n",
        "\n",
        "\n",
        "### Flowcharts (Most Common)\n",
        "``mermaid\n",
        "graph TD\n",
        "    A[\"Start\"] --> B{{\"Decision\"}}\n",
        "    B -->|\"Yes\"| C[\"Action 1\"]\n",
        "    B -->|\"No\"| D[\"Action 2\"]\n",
        "    C --> E[\"End\"]\n",
        "    D --> E\n",
        "``mermaid\n",
        "\n",
        "...\n",
        "\n",
        "## Syntax Rules\n",
        "1. **Node Labels**: Use square brackets `[Label]` for rectangular nodes\n",
        "2. **Decisions**: Use curly braces `{{Decision}}` for diamond shapes  \n",
        "3. **Databases**: Use `[(Label)]` for cylindrical database shapes\n",
        "4. **Circles**: Use `((Label))` for circular nodes\n",
        "5. **Arrows**: Use `-->` for solid arrows, `-.->` for dotted arrows\n",
        "6. **Labels on Arrows**: Use `-->|Label|` for labeled connections\n",
        "7. **Subgraphs**: Use `subgraph \"Title\"` and `end` to group elements\n",
        "8. **Comments**: Use `%%` for comments\n",
        "9. **ERD Entities**: Use `ENTITY_NAME {{ field_type field_name }}` format\n",
        "10. **ERD Relationships**: Use `||--o{{`, `||--||`, `}}o--||` for different cardinalities\n",
        "11. **Class Definitions**: Use `class ClassName {{ +type attribute +method() }}` format\n",
        "12. **Class Relationships**: Use `<|--` (inheritance), `-->` (association), `--*` (composition)\n",
        "\n",
        "## Formatting Rules\n",
        "**ALWAYS wrap node labels in double quotes `\"...\"` to prevent parsing errors:**\n",
        "\n",
        "**WRONG** (causes parse errors):\n",
        "h\n",
        "\n",
        "**Key formatting requirements:**\n",
        "- **Always use double quotes** around ALL node labels: `A[\"Label\"]`\n",
        "- **Never use semicolons** at the end of lines (they're optional and can cause issues)\n",
        "- **Quote any label** containing: parentheses `()`, commas `,`, periods `.`, colons `:`, or spaces\n",
        "- **Quote subgraph titles** as well: `subgraph \"Title\"`\n",
        "\n",
        "## Styling Guidelines\n",
        "- **Use default colors only** - do not add color specifications or custom styling\n",
        "- Do not use `fill:`, `stroke:`, `color:` or any CSS styling properties\n",
        "- Keep diagrams clean and professional with standard Mermaid appearance\n",
        "- Focus on structure and clarity, not visual customization\n",
        "\n",
        "## Key Guidelines\n",
        "- Keep node labels concise (avoid parentheses and special characters in labels)\n",
        "- Use clear, logical flow from top to bottom or left to right\n",
        "- Keep the diagram simple and easy to understand\n",
        "- Group related elements with subgraphs when helpful\n",
        "- Maintain consistent spacing and formatting\n",
        "- Choose the appropriate diagram type for the concept being illustrated\n",
        "\n",
        "## Common Mistakes to Avoid\n",
        "- **NEVER use unquoted labels** - always wrap in double quotes: `A[\"Label\"]`\n",
        "- **NEVER use semicolons** at the end of lines (causes parsing issues)\n",
        "- **NEVER put parentheses `()` in unquoted labels** - parser treats them as shape tokens\n",
        "- Don't create overly complex diagrams with too many connections\n",
        "- Avoid extremely long labels that break the visual flow\n",
        "- **Never use custom colors or styling** - stick to Mermaid's default appearance\n",
        "\n",
        "Generate a clean, professional diagram that clearly illustrates the described concept using only default Mermaid \n",
        "styling. Remember: ALWAYS use double quotes around ALL labels and NEVER use semicolons.\n",
        "\"\"\"\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example: Generating Media Items with the Orchestrator\n",
        "\n",
        "Let's see the orchestrator-worker pattern in action by generating Mermaid diagrams:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Analyzing article guideline for media requirements...\n",
            "\n",
            "Found 11 media items to generate:\n",
            "\n",
            "1. Tool: mermaid_diagram_generator_tool\n",
            "   Description: A simple LLM Workflow showing a predefined sequence of steps: starting the process, an LLM call for ...\n",
            "   Section: Understanding the Spectrum: From Workflows to Agents\n",
            "\n",
            "2. Tool: mermaid_diagram_generator_tool\n",
            "   Description: A diagram illustrating a simple Agentic System, highlighting its core components and their interacti...\n",
            "   Section: Understanding the Spectrum: From Workflows to Agents\n",
            "\n",
            "3. Tool: mermaid_diagram_generator_tool\n",
            "   Description: A graph illustrating the trade-off between application reliability and the agent's level of control....\n",
            "   Section: Choosing Your Path\n",
            "\n",
            "4. Tool: mermaid_diagram_generator_tool\n",
            "   Description: A flowchart illustrating the AI generation and human verification loop. The process starts with 'AI ...\n",
            "   Section: Choosing Your Path\n",
            "\n",
            "5. Tool: mermaid_diagram_generator_tool\n",
            "   Description: A Mermaid diagram illustrating the Chaining and Routing pattern for LLM workflows. The diagram shoul...\n",
            "   Section: Exploring Common Patterns\n",
            "\n",
            "6. Tool: mermaid_diagram_generator_tool\n",
            "   Description: A Mermaid diagram illustrating the Orchestrator-Worker pattern for LLM workflows. The diagram should...\n",
            "   Section: Exploring Common Patterns\n",
            "\n",
            "7. Tool: mermaid_diagram_generator_tool\n",
            "   Description: A Mermaid diagram illustrating the Evaluator-Optimizer loop for LLM workflows. The loop starts with ...\n",
            "   Section: Exploring Common Patterns\n",
            "\n",
            "8. Tool: mermaid_diagram_generator_tool\n",
            "   Description: A high-level Mermaid diagram illustrating the dynamics between the core components of an AI agent us...\n",
            "   Section: Exploring Common Patterns\n",
            "\n",
            "9. Tool: mermaid_diagram_generator_tool\n",
            "   Description: A suggestive Mermaid diagram illustrating the Document Summarization and Analysis workflow by Gemini...\n",
            "   Section: Zooming In on Our Favorite Examples\n",
            "\n",
            "10. Tool: mermaid_diagram_generator_tool\n",
            "   Description: A Mermaid diagram illustrating the operational loop of the Gemini CLI coding assistant. The loop beg...\n",
            "   Section: Zooming In on Our Favorite Examples\n",
            "\n",
            "11. Tool: mermaid_diagram_generator_tool\n",
            "   Description: A Mermaid diagram illustrating the iterative multi-step process of Perplexity's Deep Research agent....\n",
            "   Section: Zooming In on Our Favorite Examples\n",
            "\n",
            "Generating media items in parallel...\n",
            "\n",
            "Generated 11 media items successfully!\n",
            "\n",
            "First generated diagram:\n",
            "Location: Understanding the Spectrum: From Workflows to Agents\n",
            "Caption: An LLM Workflow showing a sequence of steps from start to end, including an LLM call for tool invocation and tool execution.\n",
            "\n",
            "Diagram code:\n",
            "```mermaid\n",
            "graph TD\n",
            "    A[\"Start Process\"] --> B[\"LLM Call for Tool Invocation\"]\n",
            "    B --> C[\"Execute Tools\"]\n",
            "    C --> D[\"End Process\"]\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "import asyncio\n",
        "\n",
        "from brown.nodes import MediaGeneratorOrchestrator, MermaidDiagramGenerator, Toolkit\n",
        "from brown.models import get_model, SupportedModels\n",
        "\n",
        "# Create worker tool\n",
        "diagram_model = get_model(SupportedModels.GOOGLE_GEMINI_25_FLASH)\n",
        "mermaid_generator = MermaidDiagramGenerator(model=diagram_model)\n",
        "toolkit = Toolkit(tools=[mermaid_generator.as_tool()])\n",
        "\n",
        "# Create orchestrator\n",
        "orchestrator_model = get_model(SupportedModels.GOOGLE_GEMINI_25_FLASH)\n",
        "orchestrator = MediaGeneratorOrchestrator(\n",
        "    article_guideline=article_guideline,\n",
        "    research=research,\n",
        "    model=orchestrator_model,\n",
        "    toolkit=toolkit,\n",
        ")\n",
        "\n",
        "# Get media generation jobs\n",
        "print(\"Analyzing article guideline for media requirements...\\n\")\n",
        "media_jobs = await orchestrator.ainvoke()\n",
        "\n",
        "print(f\"Found {len(media_jobs)} media items to generate:\\n\")\n",
        "for i, job in enumerate(media_jobs):\n",
        "    print(f\"{i + 1}. Tool: {job['name']}\")\n",
        "    print(f\"   Description: {job['args'].get('description_of_the_diagram', 'N/A')[:100]}...\")\n",
        "    print(f\"   Section: {job['args'].get('section_title', 'N/A')}\\n\")\n",
        "\n",
        "# Execute jobs in parallel\n",
        "print(\"Generating media items in parallel...\")\n",
        "coroutines = []\n",
        "for job in media_jobs:\n",
        "    tool = orchestrator.toolkit.get_tool_by_name(job[\"name\"])\n",
        "    if tool:\n",
        "        coroutines.append(tool.ainvoke(job[\"args\"]))\n",
        "\n",
        "media_items = await asyncio.gather(*coroutines)\n",
        "\n",
        "print(f\"\\nGenerated {len(media_items)} media items successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "First generated diagram:\n",
            "Location: Understanding the Spectrum: From Workflows to Agents\n",
            "Caption: An LLM Workflow showing a sequence of steps from start to end, including an LLM call for tool invocation and tool execution.\n",
            "\n",
            "Diagram code:\n",
            "```mermaid\n",
            "graph TD\n",
            "    A[\"Start Process\"] --> B[\"LLM Call for Tool Invocation\"]\n",
            "    B --> C[\"Execute Tools\"]\n",
            "    C --> D[\"End Process\"]\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "# Show first diagram\n",
        "if media_items:\n",
        "    print(f\"\\nFirst generated diagram:\")\n",
        "    print(f\"Location: {media_items[0].location}\")\n",
        "    print(f\"Caption: {media_items[0].caption}\")\n",
        "    print(f\"\\nDiagram code:\\n{media_items[0].content}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. The ArticleWriter Node: Bringing It All Together\n",
        "\n",
        "The `ArticleWriter` is the heart of the content generation system. It takes all the context we've prepared and generates a high-quality article following all the profile rules.\n",
        "\n",
        "### Class Structure\n",
        "\n",
        "From `brown.nodes.article_writer.ArticleWriter`:\n",
        "\n",
        "**1. Initialization:**\n",
        "```python\n",
        "class ArticleWriter(Node):\n",
        "    system_prompt_template = \"\"\"...\"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        article_guideline: ArticleGuideline,\n",
        "        research: Research,\n",
        "        article_profiles: ArticleProfiles,\n",
        "        media_items: MediaItems,\n",
        "        article_examples: ArticleExamples,\n",
        "        model: Runnable,\n",
        "    ) -> None:\n",
        "        super().__init__(model, toolkit=Toolkit(tools=[]))\n",
        "        \n",
        "        self.article_guideline = article_guideline\n",
        "        self.research = research\n",
        "        self.article_profiles = article_profiles\n",
        "        self.media_items = media_items\n",
        "        self.article_examples = article_examples\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### The ainvoke Method\n",
        "\n",
        "**2. Article Generation Logic:**\n",
        "```python\n",
        "async def ainvoke(self) -> Article | SelectedText:\n",
        "    # Format the system prompt with all context\n",
        "    system_prompt = self.system_prompt_template.format(\n",
        "        article_guideline=self.article_guideline.to_context(),\n",
        "        research=self.research.to_context(),\n",
        "        article_profile=self.article_profiles.article.to_context(),\n",
        "        character_profile=self.article_profiles.character.to_context(),\n",
        "        mechanics_profile=self.article_profiles.mechanics.to_context(),\n",
        "        structure_profile=self.article_profiles.structure.to_context(),\n",
        "        terminology_profile=self.article_profiles.terminology.to_context(),\n",
        "        tonality_profile=self.article_profiles.tonality.to_context(),\n",
        "        media_items=self.media_items.to_context(),\n",
        "        article_examples=self.article_examples.to_context(),\n",
        "    )\n",
        "    \n",
        "    # Build multimodal input (text + images from research)\n",
        "    user_input_content = self.build_user_input_content(\n",
        "        inputs=[system_prompt], \n",
        "        image_urls=self.research.image_urls\n",
        "    )\n",
        "    \n",
        "    inputs = [{\"role\": \"user\", \"content\": user_input_content}]\n",
        "    \n",
        "    # Generate the article\n",
        "    written_output = await self.model.ainvoke(inputs)\n",
        "    if not isinstance(written_output, AIMessage):\n",
        "        raise InvalidOutputTypeException(AIMessage, type(written_output))\n",
        "    written_output = cast(str, written_output.text)\n",
        "    \n",
        "    return Article(content=written_output)\n",
        "```\n",
        "\n",
        "The method:\n",
        "1. Combines all context using `to_context()` methods\n",
        "2. Supports multimodal input with research images]\n",
        "3. Check if we get the expected output. As the article generation is a critical step within our workflow, we want to fail the whole workflow if this node fails.\n",
        "4. Returns an `Article` entity\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### The System Prompt (Key Sections)\n",
        "\n",
        "The `ArticleWriter` uses an extensive system prompt that includes all the context. Here are the key sections (simplified for clarity):\n",
        "\n",
        "**3. System Prompt Structure:**\n",
        "\n",
        "```\n",
        "You are Brown, a professional human writer specialized in writing technical, educative and informational articles\n",
        "about AI.\n",
        "\n",
        "Your task is to write a high-quality article, while providing you the following context:\n",
        "- article guideline: the user intent\n",
        "- research: the factual data\n",
        "- article profile: rules specific to writing articles\n",
        "- character profile: the character you will impersonate\n",
        "- structure profile: Structure rules\n",
        "- mechanics profile: Mechanics rules\n",
        "- terminology profile: Terminology rules\n",
        "- tonality profile: Tonality rules\n",
        "\n",
        "## Character Profile\n",
        "{character_profile}\n",
        "\n",
        "## Research\n",
        "{research}\n",
        "\n",
        "## Article Examples\n",
        "{article_examples}\n",
        "\n",
        "## Tonality Profile\n",
        "{tonality_profile}\n",
        "\n",
        "## Terminology Profile\n",
        "{terminology_profile}\n",
        "\n",
        "## Mechanics Profile\n",
        "{mechanics_profile}\n",
        "\n",
        "## Structure Profile\n",
        "{structure_profile}\n",
        "\n",
        "## Media Items\n",
        "{media_items}\n",
        "\n",
        "## Article Profile\n",
        "{article_profile}\n",
        "\n",
        "## Article Guideline\n",
        "{article_guideline}\n",
        "\n",
        "## Chain of Thought\n",
        "1. Plan the article outline\n",
        "2. Write the article following the article outline and all the other constraints\n",
        "3. Check if all the constraints are respected. Edit the article if not\n",
        "4. Return ONLY the final version of the article\n",
        "```\n",
        "\n",
        "**Key Prompt Engineering Techniques:**\n",
        "\n",
        "1. **Clear Role Definition**: \"You are Brown, a professional human writer...\"\n",
        "2. **Structured Context**: Each piece of context has its own section that is automatically enclosed by XML tags due to the `ContextMixin.to_context()` interface. \n",
        "3. **Priority Guidance**: The \"Article guideline\" which is the user input, always has priority over everything else.\n",
        "4. **Chain of Thought**: Explicit reasoning steps\n",
        "5. **Constraint Adherence**: Multiple reminders to follow all profiles, to be 100% sure the LLM picks up on our core instructions.\n",
        "6. **Grounding in Facts**: \"Anchor exclusively in research, avoid internal knowledge\" to avoid hallucinations\n",
        "\n",
        "This comprehensive prompt ensures the LLM has all the information and instructions it needs to generate high-quality, consistent content.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Complete Workflow Example\n",
        "\n",
        "Now let's put everything together and run the complete workflow to generate an article using our test data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
            "   COMPLETE WORKFLOW: GENERATING AN ARTICLE\n",
            "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
            "\n",
            "✓ Step 1: Context loaded\n",
            "  - Article Guideline: 23127 characters\n",
            "  - Research: 211792 characters\n",
            "  - Profiles: 6 profiles loaded\n",
            "  - Examples: 2 examples\n",
            "\n",
            "✓ Step 2: Media items generated\n",
            "  - Generated 11 Mermaid diagrams\n",
            "\n",
            "⏳ Step 3: Writing article (this may take 1-2 minutes)...\n",
            "✓ Step 3: Article generated (31900 characters)\n",
            "\u001b[93m-------------------------------------------------- Generated Article --------------------------------------------------\u001b[0m\n",
            "  # Workflows vs. Agents: The AI Engineer's Critical Decision\n",
            "### Navigating the Architectural Choices for Production-Ready AI\n",
            "\n",
            "When building AI applications, engineers face a critical architectural decision early in development. Should they create a predictable, step-by-step workflow where every action is controlled, or should they build an autonomous agent that thinks and decides for itself? This choice impacts everything from development time and costs to reliability and user experience.\n",
            "\n",
            "Choosing the wrong approach can lead to significant problems. An overly rigid system might break when users deviate from expected patterns or when developers try to add new features. Conversely, an unpredictable agent could work brilliantly most of the time but fail catastrophically when it matters most. Either scenario risks months of development time, frustrating users, and executives grappling with unexpectedly high operational costs.\n",
            "\n",
            "In 2024 and 2025, many billion-dollar AI startups will succeed or fail based primarily on this architectural decision. Successful companies, teams, and AI engineers understand when to use workflows versus agents, and how to combine both approaches effectively. This lesson will equip you with the knowledge to make these critical decisions with confidence, ensuring your AI applications are not only powerful but also robust, efficient, and safe. You will learn about key architectural patterns, understand real-world examples, and gain practical insights into t\n",
            "\u001b[93m------------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
            "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
            "   WORKFLOW COMPLETE!\n",
            "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "from brown.nodes import ArticleWriter\n",
        "from brown.entities.media_items import MediaItems\n",
        "from brown.renderers import MarkdownArticleRenderer\n",
        "\n",
        "pretty_print.wrapped(\" COMPLETE WORKFLOW: GENERATING AN ARTICLE\", width=100)\n",
        "\n",
        "# Step 1: We already loaded the context (guideline, research, profiles, examples)\n",
        "print(\"\\n✓ Step 1: Context loaded\")\n",
        "print(f\"  - Article Guideline: {len(article_guideline.content)} characters\")\n",
        "print(f\"  - Research: {len(research.content)} characters\")\n",
        "print(f\"  - Profiles: 6 profiles loaded\")\n",
        "print(f\"  - Examples: {len(article_examples.examples)} examples\")\n",
        "\n",
        "# Step 2: Generate media items (we already did this in the previous section)\n",
        "# Let's create a MediaItems container from our generated items\n",
        "print(\"\\n✓ Step 2: Media items generated\")\n",
        "if \"media_items\" in locals() and media_items:\n",
        "    media_items_entity = MediaItems.build(media_items)\n",
        "    print(f\"  - Generated {len(media_items)} Mermaid diagrams\")\n",
        "else:\n",
        "    # For demo purposes, we'll use empty media items\n",
        "    media_items_entity = MediaItems.build([])\n",
        "    print(f\"  - No media items generated (using empty MediaItems)\")\n",
        "\n",
        "# Step 3: Write the article\n",
        "print(\"\\n⏳ Step 3: Writing article (this may take 1-2 minutes)...\")\n",
        "\n",
        "writer_model = get_model(SupportedModels.GOOGLE_GEMINI_25_FLASH)\n",
        "article_writer = ArticleWriter(\n",
        "    article_guideline=article_guideline,\n",
        "    research=research,\n",
        "    article_profiles=article_profiles,\n",
        "    media_items=media_items_entity,\n",
        "    article_examples=article_examples,\n",
        "    model=writer_model,\n",
        ")\n",
        "\n",
        "generated_article = await article_writer.ainvoke()\n",
        "\n",
        "print(f\"✓ Step 3: Article generated ({len(generated_article.content)} characters)\")\n",
        "pretty_print.wrapped(generated_article.content[:1500], title=\"Generated Article\", width=120)\n",
        "pretty_print.wrapped(\" WORKFLOW COMPLETE!\", width=100)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Save the article:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
            "  Article saved to inputs/tests/01_sample/article.md\n",
            "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "output_path = TEST_DIR / \"article.md\"\n",
        "renderer = MarkdownArticleRenderer()\n",
        "renderer.render(generated_article, output_uri=output_path)\n",
        "\n",
        "pretty_print.wrapped(f\"Article saved to {output_path}\", width=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now check out the article and let us know what you think!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Future Steps and Key Takeaways\n",
        "\n",
        "Congratulations! You've just learned the foundations of the Brown writing agent. Let's recap what we covered and look ahead to what's next.\n",
        "\n",
        "### What We've Learned\n",
        "\n",
        "**1. Context Engineering:**\n",
        "- How to structure entities with Pydantic for type safety\n",
        "- The `ContextMixin` pattern for XML-based context representation\n",
        "- Loading markdown content into structured entities\n",
        "- Composing multiple context elements seamlessly\n",
        "\n",
        "**2. The Orchestrator-Worker Pattern:**\n",
        "- `Node` and `ToolNode` base abstractions\n",
        "- `Toolkit` for managing collections of tools\n",
        "- `MediaGeneratorOrchestrator` analyzing requirements and delegating tasks\n",
        "- `MermaidDiagramGenerator` as a specialized worker\n",
        "- Parallel execution of multiple worker tasks\n",
        "\n",
        "**3. The ArticleWriter Node:**\n",
        "- Comprehensive system prompt engineering\n",
        "- Integration of all context components\n",
        "- Multimodal input support (text + images)\n",
        "- Profile-driven content generation\n",
        "\n",
        "**4. Supporting Infrastructure:**\n",
        "- Flexible model configuration with `ModelConfig`\n",
        "- Settings management with Pydantic\n",
        "- Loaders and renderers for I/O operations\n",
        "- Factory patterns for building components\n",
        "\n",
        "### Key Design Principles\n",
        "\n",
        "Throughout this lesson, we've seen several important design principles:\n",
        "\n",
        "1. **Separation of Concerns**: Each component has a single, well-defined responsibility\n",
        "2. **Composition Over Inheritance**: Entities compose context, nodes compose functionality\n",
        "3. **Type Safety**: Pydantic models ensure data integrity\n",
        "4. **Abstraction**: Base classes provide consistent interfaces\n",
        "5. **Flexibility**: Easy to swap models, profiles, or workers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Ideas for Extension\n",
        "\n",
        "Now that you understand the foundations, here are some ways you could extend this system:\n",
        "\n",
        "**1. Additional Media Workers:**\n",
        "- `ImageGenerator`: Generate images using Google's image generation features\n",
        "- `ChartGenerator`: Create data visualizations with matplotlib or plotly\n",
        "\n",
        "**2. Content Type Variations:**\n",
        "- Social media post generator (using different profiles)\n",
        "- Email newsletter generator\n",
        "- Technical documentation generator\n",
        "- Video transcript generator\n",
        "\n",
        "**3. Reduce Costs and Latency:**\n",
        "- Cache constant inputs between LLM calls such as the research and profiles to avoid recomputing them\n",
        "- Compress the research relative to the article guideline\n",
        "\n",
        "**4. Advanced Profiles:**\n",
        "- Industry-specific profiles (finance, healthcare, education)\n",
        "- Add your own character profile instead of ours based on Paul Iusztin\n",
        "\n",
        "### What's Next in the Course\n",
        "\n",
        "In the upcoming lesson, we'll explore:\n",
        "\n",
        "**Lesson 23: The Evaluator-Optimizer Pattern**\n",
        "- How to automatically review and edit generated articles\n",
        "- Implementing the `ArticleReviewer` node\n",
        "- Glue everything together into a LangGraph workflow\n",
        "\n",
        "### Resources\n",
        "\n",
        "- **Brown Package Documentation**: Explore `../writing_workflow/README.md`\n",
        "- **Profile Examples**: Check `inputs/profiles/` for more profile templates\n",
        "- **Test Data**: Use `inputs/tests/` for additional examples\n",
        "- **Configuration**: Review `configs/` for workflow configuration options\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
