{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 28: Creating Datasets for AI Evals\n",
    "\n",
    "In this lesson, we'll explore how to create an evaluation dataset for Brown, the writing workflow.\n",
    "\n",
    "**Learning Objectives:**\n",
    "\n",
    "- Understand the structure and format of evaluation datasets for article generation\n",
    "- Learn how to use the `EvalDataset` and `EvalSample` entities to load and manage evaluation data\n",
    "- Upload evaluation datasets to Opik for tracking and analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [!NOTE]\n",
    "> ðŸ’¡ Remember that you can also run `brown` as a standalone Python package by going to `lessons/writing_workflow/` and following the instructions from there. We have a script at `lessons/writing_workflow/scripts/brown_create_eval_dataset.py` that you can use to upload datasets to Opik as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "First, we define some standard Magic Python commands to autoreload Python packages whenever they change:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up Python Environment\n",
    "\n",
    "To set up your Python virtual environment using `uv` and load it into the Notebook, follow the step-by-step instructions from the `Course Admin` lesson from the beginning of the course.\n",
    "\n",
    "**TL/DR:** Be sure the correct kernel pointing to your `uv` virtual environment is selected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Opik\n",
    "\n",
    "To configure Opik, follow the step-by-step instructions in the `Course Admin` lesson.\n",
    "\n",
    "Here is a quick checklist of what you need to run this notebook:\n",
    "\n",
    "1.  Get your key from [Opik](https://www.comet.com/site/products/opik/).\n",
    "2.  From the root of your project, run: `cp .env.example .env` \n",
    "3.  Within the `.env` file, fill in the `OPIK_API_KEY` variable:\n",
    "\n",
    "Now, the code below will load the key from the `.env` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment variables loaded from `/Users/pauliusztin/Documents/01_projects/TAI/agentic-ai-engineering-course/.env`\n",
      "Environment variables loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "from utils import env\n",
    "\n",
    "env.load(required_env_vars=[\"OPIK_API_KEY\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Key Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "from utils import pretty_print\n",
    "\n",
    "nest_asyncio.apply()  # Allow nested async usage in notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Required Files\n",
    "\n",
    "First, let's download the configs folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "!rm -rf configs\n",
    "!curl -L -o configs.zip https://raw.githubusercontent.com/iusztinpaul/agentic-ai-engineering-course-data/main/data/configs.zip\n",
    "!unzip configs.zip\n",
    "!rm -rf configs.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to download the inputs folder containing the dataset files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "!rm -rf inputs\n",
    "!curl -L -o inputs.zip https://raw.githubusercontent.com/iusztinpaul/agentic-ai-engineering-course-data/main/data/inputs.zip\n",
    "!unzip inputs.zip\n",
    "!rm -rf inputs.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify what we downloaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[36mconfigs\u001b[m\u001b[m/        \u001b[1m\u001b[36minputs\u001b[m\u001b[m/         notebook.ipynb\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs directory exists: True\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "INPUTS_DIR = Path(\"inputs\")\n",
    "\n",
    "print(f\"Inputs directory exists: {INPUTS_DIR.exists()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples directory exists: True\n"
     ]
    }
   ],
   "source": [
    "EVALS_DATASET_DIR = Path(\"inputs/evals\")\n",
    "\n",
    "print(f\"Examples directory exists: {EVALS_DATASET_DIR.exists()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploring The Evals Dataset Dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m-------------------------------------- Evals Dataset Metadata --------------------------------------\u001b[0m\n",
      "  [\n",
      "    {\n",
      "        \"name\": \"Lesson 2: Workflows vs. Agents\",\n",
      "        \"directory\": \"data/02_workflows_vs_agents\",\n",
      "        \"article_guideline_path\": \"article_guideline.md\",\n",
      "        \"research_path\": \"research.md\",\n",
      "        \"ground_truth_article_path\": \"article_ground_truth.md\"\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"Lesson 3: Context Engineering\",\n",
      "        \"directory\": \"data/03_context_engineering\"\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"Lesson 4: Structured Outputs\",\n",
      "        \"directory\": \"data/04_structured_outputs\",\n",
      "        \"is_few_shot_example\": true\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"Lesson 5: Workflow Patterns\",\n",
      "        \"directory\": \"data/05_workflow_patterns\"\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"Lesson 6: Tools\",\n",
      "        \"directory\": \"data/06_tools\"\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"Lesson 7: Planning and Reasoning\",\n",
      "        \"directory\": \"data/07_reasoning_planning\",\n",
      "        \"is_few_shot_example\": true\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"Lesson 8: ReAct Practice\",\n",
      "        \"directory\": \"data/08_react_practice\"\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"Lesson 9: Retrieval-Augmented Generation (RAG)\",\n",
      "        \"directory\": \"data/09_RAG\"\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"Lesson 10: Memory\",\n",
      "        \"directory\": \"data/10_memory_knowledge_access\"\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"Lesson 11: Multimodal Data\",\n",
      "        \"directory\": \"data/11_multimodal\"\n",
      "    }\n",
      "]\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "metadata_path = EVALS_DATASET_DIR / \"dataset\" / \"metadata.json\"\n",
    "with open(metadata_path) as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "pretty_print.wrapped(json.dumps(metadata, indent=4), title=\"Evals Dataset Metadata\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a deeper look at our dataset, starting with it's overall structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m----------------------------------- Evals Dataset Data Directory -----------------------------------\u001b[0m\n",
      "  {\n",
      "    \"dataset_directory\": \"inputs/evals\",\n",
      "    \"metadata_file\": \"inputs/evals/dataset/metadata.json\",\n",
      "    \"data_directory\": \"inputs/evals/dataset/data\",\n",
      "    \"article_samples\": 10\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "data_dir = EVALS_DATASET_DIR / \"dataset\" / \"data\"\n",
    "\n",
    "pretty_print.wrapped(\n",
    "    json.dumps(\n",
    "        {\n",
    "            \"dataset_directory\": str(EVALS_DATASET_DIR),\n",
    "            \"metadata_file\": str(metadata_path),\n",
    "            \"data_directory\": str(data_dir),\n",
    "            \"article_samples\": len(list(data_dir.iterdir())),\n",
    "        },\n",
    "        indent=4,\n",
    "    ),\n",
    "    title=\"Evals Dataset Data Directory\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's look at each sample individually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "                                      ARTICLE SAMPLES\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "02_workflows_vs_agents/\n",
      "  - article_ground_truth.md\n",
      "  - article_guideline.md\n",
      "  - research.md\n",
      "03_context_engineering/\n",
      "  - article_ground_truth.md\n",
      "  - article_guideline.md\n",
      "  - research.md\n",
      "04_structured_outputs/\n",
      "  - article_generated.md\n",
      "  - article_ground_truth.md\n",
      "  - article_guideline.md\n",
      "  - research.md\n",
      "05_workflow_patterns/\n",
      "  - article_ground_truth.md\n",
      "  - article_guideline.md\n",
      "  - research.md\n",
      "06_tools/\n",
      "  - article_ground_truth.md\n",
      "  - article_guideline.md\n",
      "  - research.md\n",
      "07_reasoning_planning/\n",
      "  - article_generated.md\n",
      "  - article_ground_truth.md\n",
      "  - article_guideline.md\n",
      "  - research.md\n",
      "08_react_practice/\n",
      "  - article_ground_truth.md\n",
      "  - article_guideline.md\n",
      "  - research.md\n",
      "09_RAG/\n",
      "  - article_ground_truth.md\n",
      "  - article_guideline.md\n",
      "  - research.md\n",
      "10_memory_knowledge_access/\n",
      "  - article_ground_truth.md\n",
      "  - article_guideline.md\n",
      "  - research.md\n",
      "11_multimodal/\n",
      "  - article_ground_truth.md\n",
      "  - article_guideline.md\n",
      "  - research.md\n"
     ]
    }
   ],
   "source": [
    "pretty_print.wrapped(\"ARTICLE SAMPLES\", indent=38)\n",
    "\n",
    "for sample_dir in sorted(data_dir.iterdir()):\n",
    "    if sample_dir.is_dir():\n",
    "        files = [f.name for f in sample_dir.iterdir() if f.is_file()]\n",
    "        print(f\"{sample_dir.name}/\")\n",
    "        for f in sorted(files):\n",
    "            print(f\"  - {f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Uploading The Evals Dataset To Opik\n",
    "\n",
    "We will quickly go over the code used to upload the dataset described above to Opik. The code is pretty minimal. Thus, we will keep it short."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 The EvalSample Entity\n",
    "\n",
    "The `EvalSample` entity is a Pydantic model that represents a single evaluation sample containing all the data needed for article generation evaluation.\n",
    "\n",
    "Source: `brown.evals.dataset`\n",
    "```python\n",
    "class EvalSample(BaseModel):\n",
    "    name: str\n",
    "    directory: Path\n",
    "    article_guideline: str\n",
    "    research: str\n",
    "    ground_truth_article: str\n",
    "    is_few_shot_example: bool = False\n",
    "```\n",
    "\n",
    "Each sample contains:\n",
    "- `name`: A human-readable identifier for the sample\n",
    "- `directory`: The path where the sample files are located\n",
    "- `article_guideline`: The writing guidelines in markdown format\n",
    "- `research`: The research/source material in markdown format\n",
    "- `ground_truth_article`: The reference article to compare against\n",
    "- `is_few_shot_example`: Whether this sample is used for few-shot learning instead of evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 The EvalDataset Entity\n",
    "\n",
    "The `EvalDataset` entity is a Pydantic model that represents a collection of evaluation samples along with dataset metadata.\n",
    "\n",
    "Source: `brown.evals.dataset`\n",
    "```python\n",
    "class EvalDataset(BaseModel):\n",
    "    name: str\n",
    "    description: str\n",
    "    samples: list[EvalSample]\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset(cls, directory: Path, name: str, description: str) -> Self:\n",
    "        metadata_file = directory / \"metadata.json\"\n",
    "        if not metadata_file.exists():\n",
    "            raise FileNotFoundError(f\"Metadata file not found: {metadata_file}\")\n",
    "\n",
    "        with metadata_file.open() as f:\n",
    "            metadata = json.load(f)\n",
    "\n",
    "        samples = []\n",
    "        for sample_metadata in metadata:\n",
    "            sample_dir = directory / sample_metadata[\"directory\"]\n",
    "\n",
    "            article_guideline = cls._load_markdown_file(\n",
    "                sample_dir / sample_metadata.get(\"article_guideline_path\", DEFAULT_ARTICLE_GUIDELINE_PATH)\n",
    "            )\n",
    "            research = cls._load_markdown_file(sample_dir / sample_metadata.get(\"research_path\", DEFAULT_RESEARCH_PATH))\n",
    "            ground_truth_article = cls._load_markdown_file(\n",
    "                sample_dir / sample_metadata.get(\"ground_truth_article_path\", DEFAULT_GROUND_TRUTH_ARTICLE_PATH)\n",
    "            )\n",
    "\n",
    "            sample = EvalSample(\n",
    "                name=sample_metadata[\"name\"],\n",
    "                directory=sample_metadata[\"directory\"],\n",
    "                is_few_shot_example=sample_metadata.get(\"is_few_shot_example\", False),\n",
    "                article_guideline=article_guideline,\n",
    "                research=research,\n",
    "                ground_truth_article=ground_truth_article,\n",
    "            )\n",
    "            samples.append(sample)\n",
    "\n",
    "        return cls(name=name, description=description, samples=samples)\n",
    "```\n",
    "\n",
    "The `load_dataset` class method:\n",
    "- Reads the `metadata.json` file from the specified directory\n",
    "- Iterates through each sample entry and loads the corresponding markdown files\n",
    "- Creates `EvalSample` instances for each entry\n",
    "- Returns a fully populated `EvalDataset` ready for use\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 The upload_dataset Function\n",
    "\n",
    "The `upload_dataset` function uploads the evaluation dataset to the Opik observability platform for tracking and analysis.\n",
    "\n",
    "Source: `brown.observability.dataset`\n",
    "```python\n",
    "def upload_dataset(evaluation_dataset: \"EvalDataset\") -> None:\n",
    "    samples = evaluation_dataset.model_dump(mode=\"json\")[\"samples\"]\n",
    "    eval_samples = [sample for sample in samples if not sample[\"is_few_shot_example\"]]\n",
    "    logger.info(f\"Uploading `{len(eval_samples)}/{len(samples)}` evaluation samples to Opik.\")\n",
    "    training_samples = [sample for sample in samples if sample[\"is_few_shot_example\"]]\n",
    "    logger.info(f\"The following `{len(training_samples)}/{len(samples)}` samples will be used for training or as few-shot examples:\")\n",
    "    for sample in training_samples:\n",
    "        logger.info(f\"- `{sample['name']}`\")\n",
    "\n",
    "    opik_utils.update_or_create_dataset(\n",
    "        name=evaluation_dataset.name,\n",
    "        description=evaluation_dataset.description,\n",
    "        items=eval_samples,\n",
    "    )\n",
    "```\n",
    "\n",
    "The function:\n",
    "- Separates samples into evaluation samples and few-shot examples based on the `is_few_shot_example` flag\n",
    "- Only uploads evaluation samples to Opik (few-shot examples are used by the LLM judge. Thus, to avoid data leakage, we cannot compute metrics on them)\n",
    "\n",
    "While the `update_or_create_dataset` function handles updating the dataset on Opik.\n",
    "\n",
    "Source: `brown.observability.opik_utils`\n",
    "```python\n",
    "import opik\n",
    "\n",
    "def update_or_create_dataset(name: str, description: str, items: list[dict]) -> opik.Dataset:\n",
    "    \"\"\"\n",
    "    Update an existing dataset or create a new one if it doesn't exist.\n",
    "\n",
    "    Args:\n",
    "        name: The name of the dataset to update or create.\n",
    "        description: The description of the dataset.\n",
    "        items: The items to insert into the dataset.\n",
    "\n",
    "    Returns:\n",
    "        opik.Dataset: The updated or created dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    client = opik.Opik()\n",
    "    dataset = client.get_or_create_dataset(name=name, description=description)\n",
    "    dataset.clear()\n",
    "\n",
    "    dataset.insert(items)\n",
    "\n",
    "    return dataset\n",
    "```\n",
    "\n",
    "This is a simple function that gets or creates a dataset on Opik based on its name. Then it clears the dataset and reinserts all the items. As our dataset is small, doing this reinsertion every time works fine, making it a good strategy to avoid duplicates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Loading and Uploading the Dataset\n",
    "\n",
    "Now let's use the `EvalDataset` entity to load our evaluation dataset and upload it to Opik. First, let's reference our dataset directory:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('inputs/evals')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EVALS_DATASET_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_EVALS_DATASET_DIR = EVALS_DATASET_DIR / \"dataset\"\n",
    "DATASET_NAME = \"brown-course-lessons\"\n",
    "DATASET_DESCRIPTION = \"Brown evaluation dataset on course lessons format.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's load the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m----------------------------------------- Dataset Metadata -----------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"dataset_name\": \"brown-course-lessons\",\n",
      "  \"dataset_description\": \"Brown evaluation dataset on course lessons format.\",\n",
      "  \"dataset_samples\": 10\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from brown.evals.dataset import EvalDataset\n",
    "from brown.observability import upload_dataset\n",
    "from loguru import logger\n",
    "\n",
    "dataset = EvalDataset.load_dataset(INPUT_EVALS_DATASET_DIR, name=DATASET_NAME, description=DATASET_DESCRIPTION)\n",
    "\n",
    "pretty_print.wrapped(\n",
    "    {\n",
    "        \"dataset_name\": dataset.name,\n",
    "        \"dataset_description\": dataset.description,\n",
    "        \"dataset_samples\": len(dataset.samples),\n",
    "    },\n",
    "    title=\"Dataset Metadata\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's upload the dataset to Opik:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-12-23 14:47:11.863\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m1\u001b[0m - \u001b[1mUploading dataset to Opik: `brown-course-lessons`\u001b[0m\n",
      "\u001b[32m2025-12-23 14:47:11.864\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mbrown.observability.dataset\u001b[0m:\u001b[36mupload_dataset\u001b[0m:\u001b[36m25\u001b[0m - \u001b[1mUploading `8/10` evaluation samples to Opik.\u001b[0m\n",
      "\u001b[32m2025-12-23 14:47:11.864\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mbrown.observability.dataset\u001b[0m:\u001b[36mupload_dataset\u001b[0m:\u001b[36m27\u001b[0m - \u001b[1mThe following `2/10` samples will be used for training or as few-shot examples:\u001b[0m\n",
      "\u001b[32m2025-12-23 14:47:11.865\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mbrown.observability.dataset\u001b[0m:\u001b[36mupload_dataset\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1m- `Lesson 4: Structured Outputs`\u001b[0m\n",
      "\u001b[32m2025-12-23 14:47:11.865\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mbrown.observability.dataset\u001b[0m:\u001b[36mupload_dataset\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1m- `Lesson 7: Planning and Reasoning`\u001b[0m\n",
      "\u001b[32m2025-12-23 14:47:15.640\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m3\u001b[0m - \u001b[32m\u001b[1mSuccessfully uploaded dataset to Opik: `brown-course-lessons`\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "logger.info(f\"Uploading dataset to Opik: `{dataset.name}`\")\n",
    "upload_dataset(dataset)\n",
    "logger.success(f\"Successfully uploaded dataset to Opik: `{dataset.name}`\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Conclusion\n",
    "\n",
    "In this lesson, we learned how to create and manage evaluation datasets for the Brown writing workflow.\n",
    "\n",
    "In the next lesson, we'll use this dataset to run evaluations and measure the quality of Brown.\n",
    "\n",
    "### Practicing Ideas\n",
    "\n",
    "1. Extend the dataset with more diverse article samples.\n",
    "2. Change the dataset with a new set of articles that follow your format instead of our course lesson format.\n",
    "3. Do your own AI evals dataset on a different data format, such as social media posts.\n",
    "4. Update the `update_or_create_dataset` to stop `clearing` the dataset entirely when adding new items by introducing a mechanism to detect dataset item duplicates.\n",
    "5. Use Opik to version the dataset when changing it in any way, such as adding, removing or changing dataset samples.\n",
    "\n",
    "> [!NOTE]\n",
    "> ðŸ’¡ Remember that you can also run `brown` as a standalone Python package by going to `lessons/writing_workflow/` and following the instructions from there. We have a script at `lessons/writing_workflow/scripts/brown_create_eval_dataset.py` that you can use to upload datasets to Opik as well.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
