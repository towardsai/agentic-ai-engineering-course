{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3974a398",
   "metadata": {},
   "source": [
    "# Lesson 9: Memory for Agents\n",
    "\n",
    "This lesson explores the concept of adding **long-term memory** to agents, so they can persist and retrieve information over time. \n",
    "\n",
    "We’ll implement semantic, episodic, and procedural memory using the open-source mem0 library with Google's Gemini text embedding model, and a vector store that runs locally in the notebook, using ChromaDB. \n",
    "\n",
    "\n",
    "Learning Objectives:\n",
    "\n",
    "1. Understand the different types of memory \n",
    "2. How to implement them, using the mem0 library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce8c3ae",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "First, we define some standard Magic Python commands to autoreload Python packages whenever they change:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7eca0f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854700aa",
   "metadata": {},
   "source": [
    "### Set Up Python Environment\n",
    "\n",
    "To set up your Python virtual environment using `uv` and use it in the Notebook, follow the step-by-step instructions from the [Course Admin](https://academy.towardsai.net/courses/take/agent-engineering/multimedia/67469688-lesson-1-part-2-course-admin) lesson from the beginning of the course.\n",
    "\n",
    "**TL/DR:** Be sure the correct kernel pointing to your `uv` virtual environment is selected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc69dfa",
   "metadata": {},
   "source": [
    "### Configure Gemini API\n",
    "\n",
    "To configure the Gemini API, follow the step-by-step instructions from the [Course Admin](https://academy.towardsai.net/courses/take/agent-engineering/multimedia/67469688-lesson-1-part-2-course-admin) lesson.\n",
    "\n",
    "But here is a quick check on what you need to run this Notebook:\n",
    "\n",
    "1.  Get your key from [Google AI Studio](https://aistudio.google.com/app/apikey).\n",
    "2.  From the root of your project, run: `cp .env.example .env` \n",
    "3.  Within the `.env` file, fill in the `GOOGLE_API_KEY` variable:\n",
    "\n",
    "Now, the code below will load the key from the `.env` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334f7f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to load environment variables from `/Users/omar/Documents/ai_repos/course-ai-agents/.env`\n",
      "Environment variables loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "from lessons.utils import env\n",
    "\n",
    "env.load(required_env_vars=[\"GOOGLE_API_KEY\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fd0942",
   "metadata": {},
   "source": [
    "### Import Key Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9cb7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from typing import Optional\n",
    "\n",
    "from google import genai\n",
    "from mem0 import Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5708a96",
   "metadata": {},
   "source": [
    "### Initialize the Gemini Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d28218",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = genai.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4388a20",
   "metadata": {},
   "source": [
    "### Define Constants\n",
    "\n",
    "We will use the `gemini-2.5-flash` model, which is fast and cost-effective:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4feb5354",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"gemini-2.5-flash\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc334dc7",
   "metadata": {},
   "source": [
    "### Configure mem0 (Gemini LLM + embeddings + local vector store)\n",
    "\n",
    "Here we instantiate mem0 with:\n",
    "\n",
    "- LLM: our existing Gemini model (`MODEL_ID = \"gemini-2.5-flash\"`) for the summarization/extraction of facts.\n",
    "- Embeddings: Gemini’s `text-embedding-004` (dimension 768).\n",
    "- Vector store:\n",
    "    - ChromaDB with `MEM_BACKEND=chromadb` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb81ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Mem0 ready (Gemini embeddings + in-memory Chroma).\n"
     ]
    }
   ],
   "source": [
    "MEM0_CONFIG = {\n",
    "    # Use Google's text-embedding-004 (768-dim) for embeddings\n",
    "    \"embedder\": {\n",
    "        \"provider\": \"gemini\",\n",
    "        \"config\": {\n",
    "            \"model\": \"text-embedding-004\",\n",
    "            \"embedding_dims\": 768,\n",
    "            \"api_key\": os.getenv(\"GOOGLE_API_KEY\"),\n",
    "        },\n",
    "    },\n",
    "    # Use ChromaDB as a local, in-notebook vector store (ephemeral, in-memory)\n",
    "    \"vector_store\": {\n",
    "        \"provider\": \"chroma\",\n",
    "        \"config\": {\n",
    "            \"collection_name\": \"lesson9_memories\",\n",
    "        },\n",
    "    },\n",
    "    \"llm\": {\n",
    "        \"provider\": \"gemini\",\n",
    "        \"config\": {\n",
    "            \"model\": MODEL_ID,\n",
    "            \"api_key\": os.getenv(\"GOOGLE_API_KEY\"),\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "memory = Memory.from_config(MEM0_CONFIG)\n",
    "MEM_USER_ID = \"lesson9_notebook_student\"\n",
    "memory.delete_all(user_id=MEM_USER_ID)\n",
    "print(\"✅ Mem0 ready (Gemini embeddings + in-memory Chroma).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356970b6",
   "metadata": {},
   "source": [
    "### Helper functions: add/search memory\n",
    "\n",
    "A small wrapper layer around mem0 to:\n",
    "\n",
    "- **Save** a memory string (or a short conversation) and tag it with a `category` (`semantic`, `episodic`, `procedure`) plus metadata.\n",
    "    - `mem_add_text` will store the raw string, with `infer=False` (no LLM used)\n",
    "    - `mem_add_conversation` will store an LLM-generated summary (gemini-2.5-flash in our case) of the conversation with `infer=True`\n",
    "\n",
    "- **Search** memory and return results for display. It can also filter by category.\n",
    "\n",
    "For the metadata, mem0 only allows primitive types (str, int, float, bool, None) that is why we convert to `str` any non-primitive values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de83b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mem_add_text(text: str, category: str = \"semantic\", **meta) -> str:\n",
    "    \"\"\"Add a single text memory. No LLM is used for extraction or summarization.\"\"\"\n",
    "    metadata = {\"category\": category}\n",
    "    for k, v in meta.items():\n",
    "        if isinstance(v, (str, int, float, bool)) or v is None:\n",
    "            metadata[k] = v\n",
    "        else:\n",
    "            metadata[k] = str(v)\n",
    "    memory.add(text, user_id=MEM_USER_ID, metadata=metadata, infer=False)\n",
    "    return f\"Saved {category} memory.\"\n",
    "\n",
    "\n",
    "def mem_add_conversation(messages: list[dict], category: str = \"episodic\", **meta) -> str:\n",
    "    \"\"\"Add a conversation (list of {role, content}) as one episode.\"\"\"\n",
    "    metadata = {\"category\": category}\n",
    "    for k, v in meta.items():\n",
    "        metadata[k] = v if isinstance(v, (str, int, float, bool)) or v is None else str(v)\n",
    "    memory.add(messages, user_id=MEM_USER_ID, metadata=metadata, infer=True)\n",
    "    return f\"Saved {category} episode.\"\n",
    "\n",
    "\n",
    "def mem_search(query: str, limit: int = 5, category: Optional[str] = None) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Category-aware search wrapper.\n",
    "    Returns the full result dicts so we can inspect metadata.\n",
    "    \"\"\"\n",
    "    res = memory.search(query, user_id=MEM_USER_ID, limit=limit) or {}\n",
    "    items = res.get(\"results\", [])\n",
    "    if category is not None:\n",
    "        items = [r for r in items if (r.get(\"metadata\") or {}).get(\"category\") == category]\n",
    "    return items"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b21d67e",
   "metadata": {},
   "source": [
    "## 2. Semantic memory example (facts as atomic strings)\n",
    "\n",
    "**Goal**: We show semantic memory as “facts & preferences” stored as short, individual strings.\n",
    "\n",
    "- We insert a few example facts (e.g., “User has a dog named George”).\n",
    "\n",
    "- Then we search with a natural query (e.g., “brother job”) and see the relevant fact returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d46ea78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved semantic memory.\n",
      "Saved semantic memory.\n",
      "Saved semantic memory.\n",
      "Saved semantic memory.\n",
      "\n",
      "Search --> 'brother job':\n",
      "- {'id': 'c6f0edbc-e0cc-40a2-abb4-d3f364b009ba', 'memory': \"User's brother is named Mark and is a software engineer.\", 'hash': '9a01dbd8ea8b96f8ed9c84e9dcdb55a1', 'metadata': {'category': 'semantic'}, 'score': 0.9269160032272339, 'created_at': '2025-08-25T17:26:22.183416-07:00', 'updated_at': None, 'user_id': 'lesson9_notebook_student', 'role': 'user'}\n",
      "- {'id': 'bd98bf8d-8d85-4103-87b8-d6b32b8b9eb9', 'memory': 'User has a dog named George.', 'hash': '8c592a46b362bab2fd20a1a2a9214d74', 'metadata': {'category': 'semantic'}, 'score': 1.4484589099884033, 'created_at': '2025-08-25T17:26:21.261837-07:00', 'updated_at': None, 'user_id': 'lesson9_notebook_student', 'role': 'user'}\n",
      "- {'id': 'ce5ccb85-60d2-4a67-8361-be990f9100f2', 'memory': 'User prefers vegetarian meals.', 'hash': '0034073d2dbe31e303972a0599565525', 'metadata': {'category': 'semantic'}, 'score': 1.5473814010620117, 'created_at': '2025-08-25T17:26:20.721037-07:00', 'updated_at': None, 'user_id': 'lesson9_notebook_student', 'role': 'user'}\n"
     ]
    }
   ],
   "source": [
    "facts: list[str] = [\n",
    "    \"User prefers vegetarian meals.\",\n",
    "    \"User has a dog named George.\",\n",
    "    \"User is allergic to gluten.\",\n",
    "    \"User's brother is named Mark and is a software engineer.\",\n",
    "]\n",
    "for f in facts:\n",
    "    print(mem_add_text(f, category=\"semantic\"))\n",
    "\n",
    "print(\"\\nSearch --> 'brother job':\")\n",
    "print(\"\\n\".join(f\"- {m}\" for m in mem_search(\"brother job\", limit=3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c54022",
   "metadata": {},
   "source": [
    "## 3. Episodic memory example (summarize 3–4 turns → one episode)\n",
    "\n",
    "**Goal**: Demonstrate episodic memory (experiences & history).\n",
    "\n",
    "- We create a short 3–4 turn exchange between user and assistant.\n",
    "\n",
    "- We ask the LLM to produce a concise episode summary (1–2 sentences) and save it under category=\"episodic\".\n",
    "\n",
    "- Finally, we run a semantic search (e.g., “deadline stress”) to retrieve that episode, we print the memory along with its creation timestamp.\n",
    "\n",
    "This example show how an agent can compress transient chat into a single durable “moment.”\n",
    "\n",
    "Since mem0 by default creates a created_at timestamp, we have the possibility to use it to sort and filter memories.\n",
    "It would then be possible to answer questions like \"What did we talk about last week?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe99984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved episodic memory.\n",
      "\n",
      "Search --> 'deadline stress'\n",
      "- [created_at=2025-08-25T17:26:28.260452-07:00] Stressed about a looming Friday project deadline, the user identified testing as their main blocker and noted a preference for working at night. The assistant offered support by proposing they split the testing into two sessions.\n"
     ]
    }
   ],
   "source": [
    "# A short 4-turn exchange we want to compress into one \"episode\"\n",
    "dialogue = [\n",
    "    {\"role\": \"user\", \"content\": \"I'm stressed about my project deadline on Friday.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"I’m here to help—what’s the blocker?\"},\n",
    "    {\"role\": \"user\", \"content\": \"Mainly testing. I also prefer working at night.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Okay, we can split testing into two sessions.\"},\n",
    "]\n",
    "\n",
    "# Ask the LLM to write a clear episodic summary.\n",
    "episodic_prompt = f\"\"\"Summarize the following 3–4 turns as one concise 'episode' (1–2 sentences).\n",
    "Keep salient details and tone.\n",
    "\n",
    "{dialogue}\n",
    "\"\"\"\n",
    "summary_resp = client.models.generate_content(model=MODEL_ID, contents=episodic_prompt)\n",
    "episode = summary_resp.text.strip()\n",
    "\n",
    "print(\n",
    "    mem_add_text(\n",
    "        episode,\n",
    "        category=\"episodic\",\n",
    "        summarized=True,\n",
    "        turns=4,\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"\\nSearch --> 'deadline stress'\")\n",
    "hits = mem_search(\"deadline stress\", limit=3, category=\"episodic\")\n",
    "for h in hits:\n",
    "    print(f\"- [created_at={h.get('created_at')}] {h['memory']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d94159",
   "metadata": {},
   "source": [
    "## 4. Procedural memory example (learn & “run” a skill)\n",
    "\n",
    "**Goal**: Demonstrate procedural memory (skills & workflows).\n",
    "\n",
    "- We teach the agent a small procedure (e.g., monthly_report) by saving ordered steps in a single text block under category=\"procedure\".\n",
    "\n",
    "- We retrieve the procedure and parse the numbered steps to simulate “running” it.\n",
    "\n",
    "This example shows how agents can learn reusable playbooks and trigger them later by name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "efb5caa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved procedure memory.\n",
      "\n",
      "Retrieve and 'run' it:\n",
      "{'id': 'ab0940fe-c9a4-446f-b3ff-cbaea099faee', 'memory': 'Procedure: monthly_report\\nSteps:\\n1. Query sales DB for the last 30 days.\\n2. Summarize top 5 insights.\\n3. Ask user whether to email or display.', 'hash': 'e66b82a0dcc57034cfa0a54084a643b5', 'metadata': {'procedure_name': 'monthly_report', 'category': 'procedure'}, 'score': 0.9708564281463623, 'created_at': '2025-08-25T17:26:28.965650-07:00', 'updated_at': None, 'user_id': 'lesson9_notebook_student', 'role': 'user'}\n"
     ]
    }
   ],
   "source": [
    "def learn_procedure(name: str, steps: list[str]) -> str:\n",
    "    body = \"Procedure: \" + name + \"\\nSteps:\\n\" + \"\\n\".join(f\"{i + 1}. {s}\" for i, s in enumerate(steps))\n",
    "    return mem_add_text(body, category=\"procedure\", procedure_name=name)\n",
    "\n",
    "\n",
    "def find_procedure(name: str) -> dict | None:\n",
    "    # search broadly but only keep category=procedure\n",
    "    hits = mem_search(name, limit=10, category=\"procedure\")\n",
    "    # Prefer an exact name match if available\n",
    "    for h in hits:\n",
    "        if (h.get(\"metadata\") or {}).get(\"procedure_name\") == name:\n",
    "            return h\n",
    "    return hits[0] if hits else None\n",
    "\n",
    "\n",
    "def run_procedure(name: str) -> str:\n",
    "    p = find_procedure(name)\n",
    "    if not p:\n",
    "        return f\"Procedure '{name}' not found.\"\n",
    "    text = p.get(\"memory\", \"\")\n",
    "    steps = [m.group(1).strip() for m in re.finditer(r\"^\\s*\\d+\\.\\s+(.*)$\", text, flags=re.MULTILINE)]\n",
    "    if not steps:\n",
    "        return f\"Procedure '{name}' has no parseable steps.\\n\\n{text}\"\n",
    "    lines = [f\"→ {s}\" for s in steps]\n",
    "    return f\"Running procedure '{name}':\\n\" + \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "# Teach the agent a tiny recurrent skill:\n",
    "print(\n",
    "    learn_procedure(\n",
    "        \"monthly_report\",\n",
    "        [\n",
    "            \"Query sales DB for the last 30 days.\",\n",
    "            \"Summarize top 5 insights.\",\n",
    "            \"Ask user whether to email or display.\",\n",
    "        ],\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"\\nRetrieve and 'run' it:\")\n",
    "proc = find_procedure(\"monthly_report\")\n",
    "print(proc or \"Not found.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
