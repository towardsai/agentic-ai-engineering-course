{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d92a1d4",
   "metadata": {},
   "source": [
    "# Lesson 31: Continuous Integration (CI) for AI Engineering\n",
    "\n",
    "In this notebook, we'll practice the CI essentials covered in Lesson 31. You'll run formatting checks, linting, and tests to see how CI tools maintain code quality.\n",
    "\n",
    "**Learning Objectives:**\n",
    "\n",
    "- Understand Brown's CI configuration files\n",
    "- Practice running formatting and linting checks with Ruff\n",
    "- Learn to fix code quality issues automatically\n",
    "- Run unit tests with mocked LLM responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd49ca0",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "First, let's navigate to the Brown writing agent directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9383d61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70213c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/fabio/Desktop/course-ai-agents/lessons/writing_workflow\n"
     ]
    }
   ],
   "source": [
    "%cd ../writing_workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30de2077",
   "metadata": {},
   "source": [
    "## 2. Viewing Brown's CI Configuration\n",
    "\n",
    "Let's examine Brown's actual CI configuration files to understand how CI is set up."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9131df69",
   "metadata": {},
   "source": [
    "### 2.1 Pre-commit Configuration\n",
    "\n",
    "The `.pre-commit-config.yaml` file defines Git hooks that run automatically before each commit. These hooks catch issues immediately in your local development environment.\n",
    "\n",
    "Brown's pre-commit configuration includes three types of hooks:\n",
    "1. **validate-pyproject** - Validates that `pyproject.toml` is structurally correct\n",
    "2. **prettier** - Formats YAML and JSON configuration files consistently\n",
    "3. **ruff-check** and **ruff-format** - Lints and formats Python code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87335769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fail_fast: false\n",
      "\n",
      "repos:\n",
      "  - repo: https://github.com/abravalheri/validate-pyproject\n",
      "    rev: v0.24.1\n",
      "    hooks:\n",
      "      - id: validate-pyproject\n",
      "\n",
      "  - repo: https://github.com/pre-commit/mirrors-prettier\n",
      "    rev: v3.1.0\n",
      "    hooks:\n",
      "      - id: prettier\n",
      "        types_or: [yaml, json5]\n",
      "\n",
      "  - repo: https://github.com/astral-sh/ruff-pre-commit\n",
      "    # Ruff version.\n",
      "    rev: v0.12.1\n",
      "    hooks:\n",
      "      # Run the linter.\n",
      "      - id: ruff-check\n",
      "        args: [--fix, --exit-non-zero-on-fix]\n",
      "      # Run the formatter.\n",
      "      - id: ruff-format\n"
     ]
    }
   ],
   "source": [
    "# Show the content of the file\n",
    "!cat .pre-commit-config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee437e8e",
   "metadata": {},
   "source": [
    "### 2.2 Ruff Configuration\n",
    "\n",
    "The `pyproject.toml` file contains Ruff's configuration in the `[tool.ruff]` section. This defines:\n",
    "- **target-version**: Which Python version to target (py312 for Python 3.12)\n",
    "- **line-length**: Maximum line length (140 characters for modern screens)\n",
    "- **select rules**: Which linting rules to enable (F=Pyflakes, E=pycodestyle, I=isort)\n",
    "- **known-first-party**: How to group imports correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6730f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tool.ruff]\n",
      "target-version = \"py312\"\n",
      "line-length = 140\n",
      "\n",
      "[tool.ruff.lint]\n",
      "select = [\n",
      "    \"F\",    # Pyflakes\n",
      "    \"E\",    # pycodestyle errors\n",
      "    \"I\",    # isort\n",
      "]\n",
      "\n",
      "[tool.ruff.lint.isort]\n",
      "known-first-party = [\"src\", \"tests\"]\n",
      "\n",
      "[tool.pytest.ini_options]\n",
      "pythonpath = [\"src\"]\n"
     ]
    }
   ],
   "source": [
    "# Show the content of the file related to ruff\n",
    "!grep -A 20 \"\\[tool.ruff\\]\" pyproject.toml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d74bb8",
   "metadata": {},
   "source": [
    "### 2.3 Makefile QA Targets\n",
    "\n",
    "The Makefile provides convenient shortcuts for running CI commands. Instead of typing long `uv run ruff format --check src/ tests/ scripts/` commands, you can simply run `make format-check`.\n",
    "\n",
    "The Makefile defines:\n",
    "- **QA_FOLDERS** - Which directories to check (src/, tests/, scripts/)\n",
    "- **format-check/format-fix** - Formatting commands\n",
    "- **lint-check/lint-fix** - Linting commands\n",
    "- **tests** - Test suite with the correct configuration\n",
    "- **pre-commit** - Manual pre-commit hook execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31af17d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "tests: # Run tests.\n",
      "\tCONFIG_FILE=configs/debug.yaml uv run pytest\n",
      "\n",
      "pre-commit: # Run pre-commit hooks.\n",
      "\tuv run pre-commit run --all-files\n",
      "\n",
      "format-fix: # Auto-format Python code using ruff formatter.\n",
      "\tuv run ruff format $(QA_FOLDERS)\n",
      "\n",
      "lint-fix: # Auto-fix linting issues using ruff linter.\n",
      "\tuv run ruff check --fix $(QA_FOLDERS)\n",
      "\n",
      "format-check: # Check code formatting without making changes using ruff formatter.\n",
      "\tuv run ruff format --check $(QA_FOLDERS) \n",
      "\n",
      "lint-check: # Check code for linting issues without fixing them using ruff linter.\n",
      "\tuv run ruff check $(QA_FOLDERS)\n"
     ]
    }
   ],
   "source": [
    "# Show the commands in the Makefile related to QA\n",
    "!sed -n '/# --- Tests & QA ---/,$p' Makefile | tail -n +2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90593ea0",
   "metadata": {},
   "source": [
    "## 3. Pre-commit Hooks (Local Enforcement)\n",
    "\n",
    "Pre-commit hooks run automatically before each commit, catching issues before they enter version control. Let's run them manually:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3b1f030c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1m`VIRTUAL_ENV=/Users/fabio/Desktop/course-ai-agents/.venv` does not match the project environment path `.venv` and will be ignored; use `--active` to target the active environment instead\u001b[0m\n",
      "Validate pyproject.toml..............................(no files to check)\u001b[46;30mSkipped\u001b[m\n",
      "prettier.................................................................\u001b[42mPassed\u001b[m\n",
      "ruff check...............................................................\u001b[42mPassed\u001b[m\n",
      "ruff format..............................................................\u001b[42mPassed\u001b[m\n"
     ]
    }
   ],
   "source": [
    "!uv run pre-commit run --files ./**/*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e8cbe2",
   "metadata": {},
   "source": [
    "These hooks will:\n",
    "1. Validate your `pyproject.toml` structure\n",
    "2. Format all YAML/JSON files with prettier\n",
    "3. Lint Python code with ruff-check (and auto-fix issues)\n",
    "4. Format Python code with ruff-format\n",
    "\n",
    "If any hook fails, you'll see the error, fix it, re-stage the files, and commit again. This tight feedback loop keeps code quality high.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b943c1",
   "metadata": {},
   "source": [
    "## 4. Running Formatting Checks\n",
    "\n",
    "Now let's practice using Ruff's formatter. Instead of running it on Brown's existing code (which is already formatted), we'll create a simple Python file with formatting issues and fix them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b353e1",
   "metadata": {},
   "source": [
    "### 4.1 Create a Test File with Formatting Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ee28f4a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created test_formatting.py\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# Create a Python file with various formatting issues\n",
    "cat > test_formatting.py << 'EOF'\n",
    "# This file has formatting issues\n",
    "def  badly_formatted_function(x,y,z):\n",
    "    result=x+y+z\n",
    "    my_list=[1,2,3,4,5,6,7,8,9,10]\n",
    "    my_dict={\"key1\":\"value1\",\"key2\":\"value2\",\"key3\":\"value3\"}\n",
    "    if result>10:\n",
    "        print(\"Result is greater than 10\")\n",
    "    else:\n",
    "        print(\"Result is 10 or less\")\n",
    "    return result\n",
    "\n",
    "class   BadlyFormattedClass:\n",
    "    def __init__(self,name,age):\n",
    "        self.name=name\n",
    "        self.age=age\n",
    "    def get_info(self):\n",
    "        return f\"{self.name} is {self.age} years old\"\n",
    "EOF\n",
    "\n",
    "echo \"Created test_formatting.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371ceca2",
   "metadata": {},
   "source": [
    "### 4.2 Check Formatting (Without Fixing)\n",
    "\n",
    "Let's check if the file has formatting issues without modifying it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cbe4764f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1m`VIRTUAL_ENV=/Users/fabio/Desktop/course-ai-agents/.venv` does not match the project environment path `.venv` and will be ignored; use `--active` to target the active environment instead\u001b[0m\n",
      "Would reformat: \u001b[1mtest_formatting.py\u001b[0m\n",
      "1 file would be reformatted\n"
     ]
    }
   ],
   "source": [
    "!uv run ruff format --check test_formatting.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c754c1ec",
   "metadata": {},
   "source": [
    "You'll see that Ruff reports the file would be reformatted. The `--check` flag means Ruff only reports issues without changing the file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14365f9",
   "metadata": {},
   "source": [
    "### 4.3 Auto-fix Formatting Issues\n",
    "\n",
    "Now let's fix all the formatting issues automatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d99402d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1m`VIRTUAL_ENV=/Users/fabio/Desktop/course-ai-agents/.venv` does not match the project environment path `.venv` and will be ignored; use `--active` to target the active environment instead\u001b[0m\n",
      "1 file reformatted\n"
     ]
    }
   ],
   "source": [
    "!uv run ruff format test_formatting.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41bca3d",
   "metadata": {},
   "source": [
    "Ruff will reformat the file to follow consistent style rules. Let's see the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "26bb7a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# This file has formatting issues\n",
      "def badly_formatted_function(x, y, z):\n",
      "    result = x + y + z\n",
      "    my_list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "    my_dict = {\"key1\": \"value1\", \"key2\": \"value2\", \"key3\": \"value3\"}\n",
      "    if result > 10:\n",
      "        print(\"Result is greater than 10\")\n",
      "    else:\n",
      "        print(\"Result is 10 or less\")\n",
      "    return result\n",
      "\n",
      "\n",
      "class BadlyFormattedClass:\n",
      "    def __init__(self, name, age):\n",
      "        self.name = name\n",
      "        self.age = age\n",
      "\n",
      "    def get_info(self):\n",
      "        return f\"{self.name} is {self.age} years old\"\n"
     ]
    }
   ],
   "source": [
    "cat test_formatting.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc417719",
   "metadata": {},
   "source": [
    "Notice how Ruff has:\n",
    "- Fixed spacing around operators (`x+y+z` → `x + y + z`)\n",
    "- Added proper spacing in function signatures\n",
    "- Formatted lists and dictionaries consistently\n",
    "- Fixed class definition spacing\n",
    "\n",
    "Let's remove the file now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e777c149",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm test_formatting.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d84ea4",
   "metadata": {},
   "source": [
    "## 5. Running Linting Checks\n",
    "\n",
    "Linting goes beyond formatting—it checks for bugs, code quality issues, and best practices. Let's create a file with linting issues and fix them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b10186",
   "metadata": {},
   "source": [
    "### 5.1 Create a Test File with Linting Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "165fbe10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created test_linting.py\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cat > test_linting.py << 'EOF'\n",
    "import os\n",
    "import sys\n",
    "import json # Unused import\n",
    "\n",
    "def calculate_sum(numbers):\n",
    "    \"\"\"Calculate sum of numbers.\"\"\"\n",
    "    total = 0\n",
    "    for num in numbers:\n",
    "        total = total + num\n",
    "    return total\n",
    "\n",
    "def process_data(data):\n",
    "    \"\"\"Process some data.\"\"\"\n",
    "    result = calculate_sum(data)\n",
    "    print(f\"Result: {result}\")\n",
    "    _ = os.getcwd()  # Use os\n",
    "    _ = sys.argv[0]  # Use sys\n",
    "    undefined_variable = some_undefined_function()  # Using undefined name\n",
    "    return result\n",
    "\n",
    "import sys # Duplicate import\n",
    "EOF\n",
    "\n",
    "echo \"Created test_linting.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ad003e",
   "metadata": {},
   "source": [
    "### 5.2 Check Linting Issues (Without Fixing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "557f5d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1m`VIRTUAL_ENV=/Users/fabio/Desktop/course-ai-agents/.venv` does not match the project environment path `.venv` and will be ignored; use `--active` to target the active environment instead\u001b[0m\n",
      "\u001b[1m\u001b[91mI001 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mImport block is un-sorted or un-formatted\u001b[0m\n",
      " \u001b[1m\u001b[94m-->\u001b[0m test_linting.py:1:1\n",
      "  \u001b[1m\u001b[94m|\u001b[0m\n",
      "\u001b[1m\u001b[94m1 |\u001b[0m \u001b[1m\u001b[91m/\u001b[0m import os\n",
      "\u001b[1m\u001b[94m2 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m import sys\n",
      "\u001b[1m\u001b[94m3 |\u001b[0m \u001b[1m\u001b[91m|\u001b[0m import json # Unused import\n",
      "  \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m|___________^\u001b[0m\n",
      "\u001b[1m\u001b[94m4 |\u001b[0m\n",
      "\u001b[1m\u001b[94m5 |\u001b[0m   def calculate_sum(numbers):\n",
      "  \u001b[1m\u001b[94m|\u001b[0m\n",
      "\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mOrganize imports\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[91mF401 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1m`json` imported but unused\u001b[0m\n",
      " \u001b[1m\u001b[94m-->\u001b[0m test_linting.py:3:8\n",
      "  \u001b[1m\u001b[94m|\u001b[0m\n",
      "\u001b[1m\u001b[94m1 |\u001b[0m import os\n",
      "\u001b[1m\u001b[94m2 |\u001b[0m import sys\n",
      "\u001b[1m\u001b[94m3 |\u001b[0m import json # Unused import\n",
      "  \u001b[1m\u001b[94m|\u001b[0m        \u001b[1m\u001b[91m^^^^\u001b[0m\n",
      "\u001b[1m\u001b[94m4 |\u001b[0m\n",
      "\u001b[1m\u001b[94m5 |\u001b[0m def calculate_sum(numbers):\n",
      "  \u001b[1m\u001b[94m|\u001b[0m\n",
      "\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove unused import: `json`\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[91mF841 \u001b[0m\u001b[1mLocal variable `undefined_variable` is assigned to but never used\u001b[0m\n",
      "  \u001b[1m\u001b[94m-->\u001b[0m test_linting.py:18:5\n",
      "   \u001b[1m\u001b[94m|\u001b[0m\n",
      "\u001b[1m\u001b[94m16 |\u001b[0m     _ = os.getcwd()  # Use os\n",
      "\u001b[1m\u001b[94m17 |\u001b[0m     _ = sys.argv[0]  # Use sys\n",
      "\u001b[1m\u001b[94m18 |\u001b[0m     undefined_variable = some_undefined_function()  # Using undefined name\n",
      "   \u001b[1m\u001b[94m|\u001b[0m     \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "\u001b[1m\u001b[94m19 |\u001b[0m     return result\n",
      "   \u001b[1m\u001b[94m|\u001b[0m\n",
      "\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove assignment to unused variable `undefined_variable`\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[91mF821 \u001b[0m\u001b[1mUndefined name `some_undefined_function`\u001b[0m\n",
      "  \u001b[1m\u001b[94m-->\u001b[0m test_linting.py:18:26\n",
      "   \u001b[1m\u001b[94m|\u001b[0m\n",
      "\u001b[1m\u001b[94m16 |\u001b[0m     _ = os.getcwd()  # Use os\n",
      "\u001b[1m\u001b[94m17 |\u001b[0m     _ = sys.argv[0]  # Use sys\n",
      "\u001b[1m\u001b[94m18 |\u001b[0m     undefined_variable = some_undefined_function()  # Using undefined name\n",
      "   \u001b[1m\u001b[94m|\u001b[0m                          \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "\u001b[1m\u001b[94m19 |\u001b[0m     return result\n",
      "   \u001b[1m\u001b[94m|\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[91mE402 \u001b[0m\u001b[1mModule level import not at top of file\u001b[0m\n",
      "  \u001b[1m\u001b[94m-->\u001b[0m test_linting.py:21:1\n",
      "   \u001b[1m\u001b[94m|\u001b[0m\n",
      "\u001b[1m\u001b[94m19 |\u001b[0m     return result\n",
      "\u001b[1m\u001b[94m20 |\u001b[0m\n",
      "\u001b[1m\u001b[94m21 |\u001b[0m import sys # Duplicate import\n",
      "   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^\u001b[0m\n",
      "   \u001b[1m\u001b[94m|\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[91mI001 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mImport block is un-sorted or un-formatted\u001b[0m\n",
      "  \u001b[1m\u001b[94m-->\u001b[0m test_linting.py:21:1\n",
      "   \u001b[1m\u001b[94m|\u001b[0m\n",
      "\u001b[1m\u001b[94m19 |\u001b[0m     return result\n",
      "\u001b[1m\u001b[94m20 |\u001b[0m\n",
      "\u001b[1m\u001b[94m21 |\u001b[0m import sys # Duplicate import\n",
      "   \u001b[1m\u001b[94m|\u001b[0m \u001b[1m\u001b[91m^^^^^^^^^^\u001b[0m\n",
      "   \u001b[1m\u001b[94m|\u001b[0m\n",
      "\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mOrganize imports\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[91mF811 \u001b[0m[\u001b[1m\u001b[96m*\u001b[0m] \u001b[1mRedefinition of unused `sys` from line 2\u001b[0m\n",
      "  \u001b[1m\u001b[94m-->\u001b[0m test_linting.py:21:8\n",
      "   \u001b[1m\u001b[94m|\u001b[0m\n",
      "\u001b[1m\u001b[94m19 |\u001b[0m     return result\n",
      "\u001b[1m\u001b[94m20 |\u001b[0m\n",
      "\u001b[1m\u001b[94m21 |\u001b[0m import sys # Duplicate import\n",
      "   \u001b[1m\u001b[94m|\u001b[0m        \u001b[1m\u001b[91m^^^\u001b[0m \u001b[1m\u001b[91m`sys` redefined here\u001b[0m\n",
      "   \u001b[1m\u001b[94m|\u001b[0m\n",
      "  \u001b[1m\u001b[94m:::\u001b[0m test_linting.py:2:8\n",
      "   \u001b[1m\u001b[94m|\u001b[0m\n",
      "\u001b[1m\u001b[94m 1 |\u001b[0m import os\n",
      "\u001b[1m\u001b[94m 2 |\u001b[0m import sys\n",
      "   \u001b[1m\u001b[94m|\u001b[0m        \u001b[1m\u001b[33m---\u001b[0m \u001b[1m\u001b[33mprevious definition of `sys` here\u001b[0m\n",
      "\u001b[1m\u001b[94m 3 |\u001b[0m import json # Unused import\n",
      "   \u001b[1m\u001b[94m|\u001b[0m\n",
      "\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove definition: `sys`\u001b[0m\n",
      "\n",
      "Found 7 errors.\n",
      "[\u001b[36m*\u001b[0m] 4 fixable with the `--fix` option (1 hidden fix can be enabled with the `--unsafe-fixes` option).\n"
     ]
    }
   ],
   "source": [
    "!uv run ruff check test_linting.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24565b97",
   "metadata": {},
   "source": [
    "Ruff will report several issues:\n",
    "- **F401**: Unused import (`json` is imported but never used)\n",
    "- **F811**: Duplicate import (`sys` is imported twice)\n",
    "- **F821**: Undefined name (`some_undefined_function` doesn't exist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1f28e7",
   "metadata": {},
   "source": [
    "### 5.3 Auto-fix Linting Issues (Where Possible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a35657c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1m`VIRTUAL_ENV=/Users/fabio/Desktop/course-ai-agents/.venv` does not match the project environment path `.venv` and will be ignored; use `--active` to target the active environment instead\u001b[0m\n",
      "\u001b[1m\u001b[91mF841 \u001b[0m\u001b[1mLocal variable `undefined_variable` is assigned to but never used\u001b[0m\n",
      "  \u001b[1m\u001b[94m-->\u001b[0m test_linting.py:18:5\n",
      "   \u001b[1m\u001b[94m|\u001b[0m\n",
      "\u001b[1m\u001b[94m16 |\u001b[0m     _ = os.getcwd()  # Use os\n",
      "\u001b[1m\u001b[94m17 |\u001b[0m     _ = sys.argv[0]  # Use sys\n",
      "\u001b[1m\u001b[94m18 |\u001b[0m     undefined_variable = some_undefined_function()  # Using undefined name\n",
      "   \u001b[1m\u001b[94m|\u001b[0m     \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "\u001b[1m\u001b[94m19 |\u001b[0m     return result\n",
      "   \u001b[1m\u001b[94m|\u001b[0m\n",
      "\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove assignment to unused variable `undefined_variable`\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[91mF821 \u001b[0m\u001b[1mUndefined name `some_undefined_function`\u001b[0m\n",
      "  \u001b[1m\u001b[94m-->\u001b[0m test_linting.py:18:26\n",
      "   \u001b[1m\u001b[94m|\u001b[0m\n",
      "\u001b[1m\u001b[94m16 |\u001b[0m     _ = os.getcwd()  # Use os\n",
      "\u001b[1m\u001b[94m17 |\u001b[0m     _ = sys.argv[0]  # Use sys\n",
      "\u001b[1m\u001b[94m18 |\u001b[0m     undefined_variable = some_undefined_function()  # Using undefined name\n",
      "   \u001b[1m\u001b[94m|\u001b[0m                          \u001b[1m\u001b[91m^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "\u001b[1m\u001b[94m19 |\u001b[0m     return result\n",
      "   \u001b[1m\u001b[94m|\u001b[0m\n",
      "\n",
      "Found 5 errors (3 fixed, 2 remaining).\n",
      "No fixes available (1 hidden fix can be enabled with the `--unsafe-fixes` option).\n"
     ]
    }
   ],
   "source": [
    "!uv run ruff check --fix test_linting.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b399b418",
   "metadata": {},
   "source": [
    "Ruff will automatically fix:\n",
    "- Remove unused imports\n",
    "- Remove duplicate imports\n",
    "\n",
    "But it won't fix the undefined name. That requires manual intervention since it's a logic error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8408c0f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import os\n",
      "import sys\n",
      "\n",
      "\n",
      "def calculate_sum(numbers):\n",
      "    \"\"\"Calculate sum of numbers.\"\"\"\n",
      "    total = 0\n",
      "    for num in numbers:\n",
      "        total = total + num\n",
      "    return total\n",
      "\n",
      "def process_data(data):\n",
      "    \"\"\"Process some data.\"\"\"\n",
      "    result = calculate_sum(data)\n",
      "    print(f\"Result: {result}\")\n",
      "    _ = os.getcwd()  # Use os\n",
      "    _ = sys.argv[0]  # Use sys\n",
      "    undefined_variable = some_undefined_function()  # Using undefined name\n",
      "    return result\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cat test_linting.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e244f8a3",
   "metadata": {},
   "source": [
    "Let's remove the file now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e98846a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm test_linting.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba5bbf7",
   "metadata": {},
   "source": [
    "## 6. Running Unit Tests\n",
    "\n",
    "Now let's run Brown's test suite with mocked LLM responses. The tests use fake models instead of real LLMs, making them fast, deterministic, and free."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "87c1134b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1m`VIRTUAL_ENV=/Users/fabio/Desktop/course-ai-agents/.venv` does not match the project environment path `.venv` and will be ignored; use `--active` to target the active environment instead\u001b[0m\n",
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.12.11, pytest-8.4.2, pluggy-1.6.0 -- /Users/fabio/Desktop/course-ai-agents/lessons/writing_workflow/.venv/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /Users/fabio/Desktop/course-ai-agents/lessons/writing_workflow\n",
      "configfile: pyproject.toml\n",
      "plugins: asyncio-1.2.0, anyio-4.11.0, langsmith-0.4.38, opik-1.8.96\n",
      "asyncio: mode=Mode.STRICT, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function\n",
      "collected 214 items                                                            \u001b[0m\n",
      "\n",
      "tests/brown/data/test_loaders.py::TestMarkdownArticleLoader::test_article_loader_success \u001b[32mPASSED\u001b[0m\u001b[32m [  0%]\u001b[0m\n",
      "tests/brown/data/test_loaders.py::TestMarkdownArticleLoader::test_article_loader_file_not_found \u001b[32mPASSED\u001b[0m\u001b[32m [  0%]\u001b[0m\n",
      "tests/brown/data/test_loaders.py::TestMarkdownArticleGuidelineLoader::test_article_guideline_loader_success \u001b[32mPASSED\u001b[0m\u001b[32m [  1%]\u001b[0m\n",
      "tests/brown/data/test_loaders.py::TestMarkdownArticleGuidelineLoader::test_article_guideline_loader_file_not_found \u001b[32mPASSED\u001b[0m\u001b[32m [  1%]\u001b[0m\n",
      "tests/brown/data/test_loaders.py::TestMarkdownResearchLoader::test_research_loader_success \u001b[32mPASSED\u001b[0m\u001b[32m [  2%]\u001b[0m\n",
      "tests/brown/data/test_loaders.py::TestMarkdownResearchLoader::test_research_loader_with_markdown_links \u001b[32mPASSED\u001b[0m\u001b[32m [  2%]\u001b[0m\n",
      "tests/brown/data/test_loaders.py::TestMarkdownResearchLoader::test_research_loader_file_not_found \u001b[32mPASSED\u001b[0m\u001b[32m [  3%]\u001b[0m\n",
      "tests/brown/data/test_loaders.py::TestMarkdownArticleProfilesLoader::test_profiles_loader_success \u001b[32mPASSED\u001b[0m\u001b[32m [  3%]\u001b[0m\n",
      "tests/brown/data/test_loaders.py::TestMarkdownArticleProfilesLoader::test_profiles_loader_missing_file \u001b[32mPASSED\u001b[0m\u001b[32m [  4%]\u001b[0m\n",
      "tests/brown/data/test_loaders.py::TestMarkdownArticleProfilesLoader::test_profiles_loader_get_supported_profiles \u001b[32mPASSED\u001b[0m\u001b[32m [  4%]\u001b[0m\n",
      "tests/brown/data/test_loaders.py::TestMarkdownArticleExampleLoader::test_article_example_loader_success \u001b[32mPASSED\u001b[0m\u001b[32m [  5%]\u001b[0m\n",
      "tests/brown/data/test_loaders.py::TestMarkdownArticleExampleLoader::test_article_example_loader_empty_dir \u001b[32mPASSED\u001b[0m\u001b[32m [  5%]\u001b[0m\n",
      "tests/brown/data/test_loaders.py::TestMarkdownArticleExampleLoader::test_article_example_loader_dir_not_found \u001b[32mPASSED\u001b[0m\u001b[32m [  6%]\u001b[0m\n",
      "tests/brown/data/test_loaders.py::TestMarkdownArticleExampleLoader::test_article_example_loader_ignores_non_md_files \u001b[32mPASSED\u001b[0m\u001b[32m [  6%]\u001b[0m\n",
      "tests/brown/data/test_renderers.py::TestMarkdownArticleRenderer::test_article_markdown_renderer \u001b[32mPASSED\u001b[0m\u001b[32m [  7%]\u001b[0m\n",
      "tests/brown/data/test_renderers.py::TestMarkdownArticleRenderer::test_article_markdown_renderer_overwrites_existing \u001b[32mPASSED\u001b[0m\u001b[32m [  7%]\u001b[0m\n",
      "tests/brown/data/test_renderers.py::TestMarkdownArticleRenderer::test_article_markdown_renderer_empty_content \u001b[32mPASSED\u001b[0m\u001b[32m [  7%]\u001b[0m\n",
      "tests/brown/data/test_renderers.py::TestMarkdownArticleReviewsRenderer::test_article_reviews_context_renderer \u001b[32mPASSED\u001b[0m\u001b[32m [  8%]\u001b[0m\n",
      "tests/brown/data/test_renderers.py::TestMarkdownArticleReviewsRenderer::test_article_reviews_context_renderer_overwrites_existing \u001b[32mPASSED\u001b[0m\u001b[32m [  8%]\u001b[0m\n",
      "tests/brown/data/test_renderers.py::TestMarkdownArticleReviewsRenderer::test_article_reviews_context_renderer_empty_reviews \u001b[32mPASSED\u001b[0m\u001b[32m [  9%]\u001b[0m\n",
      "tests/brown/data/test_renderers.py::TestMarkdownArticleReviewsRenderer::test_article_reviews_context_renderer_without_article \u001b[32mPASSED\u001b[0m\u001b[32m [  9%]\u001b[0m\n",
      "tests/brown/domain/test_articles.py::TestArticle::test_article_creation \u001b[32mPASSED\u001b[0m\u001b[32m [ 10%]\u001b[0m\n",
      "tests/brown/domain/test_articles.py::TestArticle::test_article_to_context \u001b[32mPASSED\u001b[0m\u001b[32m [ 10%]\u001b[0m\n",
      "tests/brown/domain/test_articles.py::TestArticle::test_article_to_markdown \u001b[32mPASSED\u001b[0m\u001b[32m [ 11%]\u001b[0m\n",
      "tests/brown/domain/test_articles.py::TestArticle::test_article_str_representation \u001b[32mPASSED\u001b[0m\u001b[32m [ 11%]\u001b[0m\n",
      "tests/brown/domain/test_articles.py::TestArticle::test_article_empty_content \u001b[32mPASSED\u001b[0m\u001b[32m [ 12%]\u001b[0m\n",
      "tests/brown/domain/test_articles.py::TestSelectedText::test_selected_text_creation \u001b[32mPASSED\u001b[0m\u001b[32m [ 12%]\u001b[0m\n",
      "tests/brown/domain/test_articles.py::TestSelectedText::test_selected_text_to_context \u001b[32mPASSED\u001b[0m\u001b[32m [ 13%]\u001b[0m\n",
      "tests/brown/domain/test_articles.py::TestSelectedText::test_selected_text_single_line \u001b[32mPASSED\u001b[0m\u001b[32m [ 13%]\u001b[0m\n",
      "tests/brown/domain/test_articles.py::TestArticleExample::test_article_example_creation \u001b[32mPASSED\u001b[0m\u001b[32m [ 14%]\u001b[0m\n",
      "tests/brown/domain/test_articles.py::TestArticleExample::test_article_example_to_context \u001b[32mPASSED\u001b[0m\u001b[32m [ 14%]\u001b[0m\n",
      "tests/brown/domain/test_articles.py::TestArticleExample::test_article_example_str_representation \u001b[32mPASSED\u001b[0m\u001b[32m [ 14%]\u001b[0m\n",
      "tests/brown/domain/test_articles.py::TestArticleExamples::test_article_examples_creation \u001b[32mPASSED\u001b[0m\u001b[32m [ 15%]\u001b[0m\n",
      "tests/brown/domain/test_articles.py::TestArticleExamples::test_article_examples_to_context \u001b[32mPASSED\u001b[0m\u001b[32m [ 15%]\u001b[0m\n",
      "tests/brown/domain/test_articles.py::TestArticleExamples::test_article_examples_empty \u001b[32mPASSED\u001b[0m\u001b[32m [ 16%]\u001b[0m\n",
      "tests/brown/domain/test_articles.py::TestArticleExamples::test_article_examples_single \u001b[32mPASSED\u001b[0m\u001b[32m [ 16%]\u001b[0m\n",
      "tests/brown/domain/test_guidelines.py::TestArticleGuideline::test_article_guideline_creation \u001b[32mPASSED\u001b[0m\u001b[32m [ 17%]\u001b[0m\n",
      "tests/brown/domain/test_guidelines.py::TestArticleGuideline::test_article_guideline_to_context \u001b[32mPASSED\u001b[0m\u001b[32m [ 17%]\u001b[0m\n",
      "tests/brown/domain/test_guidelines.py::TestArticleGuideline::test_article_guideline_empty_content \u001b[32mPASSED\u001b[0m\u001b[32m [ 18%]\u001b[0m\n",
      "tests/brown/domain/test_guidelines.py::TestArticleGuideline::test_article_guideline_complex_content \u001b[32mPASSED\u001b[0m\u001b[32m [ 18%]\u001b[0m\n",
      "tests/brown/domain/test_media_items.py::TestMediaItem::test_media_item_creation \u001b[32mPASSED\u001b[0m\u001b[32m [ 19%]\u001b[0m\n",
      "tests/brown/domain/test_media_items.py::TestMediaItem::test_media_item_to_context \u001b[32mPASSED\u001b[0m\u001b[32m [ 19%]\u001b[0m\n",
      "tests/brown/domain/test_media_items.py::TestMediaItem::test_media_item_different_types \u001b[32mPASSED\u001b[0m\u001b[32m [ 20%]\u001b[0m\n",
      "tests/brown/domain/test_media_items.py::TestMediaItem::test_media_item_str_representation \u001b[32mPASSED\u001b[0m\u001b[32m [ 20%]\u001b[0m\n",
      "tests/brown/domain/test_media_items.py::TestMediaItems::test_media_items_creation \u001b[32mPASSED\u001b[0m\u001b[32m [ 21%]\u001b[0m\n",
      "tests/brown/domain/test_media_items.py::TestMediaItems::test_media_items_to_context \u001b[32mPASSED\u001b[0m\u001b[32m [ 21%]\u001b[0m\n",
      "tests/brown/domain/test_media_items.py::TestMediaItems::test_media_items_empty \u001b[32mPASSED\u001b[0m\u001b[32m [ 21%]\u001b[0m\n",
      "tests/brown/domain/test_media_items.py::TestMediaItems::test_media_items_single \u001b[32mPASSED\u001b[0m\u001b[32m [ 22%]\u001b[0m\n",
      "tests/brown/domain/test_media_items.py::TestMediaItems::test_media_items_str_representation \u001b[32mPASSED\u001b[0m\u001b[32m [ 22%]\u001b[0m\n",
      "tests/brown/domain/test_media_items.py::TestMediaItems::test_media_items_build_classmethod \u001b[32mPASSED\u001b[0m\u001b[32m [ 23%]\u001b[0m\n",
      "tests/brown/domain/test_media_items.py::TestMediaItems::test_are_available_only_in_source_property \u001b[32mPASSED\u001b[0m\u001b[32m [ 23%]\u001b[0m\n",
      "tests/brown/domain/test_media_items.py::TestMermaidDiagram::test_mermaid_diagram_creation \u001b[32mPASSED\u001b[0m\u001b[32m [ 24%]\u001b[0m\n",
      "tests/brown/domain/test_mixins.py::TestContextMixin::test_context_mixin_xml_tag \u001b[32mPASSED\u001b[0m\u001b[32m [ 24%]\u001b[0m\n",
      "tests/brown/domain/test_mixins.py::TestContextMixin::test_context_mixin_xml_tag_with_profile \u001b[32mPASSED\u001b[0m\u001b[32m [ 25%]\u001b[0m\n",
      "tests/brown/domain/test_mixins.py::TestContextMixin::test_context_mixin_xml_tag_with_suffix \u001b[32mPASSED\u001b[0m\u001b[32m [ 25%]\u001b[0m\n",
      "tests/brown/domain/test_mixins.py::TestMarkdownMixin::test_markdown_mixin_abstract \u001b[32mPASSED\u001b[0m\u001b[32m [ 26%]\u001b[0m\n",
      "tests/brown/domain/test_mixins.py::TestMarkdownMixin::test_markdown_mixin_implementation \u001b[32mPASSED\u001b[0m\u001b[32m [ 26%]\u001b[0m\n",
      "tests/brown/domain/test_mixins.py::TestMarkdownMixin::test_markdown_mixin_missing_implementation \u001b[32mPASSED\u001b[0m\u001b[32m [ 27%]\u001b[0m\n",
      "tests/brown/domain/test_profiles.py::TestProfile::test_profile_creation \u001b[32mPASSED\u001b[0m\u001b[32m [ 27%]\u001b[0m\n",
      "tests/brown/domain/test_profiles.py::TestProfile::test_profile_xml_tag \u001b[32mPASSED\u001b[0m\u001b[32m [ 28%]\u001b[0m\n",
      "tests/brown/domain/test_profiles.py::TestProfile::test_profile_to_context \u001b[32mPASSED\u001b[0m\u001b[32m [ 28%]\u001b[0m\n",
      "tests/brown/domain/test_profiles.py::TestCharacterProfile::test_character_profile_creation \u001b[32mPASSED\u001b[0m\u001b[32m [ 28%]\u001b[0m\n",
      "tests/brown/domain/test_profiles.py::TestCharacterProfile::test_character_profile_to_context \u001b[32mPASSED\u001b[0m\u001b[32m [ 29%]\u001b[0m\n",
      "tests/brown/domain/test_profiles.py::TestArticleProfile::test_article_profile_creation \u001b[32mPASSED\u001b[0m\u001b[32m [ 29%]\u001b[0m\n",
      "tests/brown/domain/test_profiles.py::TestArticleProfile::test_article_profile_to_context \u001b[32mPASSED\u001b[0m\u001b[32m [ 30%]\u001b[0m\n",
      "tests/brown/domain/test_profiles.py::TestStructureProfile::test_structure_profile_creation \u001b[32mPASSED\u001b[0m\u001b[32m [ 30%]\u001b[0m\n",
      "tests/brown/domain/test_profiles.py::TestStructureProfile::test_structure_profile_to_context \u001b[32mPASSED\u001b[0m\u001b[32m [ 31%]\u001b[0m\n",
      "tests/brown/domain/test_profiles.py::TestMechanicsProfile::test_mechanics_profile_creation \u001b[32mPASSED\u001b[0m\u001b[32m [ 31%]\u001b[0m\n",
      "tests/brown/domain/test_profiles.py::TestMechanicsProfile::test_mechanics_profile_to_context \u001b[32mPASSED\u001b[0m\u001b[32m [ 32%]\u001b[0m\n",
      "tests/brown/domain/test_profiles.py::TestTerminologyProfile::test_terminology_profile_creation \u001b[32mPASSED\u001b[0m\u001b[32m [ 32%]\u001b[0m\n",
      "tests/brown/domain/test_profiles.py::TestTerminologyProfile::test_terminology_profile_to_context \u001b[32mPASSED\u001b[0m\u001b[32m [ 33%]\u001b[0m\n",
      "tests/brown/domain/test_profiles.py::TestTonalityProfile::test_tonality_profile_creation \u001b[32mPASSED\u001b[0m\u001b[32m [ 33%]\u001b[0m\n",
      "tests/brown/domain/test_profiles.py::TestTonalityProfile::test_tonality_profile_to_context \u001b[32mPASSED\u001b[0m\u001b[32m [ 34%]\u001b[0m\n",
      "tests/brown/domain/test_profiles.py::TestArticleProfiles::test_article_profiles_creation \u001b[32mPASSED\u001b[0m\u001b[32m [ 34%]\u001b[0m\n",
      "tests/brown/domain/test_profiles.py::TestArticleProfiles::test_article_profiles_to_context \u001b[32mPASSED\u001b[0m\u001b[32m [ 35%]\u001b[0m\n",
      "tests/brown/domain/test_research.py::TestResearch::test_research_creation \u001b[32mPASSED\u001b[0m\u001b[32m [ 35%]\u001b[0m\n",
      "tests/brown/domain/test_research.py::TestResearch::test_research_to_context \u001b[32mPASSED\u001b[0m\u001b[32m [ 35%]\u001b[0m\n",
      "tests/brown/domain/test_research.py::TestResearch::test_research_empty_content \u001b[32mPASSED\u001b[0m\u001b[32m [ 36%]\u001b[0m\n",
      "tests/brown/domain/test_research.py::TestResearch::test_research_complex_content \u001b[32mPASSED\u001b[0m\u001b[32m [ 36%]\u001b[0m\n",
      "tests/brown/domain/test_research.py::TestResearch::test_research_str_representation \u001b[32mPASSED\u001b[0m\u001b[32m [ 37%]\u001b[0m\n",
      "tests/brown/domain/test_reviews.py::TestReview::test_review_creation \u001b[32mPASSED\u001b[0m\u001b[33m [ 37%]\u001b[0m\n",
      "tests/brown/domain/test_reviews.py::TestReview::test_review_to_context \u001b[32mPASSED\u001b[0m\u001b[33m [ 38%]\u001b[0m\n",
      "tests/brown/domain/test_reviews.py::TestReview::test_review_score_validation \u001b[32mPASSED\u001b[0m\u001b[33m [ 38%]\u001b[0m\n",
      "tests/brown/domain/test_reviews.py::TestReview::test_review_str_representation \u001b[32mPASSED\u001b[0m\u001b[33m [ 39%]\u001b[0m\n",
      "tests/brown/domain/test_reviews.py::TestArticleReviews::test_article_reviews_creation \u001b[32mPASSED\u001b[0m\u001b[33m [ 39%]\u001b[0m\n",
      "tests/brown/domain/test_reviews.py::TestArticleReviews::test_article_reviews_to_context \u001b[32mPASSED\u001b[0m\u001b[33m [ 40%]\u001b[0m\n",
      "tests/brown/domain/test_reviews.py::TestArticleReviews::test_article_reviews_with_article \u001b[32mPASSED\u001b[0m\u001b[33m [ 40%]\u001b[0m\n",
      "tests/brown/domain/test_reviews.py::TestArticleReviews::test_article_reviews_without_article \u001b[32mPASSED\u001b[0m\u001b[33m [ 41%]\u001b[0m\n",
      "tests/brown/domain/test_reviews.py::TestArticleReviews::test_article_reviews_empty \u001b[32mPASSED\u001b[0m\u001b[33m [ 41%]\u001b[0m\n",
      "tests/brown/domain/test_reviews.py::TestArticleReviews::test_article_reviews_single \u001b[32mPASSED\u001b[0m\u001b[33m [ 42%]\u001b[0m\n",
      "tests/brown/domain/test_reviews.py::TestArticleReviews::test_article_reviews_str_representation \u001b[32mPASSED\u001b[0m\u001b[33m [ 42%]\u001b[0m\n",
      "tests/brown/domain/test_reviews.py::TestHumanFeedback::test_human_feedback_creation \u001b[32mPASSED\u001b[0m\u001b[33m [ 42%]\u001b[0m\n",
      "tests/brown/domain/test_reviews.py::TestHumanFeedback::test_human_feedback_to_context \u001b[32mPASSED\u001b[0m\u001b[33m [ 43%]\u001b[0m\n",
      "tests/brown/domain/test_reviews.py::TestSelectedTextReviews::test_selected_text_reviews_creation \u001b[32mPASSED\u001b[0m\u001b[33m [ 43%]\u001b[0m\n",
      "tests/brown/domain/test_reviews.py::TestSelectedTextReviews::test_selected_text_reviews_to_context \u001b[32mPASSED\u001b[0m\u001b[33m [ 44%]\u001b[0m\n",
      "tests/brown/evals/metrics/follows_gt/test_follows_gt_metric.py::test_article_metric_perfect_score \u001b[32mPASSED\u001b[0m\u001b[33m [ 44%]\u001b[0m\n",
      "tests/brown/evals/metrics/follows_gt/test_follows_gt_metric.py::test_article_metric_mixed_scores \u001b[32mPASSED\u001b[0m\u001b[33m [ 45%]\u001b[0m\n",
      "tests/brown/evals/metrics/follows_gt/test_follows_gt_metric.py::test_article_metric_empty_sections \u001b[32mPASSED\u001b[0m\u001b[33m [ 45%]\u001b[0m\n",
      "tests/brown/evals/metrics/follows_gt/test_follows_gt_metric.py::test_article_metric_init_requires_mocked_response_for_fake_model \u001b[32mPASSED\u001b[0m\u001b[33m [ 46%]\u001b[0m\n",
      "tests/brown/evals/metrics/follows_gt/test_follows_gt_metric.py::test_article_metric_init_default_model \u001b[32mPASSED\u001b[0m\u001b[33m [ 46%]\u001b[0m\n",
      "tests/brown/evals/metrics/follows_gt/test_follows_gt_metric.py::test_article_metric_init_custom_name \u001b[32mPASSED\u001b[0m\u001b[33m [ 47%]\u001b[0m\n",
      "tests/brown/evals/metrics/user_intent/test_user_intent_metric.py::test_user_intent_metric_perfect_score \u001b[32mPASSED\u001b[0m\u001b[33m [ 47%]\u001b[0m\n",
      "tests/brown/evals/metrics/user_intent/test_user_intent_metric.py::test_user_intent_metric_mixed_scores \u001b[32mPASSED\u001b[0m\u001b[33m [ 48%]\u001b[0m\n",
      "tests/brown/evals/metrics/user_intent/test_user_intent_metric.py::test_user_intent_metric_poor_scores \u001b[32mPASSED\u001b[0m\u001b[33m [ 48%]\u001b[0m\n",
      "tests/brown/evals/metrics/user_intent/test_user_intent_metric.py::test_user_intent_metric_empty_sections \u001b[32mPASSED\u001b[0m\u001b[33m [ 49%]\u001b[0m\n",
      "tests/brown/evals/metrics/user_intent/test_user_intent_metric.py::test_user_intent_metric_missing_research_in_context \u001b[32mPASSED\u001b[0m\u001b[33m [ 49%]\u001b[0m\n",
      "tests/brown/evals/metrics/user_intent/test_user_intent_metric.py::test_user_intent_metric_init_requires_mocked_response_for_fake_model \u001b[32mPASSED\u001b[0m\u001b[33m [ 50%]\u001b[0m\n",
      "tests/brown/evals/metrics/user_intent/test_user_intent_metric.py::test_user_intent_metric_init_default_model \u001b[32mPASSED\u001b[0m\u001b[33m [ 50%]\u001b[0m\n",
      "tests/brown/evals/metrics/user_intent/test_user_intent_metric.py::test_user_intent_metric_init_custom_name \u001b[32mPASSED\u001b[0m\u001b[33m [ 50%]\u001b[0m\n",
      "tests/brown/evals/metrics/user_intent/test_user_intent_metric.py::test_user_intent_metric_init_custom_few_shot_examples \u001b[32mPASSED\u001b[0m\u001b[33m [ 51%]\u001b[0m\n",
      "tests/brown/evals/metrics/user_intent/test_user_intent_metric.py::test_user_intent_metric_score_result_structure \u001b[32mPASSED\u001b[0m\u001b[33m [ 51%]\u001b[0m\n",
      "tests/brown/evals/metrics/user_intent/test_user_intent_metric.py::test_user_intent_metric_async_score \u001b[32mPASSED\u001b[0m\u001b[33m [ 52%]\u001b[0m\n",
      "tests/brown/evals/metrics/user_intent/test_user_intent_metric.py::test_user_intent_scores_to_context \u001b[32mPASSED\u001b[0m\u001b[33m [ 52%]\u001b[0m\n",
      "tests/brown/evals/metrics/user_intent/test_user_intent_metric.py::test_user_intent_article_scores_to_context \u001b[32mPASSED\u001b[0m\u001b[33m [ 53%]\u001b[0m\n",
      "tests/brown/evals/test_dataset.py::test_load_dataset_success \u001b[32mPASSED\u001b[0m\u001b[33m      [ 53%]\u001b[0m\n",
      "tests/brown/evals/test_dataset.py::test_load_dataset_few_shot_example \u001b[32mPASSED\u001b[0m\u001b[33m [ 54%]\u001b[0m\n",
      "tests/brown/evals/test_dataset.py::test_load_dataset_file_not_found \u001b[32mPASSED\u001b[0m\u001b[33m [ 54%]\u001b[0m\n",
      "tests/brown/evals/test_dataset.py::test_load_dataset_metadata_not_found \u001b[32mPASSED\u001b[0m\u001b[33m [ 55%]\u001b[0m\n",
      "tests/brown/evals/test_dataset.py::test_load_dataset_empty_metadata \u001b[32mPASSED\u001b[0m\u001b[33m [ 55%]\u001b[0m\n",
      "tests/brown/evals/test_dataset.py::test_load_dataset_invalid_metadata_structure \u001b[32mPASSED\u001b[0m\u001b[33m [ 56%]\u001b[0m\n",
      "tests/brown/evals/test_tasks.py::TestEvaluationTask::test_evaluation_task_creation \u001b[32mPASSED\u001b[0m\u001b[33m [ 56%]\u001b[0m\n",
      "tests/brown/evals/test_tasks.py::TestEvaluationTask::test_evaluation_task_function_signature \u001b[32mPASSED\u001b[0m\u001b[33m [ 57%]\u001b[0m\n",
      "tests/brown/evals/test_tasks.py::TestEvaluationTask::test_create_evaluation_task_parameters \u001b[32mPASSED\u001b[0m\u001b[33m [ 57%]\u001b[0m\n",
      "tests/brown/evals/test_tasks.py::TestEvaluationTask::test_evaluation_task_async \u001b[32mPASSED\u001b[0m\u001b[33m [ 57%]\u001b[0m\n",
      "tests/brown/nodes/test_article_reviewer.py::TestArticleReviewer::test_article_reviewer_initialization \u001b[32mPASSED\u001b[0m\u001b[33m [ 58%]\u001b[0m\n",
      "tests/brown/nodes/test_article_reviewer.py::TestArticleReviewer::test_article_reviewer_ainvoke_success \u001b[32mPASSED\u001b[0m\u001b[33m [ 58%]\u001b[0m\n",
      "tests/brown/nodes/test_article_reviewer.py::TestArticleReviewer::test_article_reviewer_structured_output \u001b[32mPASSED\u001b[0m\u001b[33m [ 59%]\u001b[0m\n",
      "tests/brown/nodes/test_article_reviewer.py::TestArticleReviewer::test_article_reviewer_empty_article \u001b[32mPASSED\u001b[0m\u001b[33m [ 59%]\u001b[0m\n",
      "tests/brown/nodes/test_article_reviewer.py::TestArticleReviewer::test_article_reviewer_multiple_reviews \u001b[32mPASSED\u001b[0m\u001b[33m [ 60%]\u001b[0m\n",
      "tests/brown/nodes/test_article_reviewer.py::TestArticleReviewer::test_article_reviewer_requires_mocked_response_for_fake_model \u001b[32mPASSED\u001b[0m\u001b[33m [ 60%]\u001b[0m\n",
      "tests/brown/nodes/test_article_reviewer.py::TestArticleReviewer::test_article_reviewer_selected_text_initialization \u001b[32mPASSED\u001b[0m\u001b[33m [ 61%]\u001b[0m\n",
      "tests/brown/nodes/test_article_reviewer.py::TestArticleReviewer::test_article_reviewer_selected_text_ainvoke \u001b[32mPASSED\u001b[0m\u001b[33m [ 61%]\u001b[0m\n",
      "tests/brown/nodes/test_article_reviewer.py::TestArticleReviewer::test_article_reviewer_selected_text_with_human_feedback \u001b[32mPASSED\u001b[0m\u001b[33m [ 62%]\u001b[0m\n",
      "tests/brown/nodes/test_article_reviewer.py::TestArticleReviewer::test_article_reviewer_article_property \u001b[32mPASSED\u001b[0m\u001b[33m [ 62%]\u001b[0m\n",
      "tests/brown/nodes/test_article_writer.py::TestArticleWriter::test_article_writer_initialization \u001b[32mPASSED\u001b[0m\u001b[33m [ 63%]\u001b[0m\n",
      "tests/brown/nodes/test_article_writer.py::TestArticleWriter::test_article_writer_ainvoke_success \u001b[32mPASSED\u001b[0m\u001b[33m [ 63%]\u001b[0m\n",
      "tests/brown/nodes/test_article_writer.py::TestArticleWriter::test_article_writer_with_media_items \u001b[32mPASSED\u001b[0m\u001b[33m [ 64%]\u001b[0m\n",
      "tests/brown/nodes/test_article_writer.py::TestArticleWriter::test_article_writer_with_reviews \u001b[32mPASSED\u001b[0m\u001b[33m [ 64%]\u001b[0m\n",
      "tests/brown/nodes/test_article_writer.py::TestArticleWriter::test_article_writer_empty_input \u001b[32mPASSED\u001b[0m\u001b[33m [ 64%]\u001b[0m\n",
      "tests/brown/nodes/test_article_writer.py::TestArticleWriter::test_article_writer_requires_mocked_response_for_fake_model \u001b[32mPASSED\u001b[0m\u001b[33m [ 65%]\u001b[0m\n",
      "tests/brown/nodes/test_article_writer.py::TestArticleWriter::test_article_writer_with_selected_text_reviews \u001b[32mPASSED\u001b[0m\u001b[33m [ 65%]\u001b[0m\n",
      "tests/brown/nodes/test_article_writer.py::TestArticleWriter::test_article_writer_selected_text_preserves_line_numbers \u001b[32mPASSED\u001b[0m\u001b[33m [ 66%]\u001b[0m\n",
      "tests/brown/nodes/test_article_writer.py::TestArticleWriter::test_article_writer_selected_text_vs_article_output \u001b[32mPASSED\u001b[0m\u001b[33m [ 66%]\u001b[0m\n",
      "tests/brown/nodes/test_article_writer.py::TestArticleWriter::test_article_writer_selected_text_content \u001b[32mPASSED\u001b[0m\u001b[33m [ 67%]\u001b[0m\n",
      "tests/brown/nodes/test_base.py::TestNode::test_node_initialization \u001b[32mPASSED\u001b[0m\u001b[33m [ 67%]\u001b[0m\n",
      "tests/brown/nodes/test_base.py::TestNode::test_node_build_toolkit \u001b[32mPASSED\u001b[0m\u001b[33m [ 68%]\u001b[0m\n",
      "tests/brown/nodes/test_base.py::TestNode::test_node_build_model_client \u001b[32mPASSED\u001b[0m\u001b[33m [ 68%]\u001b[0m\n",
      "tests/brown/nodes/test_base.py::TestNode::test_node_build_model_client_fake \u001b[32mPASSED\u001b[0m\u001b[33m [ 69%]\u001b[0m\n",
      "tests/brown/nodes/test_base.py::TestNode::test_node_build_user_input_content \u001b[32mPASSED\u001b[0m\u001b[33m [ 69%]\u001b[0m\n",
      "tests/brown/nodes/test_base.py::TestNode::test_node_as_tool \u001b[32mPASSED\u001b[0m\u001b[33m       [ 70%]\u001b[0m\n",
      "tests/brown/nodes/test_base.py::TestNode::test_node_with_fake_model_requires_mocked_response \u001b[32mPASSED\u001b[0m\u001b[33m [ 70%]\u001b[0m\n",
      "tests/brown/nodes/test_base.py::TestNode::test_node_set_mocked_responses \u001b[32mPASSED\u001b[0m\u001b[33m [ 71%]\u001b[0m\n",
      "tests/brown/nodes/test_base.py::TestToolkit::test_toolkit_initialization \u001b[32mPASSED\u001b[0m\u001b[33m [ 71%]\u001b[0m\n",
      "tests/brown/nodes/test_base.py::TestToolkit::test_toolkit_abstract_methods \u001b[32mPASSED\u001b[0m\u001b[33m [ 71%]\u001b[0m\n",
      "tests/brown/nodes/test_media_generator.py::TestMermaidDiagramGenerator::test_mermaid_diagram_generator_initialization \u001b[32mPASSED\u001b[0m\u001b[33m [ 72%]\u001b[0m\n",
      "tests/brown/nodes/test_media_generator.py::TestMermaidDiagramGenerator::test_mermaid_diagram_generator_ainvoke \u001b[32mPASSED\u001b[0m\u001b[33m [ 72%]\u001b[0m\n",
      "tests/brown/nodes/test_media_generator.py::TestMermaidDiagramGenerator::test_mermaid_diagram_generator_error_handling \u001b[32mPASSED\u001b[0m\u001b[33m [ 73%]\u001b[0m\n",
      "tests/brown/nodes/test_media_generator.py::TestMermaidDiagramGenerator::test_mermaid_diagram_generator_requires_mocked_response_for_fake_model \u001b[32mPASSED\u001b[0m\u001b[33m [ 73%]\u001b[0m\n",
      "tests/brown/nodes/test_media_generator.py::TestMediaGeneratorOrchestrator::test_media_generator_orchestrator_initialization \u001b[32mPASSED\u001b[0m\u001b[33m [ 74%]\u001b[0m\n",
      "tests/brown/nodes/test_media_generator.py::TestMediaGeneratorOrchestrator::test_media_generator_orchestrator_ainvoke \u001b[32mPASSED\u001b[0m\u001b[33m [ 74%]\u001b[0m\n",
      "tests/brown/nodes/test_media_generator.py::TestMediaGeneratorOrchestrator::test_media_generator_orchestrator_requires_mocked_response_for_fake_model \u001b[32mPASSED\u001b[0m\u001b[33m [ 75%]\u001b[0m\n",
      "tests/brown/utils/test_a.py::TestAsSync::test_as_sync_decorator \u001b[32mPASSED\u001b[0m\u001b[33m   [ 75%]\u001b[0m\n",
      "tests/brown/utils/test_a.py::TestAsSync::test_as_sync_with_exception \u001b[32mPASSED\u001b[0m\u001b[33m [ 76%]\u001b[0m\n",
      "tests/brown/utils/test_a.py::TestAsSync::test_as_sync_invalid_function \u001b[32mPASSED\u001b[0m\u001b[33m [ 76%]\u001b[0m\n",
      "tests/brown/utils/test_a.py::TestAsyncioRun::test_asyncio_run_new_loop \u001b[32mPASSED\u001b[0m\u001b[33m [ 77%]\u001b[0m\n",
      "tests/brown/utils/test_a.py::TestAsyncioRun::test_asyncio_run_with_exception \u001b[32mPASSED\u001b[0m\u001b[33m [ 77%]\u001b[0m\n",
      "tests/brown/utils/test_a.py::TestAsyncioRun::test_asyncio_run_existing_loop \u001b[32mPASSED\u001b[0m\u001b[33m [ 78%]\u001b[0m\n",
      "tests/brown/utils/test_a.py::TestRunAsyncTasks::test_run_async_tasks \u001b[32mPASSED\u001b[0m\u001b[33m [ 78%]\u001b[0m\n",
      "tests/brown/utils/test_a.py::TestRunAsyncTasks::test_run_async_tasks_empty \u001b[32mPASSED\u001b[0m\u001b[33m [ 78%]\u001b[0m\n",
      "tests/brown/utils/test_a.py::TestRunAsyncTasks::test_run_async_tasks_with_progress \u001b[32mPASSED\u001b[0m\u001b[33m [ 79%]\u001b[0m\n",
      "tests/brown/utils/test_a.py::TestBatchGather::test_batch_gather \u001b[32mPASSED\u001b[0m\u001b[33m   [ 79%]\u001b[0m\n",
      "tests/brown/utils/test_a.py::TestBatchGather::test_batch_gather_single_batch \u001b[32mPASSED\u001b[0m\u001b[33m [ 80%]\u001b[0m\n",
      "tests/brown/utils/test_a.py::TestBatchGather::test_batch_gather_with_verbose \u001b[32mPASSED\u001b[0m\u001b[33m [ 80%]\u001b[0m\n",
      "tests/brown/utils/test_a.py::TestRunJobs::test_run_jobs \u001b[32mPASSED\u001b[0m\u001b[33m           [ 81%]\u001b[0m\n",
      "tests/brown/utils/test_a.py::TestRunJobs::test_run_jobs_single_worker \u001b[32mPASSED\u001b[0m\u001b[33m [ 81%]\u001b[0m\n",
      "tests/brown/utils/test_a.py::TestRunJobs::test_run_jobs_with_progress \u001b[32mPASSED\u001b[0m\u001b[33m [ 82%]\u001b[0m\n",
      "tests/brown/utils/test_a.py::TestRunJobs::test_run_jobs_empty \u001b[32mPASSED\u001b[0m\u001b[33m     [ 82%]\u001b[0m\n",
      "tests/brown/utils/test_a.py::TestRunJobs::test_run_jobs_with_exception \u001b[32mPASSED\u001b[0m\u001b[33m [ 83%]\u001b[0m\n",
      "tests/brown/utils/test_network.py::TestIsImageDomainAccepted::test_valid_domains \u001b[32mPASSED\u001b[0m\u001b[33m [ 83%]\u001b[0m\n",
      "tests/brown/utils/test_network.py::TestIsImageDomainAccepted::test_invalid_domains \u001b[32mPASSED\u001b[0m\u001b[33m [ 84%]\u001b[0m\n",
      "tests/brown/utils/test_network.py::TestIsImageDomainAccepted::test_edge_cases \u001b[32mPASSED\u001b[0m\u001b[33m [ 84%]\u001b[0m\n",
      "tests/brown/utils/test_network.py::TestPing::test_ping_success \u001b[32mPASSED\u001b[0m\u001b[33m    [ 85%]\u001b[0m\n",
      "tests/brown/utils/test_network.py::TestPing::test_ping_head_fallback_to_get \u001b[32mPASSED\u001b[0m\u001b[33m [ 85%]\u001b[0m\n",
      "tests/brown/utils/test_network.py::TestPing::test_ping_invalid_content_type \u001b[32mPASSED\u001b[0m\u001b[33m [ 85%]\u001b[0m\n",
      "tests/brown/utils/test_network.py::TestPing::test_ping_non_200_status \u001b[32mPASSED\u001b[0m\u001b[33m [ 86%]\u001b[0m\n",
      "tests/brown/utils/test_network.py::TestPing::test_ping_request_error \u001b[32mPASSED\u001b[0m\u001b[33m [ 86%]\u001b[0m\n",
      "tests/brown/utils/test_network.py::TestPing::test_ping_timeout \u001b[32mPASSED\u001b[0m\u001b[33m    [ 87%]\u001b[0m\n",
      "tests/brown/utils/test_network.py::TestPing::test_ping_invalid_url \u001b[32mPASSED\u001b[0m\u001b[33m [ 87%]\u001b[0m\n",
      "tests/brown/utils/test_network.py::TestIsImageUrlValid::test_valid_image_url \u001b[32mPASSED\u001b[0m\u001b[33m [ 88%]\u001b[0m\n",
      "tests/brown/utils/test_network.py::TestIsImageUrlValid::test_invalid_domain \u001b[32mPASSED\u001b[0m\u001b[33m [ 88%]\u001b[0m\n",
      "tests/brown/utils/test_network.py::TestIsImageUrlValid::test_valid_domain_but_unreachable \u001b[32mPASSED\u001b[0m\u001b[33m [ 89%]\u001b[0m\n",
      "tests/brown/utils/test_network.py::TestIsImageUrlValid::test_custom_timeout \u001b[32mPASSED\u001b[0m\u001b[33m [ 89%]\u001b[0m\n",
      "tests/brown/utils/test_network.py::TestIsImageUrlValid::test_empty_url \u001b[32mPASSED\u001b[0m\u001b[33m [ 90%]\u001b[0m\n",
      "tests/brown/utils/test_network.py::TestIsImageUrlValid::test_malformed_url \u001b[32mPASSED\u001b[0m\u001b[33m [ 90%]\u001b[0m\n",
      "tests/brown/utils/test_s.py::TestCamelToSnake::test_camel_to_snake_simple \u001b[32mPASSED\u001b[0m\u001b[33m [ 91%]\u001b[0m\n",
      "tests/brown/utils/test_s.py::TestCamelToSnake::test_camel_to_snake_with_numbers \u001b[32mPASSED\u001b[0m\u001b[33m [ 91%]\u001b[0m\n",
      "tests/brown/utils/test_s.py::TestCamelToSnake::test_camel_to_snake_single_word \u001b[32mPASSED\u001b[0m\u001b[33m [ 92%]\u001b[0m\n",
      "tests/brown/utils/test_s.py::TestCamelToSnake::test_camel_to_snake_empty \u001b[32mPASSED\u001b[0m\u001b[33m [ 92%]\u001b[0m\n",
      "tests/brown/utils/test_s.py::TestCamelToSnake::test_camel_to_snake_already_snake \u001b[32mPASSED\u001b[0m\u001b[33m [ 92%]\u001b[0m\n",
      "tests/brown/utils/test_s.py::TestNormalizeAnyToStr::test_normalize_any_to_str_with_string \u001b[32mPASSED\u001b[0m\u001b[33m [ 93%]\u001b[0m\n",
      "tests/brown/utils/test_s.py::TestNormalizeAnyToStr::test_normalize_any_to_str_with_list \u001b[32mPASSED\u001b[0m\u001b[33m [ 93%]\u001b[0m\n",
      "tests/brown/utils/test_s.py::TestNormalizeAnyToStr::test_normalize_any_to_str_with_dict \u001b[32mPASSED\u001b[0m\u001b[33m [ 94%]\u001b[0m\n",
      "tests/brown/utils/test_s.py::TestNormalizeAnyToStr::test_normalize_any_to_str_invalid_list \u001b[32mPASSED\u001b[0m\u001b[33m [ 94%]\u001b[0m\n",
      "tests/brown/utils/test_s.py::TestNormalizeAnyToStr::test_normalize_any_to_str_invalid_dict \u001b[32mPASSED\u001b[0m\u001b[33m [ 95%]\u001b[0m\n",
      "tests/brown/utils/test_s.py::TestNormalizeAnyToStr::test_normalize_any_to_str_invalid_type \u001b[32mPASSED\u001b[0m\u001b[33m [ 95%]\u001b[0m\n",
      "tests/brown/utils/test_s.py::TestCleanMarkdownLinks::test_clean_markdown_links_simple \u001b[32mPASSED\u001b[0m\u001b[33m [ 96%]\u001b[0m\n",
      "tests/brown/utils/test_s.py::TestCleanMarkdownLinks::test_clean_markdown_links_multiple \u001b[32mPASSED\u001b[0m\u001b[33m [ 96%]\u001b[0m\n",
      "tests/brown/utils/test_s.py::TestCleanMarkdownLinks::test_clean_markdown_links_with_images \u001b[32mPASSED\u001b[0m\u001b[33m [ 97%]\u001b[0m\n",
      "tests/brown/utils/test_s.py::TestCleanMarkdownLinks::test_clean_markdown_links_malformed \u001b[32mPASSED\u001b[0m\u001b[33m [ 97%]\u001b[0m\n",
      "tests/brown/utils/test_s.py::TestCleanMarkdownLinks::test_clean_markdown_links_no_links \u001b[32mPASSED\u001b[0m\u001b[33m [ 98%]\u001b[0m\n",
      "tests/brown/utils/test_s.py::TestCleanMarkdownLinks::test_clean_markdown_links_empty \u001b[32mPASSED\u001b[0m\u001b[33m [ 98%]\u001b[0m\n",
      "tests/brown/utils/test_s.py::TestCleanMarkdownLinks::test_clean_markdown_links_complex \u001b[32mPASSED\u001b[0m\u001b[33m [ 99%]\u001b[0m\n",
      "tests/brown/utils/test_s.py::TestCleanMarkdownLinks::test_clean_markdown_links_with_text \u001b[32mPASSED\u001b[0m\u001b[33m [ 99%]\u001b[0m\n",
      "tests/brown/utils/test_s.py::TestCleanMarkdownLinks::test_clean_markdown_links_nested_brackets \u001b[32mPASSED\u001b[0m\u001b[33m [100%]\u001b[0m\n",
      "\n",
      "\u001b[33m=============================== warnings summary ===============================\u001b[0m\n",
      "tests/brown/domain/test_research.py::TestResearch::test_research_str_representation\n",
      "  /Users/fabio/Desktop/course-ai-agents/lessons/writing_workflow/src/brown/utils/a.py:90: DeprecationWarning: There is no current event loop\n",
      "    loop = asyncio.get_event_loop()\n",
      "\n",
      "tests/brown/utils/test_network.py::TestPing::test_ping_success\n",
      "  /Users/fabio/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/unittest/mock.py:2217: RuntimeWarning: coroutine 'run_jobs.<locals>.worker' was never awaited\n",
      "    def __init__(self, name, parent):\n",
      "  Enable tracemalloc to get traceback where the object was allocated.\n",
      "  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.\n",
      "\n",
      "tests/brown/utils/test_network.py::TestPing::test_ping_non_200_status\n",
      "  /Users/fabio/Desktop/course-ai-agents/lessons/writing_workflow/src/brown/utils/network.py:39: RuntimeWarning: coroutine 'AsyncMockMixin._execute_mock_call' was never awaited\n",
      "    if response.status_code == 405 or not response.headers.get(\"Content-Type\"):\n",
      "  Enable tracemalloc to get traceback where the object was allocated.\n",
      "  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.\n",
      "\n",
      "-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n",
      "\u001b[33m======================= \u001b[32m214 passed\u001b[0m, \u001b[33m\u001b[1m3 warnings\u001b[0m\u001b[33m in 0.91s\u001b[0m\u001b[33m ========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!CONFIG_FILE=configs/debug.yaml uv run pytest -v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debef37e",
   "metadata": {},
   "source": [
    "The `-v` flag provides verbose output, showing each test as it runs. The `CONFIG_FILE=configs/debug.yaml` ensures all tests use fake models instead of real LLMs.\n",
    "\n",
    "### 6.1 Running Specific Test Files\n",
    "\n",
    "You can run tests for specific components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3a511303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1m`VIRTUAL_ENV=/Users/fabio/Desktop/course-ai-agents/.venv` does not match the project environment path `.venv` and will be ignored; use `--active` to target the active environment instead\u001b[0m\n",
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.12.11, pytest-8.4.2, pluggy-1.6.0 -- /Users/fabio/Desktop/course-ai-agents/lessons/writing_workflow/.venv/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /Users/fabio/Desktop/course-ai-agents/lessons/writing_workflow\n",
      "configfile: pyproject.toml\n",
      "plugins: asyncio-1.2.0, anyio-4.11.0, langsmith-0.4.38, opik-1.8.96\n",
      "asyncio: mode=Mode.STRICT, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function\n",
      "collected 74 items                                                             \u001b[0m\n",
      "\n",
      "tests/brown/domain/test_articles.py::TestArticle::test_article_creation \u001b[32mPASSED\u001b[0m\u001b[32m [  1%]\u001b[0m\n",
      "tests/brown/domain/test_articles.py::TestArticle::test_article_to_context \u001b[32mPASSED\u001b[0m\u001b[32m [  2%]\u001b[0m\n",
      "tests/brown/domain/test_articles.py::TestArticle::test_article_to_markdown \u001b[32mPASSED\u001b[0m\u001b[32m [  4%]\u001b[0m\n",
      "tests/brown/domain/test_articles.py::TestArticle::test_article_str_representation \u001b[32mPASSED\u001b[0m\u001b[32m [  5%]\u001b[0m\n",
      "tests/brown/domain/test_articles.py::TestArticle::test_article_empty_content \u001b[32mPASSED\u001b[0m\u001b[32m [  6%]\u001b[0m\n",
      "tests/brown/domain/test_articles.py::TestSelectedText::test_selected_text_creation \u001b[32mPASSED\u001b[0m\u001b[32m [  8%]\u001b[0m\n",
      "tests/brown/domain/test_articles.py::TestSelectedText::test_selected_text_to_context \u001b[32mPASSED\u001b[0m\u001b[32m [  9%]\u001b[0m\n",
      "tests/brown/domain/test_articles.py::TestSelectedText::test_selected_text_single_line \u001b[32mPASSED\u001b[0m\u001b[32m [ 10%]\u001b[0m\n",
      "tests/brown/domain/test_articles.py::TestArticleExample::test_article_example_creation \u001b[32mPASSED\u001b[0m\u001b[32m [ 12%]\u001b[0m\n",
      "tests/brown/domain/test_articles.py::TestArticleExample::test_article_example_to_context \u001b[32mPASSED\u001b[0m\u001b[32m [ 13%]\u001b[0m\n",
      "tests/brown/domain/test_articles.py::TestArticleExample::test_article_example_str_representation \u001b[32mPASSED\u001b[0m\u001b[32m [ 14%]\u001b[0m\n",
      "tests/brown/domain/test_articles.py::TestArticleExamples::test_article_examples_creation \u001b[32mPASSED\u001b[0m\u001b[32m [ 16%]\u001b[0m\n",
      "tests/brown/domain/test_articles.py::TestArticleExamples::test_article_examples_to_context \u001b[32mPASSED\u001b[0m\u001b[32m [ 17%]\u001b[0m\n",
      "tests/brown/domain/test_articles.py::TestArticleExamples::test_article_examples_empty \u001b[32mPASSED\u001b[0m\u001b[32m [ 18%]\u001b[0m\n",
      "tests/brown/domain/test_articles.py::TestArticleExamples::test_article_examples_single \u001b[32mPASSED\u001b[0m\u001b[32m [ 20%]\u001b[0m\n",
      "tests/brown/domain/test_guidelines.py::TestArticleGuideline::test_article_guideline_creation \u001b[32mPASSED\u001b[0m\u001b[32m [ 21%]\u001b[0m\n",
      "tests/brown/domain/test_guidelines.py::TestArticleGuideline::test_article_guideline_to_context \u001b[32mPASSED\u001b[0m\u001b[32m [ 22%]\u001b[0m\n",
      "tests/brown/domain/test_guidelines.py::TestArticleGuideline::test_article_guideline_empty_content \u001b[32mPASSED\u001b[0m\u001b[32m [ 24%]\u001b[0m\n",
      "tests/brown/domain/test_guidelines.py::TestArticleGuideline::test_article_guideline_complex_content \u001b[32mPASSED\u001b[0m\u001b[32m [ 25%]\u001b[0m\n",
      "tests/brown/domain/test_media_items.py::TestMediaItem::test_media_item_creation \u001b[32mPASSED\u001b[0m\u001b[32m [ 27%]\u001b[0m\n",
      "tests/brown/domain/test_media_items.py::TestMediaItem::test_media_item_to_context \u001b[32mPASSED\u001b[0m\u001b[32m [ 28%]\u001b[0m\n",
      "tests/brown/domain/test_media_items.py::TestMediaItem::test_media_item_different_types \u001b[32mPASSED\u001b[0m\u001b[32m [ 29%]\u001b[0m\n",
      "tests/brown/domain/test_media_items.py::TestMediaItem::test_media_item_str_representation \u001b[32mPASSED\u001b[0m\u001b[32m [ 31%]\u001b[0m\n",
      "tests/brown/domain/test_media_items.py::TestMediaItems::test_media_items_creation \u001b[32mPASSED\u001b[0m\u001b[32m [ 32%]\u001b[0m\n",
      "tests/brown/domain/test_media_items.py::TestMediaItems::test_media_items_to_context \u001b[32mPASSED\u001b[0m\u001b[32m [ 33%]\u001b[0m\n",
      "tests/brown/domain/test_media_items.py::TestMediaItems::test_media_items_empty \u001b[32mPASSED\u001b[0m\u001b[32m [ 35%]\u001b[0m\n",
      "tests/brown/domain/test_media_items.py::TestMediaItems::test_media_items_single \u001b[32mPASSED\u001b[0m\u001b[32m [ 36%]\u001b[0m\n",
      "tests/brown/domain/test_media_items.py::TestMediaItems::test_media_items_str_representation \u001b[32mPASSED\u001b[0m\u001b[32m [ 37%]\u001b[0m\n",
      "tests/brown/domain/test_media_items.py::TestMediaItems::test_media_items_build_classmethod \u001b[32mPASSED\u001b[0m\u001b[32m [ 39%]\u001b[0m\n",
      "tests/brown/domain/test_media_items.py::TestMediaItems::test_are_available_only_in_source_property \u001b[32mPASSED\u001b[0m\u001b[32m [ 40%]\u001b[0m\n",
      "tests/brown/domain/test_media_items.py::TestMermaidDiagram::test_mermaid_diagram_creation \u001b[32mPASSED\u001b[0m\u001b[32m [ 41%]\u001b[0m\n",
      "tests/brown/domain/test_mixins.py::TestContextMixin::test_context_mixin_xml_tag \u001b[32mPASSED\u001b[0m\u001b[32m [ 43%]\u001b[0m\n",
      "tests/brown/domain/test_mixins.py::TestContextMixin::test_context_mixin_xml_tag_with_profile \u001b[32mPASSED\u001b[0m\u001b[32m [ 44%]\u001b[0m\n",
      "tests/brown/domain/test_mixins.py::TestContextMixin::test_context_mixin_xml_tag_with_suffix \u001b[32mPASSED\u001b[0m\u001b[32m [ 45%]\u001b[0m\n",
      "tests/brown/domain/test_mixins.py::TestMarkdownMixin::test_markdown_mixin_abstract \u001b[32mPASSED\u001b[0m\u001b[32m [ 47%]\u001b[0m\n",
      "tests/brown/domain/test_mixins.py::TestMarkdownMixin::test_markdown_mixin_implementation \u001b[32mPASSED\u001b[0m\u001b[32m [ 48%]\u001b[0m\n",
      "tests/brown/domain/test_mixins.py::TestMarkdownMixin::test_markdown_mixin_missing_implementation \u001b[32mPASSED\u001b[0m\u001b[32m [ 50%]\u001b[0m\n",
      "tests/brown/domain/test_profiles.py::TestProfile::test_profile_creation \u001b[32mPASSED\u001b[0m\u001b[32m [ 51%]\u001b[0m\n",
      "tests/brown/domain/test_profiles.py::TestProfile::test_profile_xml_tag \u001b[32mPASSED\u001b[0m\u001b[32m [ 52%]\u001b[0m\n",
      "tests/brown/domain/test_profiles.py::TestProfile::test_profile_to_context \u001b[32mPASSED\u001b[0m\u001b[32m [ 54%]\u001b[0m\n",
      "tests/brown/domain/test_profiles.py::TestCharacterProfile::test_character_profile_creation \u001b[32mPASSED\u001b[0m\u001b[32m [ 55%]\u001b[0m\n",
      "tests/brown/domain/test_profiles.py::TestCharacterProfile::test_character_profile_to_context \u001b[32mPASSED\u001b[0m\u001b[32m [ 56%]\u001b[0m\n",
      "tests/brown/domain/test_profiles.py::TestArticleProfile::test_article_profile_creation \u001b[32mPASSED\u001b[0m\u001b[32m [ 58%]\u001b[0m\n",
      "tests/brown/domain/test_profiles.py::TestArticleProfile::test_article_profile_to_context \u001b[32mPASSED\u001b[0m\u001b[32m [ 59%]\u001b[0m\n",
      "tests/brown/domain/test_profiles.py::TestStructureProfile::test_structure_profile_creation \u001b[32mPASSED\u001b[0m\u001b[32m [ 60%]\u001b[0m\n",
      "tests/brown/domain/test_profiles.py::TestStructureProfile::test_structure_profile_to_context \u001b[32mPASSED\u001b[0m\u001b[32m [ 62%]\u001b[0m\n",
      "tests/brown/domain/test_profiles.py::TestMechanicsProfile::test_mechanics_profile_creation \u001b[32mPASSED\u001b[0m\u001b[32m [ 63%]\u001b[0m\n",
      "tests/brown/domain/test_profiles.py::TestMechanicsProfile::test_mechanics_profile_to_context \u001b[32mPASSED\u001b[0m\u001b[32m [ 64%]\u001b[0m\n",
      "tests/brown/domain/test_profiles.py::TestTerminologyProfile::test_terminology_profile_creation \u001b[32mPASSED\u001b[0m\u001b[32m [ 66%]\u001b[0m\n",
      "tests/brown/domain/test_profiles.py::TestTerminologyProfile::test_terminology_profile_to_context \u001b[32mPASSED\u001b[0m\u001b[32m [ 67%]\u001b[0m\n",
      "tests/brown/domain/test_profiles.py::TestTonalityProfile::test_tonality_profile_creation \u001b[32mPASSED\u001b[0m\u001b[32m [ 68%]\u001b[0m\n",
      "tests/brown/domain/test_profiles.py::TestTonalityProfile::test_tonality_profile_to_context \u001b[32mPASSED\u001b[0m\u001b[32m [ 70%]\u001b[0m\n",
      "tests/brown/domain/test_profiles.py::TestArticleProfiles::test_article_profiles_creation \u001b[32mPASSED\u001b[0m\u001b[32m [ 71%]\u001b[0m\n",
      "tests/brown/domain/test_profiles.py::TestArticleProfiles::test_article_profiles_to_context \u001b[32mPASSED\u001b[0m\u001b[32m [ 72%]\u001b[0m\n",
      "tests/brown/domain/test_research.py::TestResearch::test_research_creation \u001b[32mPASSED\u001b[0m\u001b[32m [ 74%]\u001b[0m\n",
      "tests/brown/domain/test_research.py::TestResearch::test_research_to_context \u001b[32mPASSED\u001b[0m\u001b[32m [ 75%]\u001b[0m\n",
      "tests/brown/domain/test_research.py::TestResearch::test_research_empty_content \u001b[32mPASSED\u001b[0m\u001b[32m [ 77%]\u001b[0m\n",
      "tests/brown/domain/test_research.py::TestResearch::test_research_complex_content \u001b[32mPASSED\u001b[0m\u001b[32m [ 78%]\u001b[0m\n",
      "tests/brown/domain/test_research.py::TestResearch::test_research_str_representation \u001b[32mPASSED\u001b[0m\u001b[32m [ 79%]\u001b[0m\n",
      "tests/brown/domain/test_reviews.py::TestReview::test_review_creation \u001b[32mPASSED\u001b[0m\u001b[33m [ 81%]\u001b[0m\n",
      "tests/brown/domain/test_reviews.py::TestReview::test_review_to_context \u001b[32mPASSED\u001b[0m\u001b[33m [ 82%]\u001b[0m\n",
      "tests/brown/domain/test_reviews.py::TestReview::test_review_score_validation \u001b[32mPASSED\u001b[0m\u001b[33m [ 83%]\u001b[0m\n",
      "tests/brown/domain/test_reviews.py::TestReview::test_review_str_representation \u001b[32mPASSED\u001b[0m\u001b[33m [ 85%]\u001b[0m\n",
      "tests/brown/domain/test_reviews.py::TestArticleReviews::test_article_reviews_creation \u001b[32mPASSED\u001b[0m\u001b[33m [ 86%]\u001b[0m\n",
      "tests/brown/domain/test_reviews.py::TestArticleReviews::test_article_reviews_to_context \u001b[32mPASSED\u001b[0m\u001b[33m [ 87%]\u001b[0m\n",
      "tests/brown/domain/test_reviews.py::TestArticleReviews::test_article_reviews_with_article \u001b[32mPASSED\u001b[0m\u001b[33m [ 89%]\u001b[0m\n",
      "tests/brown/domain/test_reviews.py::TestArticleReviews::test_article_reviews_without_article \u001b[32mPASSED\u001b[0m\u001b[33m [ 90%]\u001b[0m\n",
      "tests/brown/domain/test_reviews.py::TestArticleReviews::test_article_reviews_empty \u001b[32mPASSED\u001b[0m\u001b[33m [ 91%]\u001b[0m\n",
      "tests/brown/domain/test_reviews.py::TestArticleReviews::test_article_reviews_single \u001b[32mPASSED\u001b[0m\u001b[33m [ 93%]\u001b[0m\n",
      "tests/brown/domain/test_reviews.py::TestArticleReviews::test_article_reviews_str_representation \u001b[32mPASSED\u001b[0m\u001b[33m [ 94%]\u001b[0m\n",
      "tests/brown/domain/test_reviews.py::TestHumanFeedback::test_human_feedback_creation \u001b[32mPASSED\u001b[0m\u001b[33m [ 95%]\u001b[0m\n",
      "tests/brown/domain/test_reviews.py::TestHumanFeedback::test_human_feedback_to_context \u001b[32mPASSED\u001b[0m\u001b[33m [ 97%]\u001b[0m\n",
      "tests/brown/domain/test_reviews.py::TestSelectedTextReviews::test_selected_text_reviews_creation \u001b[32mPASSED\u001b[0m\u001b[33m [ 98%]\u001b[0m\n",
      "tests/brown/domain/test_reviews.py::TestSelectedTextReviews::test_selected_text_reviews_to_context \u001b[32mPASSED\u001b[0m\u001b[33m [100%]\u001b[0m\n",
      "\n",
      "\u001b[33m=============================== warnings summary ===============================\u001b[0m\n",
      "tests/brown/domain/test_research.py::TestResearch::test_research_str_representation\n",
      "  /Users/fabio/Desktop/course-ai-agents/lessons/writing_workflow/src/brown/utils/a.py:90: DeprecationWarning: There is no current event loop\n",
      "    loop = asyncio.get_event_loop()\n",
      "\n",
      "-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n",
      "\u001b[33m======================== \u001b[32m74 passed\u001b[0m, \u001b[33m\u001b[1m1 warning\u001b[0m\u001b[33m in 0.07s\u001b[0m\u001b[33m =========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!CONFIG_FILE=configs/debug.yaml uv run pytest tests/brown/domain/ -v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bbb947ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1m`VIRTUAL_ENV=/Users/fabio/Desktop/course-ai-agents/.venv` does not match the project environment path `.venv` and will be ignored; use `--active` to target the active environment instead\u001b[0m\n",
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.12.11, pytest-8.4.2, pluggy-1.6.0 -- /Users/fabio/Desktop/course-ai-agents/lessons/writing_workflow/.venv/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /Users/fabio/Desktop/course-ai-agents/lessons/writing_workflow\n",
      "configfile: pyproject.toml\n",
      "plugins: asyncio-1.2.0, anyio-4.11.0, langsmith-0.4.38, opik-1.8.96\n",
      "asyncio: mode=Mode.STRICT, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function\n",
      "collected 37 items                                                             \u001b[0m\n",
      "\n",
      "tests/brown/nodes/test_article_reviewer.py::TestArticleReviewer::test_article_reviewer_initialization \u001b[32mPASSED\u001b[0m\u001b[32m [  2%]\u001b[0m\n",
      "tests/brown/nodes/test_article_reviewer.py::TestArticleReviewer::test_article_reviewer_ainvoke_success \u001b[32mPASSED\u001b[0m\u001b[32m [  5%]\u001b[0m\n",
      "tests/brown/nodes/test_article_reviewer.py::TestArticleReviewer::test_article_reviewer_structured_output \u001b[32mPASSED\u001b[0m\u001b[32m [  8%]\u001b[0m\n",
      "tests/brown/nodes/test_article_reviewer.py::TestArticleReviewer::test_article_reviewer_empty_article \u001b[32mPASSED\u001b[0m\u001b[32m [ 10%]\u001b[0m\n",
      "tests/brown/nodes/test_article_reviewer.py::TestArticleReviewer::test_article_reviewer_multiple_reviews \u001b[32mPASSED\u001b[0m\u001b[32m [ 13%]\u001b[0m\n",
      "tests/brown/nodes/test_article_reviewer.py::TestArticleReviewer::test_article_reviewer_requires_mocked_response_for_fake_model \u001b[32mPASSED\u001b[0m\u001b[32m [ 16%]\u001b[0m\n",
      "tests/brown/nodes/test_article_reviewer.py::TestArticleReviewer::test_article_reviewer_selected_text_initialization \u001b[32mPASSED\u001b[0m\u001b[32m [ 18%]\u001b[0m\n",
      "tests/brown/nodes/test_article_reviewer.py::TestArticleReviewer::test_article_reviewer_selected_text_ainvoke \u001b[32mPASSED\u001b[0m\u001b[32m [ 21%]\u001b[0m\n",
      "tests/brown/nodes/test_article_reviewer.py::TestArticleReviewer::test_article_reviewer_selected_text_with_human_feedback \u001b[32mPASSED\u001b[0m\u001b[32m [ 24%]\u001b[0m\n",
      "tests/brown/nodes/test_article_reviewer.py::TestArticleReviewer::test_article_reviewer_article_property \u001b[32mPASSED\u001b[0m\u001b[32m [ 27%]\u001b[0m\n",
      "tests/brown/nodes/test_article_writer.py::TestArticleWriter::test_article_writer_initialization \u001b[32mPASSED\u001b[0m\u001b[32m [ 29%]\u001b[0m\n",
      "tests/brown/nodes/test_article_writer.py::TestArticleWriter::test_article_writer_ainvoke_success \u001b[32mPASSED\u001b[0m\u001b[32m [ 32%]\u001b[0m\n",
      "tests/brown/nodes/test_article_writer.py::TestArticleWriter::test_article_writer_with_media_items \u001b[32mPASSED\u001b[0m\u001b[32m [ 35%]\u001b[0m\n",
      "tests/brown/nodes/test_article_writer.py::TestArticleWriter::test_article_writer_with_reviews \u001b[32mPASSED\u001b[0m\u001b[32m [ 37%]\u001b[0m\n",
      "tests/brown/nodes/test_article_writer.py::TestArticleWriter::test_article_writer_empty_input \u001b[32mPASSED\u001b[0m\u001b[32m [ 40%]\u001b[0m\n",
      "tests/brown/nodes/test_article_writer.py::TestArticleWriter::test_article_writer_requires_mocked_response_for_fake_model \u001b[32mPASSED\u001b[0m\u001b[32m [ 43%]\u001b[0m\n",
      "tests/brown/nodes/test_article_writer.py::TestArticleWriter::test_article_writer_with_selected_text_reviews \u001b[32mPASSED\u001b[0m\u001b[32m [ 45%]\u001b[0m\n",
      "tests/brown/nodes/test_article_writer.py::TestArticleWriter::test_article_writer_selected_text_preserves_line_numbers \u001b[32mPASSED\u001b[0m\u001b[32m [ 48%]\u001b[0m\n",
      "tests/brown/nodes/test_article_writer.py::TestArticleWriter::test_article_writer_selected_text_vs_article_output \u001b[32mPASSED\u001b[0m\u001b[32m [ 51%]\u001b[0m\n",
      "tests/brown/nodes/test_article_writer.py::TestArticleWriter::test_article_writer_selected_text_content \u001b[32mPASSED\u001b[0m\u001b[32m [ 54%]\u001b[0m\n",
      "tests/brown/nodes/test_base.py::TestNode::test_node_initialization \u001b[32mPASSED\u001b[0m\u001b[32m [ 56%]\u001b[0m\n",
      "tests/brown/nodes/test_base.py::TestNode::test_node_build_toolkit \u001b[32mPASSED\u001b[0m\u001b[32m [ 59%]\u001b[0m\n",
      "tests/brown/nodes/test_base.py::TestNode::test_node_build_model_client \u001b[32mPASSED\u001b[0m\u001b[32m [ 62%]\u001b[0m\n",
      "tests/brown/nodes/test_base.py::TestNode::test_node_build_model_client_fake \u001b[32mPASSED\u001b[0m\u001b[32m [ 64%]\u001b[0m\n",
      "tests/brown/nodes/test_base.py::TestNode::test_node_build_user_input_content \u001b[32mPASSED\u001b[0m\u001b[32m [ 67%]\u001b[0m\n",
      "tests/brown/nodes/test_base.py::TestNode::test_node_as_tool \u001b[32mPASSED\u001b[0m\u001b[32m       [ 70%]\u001b[0m\n",
      "tests/brown/nodes/test_base.py::TestNode::test_node_with_fake_model_requires_mocked_response \u001b[32mPASSED\u001b[0m\u001b[32m [ 72%]\u001b[0m\n",
      "tests/brown/nodes/test_base.py::TestNode::test_node_set_mocked_responses \u001b[32mPASSED\u001b[0m\u001b[32m [ 75%]\u001b[0m\n",
      "tests/brown/nodes/test_base.py::TestToolkit::test_toolkit_initialization \u001b[32mPASSED\u001b[0m\u001b[32m [ 78%]\u001b[0m\n",
      "tests/brown/nodes/test_base.py::TestToolkit::test_toolkit_abstract_methods \u001b[32mPASSED\u001b[0m\u001b[32m [ 81%]\u001b[0m\n",
      "tests/brown/nodes/test_media_generator.py::TestMermaidDiagramGenerator::test_mermaid_diagram_generator_initialization \u001b[32mPASSED\u001b[0m\u001b[32m [ 83%]\u001b[0m\n",
      "tests/brown/nodes/test_media_generator.py::TestMermaidDiagramGenerator::test_mermaid_diagram_generator_ainvoke \u001b[32mPASSED\u001b[0m\u001b[32m [ 86%]\u001b[0m\n",
      "tests/brown/nodes/test_media_generator.py::TestMermaidDiagramGenerator::test_mermaid_diagram_generator_error_handling \u001b[32mPASSED\u001b[0m\u001b[32m [ 89%]\u001b[0m\n",
      "tests/brown/nodes/test_media_generator.py::TestMermaidDiagramGenerator::test_mermaid_diagram_generator_requires_mocked_response_for_fake_model \u001b[32mPASSED\u001b[0m\u001b[32m [ 91%]\u001b[0m\n",
      "tests/brown/nodes/test_media_generator.py::TestMediaGeneratorOrchestrator::test_media_generator_orchestrator_initialization \u001b[32mPASSED\u001b[0m\u001b[32m [ 94%]\u001b[0m\n",
      "tests/brown/nodes/test_media_generator.py::TestMediaGeneratorOrchestrator::test_media_generator_orchestrator_ainvoke \u001b[32mPASSED\u001b[0m\u001b[32m [ 97%]\u001b[0m\n",
      "tests/brown/nodes/test_media_generator.py::TestMediaGeneratorOrchestrator::test_media_generator_orchestrator_requires_mocked_response_for_fake_model \u001b[32mPASSED\u001b[0m\u001b[32m [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m37 passed\u001b[0m\u001b[32m in 0.13s\u001b[0m\u001b[32m ==============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!CONFIG_FILE=configs/debug.yaml uv run pytest tests/brown/nodes/ -v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be97003",
   "metadata": {},
   "source": [
    "### 6.3 Understanding What's Being Tested\n",
    "\n",
    "Brown's test suite includes:\n",
    "- **Domain tests** (`tests/brown/domain/`): Testing Pydantic models and data structures without any LLM calls\n",
    "- **Node tests** (`tests/brown/nodes/`): Testing agent nodes like ArticleWriter and ArticleReviewer with mocked LLM responses\n",
    "- **Utility tests** (`tests/brown/utils/`): Testing helper functions\n",
    "- **Evaluation tests** (`tests/brown/evals/`): Testing evaluation metrics and dataset handling\n",
    "\n",
    "The complete test suite runs in under a minute and requires no API keys. Every test is deterministic."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
