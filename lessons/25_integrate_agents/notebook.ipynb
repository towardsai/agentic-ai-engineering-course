{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "564e6695",
      "metadata": {},
      "source": [
        "# Lesson 25: Integrating Multiple AI Agents with MCP\n",
        "\n",
        "Throughout the course, we've built two agents: Nova for research and Brown for writing. Now it's time to integrate them into a unified system. In this lesson, we will explore how to use both the Nova research agent and the Brown writing workflow together by leveraging the Model Context Protocol (MCP).\n",
        "\n",
        "We've already seen how each agent works as an MCP server in previous lessons. Nova exposes 11 tools for research tasks, while Brown provides 3 tools for article generation and editing. The beauty of MCP is that it makes integration straightforward. We'll explore two approaches:\n",
        "\n",
        "1. **Multi-Server MCP Client**: A single MCP client that connects to multiple independent MCP servers. To run both agents together, you execute their MCP prompts in sequence.\n",
        "2. **Composed MCP Server**: A single MCP client that connects to a single MCP server that composes multiple MCP servers together using FastMCP's composition features. The composed server can add new capabilitiesâ€”like a combined workflow prompt that orchestrates both agents with a single command.\n",
        "\n",
        "Both approaches have their use cases, and by the end of this lesson, you'll understand when to use each one.\n",
        "\n",
        "Learning objectives:\n",
        "- Learn how to connect an MCP client to multiple MCP servers simultaneously\n",
        "- Understand how to use FastMCP's composition features to create a unified MCP server\n",
        "- See how composed servers can add orchestration prompts that coordinate multiple agents\n",
        "- Compare multi-server client vs composed server approaches\n",
        "- See the practical benefits of MCP for agent integration"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec74cbeb",
      "metadata": {},
      "source": [
        "## 1. Setup\n",
        "\n",
        "First, we define some standard Magic Python commands to autoreload Python packages whenever they change:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24fcc3b3",
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81250056",
      "metadata": {},
      "source": [
        "### Import Key Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da957b6e",
      "metadata": {},
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply() # Allow nested async usage in notebooks"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3ff4c59",
      "metadata": {},
      "source": [
        "## 2. Understanding Multi-Agent Orchestration: The MCP Approach\n",
        "\n",
        "Before we dive into the technical implementation, let's understand the orchestration model we're using and why it matters.\n",
        "\n",
        "### The Central LLM Orchestration Pattern\n",
        "\n",
        "In this lesson, we're implementing what's known as a **Central LLM Orchestration** pattern. In this approach, a single, central LLM (e.g. the one powering your IDE assistant, like Claude in Cursor) has access to tools from multiple specialized agents. When you give it a task, the LLM dynamically decides which agent's tools to use based on the task requirements, maintaining a single conversation context and orchestrating the workflow by selecting the appropriate tools as needed.\n",
        "\n",
        "This is fundamentally different from other orchestration patterns. A **Supervisor Agent** pattern would have one agent explicitly delegate entire sub-tasks to worker agents. A **Sequential Pipeline** would force agents to always execute in a fixed order. **Peer-to-Peer Communication** would allow agents to directly message each other. Our central LLM orchestration is simpler: the LLM acts as a intelligent tool selector rather than an explicit coordinator.\n",
        "\n",
        "We'll learn more about these other patterns in another lesson.\n",
        "\n",
        "### Why This Pattern Works Well with MCP\n",
        "\n",
        "The Model Context Protocol makes this orchestration pattern particularly elegant. Both Nova and Brown expose their capabilities as MCP tools with clear descriptions, allowing the central LLM to discover all available tools through a single, standardized protocol. This unified tool interface eliminates the need for custom integration code: because both agents speak MCP, we don't need to write adapters or APIs. The client simply connects to both servers and aggregates their tools.\n",
        "\n",
        "The LLM's natural reasoning ability handles the orchestration logic. For example, it intuitively understands that it should use Nova's research tools first, review the results, and then use Brown's writing tools with that research as input. There's no need to explicitly program this workflow or create complex state machines.\n",
        "\n",
        "The flexibility is another key advantage. You can easily add or remove agents by simply updating the configuration file without touching any code. This pattern essentially treats specialized agents as \"tool libraries\" that a central reasoning engine can draw from as needed, making the system both modular and maintainable."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d263e349",
      "metadata": {},
      "source": [
        "### The Rationale: Why Choose Central LLM Orchestration?\n",
        "\n",
        "This orchestration pattern offers several compelling advantages that make it ideal for our use case of integrating Nova and Brown.\n",
        "\n",
        "The most immediate benefit is **simplicity and maintainability**. All decision-making happens in one placeâ€”the central LLM, which means the workflow is transparent and easy to understand. You can see which tools the LLM chooses in real-time as it works through a task. There's no need for complex state management or inter-agent communication protocols, which reduces the cognitive overhead of understanding and debugging the system.\n",
        "\n",
        "The pattern also enables **natural task decomposition**. The central LLM can break down complex requests on the fly without requiring predefined workflows. Even more importantly, it can adapt its strategy based on intermediate results. For example, if research reveals unexpected information, the LLM can adjust its writing approach accordingly. This adaptive behavior is especially valuable for human-in-the-loop workflows common in IDE environments. You can provide feedback at any point in the process, and the LLM incorporates it naturally without requiring explicit handoff protocols.\n",
        "\n",
        "The central LLM maintains a **single, unified context window**, which means it can reference information from Nova's research when calling Brown's tools without requiring explicit data passing between agents. This avoids the notorious \"siloed knowledge\" problem, where critical information gets trapped in one agent's context and becomes unavailable to others who need it.\n",
        "\n",
        "Perhaps most importantly for our specific use case, this pattern is **perfect for sequential, interdependent tasks**. Our workflow (research followed by writing) is inherently sequential, and the writing task depends heavily on research results. A central orchestrator can easily manage these dependencies because it sees the entire workflow and can make informed decisions about when to transition from research to writing."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3aefe0c",
      "metadata": {},
      "source": [
        "### The Tradeoffs: Understanding the Limitations\n",
        "\n",
        "While central LLM orchestration with MCP is powerful, it's important to understand its limitations and recognize when you might need a different approach.\n",
        "\n",
        "The first limitation arises from **tool overload**. As the number of available tools grows beyond approximately 15-20, LLMs begin to struggle with reliable tool selection. They may choose suboptimal tools or miss relevant ones entirely. This degradation in performance is well-documented in the research on agent systems. Our system, with 14 tools total, is comfortably within the limit, but if you were to add many more specialized agents, you'd eventually hit this ceiling and need to consider a different pattern.\n",
        "\n",
        "Another significant limitation is the pattern's inherently **sequential execution model**. If you need to research 50 companies simultaneously, a single LLM executing tools one at a time would be painfully inefficient. In such scenarios, a Supervisor-Worker pattern with parallel execution would be better.\n",
        "\n",
        "The pattern also struggles with **complex inter-agent dependencies**. If agents need to negotiate with each other, engage in debate, or iteratively refine each other's work through back-and-forth exchanges, direct agent-to-agent communication would be more natural. Our pattern handles simple, linear dependencies well (Nova's output feeds into Brown) but complex multi-way interactions would become awkward and difficult to manage.\n",
        "\n",
        "However, central LLM orchestration with MCP is the **right default choice** for most agent integration scenarios. It's simple, maintainable, and leverages the LLM's natural reasoning abilities without adding unnecessary complexity. You should only consider more elaborate patterns when you hit clear scaling limits (e.g. too many tools causing selection problems) or have fundamentally different requirements like massive parallelism or complex agent negotiations.\n",
        "\n",
        "We'll learn more about these other patterns in another lesson.\n",
        "\n",
        "### Integrating Nova and Brown\n",
        "\n",
        "In the previous lessons, we built two specialized agents: Nova for research (which ingests article guidelines, performs web research, scrapes sources, and compiles comprehensive research files) and Brown for writing (which takes research and guidelines to generate, review, and edit articles with human-in-the-loop feedback). These agents were designed to work in sequenceâ€”Nova gathers the research, and Brown uses that research to write the articleâ€”but we've been running them separately. Now we'll orchestrate them together using the central LLM orchestration pattern. Because both agents are already exposed as MCP servers, integration is straightforward: we simply connect to both servers and let the central LLM decide which tools to use and when.\n",
        "\n",
        "We'll explore two different approaches for achieving this integration.\n",
        "1. The first is a **Multi-Server MCP Client**, where a single client connects directly to multiple independent MCP servers simultaneouslyâ€”the client aggregates all tools from both servers and presents them to the LLM.\n",
        "2. The second is a **Composed MCP Server**, where we create a new server that internally connects to both Nova and Brown, exposing their combined capabilities as a single unified endpoint.\n",
        "\n",
        "Both approaches achieve the same goal but differ in their deployment and configuration patterns."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8450a342",
      "metadata": {},
      "source": [
        "## 3. Approach 1: Multi-Server MCP Client\n",
        "\n",
        "The first approach is to create an MCP client that connects to multiple MCP servers simultaneously. FastMCP's `Client` class supports this out of the box by accepting a configuration object that specifies multiple servers."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e1416b6",
      "metadata": {},
      "source": [
        "### 3.1 Multi-Server Configuration File\n",
        "\n",
        "Let's look at the configuration file that defines both Nova and Brown servers.\n",
        "\n",
        "Source: _lessons/25_integrate_agents/mcp_servers_config_http.json_\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"mcpServers\": {\n",
        "    \"nova-research-agent\": {\n",
        "      \"url\": \"http://localhost:8001/mcp\"\n",
        "    },\n",
        "    \"brown-writing-workflow\": {\n",
        "      \"url\": \"http://localhost:8002/mcp\"\n",
        "    }\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "This configuration tells the MCP client how to connect to both servers via HTTP. Each server:\n",
        "- Has a unique name (`nova-research-agent`, `brown-writing-workflow`)\n",
        "- Specifies the HTTP URL where the server is listening"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90964e2b",
      "metadata": {},
      "source": [
        "### 3.2 Creating the Multi-Server Client\n",
        "\n",
        "Now let's see how the client code loads this configuration and connects to both servers.\n",
        "\n",
        "Source: _lessons/agents_integration/mcp_client/src/client.py_\n",
        "\n",
        "```python\n",
        "import argparse\n",
        "import json\n",
        "from pathlib import Path\n",
        "from fastmcp import Client\n",
        "\n",
        "# Parse command line arguments for config file\n",
        "parser = argparse.ArgumentParser(description=\"Multi-Server MCP Client\")\n",
        "parser.add_argument(\"--config\", \"-c\", type=str, default=None,\n",
        "                    help=\"Path to MCP servers config file\")\n",
        "args = parser.parse_args()\n",
        "\n",
        "# Load configuration from JSON file\n",
        "config_path = Path(args.config) if args.config else Path(\"mcp_servers_config.json\")\n",
        "with open(config_path) as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "server_names = list(config[\"mcpServers\"].keys())\n",
        "logging.info(f\"Found {len(server_names)} MCP servers in configuration: {', '.join(server_names)}\")\n",
        "\n",
        "# Create a single client with multi-server configuration\n",
        "client = Client(config)\n",
        "\n",
        "# Connect and fetch capabilities from all servers\n",
        "async with client:\n",
        "    tools = await client.list_tools()\n",
        "    resources = await client.list_resources()\n",
        "    prompts = await client.list_prompts()\n",
        "    \n",
        "    logging.info(\n",
        "        f\"Total capabilities: {len(tools)} tools, {len(resources)} resources, {len(prompts)} prompts\"\n",
        "    )\n",
        "    \n",
        "    # Main conversation loop\n",
        "    while True:\n",
        "        user_input = input(\"ðŸ‘¤ You: \").strip()\n",
        "        parsed_input = parse_user_input(user_input)\n",
        "        \n",
        "        # Handle the user message (commands, prompts, or normal messages)\n",
        "        should_continue, thinking_enabled = await handle_user_message(\n",
        "            parsed_input=parsed_input,\n",
        "            tools=tools,\n",
        "            resources=resources,\n",
        "            prompts=prompts,\n",
        "            conversation_history=conversation_history,\n",
        "            mcp_client=client,\n",
        "            thinking_enabled=thinking_enabled,\n",
        "            server_names=server_names,\n",
        "        )\n",
        "        \n",
        "        if not should_continue:\n",
        "            break\n",
        "```\n",
        "\n",
        "The key insight here is that `Client(config)` accepts a multi-server configuration. When you call `list_tools()`, `list_resources()`, or `list_prompts()`, the client automatically aggregates capabilities from all connected servers.\n",
        "\n",
        "The client supports several interactive commands:\n",
        "- `/tools`, `/resources`, `/prompts` - List available capabilities\n",
        "- `/prompt/<name>?arg=value` - Load and execute an MCP prompt with optional arguments\n",
        "- `/resource/<uri>` - Read a resource's content\n",
        "- `/model-thinking-switch` - Toggle the LLM's thinking mode\n",
        "- `/quit` - Exit the client\n",
        "\n",
        "When a prompt is loaded via `/prompt/<name>`, the client retrieves its content from the MCP server and sends it to the LLM, which then executes the workflow using the available tools."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b510072a",
      "metadata": {},
      "source": [
        "### 3.3 How Capabilities Are Named\n",
        "\n",
        "When you have multiple servers, how do you distinguish which tool belongs to which server? FastMCP handles this automatically by prefixing the capability names with the server name from your configuration.\n",
        "\n",
        "For example, with our configuration having `nova-research-agent` and `brown-writing-workflow` as server names:\n",
        "- Nova's `extract_guidelines_urls` tool becomes `nova-research-agent_extract_guidelines_urls`\n",
        "- Brown's `generate_article` tool becomes `brown-writing-workflow_generate_article`\n",
        "- Nova's `full_research_instructions_prompt` prompt becomes `nova-research-agent_full_research_instructions_prompt`\n",
        "- Brown's `generate_article_prompt` prompt becomes `brown-writing-workflow_generate_article_prompt`\n",
        "\n",
        "This naming convention makes it easy to identify which capability comes from which server, and ensures there are no naming collisions if both servers happen to expose capabilities with the same base name."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa268db3",
      "metadata": {},
      "source": [
        "### 3.4 Running the Multi-Server Client\n",
        "\n",
        "When you run the multi-server client, you'll see output like this:\n",
        "\n",
        "```bash\n",
        "$ cd agents_integration/mcp_client\n",
        "$ uv run -m src.client --config ../../25_integrate_agents/mcp_servers_config_http.json\n",
        "```\n",
        "\n",
        "```terminal\n",
        "INFO:root:Loading MCP server configuration from: mcp_servers_config_http.json\n",
        "INFO:root:Found 2 MCP servers in configuration: nova-research-agent, brown-writing-workflow\n",
        "INFO:root:Connecting to MCP servers...\n",
        "INFO:root:Fetching capabilities from all servers...\n",
        "INFO:root:Total capabilities: 14 tools, 4 resources, 4 prompts\n",
        "\n",
        "============================================================\n",
        "All Servers\n",
        "============================================================\n",
        "\n",
        "  - 14 tools available\n",
        "  - 4 resources available\n",
        "  - 4 prompts available\n",
        "\n",
        "Available Commands: /tools, /resources, /prompts, /prompt/<name>?arg=value, /resource/<uri>, /model-thinking-switch, /quit\n",
        "\n",
        "ðŸ‘¤ You:\n",
        "```\n",
        "\n",
        "When you type `/prompts`, you'll see all prompts from both servers:\n",
        "\n",
        "```terminal\n",
        "ðŸ‘¤ You: /prompts\n",
        "============================================================\n",
        "Prompts (4)\n",
        "============================================================\n",
        "\n",
        "1. nova-research-agent_full_research_instructions_prompt\n",
        "   Complete Nova research agent workflow instructions.\n",
        "\n",
        "2. brown-writing-workflow_generate_article_prompt\n",
        "   Retrieve a prompt that will trigger the article generation workflow using Brown.\n",
        "\n",
        "3. brown-writing-workflow_edit_article_prompt\n",
        "   Retrieve a prompt that will trigger the article editing workflow using Brown.\n",
        "\n",
        "4. brown-writing-workflow_edit_selected_text_prompt\n",
        "   Retrieve a prompt that will trigger the selected text editing workflow using Brown.\n",
        "\n",
        "```\n",
        "\n",
        "This demonstrates that the client successfully connected to both servers and aggregated their capabilities. Notice that each prompt is prefixed with its server name, making it clear which agent provides each capability."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ba681d4",
      "metadata": {},
      "source": [
        "### 3.5 Running the Complete Workflow: Two Prompts in Sequence\n",
        "\n",
        "This section assumes the client is connected using the configuration from `lessons/25_integrate_agents/mcp_servers_config_http.json`.\n",
        "\n",
        "To run both agents togetherâ€”first Nova for research, then Brown for article generationâ€”you need to execute their prompts in sequence.\n",
        "\n",
        "**Step 1: Run Nova's Research Workflow**\n",
        "\n",
        "First, trigger Nova's research prompt:\n",
        "\n",
        "```terminal\n",
        "ðŸ‘¤ You: /prompt/nova-research-agent_full_research_instructions_prompt\n",
        "```\n",
        "\n",
        "The LLM receives the prompt instructions and responds:\n",
        "\n",
        "```terminal\n",
        "ðŸ’¬ LLM Response: Hello! I will help you execute the Nova research agent workflow. Here are the steps involved:\n",
        "\n",
        "1.  **Setup:** Extract URLs and local file references from your article guidelines.\n",
        "2.  **Process Resources:** Concurrently process local files, scrape other web URLs, process GitHub URLs, and transcribe YouTube URLs found in the guidelines.\n",
        "3.  **Research Loop:** Conduct 3 rounds of web research using Perplexity to generate new queries and gather results.\n",
        "4.  **Filter Results:** Evaluate and select high-quality sources from the Perplexity research.\n",
        "5.  **Scrape Research Sources:** Identify and scrape the full content of the most valuable research sources.\n",
        "6.  **Final Research File:** Compile all gathered and processed research into a comprehensive markdown file.\n",
        "\n",
        "To begin, please provide the path to your research directory. Also, let me know if you need any modifications to this workflow, such as starting from a specific step or adding user feedback.\n",
        "\n",
        "ðŸ‘¤ You:\n",
        "```\n",
        "\n",
        "You then provide the research directory path and let Nova complete the research workflow.\n",
        "\n",
        "**Step 2: Run Brown's Article Generation Workflow**\n",
        "\n",
        "Once Nova finishes, you trigger Brown's article generation prompt with the same directory:\n",
        "\n",
        "```terminal\n",
        "ðŸ‘¤ You: /prompt/brown-writing-workflow_generate_article_prompt?dir_path=/path/to/your/research/directory\n",
        "```\n",
        "\n",
        "The LLM receives Brown's prompt and calls the article generation tool:\n",
        "\n",
        "```terminal\n",
        "ðŸ”§ Function Call (Tool):\n",
        "  Tool: brown-writing-workflow_generate_article\n",
        "  Arguments: {\n",
        "    \"dir_path\": \"/path/to/your/research/directory\"\n",
        "  }\n",
        "\n",
        "âš¡ Executing tool 'brown-writing-workflow_generate_article' via MCP server...\n",
        "```\n",
        "\n",
        "Brown then generates the article using the research files Nova created."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f40eb6bd",
      "metadata": {},
      "source": [
        "### 3.6 The Limitation: No Single Unified Command\n",
        "\n",
        "While this two-step approach works, it has a significant limitation: **you must manually run two separate prompts in the correct order**. There's no single command that orchestrates the entire research-to-article workflow.\n",
        "\n",
        "You might think: \"Why not just add a combined prompt to one of the servers?\" But this creates a design problem:\n",
        "\n",
        "1. **Adding to Nova?** Nova is a research agent. It shouldn't need to know about Brown's article generation internals. This would create tight coupling between agents.\n",
        "\n",
        "2. **Adding to Brown?** Similarly, Brown is a writing workflow. It shouldn't need to know how Nova's research steps work.\n",
        "\n",
        "3. **Writing a manual prompt?** You could craft a long prompt that describes both workflows, but this duplicates the logic already encoded in each agent's prompts. It's error-prone and hard to maintain.\n",
        "\n",
        "**The clean solution is Approach 2**: create a composed MCP server that proxies both Nova and Brown. This composed server can own the combined workflow prompt without polluting either agent's codebase. The orchestration logic lives in the right placeâ€”the integration layerâ€”not in the individual agents.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b5c75d1",
      "metadata": {},
      "source": [
        "### 3.7 Running the Multi-Server Client in the Notebook\n",
        "\n",
        "Now let's run the multi-server client directly from this notebook. The client will connect to both Nova and Brown servers and display their capabilities.\n",
        "\n",
        "In this setup, we use HTTP transport (Streamable HTTP) for communication between the MCP client and servers. The first code cell starts Nova and Brown as HTTP servers on ports 8001 and 8002 respectively. The second code cell runs the client, which connects to these servers via HTTP URLs defined in `mcp_servers_config_http.json`.\n",
        "\n",
        "*Note*: The client is interactive, so you can type commands like `/tools`, `/resources`, `/prompts`, or `/quit` when prompted. Type `/quit` to exit the client."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9a9e32d",
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import time\n",
        "\n",
        "print(\"Starting MCP servers as HTTP services...\")\n",
        "\n",
        "# Start Nova MCP server on port 8001\n",
        "nova_proc = subprocess.Popen([\n",
        "    \"uv\", \"--directory\", \"/path/to/course-ai-agents/lessons/research_agent_part_2/mcp_server\",\n",
        "    \"run\", \"-m\", \"src.server\", \"--transport\", \"streamable-http\", \"--port\", \"8001\"\n",
        "], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
        "\n",
        "# Start Brown MCP server on port 8002\n",
        "brown_proc = subprocess.Popen([\n",
        "    \"uv\", \"--directory\", \"/path/to/course-ai-agents/lessons/writing_workflow\",\n",
        "    \"run\", \"-m\", \"brown.mcp.server\", \"--transport\", \"streamable-http\", \"--port\", \"8002\"\n",
        "], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
        "\n",
        "print(\"Waiting for servers to start...\")\n",
        "time.sleep(10)\n",
        "print(\"Servers should be running on ports 8001 (Nova) and 8002 (Brown)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "263f355e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run the MCP client\n",
        "import sys\n",
        "from agents_integration.mcp_client.src.client import main as client_main\n",
        "\n",
        "\n",
        "async def run_client():\n",
        "    _argv_backup = sys.argv[:]\n",
        "    sys.argv = [\"client\", \"--config\", \"mcp_servers_config_http.json\"]\n",
        "    try:\n",
        "        await client_main()\n",
        "    finally:\n",
        "        sys.argv = _argv_backup\n",
        "\n",
        "await run_client()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eafcf7d8",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Stopping MCP servers...\")\n",
        "try:\n",
        "    nova_proc.terminate()\n",
        "    brown_proc.terminate()\n",
        "    nova_proc.wait(timeout=5)\n",
        "    brown_proc.wait(timeout=5)\n",
        "    print(\"Servers stopped\")\n",
        "except Exception as e:\n",
        "    print(f\"Error stopping servers: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8080511",
      "metadata": {},
      "source": [
        "After running this cell, you should see:\n",
        "1. Both servers starting up (Nova Research MCP Server and Brown MCP Server)\n",
        "2. The total capabilities summary (14 tools, 4 resources, 4 prompts)\n",
        "3. A welcome message showing all combined capabilities\n",
        "4. An interactive prompt where you can type commands\n",
        "\n",
        "Try typing:\n",
        "- `/prompts` to see all prompts from both servers\n",
        "- `/prompt/nova-research-agent_full_research_instructions_prompt` to start the research workflow\n",
        "- `/prompt/brown-writing-workflow_generate_article_prompt?dir_path=/path/to/dir` to generate an article\n",
        "- `/tools` to see all tools from both servers\n",
        "- `/resources` to see all resources\n",
        "- `/quit` to exit"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a13136f6",
      "metadata": {},
      "source": [
        "## 4. Approach 2: Composed MCP Server\n",
        "\n",
        "The second approach is to create a new MCP server that composes the Nova and Brown servers together. Instead of the client connecting to multiple servers, you create a single composed server that internally proxies requests to the underlying servers.\n",
        "\n",
        "This approach is useful when you want to:\n",
        "- Package multiple agents as a single deployable unit\n",
        "- Simplify the client-side configuration (client only needs to know about one server)\n",
        "- Add a layer of coordination or orchestration between agents"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71c5c484",
      "metadata": {},
      "source": [
        "### 4.1 Server Composition Configuration\n",
        "\n",
        "First, we define which servers to compose. The composed server will connect to Nova and Brown via HTTP:\n",
        "\n",
        "Source: _lessons/25_integrate_agents/mcp_servers_config_http.json_\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"mcpServers\": {\n",
        "    \"nova-research-agent\": {\n",
        "      \"url\": \"http://localhost:8001/mcp\"\n",
        "    },\n",
        "    \"brown-writing-workflow\": {\n",
        "      \"url\": \"http://localhost:8002/mcp\"\n",
        "    }\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "This is identical to the multi-server client config, but it's used differently: here it defines the servers the composed server will proxy to via HTTP.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca45bcd4",
      "metadata": {},
      "source": [
        "### 4.2 Creating the Composed Server\n",
        "\n",
        "Now let's see how to create a composed server using FastMCP's composition features.\n",
        "\n",
        "Source: _lessons/agents_integration/mcp_server/src/main.py_\n",
        "\n",
        "```python\n",
        "import json\n",
        "import logging\n",
        "from pathlib import Path\n",
        "from fastmcp import Client, FastMCP\n",
        "\n",
        "def load_server_config() -> dict:\n",
        "    \"\"\"Load the MCP servers configuration from JSON file.\"\"\"\n",
        "    config_path = Path(__file__).parent.parent / \"mcp_servers_to_compose.json\"\n",
        "    with open(config_path) as f:\n",
        "        return json.load(f)\n",
        "\n",
        "def create_composed_server() -> FastMCP:\n",
        "    \"\"\"Create a composed MCP server by mounting Nova and Brown servers.\"\"\"\n",
        "    # Create the main composed server\n",
        "    mcp = FastMCP(\n",
        "        name=\"Nova+Brown Composed Server\",\n",
        "        version=\"0.1.0\",\n",
        "    )\n",
        "    \n",
        "    # Load configuration\n",
        "    config = load_server_config()\n",
        "    servers_config = config.get(\"mcpServers\", {})\n",
        "    \n",
        "    # Create proxies and mount each server\n",
        "    for server_name, server_config in servers_config.items():\n",
        "        # Wrap the server config in the structure expected by Client\n",
        "        client_config = {\"mcpServers\": {server_name: server_config}}\n",
        "        \n",
        "        # Create a client for this server\n",
        "        client = Client(client_config)\n",
        "        \n",
        "        # Create a proxy from the client\n",
        "        proxy = FastMCP.as_proxy(client)\n",
        "        \n",
        "        # Mount the proxy WITHOUT a prefix - capabilities keep their original names\n",
        "        mcp.mount(proxy)\n",
        "    \n",
        "    # Register the combined workflow prompt\n",
        "    register_combined_prompt(mcp)\n",
        "    \n",
        "    return mcp\n",
        "\n",
        "def register_combined_prompt(mcp: FastMCP) -> None:\n",
        "    '''Register the combined research and writing workflow prompt.'''\n",
        "    \n",
        "    @mcp.prompt()\n",
        "    def full_research_and_writing_workflow(dir_path: Path) -> str:\n",
        "        \"\"\"Complete workflow for research and article generation.\n",
        "        \n",
        "        This prompt combines the Nova research agent workflow with the Brown\n",
        "        article generation workflow, providing end-to-end instructions for\n",
        "        conducting comprehensive research and generating an article from that research.\n",
        "        \n",
        "        Args:\n",
        "            dir_path: Path to the directory that will contain research resources\n",
        "                     and the final article.\n",
        "        \n",
        "        Returns:\n",
        "            A formatted prompt string with complete workflow instructions.\n",
        "        \"\"\"\n",
        "        return f\"\"\"\n",
        "# Complete Research and Article Generation Workflow\n",
        "\n",
        "This workflow combines two phases: research (Nova) and article generation (Brown).\n",
        "\n",
        "## PHASE 1: Research (Nova)\n",
        "\n",
        "Your job is to execute the workflow below...\n",
        "[Full Nova workflow instructions]\n",
        "\n",
        "## PHASE 2: Article Generation (Brown)\n",
        "\n",
        "Once the research phase is complete, use Brown to generate an article\n",
        "using the research from: {dir_path}\n",
        "\"\"\".strip()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    composed_server = create_composed_server()\n",
        "    composed_server.run()\n",
        "```\n",
        "\n",
        "Let's break down the key steps:\n",
        "\n",
        "1. First we create a FastMCP instance. This is our composed server.\n",
        "2. For each server to compose:\n",
        "   - Create a `Client` that connects to that server\n",
        "   - Use `FastMCP.as_proxy(client)` to create a proxy object\n",
        "   - Use `mcp.mount(proxy)` to mount it **without a prefix**. When you mount a proxy without a prefix, all capabilities keep their original names. This means tools, resources, and prompts from both Nova and Brown are exposed exactly as they were defined in each agent.\n",
        "3. After mounting both servers, we add a new capability: the `full_research_and_writing_workflow` prompt. This is the key advantage of the composed server approachâ€”we can add orchestration logic that coordinates both agents without modifying either agent's code. The `@mcp.prompt()` decorator registers a new MCP prompt that combines instructions for both Nova's research workflow and Brown's article generation. When a user triggers this prompt, the LLM receives comprehensive instructions to run both agents end-to-end in a single conversation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb22fe33",
      "metadata": {},
      "source": [
        "### 4.3 Running the Composed Server\n",
        "\n",
        "To use the composed server, you need a client config that points to it:\n",
        "\n",
        "Source: _lessons/25_integrate_agents/mcp_composed_server_config_http.json_\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"mcpServers\": {\n",
        "    \"nova-brown-composed\": {\n",
        "      \"url\": \"http://localhost:8003/mcp\"\n",
        "    }\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "Now when you run the client with this config:\n",
        "\n",
        "```bash\n",
        "$ cd agents_integration/mcp_client\n",
        "$ uv run -m src.client --config ../../25_integrate_agents/mcp_composed_server_config_http.json\n",
        "```\n",
        "\n",
        "You'll see:\n",
        "\n",
        "```terminal\n",
        "INFO:root:Loading MCP server configuration from: mcp_composed_server_config_http.json\n",
        "INFO:root:Found 1 MCP servers in configuration: nova-brown-composed\n",
        "INFO:root:Connecting to MCP servers...\n",
        "INFO:__main__:Starting composed MCP server...\n",
        "INFO:__main__:Loading server configuration...\n",
        "INFO:__main__:Found 2 servers to compose: ['nova-research-agent', 'brown-writing-workflow']\n",
        "INFO:__main__:Creating proxy for nova-research-agent...\n",
        "INFO:__main__:Mounting nova-research-agent without prefix...\n",
        "INFO:__main__:Creating proxy for brown-writing-workflow...\n",
        "INFO:__main__:Mounting brown-writing-workflow without prefix...\n",
        "INFO:__main__:Composed server created successfully!\n",
        "INFO:__main__:Running composed server...\n",
        "\n",
        "INFO:root:Fetching capabilities from all servers...\n",
        "INFO:root:Total capabilities: 14 tools, 4 resources, 5 prompts\n",
        "\n",
        "============================================================\n",
        "All Servers\n",
        "============================================================\n",
        "\n",
        "  - 14 tools available\n",
        "  - 4 resources available\n",
        "  - 5 prompts available\n",
        "\n",
        "Available Commands: /tools, /resources, /prompts, /prompt/<name>?arg=value, /resource/<uri>, /model-thinking-switch, /quit\n",
        "\n",
        "ðŸ‘¤ You:\n",
        "```\n",
        "\n",
        "Notice that:\n",
        "- The composed server mounts both agents **without prefixes**, so capabilities keep their original names\n",
        "- There are now **5 prompts** (not 4)â€”the composed server has added a new `full_research_and_writing_workflow` prompt\n",
        "- The client sees a single unified interface with all capabilities from both agents plus the new orchestration prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd476dd4",
      "metadata": {},
      "source": [
        "### 4.4 Listing Prompts from the Composed Server\n",
        "\n",
        "When you type `/prompts`, you'll see all 5 prompts available:\n",
        "\n",
        "```terminal\n",
        "ðŸ‘¤ You: /prompts\n",
        "============================================================\n",
        "Prompts (5)\n",
        "============================================================\n",
        "\n",
        "1. full_research_and_writing_workflow\n",
        "   Complete workflow for research and article generation.\n",
        "\n",
        "   This prompt combines the Nova research agent workflow with the Brown\n",
        "   article generation workflow, providing end-to-end instructions for\n",
        "   conducting comprehensive research and generating an article from that research.\n",
        "\n",
        "2. full_research_instructions_prompt\n",
        "   Complete Nova research agent workflow instructions.\n",
        "\n",
        "3. generate_article_prompt\n",
        "   Retrieve a prompt that will trigger the article generation workflow using Brown.\n",
        "\n",
        "4. edit_article_prompt\n",
        "   Retrieve a prompt that will trigger the article editing workflow using Brown.\n",
        "\n",
        "5. edit_selected_text_prompt\n",
        "   Retrieve a prompt that will trigger the selected text editing workflow using Brown.\n",
        "```\n",
        "\n",
        "Notice that `full_research_and_writing_workflow` is **new**â€”it's not from Nova or Brown, but added by the composed server itself. This prompt orchestrates both agents, instructing the LLM to:\n",
        "1. Run the complete Nova research workflow\n",
        "2. Then use Brown to generate an article from that research\n",
        "\n",
        "This is the key advantage of the composed server approach: you can add coordination logic at the integration layer without modifying the individual agents."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04fcc2e3",
      "metadata": {},
      "source": [
        "### 4.5 Running the Complete Workflow with One Prompt\n",
        "\n",
        "Now let's see the power of the composed server approach. Instead of running two separate prompts like in Approach 1, we can trigger the complete end-to-end workflow with a single command:\n",
        "\n",
        "```terminal\n",
        "ðŸ‘¤ You: /prompt/full_research_and_writing_workflow?dir_path=/absolute/path/to/research_folder\n",
        "```\n",
        "\n",
        "The LLM receives the combined prompt and immediately understands it needs to execute both phases:\n",
        "\n",
        "```terminal\n",
        "ðŸ¤” LLM's Thoughts:\n",
        "Okay, I'm looking at a two-phase operation here: Research (Nova) and Article Generation (Brown).\n",
        "This is a well-defined process, and I need to execute it methodically. The first phase, Nova, \n",
        "is the more involved one, and it's where my focus will be initially...\n",
        "\n",
        "ðŸ”§ Function Call (Tool):\n",
        "  Tool: extract_guidelines_urls\n",
        "  Arguments: {\n",
        "    \"research_directory\": \"/absolute/path/to/research_folder\"\n",
        "  }\n",
        "\n",
        "âš¡ Executing tool 'extract_guidelines_urls' via MCP server...\n",
        "âœ… Tool execution successful!\n",
        "\n",
        "ðŸ”§ Function Call (Tool):\n",
        "  Tool: process_local_files\n",
        "  Arguments: {\n",
        "    \"research_directory\": \"/absolute/path/to/research_folder\"\n",
        "  }\n",
        "\n",
        "âš¡ Executing tool 'process_local_files' via MCP server...\n",
        "âœ… Tool execution successful!\n",
        "\n",
        "ðŸ”§ Function Call (Tool):\n",
        "  Tool: scrape_and_clean_other_urls\n",
        "  Arguments: {\n",
        "    \"research_directory\": \"/absolute/path/to/research_folder\"\n",
        "  }\n",
        "\n",
        "âš¡ Executing tool 'scrape_and_clean_other_urls' via MCP server...\n",
        "```\n",
        "\n",
        "The LLM continues executing the Nova research workflow tools, then seamlessly transitions to Brown's article generation. **All with a single prompt command.**\n",
        "\n",
        "This solves the limitation we saw in Approach 1, where you had to manually run two prompts in sequence. The composed server encapsulates the orchestration logic, providing a clean, single-command interface to the complete workflow."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c351f5a2",
      "metadata": {},
      "source": [
        "### 4.6 Running the Composed Server Client in the Notebook\n",
        "\n",
        "Now let's run the client with the composed server configuration. This time, the client connects to a single composed server that internally proxies to both Nova and Brown.\n",
        "\n",
        "In this setup, we use HTTP transport (Streamable HTTP) for all MCP communication. The first code cell starts three servers:\n",
        "1. Nova MCP server on port 8001\n",
        "2. Brown MCP server on port 8002\n",
        "3. The composed MCP server on port 8003, which connects to Nova and Brown via HTTP\n",
        "\n",
        "The second code cell runs the MCP client, which connects to the composed server at `http://localhost:8003/mcp` using the configuration in `mcp_composed_server_config_http.json`. The composed server then proxies all requests to Nova and Brown as needed.\n",
        "\n",
        "*Note*: This is also interactive. Type `/quit` to exit when you're done exploring."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5598468a",
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import time\n",
        "\n",
        "print(\"Starting MCP servers as HTTP services...\")\n",
        "\n",
        "# Start Nova MCP server on port 8001\n",
        "nova_proc = subprocess.Popen([\n",
        "    \"uv\", \"--directory\", \"/path/to/course-ai-agents/lessons/research_agent_part_2/mcp_server\",\n",
        "    \"run\", \"-m\", \"src.server\", \"--transport\", \"streamable-http\", \"--port\", \"8001\"\n",
        "], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "\n",
        "# Start Brown MCP server on port 8002\n",
        "brown_proc = subprocess.Popen([\n",
        "    \"uv\", \"--directory\", \"/path/to/course-ai-agents/lessons/writing_workflow\",\n",
        "    \"run\", \"-m\", \"brown.mcp.server\", \"--transport\", \"streamable-http\", \"--port\", \"8002\"\n",
        "], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "\n",
        "print(\"Waiting for Nova and Brown servers to start...\")\n",
        "time.sleep(10)\n",
        "print(\"Nova and Brown servers should be running on ports 8001 and 8002\")\n",
        "\n",
        "# Start Composed MCP server on port 8003\n",
        "# The composed server connects to Nova and Brown via HTTP\n",
        "composed_proc = subprocess.Popen([\n",
        "    \"uv\", \"--directory\", \"/path/to/course-ai-agents/lessons/agents_integration/mcp_server\",\n",
        "    \"run\", \"python\", \"-m\", \"src.main\",\n",
        "    \"--transport\", \"streamable-http\",\n",
        "    \"--port\", \"8003\",\n",
        "    \"--config\", \"/path/to/course-ai-agents/lessons/25_integrate_agents/mcp_servers_config_http.json\"\n",
        "], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "\n",
        "print(\"Waiting for Composed server to start...\")\n",
        "time.sleep(10)\n",
        "print(\"Composed server should be running on port 8003\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d2bd785",
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "from agents_integration.mcp_client.src.client import main as client_main\n",
        "\n",
        "\n",
        "async def run_client():\n",
        "    _argv_backup = sys.argv[:]\n",
        "    sys.argv = [\"client\", \"--config\", \"mcp_composed_server_config_http.json\"]\n",
        "    try:\n",
        "        await client_main()\n",
        "    finally:\n",
        "        sys.argv = _argv_backup\n",
        "\n",
        "\n",
        "# Start client with in-memory server\n",
        "await run_client()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cff41f1c",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Stopping MCP servers...\")\n",
        "try:\n",
        "    nova_proc.terminate()\n",
        "    brown_proc.terminate()\n",
        "    composed_proc.terminate()\n",
        "    nova_proc.wait(timeout=5)\n",
        "    brown_proc.wait(timeout=5)\n",
        "    composed_proc.wait(timeout=5)\n",
        "    print(\"Servers stopped\")\n",
        "except Exception as e:\n",
        "    print(f\"Error stopping servers: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e8d0f6b",
      "metadata": {},
      "source": [
        "After running this cell, you should see:\n",
        "1. The composed server starting (Nova+Brown Composed Server)\n",
        "2. Log messages showing the composition process (creating proxies, mounting servers **without prefixes**)\n",
        "3. Both Nova and Brown servers starting up in the background\n",
        "4. The total capabilities summary (14 tools, 4 resources, **5 prompts**)\n",
        "5. A welcome message showing all combined capabilities\n",
        "6. An interactive prompt for commands\n",
        "\n",
        "Notice the key differences compared to the multi-server client (Approach 1):\n",
        "- Only one server name in the config (`nova-brown-composed`)\n",
        "- Capabilities keep their original names (no prefixes added)\n",
        "- **5 prompts instead of 4**â€”the new `full_research_and_writing_workflow` prompt is added by the composed server\n",
        "- Single command to run both agents end-to-end\n",
        "\n",
        "Try these commands:\n",
        "- `/prompts` to see all 5 prompts (including the new combined workflow prompt)\n",
        "- `/prompt/full_research_and_writing_workflow?dir_path=/path/to/dir` to run both agents with one command\n",
        "- `/tools` to see all tools from both agents\n",
        "- `/resources` to see all resources\n",
        "- `/quit` to exit"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b09861a0",
      "metadata": {},
      "source": [
        "## 5. Multi-Server Client vs Composed Server: When to Use Each\n",
        "\n",
        "Both approaches achieve the same goal: using Nova and Brown together. But they have different use cases and trade-offs.\n",
        "\n",
        "| Aspect | Multi-Server Client | Composed Server |\n",
        "|--------|---------------------|-----------------|\n",
        "| **How it works** | The client connects to multiple independent servers simultaneously | A single server internally proxies to multiple underlying servers |\n",
        "| **Orchestration prompts** | No built-in way to add combined promptsâ€”must run agent prompts sequentially | Can add prompts that orchestrate multiple agents (e.g., `full_research_and_writing_workflow`) |\n",
        "| **Single command workflow** | Requires running multiple prompts in sequence (Nova, then Brown) | Single `full_research_and_writing_workflow` prompt runs both agents end-to-end |\n",
        "| **Client configuration** | Client needs to know about all servers | Single endpoint (simpler client config) |\n",
        "| **Flexibility** | Easy to add/remove servers without changing code | Requires code changes to modify composition |\n",
        "| **Use when** | â€¢ Developing or testing agents<br>â€¢ Quickly combining existing agents<br>â€¢ Need flexibility to add/remove agents dynamically | â€¢ Deploying agents as a unified system<br>â€¢ Need coordination logic between agents<br>â€¢ Want simpler client experience<br>â€¢ Building a product that packages multiple agents |"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5dff58cc",
      "metadata": {},
      "source": [
        "## 6. Conclusion\n",
        "\n",
        "In this lesson, we explored how to integrate the Nova research agent and Brown writing workflow using MCP. We learned:\n",
        "\n",
        "1. **Multi-Server MCP Client**: How to connect a single client to multiple MCP servers using FastMCP's multi-server configuration. Capabilities are prefixed with server names, and you run agent workflows by triggering their prompts sequentially.\n",
        "2. **Composed MCP Server**: How to use FastMCP's composition features (`as_proxy()` and `mount()`) to create a unified server that proxies multiple agents. Crucially, composed servers can add new orchestration promptsâ€”like `full_research_and_writing_workflow`â€”that coordinate multiple agents without modifying their code.\n",
        "3. **Use Cases**: When to use multi-server client vs composed server. The multi-server approach is better for flexibility and independence, while the composed server is better for having a unified interface, orchestration capabilities, and single-command workflows.\n",
        "\n",
        "The key insight is that MCP makes agent integration straightforward. Because both Nova and Brown are already MCP servers, we don't need to write custom integration code. We simply leverage MCP's standardized protocol and FastMCP's composition features. The composed server approach goes further by allowing us to add orchestration logic at the integration layerâ€”providing a single-command interface to run both agents end-to-endâ€”without polluting either agent's codebase.\n",
        "\n",
        "In a real-world scenario, you would use these integrated agents within an IDE like Cursor. You could:\n",
        "- Configure the composed server in Cursor's MCP settings\n",
        "- Trigger the `full_research_and_writing_workflow` prompt with a single command\n",
        "- Watch as the LLM automatically orchestrates Nova's research and Brown's article generation\n",
        "- Review the research file and generated article\n",
        "- Iterate on the article with Brown's editing tools\n",
        "- All from within your editor, with beautiful diff views and human-in-the-loop feedback\n",
        "\n",
        "In the next lesson, you'll see how to use both agents from Cursor to work on an article end-to-end. In Part 3 of this course, we'll explore production deployment, including how to deploy these composed servers remotely, add monitoring with Opik, and implement security measures."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
