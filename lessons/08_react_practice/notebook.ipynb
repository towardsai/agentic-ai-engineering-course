{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOgiJKN3MqB1"
      },
      "source": [
        "# Lesson 8: ReAct Practice\n",
        "\n",
        "This notebook explores practical ReAct (Reasoning and Acting) with Google's Gemini. We will use the `google-genai` library to interact with Gemini models. It includes a mock search tool, a thought generation phase using structured outputs, and an action phase with function calling, all orchestrated by a ReAct control loop."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Learning Objectives:**\n",
        "\n",
        "1. Understand how ReAct breaks problems into Thought → Action → Observation.\n",
        "2. Practice orchestrating the full ReAct loop end-to-end."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup\n",
        "\n",
        "First, we define some standard Magic Python commands to autoreload Python packages whenever they change:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GRVoMgfLMsBP"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Set Up Python Environment\n",
        "\n",
        "To set up your Python virtual environment using `uv` and load it into the Notebook, follow the step-by-step instructions from the `Course Admin` lesson from the beginning of the course.\n",
        "\n",
        "**TL/DR:** Be sure the correct kernel pointing to your `uv` virtual environment is selected."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Configure Gemini API\n",
        "\n",
        "To configure the Gemini API, follow the step-by-step instructions from the `Course Admin` lesson.\n",
        "\n",
        "But here is a quick check on what you need to run this Notebook:\n",
        "\n",
        "1.  Get your key from [Google AI Studio](https://aistudio.google.com/app/apikey).\n",
        "2.  From the root of your project, run: `cp .env.example .env` \n",
        "3.  Within the `.env` file, fill in the `GOOGLE_API_KEY` variable:\n",
        "\n",
        "Now, the code below will load the key from the `.env` file:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trying to load environment variables from `/Users/fabio/Desktop/course-ai-agents/.env`\n",
            "Environment variables loaded successfully.\n"
          ]
        }
      ],
      "source": [
        "from lessons.utils import env\n",
        "\n",
        "env.load(required_env_vars=[\"GOOGLE_API_KEY\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBAXJg6nMqB2"
      },
      "source": [
        "### Import Key Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wdZUi2E44tIQ",
        "outputId": "2059055b-6b66-4956-ca87-ce4b78335d34"
      },
      "outputs": [],
      "source": [
        "from enum import Enum\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import List\n",
        "\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "\n",
        "from lessons.utils import pretty_print"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Initialize the Gemini Client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "FOx4eD-yM-jO"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Both GOOGLE_API_KEY and GEMINI_API_KEY are set. Using GOOGLE_API_KEY.\n"
          ]
        }
      ],
      "source": [
        "client = genai.Client()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kLgTbeTBvwt"
      },
      "source": [
        "### Define Constants\n",
        "\n",
        "We will use the `gemini-2.5-flash` model, which is fast and cost-effective:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "09szFdH86K6A"
      },
      "outputs": [],
      "source": [
        "MODEL_ID = \"gemini-2.5-flash\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Id1R6LRKDRcv"
      },
      "source": [
        "## 2. Tools Definition\n",
        "\n",
        "Let's implement our mock search tool that will serve as the external knowledge source for our agent. This simplified version focuses on the ReAct mechanics rather than real API integration:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def search(query: str) -> str:\n",
        "    \"\"\"Search for information about a specific topic or query.\n",
        "\n",
        "    Args:\n",
        "        query (str): The search query or topic to look up.\n",
        "    \"\"\"\n",
        "    query_lower = query.lower()\n",
        "\n",
        "    # Predefined responses for demonstration\n",
        "    if all(word in query_lower for word in [\"capital\", \"france\"]):\n",
        "        return \"Paris is the capital of France and is known for the Eiffel Tower.\"\n",
        "    elif \"react\" in query_lower:\n",
        "        return \"The ReAct (Reasoning and Acting) framework enables LLMs to solve complex tasks by interleaving thought generation, action execution, and observation processing.\"\n",
        "\n",
        "    # Generic response for unhandled queries\n",
        "    return f\"Information about '{query}' was not found.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We maintain a mapping from tool name to tool function (the tool registry). This lets the model plan with symbolic tool names, while our code safely resolves those names to actual Python functions to execute."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "TOOL_REGISTRY = {\n",
        "    search.__name__: search,\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyqJqrKRMqB5"
      },
      "source": [
        "## 3. ReAct Thought Phase\n",
        "\n",
        "Now let's implement the thought generation phase. This component analyzes the current situation and determines what the agent should do next, potentially suggesting using tools."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, we prepare the prompt for the thinking part. We implement a function that converts the `TOOL_REGISTRY` to a string XML representation of it, which we insert into the prompt. This way, the LLM knows which tools available and can reason around them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_tools_xml_description(tools: dict[str, callable]) -> str:\n",
        "    \"\"\"Build a minimal XML description of tools using only their docstrings.\"\"\"\n",
        "    lines = []\n",
        "    for tool_name, fn in tools.items():\n",
        "        doc = (fn.__doc__ or \"\").strip()\n",
        "        lines.append(f\"\\t<tool name=\\\"{tool_name}\\\">\")\n",
        "        if doc:\n",
        "            lines.append(f\"\\t\\t<description>\")\n",
        "            for line in doc.split(\"\\n\"):\n",
        "                lines.append(f\"\\t\\t\\t{line}\")\n",
        "            lines.append(f\"\\t\\t</description>\")\n",
        "        lines.append(\"\\t</tool>\")\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "tools_xml = build_tools_xml_description(TOOL_REGISTRY)\n",
        "\n",
        "PROMPT_TEMPLATE_THOUGHT = f\"\"\"\n",
        "You are deciding the next best step for reaching the user goal. You have some tools available to you.\n",
        "\n",
        "Available tools:\n",
        "<tools>\n",
        "{tools_xml}\n",
        "</tools>\n",
        "\n",
        "Conversation so far:\n",
        "<conversation>\n",
        "{{conversation}}\n",
        "</conversation>\n",
        "\n",
        "State your next thought about what to do next as one short paragraph focused on the next action you intend to take and why.\n",
        "Avoid repeating the same strategies that didn't work previously. Prefer different approaches.\n",
        "\"\"\".strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we `print` the full prompt with the tool definitions inside."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You are deciding the next best step for reaching the user goal. You have some tools available to you.\n",
            "\n",
            "Available tools:\n",
            "<tools>\n",
            "\t<tool name=\"search\">\n",
            "\t\t<description>\n",
            "\t\t\tSearch for information about a specific topic or query.\n",
            "\t\t\t\n",
            "\t\t\tArgs:\n",
            "\t\t\t    query (str): The search query or topic to look up.\n",
            "\t\t</description>\n",
            "\t</tool>\n",
            "</tools>\n",
            "\n",
            "Conversation so far:\n",
            "<conversation>\n",
            "{conversation}\n",
            "</conversation>\n",
            "\n",
            "State your next thought about what to do next as one short paragraph focused on the next action you intend to take and why.\n",
            "Avoid repeating the same strategies that didn't work previously. Prefer different approaches.\n"
          ]
        }
      ],
      "source": [
        "print(PROMPT_TEMPLATE_THOUGHT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now implement the `generate_thought` function, which reasons on the best next action to take according to the conversation history."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "lvS9ww0xflyx"
      },
      "outputs": [],
      "source": [
        "def generate_thought(conversation: str, tool_registry: dict[str, callable]) -> str:\n",
        "    \"\"\"Generate a thought as plain text (no structured output).\"\"\"\n",
        "    tools_xml = build_tools_xml_description(tool_registry)\n",
        "    prompt = PROMPT_TEMPLATE_THOUGHT.format(conversation=conversation, tools_xml=tools_xml)\n",
        "\n",
        "    response = client.models.generate_content(\n",
        "        model=MODEL_ID,\n",
        "        contents=prompt\n",
        "    )\n",
        "    return response.text.strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtGC4m-Xsz8R"
      },
      "source": [
        "## 4. ReAct Action Phase"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-qP8k3ZMqB5"
      },
      "source": [
        "Next, let's implement the action phase using function calling. This component determines whether to use a tool or provide a final answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "gMH6u54tDQyr"
      },
      "outputs": [],
      "source": [
        "PROMPT_TEMPLATE_ACTION = \"\"\"\n",
        "You are selecting the best next action to reach the user goal.\n",
        "\n",
        "Conversation so far:\n",
        "<conversation>\n",
        "{conversation}\n",
        "</conversation>\n",
        "\n",
        "Respond either with a tool call (with arguments) or a final answer if you can confidently conclude.\n",
        "\"\"\".strip()\n",
        "\n",
        "# Dedicated prompt used when we must force a final answer\n",
        "PROMPT_TEMPLATE_ACTION_FORCED = \"\"\"\n",
        "You must now provide a final answer to the user.\n",
        "\n",
        "Conversation so far:\n",
        "<conversation>\n",
        "{conversation}\n",
        "</conversation>\n",
        "\n",
        "Provide a concise final answer that best addresses the user's goal.\n",
        "\"\"\".strip()\n",
        "\n",
        "\n",
        "class ToolCallRequest(BaseModel):\n",
        "    \"\"\"A request to call a tool with its name and arguments.\"\"\"\n",
        "    tool_name: str = Field(description=\"The name of the tool to call.\")\n",
        "    arguments: dict = Field(description=\"The arguments to pass to the tool.\")\n",
        "\n",
        "\n",
        "class FinalAnswer(BaseModel):\n",
        "    \"\"\"A final answer to present to the user when no further action is needed.\"\"\"\n",
        "    text: str = Field(description=\"The final answer text to present to the user.\")\n",
        "\n",
        "\n",
        "def generate_action(conversation: str, tool_registry: dict[str, callable] | None = None, force_final: bool = False) -> (ToolCallRequest | FinalAnswer):\n",
        "    \"\"\"Generate an action by passing tools to the LLM and parsing function calls or final text.\n",
        "\n",
        "    When force_final is True or no tools are provided, the model is instructed to produce a final answer and tool calls are disabled.\n",
        "    \"\"\"\n",
        "    # Use a dedicated prompt when forcing a final answer or no tools are provided\n",
        "    if force_final or not tool_registry:\n",
        "        prompt = PROMPT_TEMPLATE_ACTION_FORCED.format(conversation=conversation)\n",
        "        response = client.models.generate_content(\n",
        "            model=MODEL_ID,\n",
        "            contents=prompt\n",
        "        )\n",
        "        return FinalAnswer(text=response.text.strip())\n",
        "\n",
        "    # Default action prompt\n",
        "    prompt = PROMPT_TEMPLATE_ACTION.format(conversation=conversation)\n",
        "\n",
        "    # Provide the available tools to the model; disable auto-calling so we can parse and run ourselves\n",
        "    tools = list(tool_registry.values())\n",
        "    config = types.GenerateContentConfig(\n",
        "        tools=tools,\n",
        "        automatic_function_calling={\"disable\": True}\n",
        "    )\n",
        "    response = client.models.generate_content(\n",
        "        model=MODEL_ID,\n",
        "        contents=prompt,\n",
        "        config=config\n",
        "    )\n",
        "\n",
        "    # Extract the function call from the response (if present)\n",
        "    candidate = response.candidates[0]\n",
        "    parts = candidate.content.parts\n",
        "    if parts and getattr(parts[0], \"function_call\", None):\n",
        "        name = parts[0].function_call.name\n",
        "        args = dict(parts[0].function_call.args) if parts[0].function_call.args is not None else {}\n",
        "        return ToolCallRequest(tool_name=name, arguments=args)\n",
        "    \n",
        "    # Otherwise, it's a final answer\n",
        "    final_answer = \"\".join(part.text for part in candidate.content.parts)\n",
        "    return FinalAnswer(text=final_answer.strip())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Why we provide an option to force the final answer? In a ReAct loop we sometimes need to terminate cleanly after a budget of turns (e.g., to avoid infinite loops or excessive tool calls). The force flag lets us ask the model to conclude with a final answer even if, under normal conditions, it might keep calling tools. This ensures graceful shutdown and a usable output at the end of the loop."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note: In the Action phase we do not inline tool descriptions into the prompt (unlike the Thought phase). Instead, we pass the available Python tool functions through the `tools` parameter to `generate_content`. The client automatically parses these tools and incorporates their definitions/arguments into the model's prompt context, enabling function calling without duplicating tool specs in our prompt text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "moxbYJ2_EqYi"
      },
      "source": [
        "## 5. ReAct Control Loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmJ6zg2YMqB6"
      },
      "source": [
        "Now we build the main ReAct control loop that orchestrates the Thought → Action → Observation cycle end-to-end. We treat the conversation between the user and the agent as a sequence of messages. Each message is a step in the dialogue, and each step corresponds to one ReAct unit: it can be a user message, an internal thought, a tool request, the tool's observation, or the final answer.\n",
        "\n",
        "We'll start by defining the data structures for these messages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "iH0rKt2Pcf3W"
      },
      "outputs": [],
      "source": [
        "class MessageRole(str, Enum):\n",
        "    \"\"\"Enumeration for the different roles a message can have.\"\"\"\n",
        "    USER = \"user\"\n",
        "    THOUGHT = \"thought\"\n",
        "    TOOL_REQUEST = \"tool request\"\n",
        "    OBSERVATION = \"observation\"\n",
        "    FINAL_ANSWER = \"final answer\"\n",
        "\n",
        "\n",
        "class Message(BaseModel):\n",
        "    \"\"\"A message with a role and content, used for all message types.\"\"\"\n",
        "    role: MessageRole = Field(description=\"The role of the message in the ReAct loop.\")\n",
        "    content: str = Field(description=\"The textual content of the message.\")\n",
        "\n",
        "    def __str__(self) -> str:\n",
        "        \"\"\"Provides a user-friendly string representation of the message.\"\"\"\n",
        "        return f\"{self.role.value.capitalize()}: {self.content}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We also add a small printer that uses our `pretty_print` module to render each message nicely in the notebook. This makes it easy to follow how the agent alternates between Thought, Action (tool call), and Observation across turns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "def pretty_print_message(message: Message, turn: int, max_turns: int, header_color: str = pretty_print.Color.YELLOW, is_forced_final_answer: bool = False) -> None:\n",
        "    if not is_forced_final_answer:\n",
        "        title = f\"{message.role.value.capitalize()} (Turn {turn}/{max_turns}):\"\n",
        "    else:\n",
        "        title = f\"{message.role.value.capitalize()} (Forced):\"\n",
        "\n",
        "    pretty_print.wrapped(\n",
        "        text=message.content,\n",
        "        title=title,\n",
        "        header_color=header_color,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now use a `Scratchpad` class that wraps a list of `Message` objects and provides `append(..., verbose=False)` to both store and (optionally) pretty-print messages with role-based colors. The scratchpad is serialized each turn so the model can plan the next step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Scratchpad:\n",
        "    \"\"\"Container for ReAct messages with optional pretty-print on append.\"\"\"\n",
        "\n",
        "    def __init__(self, max_turns: int) -> None:\n",
        "        self.messages: List[Message] = []\n",
        "        self.max_turns: int = max_turns\n",
        "        self.current_turn: int = 1\n",
        "\n",
        "    def set_turn(self, turn: int) -> None:\n",
        "        self.current_turn = turn\n",
        "\n",
        "    def append(self, message: Message, verbose: bool = False, is_forced_final_answer: bool = False) -> None:\n",
        "        self.messages.append(message)\n",
        "        if verbose:\n",
        "            role_to_color = {\n",
        "                MessageRole.USER: pretty_print.Color.RESET,\n",
        "                MessageRole.THOUGHT: pretty_print.Color.ORANGE,\n",
        "                MessageRole.TOOL_REQUEST: pretty_print.Color.GREEN,\n",
        "                MessageRole.OBSERVATION: pretty_print.Color.YELLOW,\n",
        "                MessageRole.FINAL_ANSWER: pretty_print.Color.CYAN,\n",
        "            }\n",
        "            header_color = role_to_color.get(message.role, pretty_print.Color.YELLOW)\n",
        "            pretty_print_message(\n",
        "                message=message,\n",
        "                turn=self.current_turn,\n",
        "                max_turns=self.max_turns,\n",
        "                header_color=header_color,\n",
        "                is_forced_final_answer=is_forced_final_answer,\n",
        "            )\n",
        "\n",
        "    def to_string(self) -> str:\n",
        "        return \"\\n\".join(str(m) for m in self.messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now implement the control loop.\n",
        "- On the first turn, we add the user question.\n",
        "- Then, at each turn: (1) we get a Thought from the model; (2) we get an Action. If the action is a `FinalAnswer`, we stop. If it's a `ToolCallRequest`, we execute the tool and append the resulting `Observation`, then continue. If we reach the maximum number of turns, we run the action selector one last time with a flag that forces a final answer (no tool calls)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "W-PSlQ34f1RM"
      },
      "outputs": [],
      "source": [
        "def react_agent_loop(initial_question: str, tool_registry: dict[str, callable], max_turns: int = 5, verbose: bool = False) -> str:\n",
        "    \"\"\"\n",
        "    Implements the main ReAct (Thought -> Action -> Observation) control loop.\n",
        "    Uses a unified message class for the scratchpad.\n",
        "    \"\"\"\n",
        "    scratchpad = Scratchpad(max_turns=max_turns)\n",
        "\n",
        "    # Add the user's question to the scratchpad\n",
        "    user_message = Message(role=MessageRole.USER, content=initial_question)\n",
        "    scratchpad.append(user_message, verbose=verbose)\n",
        "\n",
        "    for turn in range(1, max_turns + 1):\n",
        "        scratchpad.set_turn(turn)\n",
        "\n",
        "        # Generate a thought based on the current scratchpad\n",
        "        thought_content = generate_thought(\n",
        "            scratchpad.to_string(),\n",
        "            tool_registry,\n",
        "        )\n",
        "        thought_message = Message(role=MessageRole.THOUGHT, content=thought_content)\n",
        "        scratchpad.append(thought_message, verbose=verbose)\n",
        "\n",
        "        # Generate an action based on the current scratchpad\n",
        "        action_result = generate_action(\n",
        "            scratchpad.to_string(),\n",
        "            tool_registry=tool_registry,\n",
        "        )\n",
        "\n",
        "        # If the model produced a final answer, return it\n",
        "        if isinstance(action_result, FinalAnswer):\n",
        "            final_answer = action_result.text\n",
        "            final_message = Message(role=MessageRole.FINAL_ANSWER, content=final_answer)\n",
        "            scratchpad.append(final_message, verbose=verbose)\n",
        "            return final_answer\n",
        "\n",
        "        # Otherwise, it is a tool request\n",
        "        if isinstance(action_result, ToolCallRequest):\n",
        "            action_name = action_result.tool_name\n",
        "            action_params = action_result.arguments\n",
        "\n",
        "            # Add the action to the scratchpad\n",
        "            params_str = \", \".join([f\"{k}='{v}'\" for k, v in action_params.items()])\n",
        "            action_content = f\"{action_name}({params_str})\"\n",
        "            action_message = Message(role=MessageRole.TOOL_REQUEST, content=action_content)\n",
        "            scratchpad.append(action_message, verbose=verbose)\n",
        "\n",
        "            # Run the action and get the observation\n",
        "            observation_content = \"\"\n",
        "            tool_function = tool_registry[action_name]\n",
        "            try:\n",
        "                observation_content = tool_function(**action_params)\n",
        "            except Exception as e:\n",
        "                observation_content = f\"Error executing tool '{action_name}': {e}\"\n",
        "\n",
        "            # Add the observation to the scratchpad\n",
        "            observation_message = Message(role=MessageRole.OBSERVATION, content=observation_content)\n",
        "            scratchpad.append(observation_message, verbose=verbose)\n",
        "\n",
        "        # Check if the maximum number of turns has been reached. If so, force the action selector to produce a final answer\n",
        "        if turn == max_turns:\n",
        "            forced_action = generate_action(\n",
        "                scratchpad.to_string(),\n",
        "                force_final=True,\n",
        "            )\n",
        "            if isinstance(forced_action, FinalAnswer):\n",
        "                final_answer = forced_action.text\n",
        "            else:\n",
        "                final_answer = \"Unable to produce a final answer within the allotted turns.\"\n",
        "            final_message = Message(role=MessageRole.FINAL_ANSWER, content=final_answer)\n",
        "            scratchpad.append(final_message, verbose=verbose, is_forced_final_answer=True)\n",
        "            return final_answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utJSwfTGTAhh"
      },
      "source": [
        "Let's test our ReAct agent with a simple factual question that requires a search:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGC0R4VO4oEo",
        "outputId": "3d245e8e-1c7f-442d-cef9-160ac144abf4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[0m----------------------------------------- User (Turn 1/2): -----------------------------------------\u001b[0m\n",
            "  What is the capital of France?\n",
            "\u001b[0m----------------------------------------------------------------------------------------------------\u001b[0m\n",
            "\u001b[38;5;208m--------------------------------------- Thought (Turn 1/2): ---------------------------------------\u001b[0m\n",
            "  I need to find the capital of France to answer the user's question. The `search` tool can be used to retrieve this factual information.\n",
            "\u001b[38;5;208m----------------------------------------------------------------------------------------------------\u001b[0m\n",
            "\u001b[92m------------------------------------- Tool request (Turn 1/2): -------------------------------------\u001b[0m\n",
            "  search(query='capital of France')\n",
            "\u001b[92m----------------------------------------------------------------------------------------------------\u001b[0m\n",
            "\u001b[93m------------------------------------- Observation (Turn 1/2): -------------------------------------\u001b[0m\n",
            "  Paris is the capital of France and is known for the Eiffel Tower.\n",
            "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
            "\u001b[38;5;208m--------------------------------------- Thought (Turn 2/2): ---------------------------------------\u001b[0m\n",
            "  I have successfully found the capital of France using the search tool. The next step is to communicate this answer to the user.\n",
            "\u001b[38;5;208m----------------------------------------------------------------------------------------------------\u001b[0m\n",
            "\u001b[96m------------------------------------- Final answer (Turn 2/2): -------------------------------------\u001b[0m\n",
            "  Paris is the capital of France.\n",
            "\u001b[96m----------------------------------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# A straightforward question requiring a search.\n",
        "question = \"What is the capital of France?\"\n",
        "final_answer = react_agent_loop(question, TOOL_REGISTRY, max_turns=2, verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Last, let's test it with a question that our mock search tool doesn't have knowledge about:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M0ubuT6akHli",
        "outputId": "1b84cc8f-e730-4df7-ace6-941ff7ce4982"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[0m----------------------------------------- User (Turn 1/2): -----------------------------------------\u001b[0m\n",
            "  What is the capital of Italy?\n",
            "\u001b[0m----------------------------------------------------------------------------------------------------\u001b[0m\n",
            "\u001b[38;5;208m--------------------------------------- Thought (Turn 1/2): ---------------------------------------\u001b[0m\n",
            "  I need to find the capital of Italy to answer the user's question. The `search` tool can provide this information efficiently.I will use the `search` tool to find the capital of Italy.\n",
            "\u001b[38;5;208m----------------------------------------------------------------------------------------------------\u001b[0m\n",
            "\u001b[92m------------------------------------- Tool request (Turn 1/2): -------------------------------------\u001b[0m\n",
            "  search(query='capital of Italy')\n",
            "\u001b[92m----------------------------------------------------------------------------------------------------\u001b[0m\n",
            "\u001b[93m------------------------------------- Observation (Turn 1/2): -------------------------------------\u001b[0m\n",
            "  Information about 'capital of Italy' was not found.\n",
            "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
            "\u001b[38;5;208m--------------------------------------- Thought (Turn 2/2): ---------------------------------------\u001b[0m\n",
            "  The previous search query \"capital of Italy\" did not return information, which is unexpected for a common fact. I will try a broader search query like \"Italy\" to see if more general information or a list of facts about Italy can provide the capital, as the tool might respond better to less specific phrasing or have had a temporary issue with the exact previous query.I will try a broader search query like \"Italy\" to see if more general information or a list of facts about Italy can provide the capital, as the tool might respond better to less specific phrasing or have had a temporary issue with the exact previous query.\n",
            "\u001b[38;5;208m----------------------------------------------------------------------------------------------------\u001b[0m\n",
            "\u001b[92m------------------------------------- Tool request (Turn 2/2): -------------------------------------\u001b[0m\n",
            "  search(query='Italy')\n",
            "\u001b[92m----------------------------------------------------------------------------------------------------\u001b[0m\n",
            "\u001b[93m------------------------------------- Observation (Turn 2/2): -------------------------------------\u001b[0m\n",
            "  Information about 'Italy' was not found.\n",
            "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
            "\u001b[96m-------------------------------------- Final answer (Forced): --------------------------------------\u001b[0m\n",
            "  I'm sorry, but I couldn't find information about the capital of Italy.\n",
            "\u001b[96m----------------------------------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# A question about a concept the mock search tool doesn't know.\n",
        "question = \"What is the capital of Italy?\"\n",
        "final_answer = react_agent_loop(question, TOOL_REGISTRY, max_turns=2, verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notice how the ReAct agent tried different strategies to find an answer for the user query, demonstrating live adaptation."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
