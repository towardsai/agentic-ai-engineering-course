{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOgiJKN3MqB1"
      },
      "source": [
        "# Lesson 8: ReAct Practice\n",
        "\n",
        "This notebook explores practical the ReAct (Reasoning and Acting) pattern with Google's Gemini API. We will use the `google-genai` library to interact with Gemini models. It includes a mock search tool, a thought generation phase using structured outputs, and an action phase with function calling, all orchestrated by a ReAct control loop."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Learning Objectives:**\n",
        "\n",
        "1. Understand how ReAct breaks problems into Thought → Action → Observation.\n",
        "2. Practice orchestrating the full ReAct loop end-to-end."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup\n",
        "\n",
        "First, we define some standard Magic Python commands to autoreload Python packages whenever they change:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GRVoMgfLMsBP"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Set Up Python Environment\n",
        "\n",
        "To set up your Python virtual environment using `uv` and use it in the Notebook, follow the step-by-step instructions from the [Course Admin](https://academy.towardsai.net/courses/take/agent-engineering/multimedia/67469688-lesson-1-part-2-course-admin) lesson from the beginning of the course.\n",
        "\n",
        "**TL/DR:** Be sure the correct kernel pointing to your `uv` virtual environment is selected."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Configure Gemini API\n",
        "\n",
        "To configure the Gemini API, follow the step-by-step instructions from the [Course Admin](https://academy.towardsai.net/courses/take/agent-engineering/multimedia/67469688-lesson-1-part-2-course-admin) lesson.\n",
        "\n",
        "But here is a quick check on what you need to run this Notebook:\n",
        "\n",
        "1.  Get your key from [Google AI Studio](https://aistudio.google.com/app/apikey).\n",
        "2.  From the root of your project, run: `cp .env.example .env` \n",
        "3.  Within the `.env` file, fill in the `GOOGLE_API_KEY` variable:\n",
        "\n",
        "Now, the code below will load the key from the `.env` file:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trying to load environment variables from `/Users/omar/Documents/ai_repos/course-ai-agents/.env`\n",
            "Environment variables loaded successfully.\n"
          ]
        }
      ],
      "source": [
        "from lessons.utils import env\n",
        "\n",
        "env.load(required_env_vars=[\"GOOGLE_API_KEY\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBAXJg6nMqB2"
      },
      "source": [
        "### Import Key Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wdZUi2E44tIQ",
        "outputId": "2059055b-6b66-4956-ca87-ce4b78335d34"
      },
      "outputs": [],
      "source": [
        "from enum import Enum\n",
        "from typing import Callable\n",
        "\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "from lessons.utils import pretty_print"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Initialize the Gemini Client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FOx4eD-yM-jO"
      },
      "outputs": [],
      "source": [
        "client = genai.Client()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kLgTbeTBvwt"
      },
      "source": [
        "### Define Constants\n",
        "\n",
        "We will use the `gemini-2.5-flash` model, which is fast and cost-effective:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "09szFdH86K6A"
      },
      "outputs": [],
      "source": [
        "MODEL_ID = \"gemini-2.5-flash\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Id1R6LRKDRcv"
      },
      "source": [
        "## 2. Tools Definition\n",
        "\n",
        "Let's implement our mock search tool that will serve as the external knowledge source for our agent. This simplified version focuses on the ReAct mechanics rather than real API integration:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def search(query: str) -> str:\n",
        "    \"\"\"Search for information about a specific topic or query.\n",
        "\n",
        "    Args:\n",
        "        query (str): The search query or topic to look up.\n",
        "    \"\"\"\n",
        "    query_lower = query.lower()\n",
        "\n",
        "    # Predefined responses for demonstration\n",
        "    if all(word in query_lower for word in [\"capital\", \"france\"]):\n",
        "        return \"Paris is the capital of France and is known for the Eiffel Tower.\"\n",
        "    elif \"react\" in query_lower:\n",
        "        return \"The ReAct (Reasoning and Acting) framework enables LLMs to solve complex tasks by interleaving thought generation, action execution, and observation processing.\"\n",
        "\n",
        "    # Generic response for unhandled queries\n",
        "    return f\"Information about '{query}' was not found.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We maintain a mapping from tool name to tool function (the tool registry). This lets the model plan with symbolic tool names, while our code safely resolves those names to actual Python functions to execute."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TOOL_REGISTRY: dict[str, Callable[..., str]] = {\n",
        "    search.__name__: search,\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyqJqrKRMqB5"
      },
      "source": [
        "## 3. ReAct Thought Phase\n",
        "\n",
        "Now let's implement the thought generation phase. This component analyzes the current situation and determines what the agent should do next, potentially suggesting using tools."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, we prepare the prompt for the thinking part. We implement a function that converts the `TOOL_REGISTRY` to a string XML representation of it, which we insert into the prompt. This way, the LLM knows which tools available and can reason around them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_tools_xml_description(tool_registry: dict[str, Callable[..., str]]) -> str:\n",
        "    \"\"\"Build a minimal XML description of tools using only their docstrings.\"\"\"\n",
        "    lines = []\n",
        "    for tool_name, fn in tool_registry.items():\n",
        "        doc = (fn.__doc__ or \"\").strip()\n",
        "        lines.append(f'\\t<tool name=\"{tool_name}\">')\n",
        "        if doc:\n",
        "            lines.append(\"\\t\\t<description>\")\n",
        "            for line in doc.split(\"\\n\"):\n",
        "                lines.append(f\"\\t\\t\\t{line}\")\n",
        "            lines.append(\"\\t\\t</description>\")\n",
        "        lines.append(\"\\t</tool>\")\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "\n",
        "tools_xml = build_tools_xml_description(TOOL_REGISTRY)\n",
        "\n",
        "PROMPT_TEMPLATE_THOUGHT = \"\"\"\n",
        "You are deciding the next best step for reaching the user goal. You have some tools available to you.\n",
        "\n",
        "Available tools:\n",
        "<tools>\n",
        "{tools_xml}\n",
        "</tools>\n",
        "\n",
        "Conversation so far:\n",
        "<conversation>\n",
        "{conversation}\n",
        "</conversation>\n",
        "\n",
        "State your next thought about what to do next as one short paragraph focused on the next action you intend to take and why.\n",
        "Avoid repeating the same strategies that didn't work previously. Prefer different approaches.\n",
        "\"\"\".strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we `print` the prompt with the tool definitions inside."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You are deciding the next best step for reaching the user goal. You have some tools available to you.\n",
            "\n",
            "Available tools:\n",
            "<tools>\n",
            "\t<tool name=\"search\">\n",
            "\t\t<description>\n",
            "\t\t\tSearch for information about a specific topic or query.\n",
            "\t\t\t\n",
            "\t\t\tArgs:\n",
            "\t\t\t    query (str): The search query or topic to look up.\n",
            "\t\t</description>\n",
            "\t</tool>\n",
            "</tools>\n",
            "\n",
            "Conversation so far:\n",
            "<conversation>\n",
            "\n",
            "</conversation>\n",
            "\n",
            "State your next thought about what to do next as one short paragraph focused on the next action you intend to take and why.\n",
            "Avoid repeating the same strategies that didn't work previously. Prefer different approaches.\n"
          ]
        }
      ],
      "source": [
        "print(PROMPT_TEMPLATE_THOUGHT.format(tools_xml=tools_xml, conversation=\"\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now implement the `generate_thought` function, which reasons on the best next action to take according to the conversation history."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lvS9ww0xflyx"
      },
      "outputs": [],
      "source": [
        "def generate_thought(conversation: str, tool_registry: dict[str, Callable[..., str]]) -> str:\n",
        "    \"\"\"Generate a thought as plain text (no structured output).\"\"\"\n",
        "    tools_xml: str = build_tools_xml_description(tool_registry)\n",
        "    prompt: str = PROMPT_TEMPLATE_THOUGHT.format(tools_xml=tools_xml, conversation=conversation)\n",
        "\n",
        "    response: types.GenerateContentResponse = client.models.generate_content(\n",
        "        model=MODEL_ID,\n",
        "        contents=prompt,\n",
        "    )\n",
        "    if response.text is None:\n",
        "        print(response)\n",
        "        raise ValueError(\"Model did not return a thought\")\n",
        "\n",
        "    return response.text.strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtGC4m-Xsz8R"
      },
      "source": [
        "## 4. ReAct Action Phase"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-qP8k3ZMqB5"
      },
      "source": [
        "Next, let's implement the action phase using function calling. This component determines whether to use a tool or provide a final answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gMH6u54tDQyr"
      },
      "outputs": [],
      "source": [
        "PROMPT_TEMPLATE_ACTION = \"\"\"\n",
        "You are selecting the best next action to reach the user goal.\n",
        "\n",
        "Conversation so far:\n",
        "<conversation>\n",
        "{conversation}\n",
        "</conversation>\n",
        "\n",
        "Respond either with a tool call (with arguments) or a final answer, but only if you can confidently conclude.\n",
        "\"\"\".strip()\n",
        "\n",
        "# Dedicated prompt used when we must force a final answer\n",
        "PROMPT_TEMPLATE_ACTION_FORCED = \"\"\"\n",
        "You must now provide a final answer to the user.\n",
        "\n",
        "Conversation so far:\n",
        "<conversation>\n",
        "{conversation}\n",
        "</conversation>\n",
        "\n",
        "Provide a concise final answer that best addresses the user's goal.\n",
        "\"\"\".strip()\n",
        "\n",
        "\n",
        "class ToolCallRequest(BaseModel):\n",
        "    \"\"\"A request to call a tool with its name and arguments.\"\"\"\n",
        "\n",
        "    tool_name: str = Field(description=\"The name of the tool to call.\")\n",
        "    arguments: dict = Field(description=\"The arguments to pass to the tool.\")\n",
        "\n",
        "\n",
        "class FinalAnswer(BaseModel):\n",
        "    \"\"\"A final answer to present to the user when no further action is needed.\"\"\"\n",
        "\n",
        "    text: str = Field(description=\"The final answer text to present to the user.\")\n",
        "\n",
        "\n",
        "def generate_action(\n",
        "    conversation: str, tool_registry: dict[str, Callable[..., str]] | None = None, force_final: bool = False\n",
        ") -> ToolCallRequest | FinalAnswer:\n",
        "    \"\"\"Generate an action by passing tools to the LLM and parsing function calls or final text.\n",
        "\n",
        "    When force_final is True or no tools are provided, the model is instructed to produce a final answer\n",
        "    and tool calls are disabled.\n",
        "    \"\"\"\n",
        "    # Use a dedicated prompt when forcing a final answer or no tools are provided\n",
        "    if force_final or not tool_registry:\n",
        "        prompt: str = PROMPT_TEMPLATE_ACTION_FORCED.format(conversation=conversation)\n",
        "        response = client.models.generate_content(model=MODEL_ID, contents=prompt)\n",
        "        return FinalAnswer(text=response.text.strip())\n",
        "\n",
        "    # Default action prompt\n",
        "    prompt = PROMPT_TEMPLATE_ACTION.format(conversation=conversation)\n",
        "\n",
        "    # Provide the available tools to the model; disable auto-calling so we can parse and run ourselves\n",
        "    tools: list[Callable[..., str]] = list(tool_registry.values())\n",
        "    config = types.GenerateContentConfig(\n",
        "        tools=tools, automatic_function_calling=types.AutomaticFunctionCallingConfig(disable=True)\n",
        "    )\n",
        "    response: types.GenerateContentResponse = client.models.generate_content(\n",
        "        model=MODEL_ID, contents=prompt, config=config\n",
        "    )\n",
        "\n",
        "    # Extract the function call from the response (if present)\n",
        "    candidate = response.candidates[0]\n",
        "    parts = candidate.content.parts\n",
        "    if parts and getattr(parts[0], \"function_call\", None):\n",
        "        name = parts[0].function_call.name\n",
        "        args = dict(parts[0].function_call.args) if parts[0].function_call.args is not None else {}\n",
        "        return ToolCallRequest(tool_name=name, arguments=args)\n",
        "\n",
        "    # Otherwise, it's a final answer\n",
        "    return FinalAnswer(text=(response.text or \"\").strip())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Why we provide an option to force the final answer? In a ReAct loop we sometimes need to terminate cleanly after a budget of turns (e.g., to avoid infinite loops or excessive tool calls). The force flag lets us ask the model to conclude with a final answer even if, under normal conditions, it might keep calling tools. This ensures graceful shutdown and a usable output at the end of the loop."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note: In the Action phase we do not inline tool descriptions into the prompt (unlike the Thought phase). Instead, we pass the available Python tool functions through the `tools` parameter to `generate_content`. The client automatically parses these tools and incorporates their definitions/arguments into the model's prompt context, enabling function calling without duplicating tool specs in our prompt text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "moxbYJ2_EqYi"
      },
      "source": [
        "## 5. ReAct Control Loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmJ6zg2YMqB6"
      },
      "source": [
        "Now we build the main ReAct control loop that orchestrates the Thought → Action → Observation cycle end-to-end. We treat the conversation between the user and the agent as a sequence of messages. Each message is a step in the dialogue, and each step corresponds to one ReAct unit: it can be a user message, an internal thought, a tool request, the tool's observation, or the final answer.\n",
        "\n",
        "We'll start by defining the data structures for these messages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iH0rKt2Pcf3W"
      },
      "outputs": [],
      "source": [
        "class MessageRole(str, Enum):\n",
        "    \"\"\"Enumeration for the different roles a message can have.\"\"\"\n",
        "\n",
        "    USER = \"user\"\n",
        "    THOUGHT = \"thought\"\n",
        "    TOOL_REQUEST = \"tool request\"\n",
        "    OBSERVATION = \"observation\"\n",
        "    FINAL_ANSWER = \"final answer\"\n",
        "\n",
        "\n",
        "class Message(BaseModel):\n",
        "    \"\"\"A message with a role and content, used for all message types.\"\"\"\n",
        "\n",
        "    role: MessageRole = Field(description=\"The role of the message in the ReAct loop.\")\n",
        "    content: str = Field(description=\"The textual content of the message.\")\n",
        "\n",
        "    def __str__(self) -> str:\n",
        "        \"\"\"Provides a user-friendly string representation of the message.\"\"\"\n",
        "        return f\"{self.role.value.capitalize()}: {self.content}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We also add a small printer that uses our `pretty_print` module to render each message nicely in the notebook. This makes it easy to follow how the agent alternates between Thought, Action (tool call), and Observation across turns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def pretty_print_message(\n",
        "    message: Message,\n",
        "    turn: int,\n",
        "    max_turns: int,\n",
        "    header_color: str = pretty_print.Color.YELLOW,\n",
        "    is_forced_final_answer: bool = False,\n",
        ") -> None:\n",
        "    if not is_forced_final_answer:\n",
        "        title = f\"{message.role.value.capitalize()} (Turn {turn}/{max_turns}):\"\n",
        "    else:\n",
        "        title = f\"{message.role.value.capitalize()} (Forced):\"\n",
        "\n",
        "    pretty_print.wrapped(\n",
        "        text=message.content,\n",
        "        title=title,\n",
        "        header_color=header_color,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now use a `Scratchpad` class that wraps a list of `Message` objects and provides `append(..., verbose=False)` to both store and (optionally) pretty-print messages with role-based colors. The scratchpad is serialized each turn so the model can plan the next step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Scratchpad:\n",
        "    \"\"\"Container for ReAct messages with optional pretty-print on append.\"\"\"\n",
        "\n",
        "    def __init__(self, max_turns: int) -> None:\n",
        "        self.messages: list[Message] = []\n",
        "        self.max_turns: int = max_turns\n",
        "        self.current_turn: int = 1\n",
        "\n",
        "    def set_turn(self, turn: int) -> None:\n",
        "        self.current_turn = turn\n",
        "\n",
        "    def append(self, message: Message, verbose: bool = False, is_forced_final_answer: bool = False) -> None:\n",
        "        self.messages.append(message)\n",
        "        if verbose:\n",
        "            role_to_color = {\n",
        "                MessageRole.USER: pretty_print.Color.RESET,\n",
        "                MessageRole.THOUGHT: pretty_print.Color.ORANGE,\n",
        "                MessageRole.TOOL_REQUEST: pretty_print.Color.GREEN,\n",
        "                MessageRole.OBSERVATION: pretty_print.Color.YELLOW,\n",
        "                MessageRole.FINAL_ANSWER: pretty_print.Color.CYAN,\n",
        "            }\n",
        "            header_color = role_to_color.get(message.role, pretty_print.Color.YELLOW)\n",
        "            pretty_print_message(\n",
        "                message=message,\n",
        "                turn=self.current_turn,\n",
        "                max_turns=self.max_turns,\n",
        "                header_color=header_color,\n",
        "                is_forced_final_answer=is_forced_final_answer,\n",
        "            )\n",
        "\n",
        "    def to_string(self) -> str:\n",
        "        return \"\\n\".join(str(m) for m in self.messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now implement the control loop.\n",
        "- On the first turn, we add the user question.\n",
        "- Then, at each turn: (1) we get a Thought from the model; (2) we get an Action. If the action is a `FinalAnswer`, we stop. If it's a `ToolCallRequest`, we execute the tool and append the resulting `Observation`, then continue. If we reach the maximum number of turns, we run the action selector one last time with a flag that forces a final answer (no tool calls)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W-PSlQ34f1RM"
      },
      "outputs": [],
      "source": [
        "def react_agent_loop(\n",
        "    initial_question: str, tool_registry: dict[str, Callable[..., str]], max_turns: int = 5, verbose: bool = False\n",
        ") -> str | None:\n",
        "    \"\"\"\n",
        "    Implements the main ReAct (Thought -> Action -> Observation) control loop.\n",
        "    Uses a unified message class for the scratchpad.\n",
        "    \"\"\"\n",
        "    scratchpad = Scratchpad(max_turns=max_turns)\n",
        "\n",
        "    # Add the user's question to the scratchpad\n",
        "    user_message = Message(role=MessageRole.USER, content=initial_question)\n",
        "    scratchpad.append(user_message, verbose=verbose)\n",
        "\n",
        "    for turn in range(1, max_turns + 1):\n",
        "        scratchpad.set_turn(turn)\n",
        "\n",
        "        # Generate a thought based on the current scratchpad\n",
        "        thought_content = generate_thought(\n",
        "            scratchpad.to_string(),\n",
        "            tool_registry,\n",
        "        )\n",
        "        thought_message = Message(role=MessageRole.THOUGHT, content=thought_content)\n",
        "        scratchpad.append(thought_message, verbose=verbose)\n",
        "\n",
        "        # Generate an action based on the current scratchpad\n",
        "        action_result = generate_action(\n",
        "            scratchpad.to_string(),\n",
        "            tool_registry=tool_registry,\n",
        "        )\n",
        "\n",
        "        # If the model produced a final answer, return it\n",
        "        if isinstance(action_result, FinalAnswer):\n",
        "            final_answer = action_result.text\n",
        "            final_message = Message(role=MessageRole.FINAL_ANSWER, content=final_answer)\n",
        "            scratchpad.append(final_message, verbose=verbose)\n",
        "            return final_answer\n",
        "\n",
        "        # Otherwise, it is a tool request\n",
        "        if isinstance(action_result, ToolCallRequest):\n",
        "            action_name = action_result.tool_name\n",
        "            action_params = action_result.arguments\n",
        "\n",
        "            # Add the action to the scratchpad\n",
        "            params_str = \", \".join([f\"{k}='{v}'\" for k, v in action_params.items()])\n",
        "            action_content = f\"{action_name}({params_str})\"\n",
        "            action_message = Message(role=MessageRole.TOOL_REQUEST, content=action_content)\n",
        "            scratchpad.append(action_message, verbose=verbose)\n",
        "\n",
        "            # Run the action and get the observation\n",
        "            if action_name not in tool_registry:\n",
        "                observation_content = f\"Unknown tool '{action_name}'. Available: {', '.join(tool_registry)}\"\n",
        "            else:\n",
        "                try:\n",
        "                    observation_content = tool_registry[action_name](**action_params)\n",
        "                except Exception as e:\n",
        "                    observation_content = f\"Error executing tool '{action_name}': {e}\"\n",
        "\n",
        "            # Add the observation to the scratchpad\n",
        "            observation_message = Message(role=MessageRole.OBSERVATION, content=observation_content)\n",
        "            scratchpad.append(observation_message, verbose=verbose)\n",
        "\n",
        "        # Check if the maximum number of turns has been reached. If so, force the action selector to produce a final answer\n",
        "        if turn == max_turns:\n",
        "            forced_action = generate_action(\n",
        "                scratchpad.to_string(),\n",
        "                force_final=True,\n",
        "            )\n",
        "            if isinstance(forced_action, FinalAnswer):\n",
        "                final_answer = forced_action.text\n",
        "            else:\n",
        "                final_answer = \"Unable to produce a final answer within the allotted turns.\"\n",
        "            final_message = Message(role=MessageRole.FINAL_ANSWER, content=final_answer)\n",
        "            scratchpad.append(final_message, verbose=verbose, is_forced_final_answer=True)\n",
        "            return final_answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utJSwfTGTAhh"
      },
      "source": [
        "Let's test our ReAct agent with a simple factual question that requires a search:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGC0R4VO4oEo",
        "outputId": "3d245e8e-1c7f-442d-cef9-160ac144abf4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[0m----------------------------------------- User (Turn 1/2): -----------------------------------------\u001b[0m\n",
            "  What is the capital of France?\n",
            "\u001b[0m----------------------------------------------------------------------------------------------------\u001b[0m\n",
            "\u001b[38;5;208m--------------------------------------- Thought (Turn 1/2): ---------------------------------------\u001b[0m\n",
            "  I will use the `search` tool to find the capital of France, as this is a factual question directly answerable by a search query.\n",
            "\u001b[38;5;208m----------------------------------------------------------------------------------------------------\u001b[0m\n",
            "\u001b[92m------------------------------------- Tool request (Turn 1/2): -------------------------------------\u001b[0m\n",
            "  search(query='capital of France')\n",
            "\u001b[92m----------------------------------------------------------------------------------------------------\u001b[0m\n",
            "\u001b[93m------------------------------------- Observation (Turn 1/2): -------------------------------------\u001b[0m\n",
            "  Paris is the capital of France and is known for the Eiffel Tower.\n",
            "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
            "\u001b[38;5;208m--------------------------------------- Thought (Turn 2/2): ---------------------------------------\u001b[0m\n",
            "  Paris is the capital of France. I will respond to the user with this information.\n",
            "\u001b[38;5;208m----------------------------------------------------------------------------------------------------\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Warning: there are non-text parts in the response: ['thought_signature'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[96m------------------------------------- Final answer (Turn 2/2): -------------------------------------\u001b[0m\n",
            "  The capital of France is Paris.\n",
            "\u001b[96m----------------------------------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# A straightforward question requiring a search.\n",
        "question = \"What is the capital of France?\"\n",
        "final_answer = react_agent_loop(question, TOOL_REGISTRY, max_turns=2, verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Last, let's test it with a question that our mock search tool doesn't have knowledge about:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M0ubuT6akHli",
        "outputId": "1b84cc8f-e730-4df7-ace6-941ff7ce4982"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[0m----------------------------------------- User (Turn 1/2): -----------------------------------------\u001b[0m\n",
            "  What is the capital of Italy?\n",
            "\u001b[0m----------------------------------------------------------------------------------------------------\u001b[0m\n",
            "\u001b[38;5;208m--------------------------------------- Thought (Turn 1/2): ---------------------------------------\u001b[0m\n",
            "  I should use the `search` tool to find the capital of Italy, as this is a factual question that can be answered by a search query.\n",
            "\u001b[38;5;208m----------------------------------------------------------------------------------------------------\u001b[0m\n",
            "\u001b[92m------------------------------------- Tool request (Turn 1/2): -------------------------------------\u001b[0m\n",
            "  search(query='capital of Italy')\n",
            "\u001b[92m----------------------------------------------------------------------------------------------------\u001b[0m\n",
            "\u001b[93m------------------------------------- Observation (Turn 1/2): -------------------------------------\u001b[0m\n",
            "  Information about 'capital of Italy' was not found.\n",
            "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
            "\u001b[38;5;208m--------------------------------------- Thought (Turn 2/2): ---------------------------------------\u001b[0m\n",
            "  The previous search query 'capital of Italy' failed to retrieve information. To avoid repeating the same strategy, I will try a rephrased search query, 'Italy capital city', to see if a different phrasing allows the search tool to find the correct information.\n",
            "\u001b[38;5;208m----------------------------------------------------------------------------------------------------\u001b[0m\n",
            "\u001b[92m------------------------------------- Tool request (Turn 2/2): -------------------------------------\u001b[0m\n",
            "  search(query='Italy capital city')\n",
            "\u001b[92m----------------------------------------------------------------------------------------------------\u001b[0m\n",
            "\u001b[93m------------------------------------- Observation (Turn 2/2): -------------------------------------\u001b[0m\n",
            "  Information about 'Italy capital city' was not found.\n",
            "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
            "\u001b[96m-------------------------------------- Final answer (Forced): --------------------------------------\u001b[0m\n",
            "  I am sorry, but I couldn't find information about the capital of Italy.\n",
            "\u001b[96m----------------------------------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# A question about a concept the mock search tool doesn't know.\n",
        "question = \"What is the capital of Italy?\"\n",
        "final_answer = react_agent_loop(question, TOOL_REGISTRY, max_turns=2, verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notice how the ReAct agent tried different strategies to find an answer for the user query, demonstrating live adaptation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. ReAct with a reasoning model (use built‑in “thinking”)\n",
        "\n",
        "In our previous implementation, generate_thought() built a prompt and forced the model to write a thought paragraph.\n",
        "\n",
        "Now we remove that step. In generate_action(), we turn on the model’s thinking and capture the model‑generated thought summary from the response. We log that as the “Thought” in our trace, then proceed exactly as before (tool call or final answer).\n",
        "\n",
        "You can also both control the \"thnking\" budget and ask for summaries of the model’s internal thinking."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_thought_summary(response: types.GenerateContentResponse) -> str | None:\n",
        "    parts = getattr(response.candidates[0].content, \"parts\", []) or []\n",
        "    chunks = [p.text for p in parts if getattr(p, \"thought\", False) and getattr(p, \"text\", None)]\n",
        "    return \"\\n\".join(chunks).strip() if chunks else None\n",
        "\n",
        "\n",
        "def extract_first_function_call(response: types.GenerateContentResponse):\n",
        "    # Prefer the convenience accessor\n",
        "    if getattr(response, \"function_calls\", None):\n",
        "        fc = response.function_calls[0]\n",
        "        return fc.name, dict(fc.args or {})\n",
        "    # Fallback: scan parts\n",
        "    parts = getattr(response.candidates[0].content, \"parts\", []) or []\n",
        "    for p in parts:\n",
        "        if getattr(p, \"function_call\", None):\n",
        "            return p.function_call.name, dict(p.function_call.args or {})\n",
        "    return None\n",
        "\n",
        "\n",
        "def build_config_with_tools(tools):\n",
        "    return types.GenerateContentConfig(\n",
        "        tools=tools,\n",
        "        thinking_config=types.ThinkingConfig(\n",
        "            include_thoughts=True,  # return thought summaries in parts where part.thought == True\n",
        "            thinking_budget=1024,  # -1 = dynamic, 0 disables on Flash/Flash-Lite\n",
        "        ),\n",
        "        automatic_function_calling=types.AutomaticFunctionCallingConfig(disable=True),\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def react_agent_loop_thinking(\n",
        "    initial_question: str,\n",
        "    tool_registry: dict[str, Callable[..., str]],\n",
        "    max_turns: int = 5,\n",
        "    verbose: bool = True,\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    ReAct loop using gemini-2.5-flash's built-in thinking.\n",
        "    \"\"\"\n",
        "\n",
        "    scratchpad = Scratchpad(max_turns=max_turns)\n",
        "    scratchpad.append(Message(role=MessageRole.USER, content=initial_question), verbose=verbose)\n",
        "\n",
        "    # Structured \"contents\" conversation for thought signatures\n",
        "    contents: list[types.Content] = [types.Content(role=\"user\", parts=[types.Part(text=initial_question)])]\n",
        "    tools = list(tool_registry.values())\n",
        "    config = build_config_with_tools(tools)\n",
        "\n",
        "    for turn in range(1, max_turns + 1):\n",
        "        scratchpad.set_turn(turn)\n",
        "\n",
        "        response = client.models.generate_content(\n",
        "            model=MODEL_ID,\n",
        "            contents=contents,\n",
        "            config=config,\n",
        "        )\n",
        "\n",
        "        # 1) Thought summary (if any) — log as your THOUGHT message\n",
        "        thoughts = extract_thought_summary(response)\n",
        "        if thoughts:\n",
        "            scratchpad.append(Message(role=MessageRole.THOUGHT, content=thoughts), verbose=verbose)\n",
        "\n",
        "        # 2) Tool call?\n",
        "        fc = extract_first_function_call(response)\n",
        "        if fc:\n",
        "            name, args = fc\n",
        "\n",
        "            # Keep the model's full response content to preserve *thought signatures*\n",
        "            contents.append(response.candidates[0].content)\n",
        "\n",
        "            # Log the tool request\n",
        "            params_str = \", \".join(f\"{k}={repr(v)}\" for k, v in args.items())\n",
        "            scratchpad.append(\n",
        "                Message(role=MessageRole.TOOL_REQUEST, content=f\"{name}({params_str})\"),\n",
        "                verbose=verbose,\n",
        "            )\n",
        "\n",
        "            # Execute the tool\n",
        "            if name not in tool_registry:\n",
        "                observation = f\"Unknown tool '{name}'. Available: {', '.join(tool_registry)}\"\n",
        "            else:\n",
        "                try:\n",
        "                    observation = tool_registry[name](**args)\n",
        "                except Exception as e:\n",
        "                    observation = f\"Error executing tool '{name}': {e}\"\n",
        "\n",
        "            # Log observation\n",
        "            scratchpad.append(Message(role=MessageRole.OBSERVATION, content=observation), verbose=verbose)\n",
        "\n",
        "            # Send the function response back (standard function-calling protocol)\n",
        "            fn_resp = types.Part.from_function_response(\n",
        "                name=name,\n",
        "                response={\"result\": observation},\n",
        "            )\n",
        "            contents.append(types.Content(role=\"user\", parts=[fn_resp]))\n",
        "            continue  # next turn\n",
        "\n",
        "        # 3) No function call => final text\n",
        "        final_text = (response.text or \"\").strip()\n",
        "        scratchpad.append(Message(role=MessageRole.FINAL_ANSWER, content=final_text), verbose=verbose)\n",
        "        return final_text\n",
        "\n",
        "    # 4) Forced finish if we hit max turns: disable tool-calling for the last shot\n",
        "    forced_config = types.GenerateContentConfig(\n",
        "        thinking_config=types.ThinkingConfig(\n",
        "            include_thoughts=True,  # return thought summaries in parts where part.thought == True\n",
        "            thinking_budget=1024,  # -1 = dynamic, 0 disables on Flash/Flash-Lite\n",
        "        ),\n",
        "        tool_config=types.ToolConfig(\n",
        "            function_calling_config=types.FunctionCallingConfig(mode=types.FunctionCallingConfigMode.NONE)\n",
        "        ),\n",
        "    )\n",
        "    forced_response = client.models.generate_content(model=MODEL_ID, contents=contents, config=forced_config)\n",
        "    final_text = (forced_response.text or \"Unable to produce a final answer within the allotted turns.\").strip()\n",
        "    scratchpad.append(\n",
        "        Message(role=MessageRole.FINAL_ANSWER, content=final_text),\n",
        "        verbose=verbose,\n",
        "        is_forced_final_answer=True,\n",
        "    )\n",
        "    return final_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[0m----------------------------------------- User (Turn 1/3): -----------------------------------------\u001b[0m\n",
            "  What is the capital of France?\n",
            "\u001b[0m----------------------------------------------------------------------------------------------------\u001b[0m\n",
            "\u001b[38;5;208m--------------------------------------- Thought (Turn 1/3): ---------------------------------------\u001b[0m\n",
            "  **Navigating a Simple Inquiry**\n",
            "\n",
            "Alright, a straightforward question: \"What is the capital of France?\" That's elementary.  A quick search should handle this. Seems like the `default_api.search` tool is perfect for this type of query.  I'll use \"capital of France\" as the search term and let the tool do its thing.  No need for complex reasoning here; factual retrieval is all that's required.\n",
            "\u001b[38;5;208m----------------------------------------------------------------------------------------------------\u001b[0m\n",
            "\u001b[92m------------------------------------- Tool request (Turn 1/3): -------------------------------------\u001b[0m\n",
            "  search(query='capital of France')\n",
            "\u001b[92m----------------------------------------------------------------------------------------------------\u001b[0m\n",
            "\u001b[93m------------------------------------- Observation (Turn 1/3): -------------------------------------\u001b[0m\n",
            "  Paris is the capital of France and is known for the Eiffel Tower.\n",
            "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
            "\u001b[96m------------------------------------- Final answer (Turn 2/3): -------------------------------------\u001b[0m\n",
            "  Paris.\n",
            "\u001b[96m----------------------------------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "question = \"What is the capital of France?\"\n",
        "final_answer = react_agent_loop_thinking(question, TOOL_REGISTRY, max_turns=3, verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[0m----------------------------------------- User (Turn 1/3): -----------------------------------------\u001b[0m\n",
            "  What is the capital of Italy?\n",
            "\u001b[0m----------------------------------------------------------------------------------------------------\u001b[0m\n",
            "\u001b[38;5;208m--------------------------------------- Thought (Turn 1/3): ---------------------------------------\u001b[0m\n",
            "  **Determining the Answer: The Capital of Italy**\n",
            "\n",
            "Alright, a straightforward question! The user wants to know the capital of Italy. This is a classic factual query, easily handled. I can leverage my access to the `search` tool for this. My plan is simple: I'll use `search` and feed it the query \"capital of Italy.\" This should give me the correct answer quickly and efficiently. Time to get this information!\n",
            "\u001b[38;5;208m----------------------------------------------------------------------------------------------------\u001b[0m\n",
            "\u001b[92m------------------------------------- Tool request (Turn 1/3): -------------------------------------\u001b[0m\n",
            "  search(query='capital of Italy')\n",
            "\u001b[92m----------------------------------------------------------------------------------------------------\u001b[0m\n",
            "\u001b[93m------------------------------------- Observation (Turn 1/3): -------------------------------------\u001b[0m\n",
            "  Information about 'capital of Italy' was not found.\n",
            "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
            "\u001b[38;5;208m--------------------------------------- Thought (Turn 2/3): ---------------------------------------\u001b[0m\n",
            "  **Troubleshooting a Knowledge Query**\n",
            "\n",
            "Okay, so I started with a straightforward question, \"What's the capital of Italy?\", expecting a direct answer from the search tool.  Instead, I got a disappointing \"information not found\" message.  That's...unexpected.  A question this basic should have a clear answer in any decent knowledge base.\n",
            "\n",
            "My first thought is to rephrase the query. Perhaps the specific phrasing threw it off? I could try something like \"capital of Italy\" - maybe using the same exact keywords will help.  Alternatively, I need to broaden my approach.  Instead of targeting the specific capital, I should look for general information. \"Italy\" is the key.  A basic overview of a country *always* includes its capital city, right?  Failing a direct hit, a broader search is the logical next step.\n",
            "\u001b[38;5;208m----------------------------------------------------------------------------------------------------\u001b[0m\n",
            "\u001b[92m------------------------------------- Tool request (Turn 2/3): -------------------------------------\u001b[0m\n",
            "  search(query='Italy')\n",
            "\u001b[92m----------------------------------------------------------------------------------------------------\u001b[0m\n",
            "\u001b[93m------------------------------------- Observation (Turn 2/3): -------------------------------------\u001b[0m\n",
            "  Information about 'Italy' was not found.\n",
            "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Warning: there are non-text parts in the response: ['thought_signature'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[38;5;208m--------------------------------------- Thought (Turn 3/3): ---------------------------------------\u001b[0m\n",
            "  **Unable to Locate Information Using Available Tools**\n",
            "\n",
            "Okay, so I'm running into a brick wall here. The search function isn't delivering, not even for broad topics like \"Italy\" or something simple like \"capital of Italy.\" That's a red flag. It points to a problem with the tool itself, or maybe how it's set up, rather than the query I'm using. I've already tested with a couple of different approaches, just to be sure, but nothing. \n",
            "\n",
            "Given my limitations – I can't actually get in there and troubleshoot the tool or try other external sources – I have to be straightforward. I'll need to let the user know that the search is failing me. I certainly can't make something up, or try a workaround, because that's not how I operate. Transparency is key.\n",
            "\u001b[38;5;208m----------------------------------------------------------------------------------------------------\u001b[0m\n",
            "\u001b[96m------------------------------------- Final answer (Turn 3/3): -------------------------------------\u001b[0m\n",
            "  I'm sorry, I was unable to find the capital of Italy using my tools.\n",
            "\u001b[96m----------------------------------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "question = \"What is the capital of Italy?\"\n",
        "final_answer = react_agent_loop_thinking(question, TOOL_REGISTRY, max_turns=3, verbose=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
