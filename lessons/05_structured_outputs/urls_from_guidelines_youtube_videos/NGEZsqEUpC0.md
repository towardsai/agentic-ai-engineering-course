All right folks. In this video, what we're going to be doing is we're going to be discussing OpenAI function calling and structured prompting. Now, the idea here is that we want to first discuss what function calling is and how OpenAI implements it. And then discuss structured prompting using the Pydantic library as an example on how that can work in terms of writing code and what does it mean to structure the prompt, etc. Right?

[00:30] So. Um, what we're going to be talking about first is function calling. So, function calling means connecting large language models like ChatGPT to tools that perform actions in the outside world like checking the weather or searching the web. Right? And OpenAI function calling involves four simple steps. We call the model with the query and a set of functions that are defined in the functions parameter. So we explain to the model, these are the tools that are available to you.

[01:00] Then, step number two, the model can choose whether or not to call one or more functions that it has available to it. And the content of that call will be a stringified JSON object that adheres to a custom schema. All right? And actually this has now become a an industry standard. So, we then parse the string into JSON in the code and call the function with the provided arguments if they exist. And then we call the model again by appending the function response as a new message and let the model summarize the results back to the user. So essentially we say the model, look, these are the tools that you have available. We have a structured way to connect the model to those functions and to send inputs to those functions once it has identified through reading through processing the prompt that a function should be called to solve the problem.

[01:50] And then the model sends the correct arguments to that function, calls the function, the output which is called an observation is integrated into a response that gets summarized to the user. Right? So, we're going to be seeing an example here in Python code on how to do that. I'm going to initialize my OpenAI client. We're going to have, I'm going to have a very simple function that creates a directory in my current folder. And then I'm going to write it as in the JSON schema for the OpenAI function calling API.

[02:17] So, it will have a dictionary with the type, the function, the name of the function, the description, the parameters. Within the parameters, each the type of the parameter, the object, the properties of that function of the arguments, so this is directory, it is a type of string and describes what that uh parameter does. And we set up also a key called required which indicates which arguments are required in that particular function.

[02:46] And then we put this function definition for the OpenAI function calling API inside of a list, which is pretty cool. Now, what we have is this little function called run terminal task. We create a variable called messages and inside that variable we give it a list with the prompts to the model. In this case, we're just saying uh create a folder called lucas-loves-llms, which is, you know, why not?

[03:15] And then we set up the tools inside of a list. We call the model, we're calling the GPT 3.5 turbo 16k. We give the messages parameter that will contain this message with our prompt to the model. We set up the tools and we set up the tool choice to automatic, so that the model can automatically choose to call a function or not. Then we gather the response and we identify, we we check whether or not tool calls were made in that response.

[03:45] Right? And if they were made, what we do is we have a dictionary with the available functions that the model can use, and then we append that response to the messages object, to the messages list. We loop over the tool calls that were made. We gather the name, the function, the arguments of the function, and we call the tool, getting the function response, right? We append everything under the messages list and we call the model with all of that information to integrate and summarize the response which is then returned to the user, like we are doing right here.

[04:24] And we get the output. Right? So when I call we'll get uh chat completion object like this. And if we inspect the string that was returned here we say lucas loves llms has been created, the folder has been created. And if I check my current folders, we see that the folder was indeed created, which is awesome, right? Now.

[04:47] Folks, this is great. Function calling is amazing, right? Function calling introduces this idea of trying to add structure and determinism to the process of interacting with large language models, right? And in the theme of that structured interaction with large language models, a library that has been extremely popular in not only in Python but now in the large language model universe in terms of frameworks is a library called Pydantic.

[05:21] Now, what this, uh this is a data validation library in Python that allows you that allows us to do some pretty interesting stuff. So, essentially what it allows us to do is it allows us to give, uh, set up data structures that we can have. And when connected with the OpenAI function calling API, Pydantic allows us to define specifically what is the object we want returned when we prompt the model with something. So you will understand that in a second. So what we're going to be doing is, in addition to OpenAI and Pydantic, we're also going to use the instructor package for this demonstration.

[06:05] And we're going to set up two classes in Pydantic. We're going to set up a class called Question that contains three attributes: the question attribute that holds the quiz question, the options for that question, imagine multiple choice, and the correct answer as an integer that refers to the index of the correct answer in the options list. All right? So, after uh having done that, what we're going to do is we're going to have a second class called Quiz that contains the topic in question for this quiz that we want to create from a webpage article or paper, and a list of questions which in each of those elements inside of this list will have the an object of the Question type.

[07:07] So folks, this is a lot of information but what we're doing here is we're setting up data types, right? And we're constructing these customizable, these custom data types with Pydantic. And why this is so cool? Because it allows us to prompt a model like ChatGPT and ask for that model to create something structured out of a prompt that was made in natural language. So I can say, uh, so let's understand that by in practice. So, I'm going to set up my client and now to interact with this, uh, and connect OpenAI function calling and the Pydantic API, we're going to be using the instructor package.

[07:48] So I'm going to set up the client with the instructor.from_openai method and then I'm going to give the OpenAI client to that method. And then I'm going to define a function called generate a quiz which calls the ChatGPT API with the chat.completions.create method. It sets up the model as GPT4 turbo. Oh, sorry, let's go back there. And it sets up the model GPT4 turbo and then it sets up the messages list and in that messages list, we feed it a dictionary containing the system message in which we say you're a quiz generation engine specialized in fostering understanding in students given a source of content to be studied.

[08:31] You will be fed content like articles or scientific papers and you will output quizzes aimed at eliciting full understanding of the material. Right? Pretty cool system message. And then we're going to give the prompt to the model. And the prompt is going to contain a prompt, right? Just like we've talked about in the in the initial lesson for this um live training for this uh video course about prompt engineering. But it will also contain the contents of the article or paper. So, in the prompt, we're going to say, I want you to do the following: identify the main topic of the following article just like we've discussed in the beginning uh of this series, we're breaking the problem down into tasks.

[09:14] Right? So identify the main topic of the following article, and then I give all the contents of the following article under the delimiters uh quotes so that we organize what is input text, remember? And what is the instruction? Then, for the second step, I want the model to create a quiz with five questions that revolve around the main topic and can be answered simply by carefully reading and understanding the article because I want the questions to be grounded on the reference text. Remember our best practices video where we talked about the strategy of grounding questions in, you know, grounding answers in reference text. So that's what we're doing here to create this quiz.

[10:00] One of these questions should check if the student understood the main ideas by testing if the student can transfer its knowledge in a different context because the idea with this quiz is to have a quiz that's comprehensive and helps the student learn um something new, right? And then we give our little output indicator which we just say output colon, right? So, when I call this, we can take a look at the output. And what's interesting about this output is, and we'll see it in just a second, is that we will see the structure that we defined with using the Pydantic library.

[10:44] And that's what makes this approach a structured prompting approach because we're uh getting an output that has structured. Right? And obviously we could talk about structured prompting as adding structure in the prompt itself, but when I say structured prompting in this context, I mean using libraries like the Pydantic OpenAI API to add structure to the output that we get from OpenAI or from ChatGPT. So, as we see here, the output is a Quiz object, which is the object that we've defined in the beginning, and it has a topic, it has a list of questions, and each question here is going to be of that Question object that we've defined earlier as well.

[11:28] So what I can do is I can loop over each question. So I can say for q in quiz_output.questions. So for q in and then we can print q.question as well as print q. and as well as loop over the options in that question. So for I, o in enumerate q.options.

[12:14] We can print I, o, we can print the option. And then at the end, we can print the correct answer by saying correct by saying correct answer and then here we can say q.correct_answer, which actually is not q.correct_answer is q.question. q.correct_answer. Yeah, I think that's correct. Perfect. So now we get the question, we get the options and we get the correct answer. Which if you ask me, this is a pretty cool application for large language model, as well as for a structured prompting approach that leverages OpenAI function calling, Pydantic, and that's it for this video and see you in the next video.