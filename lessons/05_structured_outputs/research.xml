<?xml version="1.0" encoding="UTF-8"?>
<research>
  <research_results>
    <query_group>
      <query><![CDATA[What are the most frequent failure modes and best-practice prompting techniques for getting large language models to emit strictly valid JSON (or other structured formats) without hallucinations?]]></query>
      <source_blocks>
        <source_block><![CDATA[### Source [2]: https://faktion.com/blog/guided-text-generation-with-large-language-models/

Query: What are the most frequent failure modes and best-practice prompting techniques for getting large language models to emit strictly valid JSON (or other structured formats) without hallucinations?

Answer: **Frequent failure modes** when prompting LLMs for valid JSON include:
- **Syntactic errors** such as missing brackets or commas.
- **Semantic errors** like wrong types or hallucinated fields.
- Partial output that cannot be parsed as JSON.

**Best-practice prompting techniques and tools**:
- For OpenAI models, use **Function Calling**: This approach programmatically constrains outputs to JSON, virtually eliminating hallucinations and syntactic errors.
- For transformer-based models, tools like **Jsonformer** generate only attribute values, improving reliability over plain prompt engineering, though with limited schema support.
- Libraries like **Guidance** and **Outlines** provide scripting and token constraints, allowing more precise control over JSON structure and content. Outlines, in particular, enables token-level constraints using regex or predefined vocabularies and can enforce Pydantic model compliance.
- **Prompt engineering** alone remains inconsistent and not always fully reliable, especially with smaller models.

**Other solutions** include **LMQL** and **guardrails**, which offer their own structured output enforcement mechanisms.

The source highlights that while function calling offers the highest reliability for OpenAI models, more general or open-source approaches rely on advanced libraries to achieve high accuracy in JSON output. Prompt-only solutions are less reliable for strict requirements[2].

-----

-----]]></source_block>
        <source_block><![CDATA[### Source [3]: https://arxiv.org/html/2408.11061v1

Query: What are the most frequent failure modes and best-practice prompting techniques for getting large language models to emit strictly valid JSON (or other structured formats) without hallucinations?

Answer: When evaluating LLMs' ability to **emit strictly valid JSON** in zero-shot settings (without structured decoding), the following **failure modes** are observed:
- Generated responses sometimes **fail to match the requested keys** or **produce incorrect value types**.
- Outputs may be **unparsable** as JSON due to syntactic errors.

The study finds that **structured decoding methods** (such as DOMINO) can improve compliance but may introduce system complexity, reduce throughput, and limit prompt optimization. In pure zero-shot prompting, **success rates vary** depending on prompt design and model size.

The benchmark tasks include:
- Simple value types (string, integer, boolean).
- Lists of values.
- Composite objects (e.g., JSON objects with both string and integer fields).
- Lists of composite objects.

A valid JSON output must have **all required keys and the correct value types**. The **success rate is measured by whether the output can be parsed into the requested JSON format**.

The study suggests that strict format adherence in zero-shot settings is non-trivial and benefits from either prompt optimization or structured decoding. It does not focus on hallucinations per se, but notes that structural errors are a significant concern[3].

-----

-----]]></source_block>
      </source_blocks>
    </query_group>
    <query_group>
      <query><![CDATA[How does Pydantic generate JSON Schema definitions and perform runtime type- and value-validation, and what examples exist of integrating this with LLM outputs in Python?]]></query>
      <source_blocks>
        <source_block><![CDATA[### Source [5]: https://docs.pydantic.dev/latest/api/json_schema/

Query: How does Pydantic generate JSON Schema definitions and perform runtime type- and value-validation, and what examples exist of integrating this with LLM outputs in Python?

Answer: Pydantic provides a class for generating **JSON Schemas** from models, with the main interface being the `generate` function. This function takes a `CoreSchema` (a Pydantic model) and an optional `mode` parameter (defaulting to `"validation"`), and returns a JSON schema representing the specified model. The generated schema is compliant with the JSON Schema specification and can be used for purposes like OpenAPI integration, documentation, and validation across systems. The process is designed to be robust and will raise a `PydanticUserError` if the generator is misused, such as by attempting to generate a schema multiple times from the same instance.

-----

-----]]></source_block>
        <source_block><![CDATA[### Source [6]: https://docs.pydantic.dev/1.10/usage/schema/

Query: How does Pydantic generate JSON Schema definitions and perform runtime type- and value-validation, and what examples exist of integrating this with LLM outputs in Python?

Answer: Pydantic models **automatically create JSON Schemas** that describe their structure, types, and constraints. The schema generation logic is invoked via methods like `.schema()` or `.schema_json()`. For example, when you define a model with fields, enums, and constraints (such as range limits), calling `MainModel.schema_json(indent=2)` outputs the JSON Schema, including properties, types, required fields, and descriptions. This schema reflects all field constraints (like `gt`, `lt` for numeric ranges), field aliases, and enum values, making it suitable for use in API documentation and validation scenarios.

-----

-----]]></source_block>
        <source_block><![CDATA[### Source [7]: https://docs.pydantic.dev/latest/concepts/json_schema/

Query: How does Pydantic generate JSON Schema definitions and perform runtime type- and value-validation, and what examples exist of integrating this with LLM outputs in Python?

Answer: Pydantic supports **automatic creation and customization** of JSON schemas from models. The primary functions for schema generation are `BaseModel.model_json_schema` (returns a jsonable dict of a model's schema) and `TypeAdapter.json_schema` (for adapted types). These functions differ from serialization methods (`model_dump_json`), as they generate a schema definition rather than instance data. The output of `model_json_schema` is compatible with `json.dumps` for conversion to a JSON string and can be directly used for documentation or integration with API standards like OpenAPI. Pydantic provides mechanisms for both fine-grained customization (per-model or type) and broader customizations for schema generation overall, enabling flexible adaptation to different requirements.

-----

-----]]></source_block>
      </source_blocks>
    </query_group>
    <query_group>
      <query><![CDATA[What capabilities, configuration options, and current limitations are documented for Google Gemini’s responseSchema-based structured output compared with manual prompt-based JSON generation?]]></query>
      <source_blocks>
        <source_block><![CDATA[### Source [9]: https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/control-generated-output

Query: What capabilities, configuration options, and current limitations are documented for Google Gemini’s responseSchema-based structured output compared with manual prompt-based JSON generation?

Answer: **Gemini’s responseSchema-based structured output** allows you to explicitly define the structure of the model’s response, including field names and expected data types for each field. This is done by specifying a `responseSchema` parameter in your request, which serves as a blueprint for the expected output. You can also set the `responseMimeType` to control the output format (such as JSON).

Key configuration options include:
- **`GENERATE_RESPONSE_METHOD`**: Choose between streaming (`streamGenerateContent`) or batch (`generateContent`) response modes.
- **Region, project, model, and conversational role**: These must be specified in the request.
- **Text prompt**: Standard input for the instructions to the model.
- **`responseMimeType`**: Controls the format of the output (e.g., `application/json`).
- **`responseSchema`**: Provides a schema reference to define the structured output.

The schema ensures that the model’s response is returned in a predictable, structured format, making it easier to parse and use programmatically. This is distinct from manual prompt-based JSON generation, where you must rely on careful prompt engineering to coax the model into producing valid, consistent JSON—often requiring post-processing and error handling for malformed outputs[1].

-----

-----]]></source_block>
        <source_block><![CDATA[### Source [10]: https://firebase.google.com/docs/ai-logic/generate-structured-output

Query: What capabilities, configuration options, and current limitations are documented for Google Gemini’s responseSchema-based structured output compared with manual prompt-based JSON generation?

Answer: By default, the Gemini API returns unstructured text, but for use cases needing structured text (like JSON), you can provide a `responseSchema` in your request. The schema acts as a **blueprint** that the model adheres to, ensuring the output always conforms to a specified structure.

Capabilities and options:
- **Enforce valid JSON**: The schema ensures all required fields and data types are present.
- **Reduce post-processing**: Structured output can be parsed directly, unlike prompt-based methods that may produce inconsistent formatting.
- **Control classification tasks**: The schema can restrict responses to a fixed set of enums or labels, avoiding ambiguity (e.g., only “positive” or “negative” instead of a wider, less controlled vocabulary).
- **Supports multimodal input**: While the guide focuses on text, the same approach applies when input includes images, video, or audio.

The major advantage over manual prompt-based JSON generation is that you no longer need to rely on the model’s ability to follow instructions for output formatting, which can be error-prone and inconsistent. The schema-based method produces outputs that are reliably structured for downstream tasks[2].

-----

-----]]></source_block>
        <source_block><![CDATA[### Source [11]: https://ai.google.dev/api/generate-content

Query: What capabilities, configuration options, and current limitations are documented for Google Gemini’s responseSchema-based structured output compared with manual prompt-based JSON generation?

Answer: The **Gemini API** allows you to define an explicit JSON schema (as a Go struct, JSON object, or similar) for the expected output. In your request configuration, you can provide:
- **`ResponseMIMEType`**: For example, `application/json`.
- **`ResponseSchema`**: A detailed schema object specifying types and required fields.

Example use case:
- You can instruct Gemini to list recipes, and the schema can require each object to have `recipe_name` (string) and `ingredients` (array of strings).
- The model then returns an array of objects matching this structure.

This stands in contrast to manual prompt-based methods, where you would have to instruct the model in natural language to “output JSON in the following format,” which is less reliable and may require extra validation and parsing logic to handle malformed or inconsistent outputs.

**Current limitations**:
- The schema approach is most effective for tasks where the output format and data types can be strictly defined.
- If your schema is too generic or the task is open-ended, the model may still produce output that requires validation. However, in practice, schema enforcement provides significant improvements over manual prompt-based JSON generation in structure and reliability[3].

-----

-----]]></source_block>
      </source_blocks>
    </query_group>
    <query_group>
      <query><![CDATA[Have any benchmark studies measured token counts, latency, or cost differences when requesting identical data in JSON versus YAML versus XML from LLMs?]]></query>
      <source_blocks>
        <source_block><![CDATA[### Source [12]: https://betterprogramming.pub/yaml-vs-json-which-is-more-efficient-for-language-models-5bc11dd0f6df

Query: Have any benchmark studies measured token counts, latency, or cost differences when requesting identical data in JSON versus YAML versus XML from LLMs?

Answer: This source presents an **empirical study comparing JSON and YAML efficiency for language models**. By asking GPT to generate identical content (a list of month names) in both JSON and YAML, and analyzing the result with OpenAI’s Tokenizer, the author found that **YAML resulted in about a 50% reduction in token count and associated costs compared to JSON**. The study concludes that, although JSON is generally faster to parse and consume programmatically, YAML is more **cost- and time-efficient for LLM interactions**. It recommends generating output in YAML and converting it to JSON in post-processing if strict data typing is needed. The author notes potential issues, such as YAML's less strict typing (numbers might be represented as strings), which can be resolved with post-processing or schema enforcement. No direct comparisons with XML or latency measurements are provided in this source.

-----

-----]]></source_block>
        <source_block><![CDATA[### Source [15]: https://arxiv.org/html/2408.02442v1

Query: Have any benchmark studies measured token counts, latency, or cost differences when requesting identical data in JSON versus YAML versus XML from LLMs?

Answer: This academic paper directly compares **JSON, XML, and YAML output formats in LLMs**. The study presents a table with empirical results for various models, including LLaMA 3 8B and Gemma2 9B IT. The metrics, which likely include **token counts and performance/accuracy measures across text, JSON, XML, and YAML formats**, show that:

- For **LLaMA 3 8B**, the token count (first column) for JSON is 23.37, XML is 11.35, YAML is 13.08, and plain Text is 12.04.
- For **Gemma2 9B IT**, the token count for JSON is higher than for XML and YAML, though YAML is slightly higher than XML.
- The study confirms that **requesting identical data in JSON format generally produces higher token counts than YAML or XML**, which has direct implications for **cost and potential latency** in LLM APIs.
- YAML and XML are typically more concise in terms of token usage compared to JSON, though the precise efficiency varies by model and prompt.

While the full latency and cost breakdowns are not explicitly detailed in the excerpt, the **token count differences directly affect both cost (in token-based pricing models) and potentially latency**, as shorter outputs are processed and transmitted more quickly.

-----

-----]]></source_block>
      </source_blocks>
    </query_group>
    <query_group>
      <query><![CDATA[In production AI pipelines (e.g., entity extraction for knowledge graphs or GraphRAG systems), how are structured outputs from LLMs parsed and used downstream, and what reliability or monitoring practices are recommended?]]></query>
      <source_blocks>
        <source_block><![CDATA[### Source [16]: https://www.instill-ai.com/blog/llm-structured-outputs

Query: In production AI pipelines (e.g., entity extraction for knowledge graphs or GraphRAG systems), how are structured outputs from LLMs parsed and used downstream, and what reliability or monitoring practices are recommended?

Answer: Instill AI recommends a **multi-step pipeline** for robust structured output from LLMs in production. The process is divided into two main stages:

- **Reasoning Step:** The LLM focuses purely on solving the reasoning task, unconstrained by output format. This maximizes the model's reasoning performance.
- **Structuring Step:** The unstructured output from the LLM is then passed to a dedicated component that structures the data into the required schema or format.

This separation addresses the typical failures of single-step approaches, where LLMs often struggle to simultaneously reason and adhere strictly to a structured output format. The multi-step method enables adherence to the specified data model while leveraging the LLM’s full reasoning capacity.

**Monitoring and reliability practices:** The article notes that, compared to tools that repeatedly prompt LLMs until output is correctly structured (increasing latency and cost), the multi-step pipeline achieves correctness and structure more efficiently and reliably. By having a dedicated structuring step, errors and format violations can be systematically detected and handled, improving overall pipeline robustness[1].

-----

-----]]></source_block>
        <source_block><![CDATA[### Source [18]: https://haystack.deepset.ai/tutorials/28_structured_output_with_loop

Query: In production AI pipelines (e.g., entity extraction for knowledge graphs or GraphRAG systems), how are structured outputs from LLMs parsed and used downstream, and what reliability or monitoring practices are recommended?

Answer: The Haystack tutorial demonstrates using **loop-based auto-correction** for structured LLM output:

- **Process:**
  - Use an LLM to generate a structured output (e.g., JSON) according to a predefined schema (such as Pydantic).
  - A custom **OutputValidator** component checks if the output matches the schema.
  - If the output is invalid, the pipeline loops back to the LLM to request corrections, repeating until the output is valid.

- **Reliability and Monitoring:**
  - This method ensures that structured outputs consistently adhere to required schemas before they are used downstream.
  - The auto-correction loop acts as a **built-in monitoring and error correction mechanism**, catching and correcting format deviations automatically.
  - The output is only forwarded downstream (e.g., to a knowledge graph or graph-based retrieval system) once it passes all validation checks[3].

-----

-----]]></source_block>
        <source_block><![CDATA[### Source [19]: https://www.youtube.com/watch?v=zuXW0Hwpme4

Query: In production AI pipelines (e.g., entity extraction for knowledge graphs or GraphRAG systems), how are structured outputs from LLMs parsed and used downstream, and what reliability or monitoring practices are recommended?

Answer: The workflow involves using LLMs with explicit prompts to generate structured outputs—such as sentiment labels or boolean flags—which are then parsed and validated using a JSON schema (e.g., Pydantic base models):

- The output from the LLM is **validated against the schema** before being appended to the results.
- The structured results can be easily converted into tabular data for further downstream processing or integration into machine learning pipelines.
- **Reliability Considerations:** The approach acknowledges that LLM outputs are "still experimental and prone to error", so **human validation** may be required for critical tasks. However, schema validation and automated checks help catch many common mistakes before data is used downstream[4].

-----

-----]]></source_block>
      </source_blocks>
    </query_group>
    <query_group>
      <query><![CDATA[What are the most common pitfalls and effective prompt-engineering patterns for getting large language models (e.g., GPT-4, Llama-3) to emit strictly valid JSON when no native schema enforcement is available?]]></query>
      <source_blocks>
        <source_block><![CDATA[### Source [21]: https://community.openai.com/t/best-practices-to-help-gpt-understand-heavily-nested-json-data-and-analyse-such-data/922339

Query: What are the most common pitfalls and effective prompt-engineering patterns for getting large language models (e.g., GPT-4, Llama-3) to emit strictly valid JSON when no native schema enforcement is available?

Answer: **Common pitfalls** when asking large language models to emit strictly valid JSON include:
- **Context size limitations:** Deeply nested or extremely large JSON structures can quickly exceed the model’s context window, causing confusion and incomplete or invalid output.
- **Complex nesting:** Even the latest models (e.g., GPT-4o) struggle with deeply hierarchical or nested JSON, as the structure can confuse the model’s token prediction, resulting in errors or malformed output.
- **Unfiltered technical data:** Providing raw, unprocessed JSON (especially with unnecessary or irrelevant fields) can degrade output quality, as the model may struggle to focus on the essential parts of the schema.

**Effective prompt-engineering patterns** recommended:
- **Pre-process and flatten JSON:** Before feeding data to the model, flatten or simplify the structure and retain only the relevant, human-readable parts. This helps the model focus and reduces confusion.
- **Trim irrelevant fields:** Remove unnecessary technical details from the input. This reduces cognitive load on the model and improves response quality.
- **Present key-value pairs:** A flat structure with straightforward key-value pairs works best, as it aligns with how LLMs parse and generate structured data.
- **Pre-processing logic:** Where possible, perform as much pre-processing as you can before passing data to the model, ensuring clarity and minimizing unnecessary complexity.

Models are not inherently magical at JSON handling—they rely heavily on the clarity of the prompt and the structure of the data provided[1].

-----

-----]]></source_block>
        <source_block><![CDATA[### Source [22]: https://www.vellum.ai/blog/llama-3-70b-vs-gpt-4-comparison-analysis

Query: What are the most common pitfalls and effective prompt-engineering patterns for getting large language models (e.g., GPT-4, Llama-3) to emit strictly valid JSON when no native schema enforcement is available?

Answer: **Prompting tips for Llama 3 70B** (also applicable to GPT-4):
- **Clear, concise prompts:** Both Llama 3 70B and GPT-4 respond best to unambiguous, direct instructions. Overly complex or “over-engineered” prompts are often unnecessary and can even reduce reliability.
- **Explicit formatting instructions:** Llama 3 70B, in particular, is very good at following format constraints—if the prompt clearly states the requirement for valid JSON output, the model is much more likely to comply.
- **Few-shot and chain-of-thought prompting:** Including a few explicit, correct JSON examples in the prompt can help reinforce the pattern you want the model to follow. For reasoning tasks, chain-of-thought steps within the prompt can boost accuracy.
- **Avoid boilerplate text:** Llama 3 tends to avoid extraneous text in its outputs if you specify *only* JSON output in the prompt.

**Pitfalls**:
- **Ambiguous instructions:** If instructions are unclear or allow for additional explanatory text, the model may insert non-JSON content.
- **Overly complex prompts:** Unnecessary complexity in prompt structure is not required and may degrade JSON accuracy for Llama 3 70B.

The importance of clear, explicit instructions is emphasized for both models to achieve reliable, schema-consistent JSON output[2].

-----

-----]]></source_block>
        <source_block><![CDATA[### Source [24]: https://community.openai.com/t/how-to-get-100-valid-json-answers/554379

Query: What are the most common pitfalls and effective prompt-engineering patterns for getting large language models (e.g., GPT-4, Llama-3) to emit strictly valid JSON when no native schema enforcement is available?

Answer: This thread addresses challenges and best practices for obtaining 100% valid JSON from models such as GPT-4:
- **Reliability issues:** Even with strict prompting, models may sometimes emit malformed JSON due to hallucinations, truncation, or the inclusion of explanatory text.
- **Prompt engineering tips:** Directly instructing the model to “output only valid JSON” and providing a specific schema or example JSON in the prompt improves compliance.
- **Validation tools:** Downstream, always validate the output using a JSON parser. The community recommends using try-catch logic or specialized tools to catch and handle invalid JSON before further processing.
- **Chunking output:** For larger responses, generating output in smaller, manageable chunks can prevent truncation and reduce the risk of invalid JSON.

**Pitfalls**:
- Relying solely on prompt design cannot guarantee 100% valid JSON in all cases, as model outputs are inherently probabilistic and may deviate from strict requirements[4].

-----]]></source_block>
      </source_blocks>
    </query_group>
    <query_group>
      <query><![CDATA[How does Google Gemini’s responseSchema structured-output feature perform in real-world use cases, and what published examples or case studies document its accuracy, cost impact, and current limitations compared with prompt-only JSON generation?]]></query>
      <source_blocks>
        <source_block><![CDATA[### Source [27]: https://cloud.google.com/vertex-ai/generative-ai/docs/maas/capabilities/structured-output

Query: How does Google Gemini’s responseSchema structured-output feature perform in real-world use cases, and what published examples or case studies document its accuracy, cost impact, and current limitations compared with prompt-only JSON generation?

Answer: According to Google Cloud Vertex AI documentation, structured outputs ensure that model responses always adhere to a specific schema, such as requiring a JSON object with fields like name, date, and participants. 

**Key features:**
- The documentation provides a code example that uses a schema to extract structured data from unstructured text ("Alice and Bob are going to a science fair on Friday").
- This method guarantees that the output matches the schema definition, which improves reliability for further processing.
- All open models on Vertex AI Model as a Service (MaaS), including Gemini, support this feature.
- The documentation does not include empirical benchmarks, published case studies, or direct comparisons to prompt-only JSON generation in terms of accuracy or cost. The focus is on ease of schema adherence.

-----

-----]]></source_block>
      </source_blocks>
    </query_group>
    <query_group>
      <query><![CDATA[In production Python systems, how are Pydantic models used to parse and validate LLM outputs, and what advantages over plain dictionaries, dataclasses, or TypedDicts have engineers reported?]]></query>
      <source_blocks>
        <source_block><![CDATA[### Source [34]: https://xebia.com/blog/enforce-and-validate-llm-output-with-pydantic/

Query: In production Python systems, how are Pydantic models used to parse and validate LLM outputs, and what advantages over plain dictionaries, dataclasses, or TypedDicts have engineers reported?

Answer: **Pydantic models are used in production Python systems to parse and validate Large Language Model (LLM) outputs by defining strict schemas for expected responses.** By creating Pydantic models with explicit field types and value constraints (such as using `Literal` for enumerated values), engineers can ensure that LLM-generated data conforms to the desired structure and content. For example, if an LLM is expected to return a "difficulty" field with only specific allowed values (“easy”, “medium”, “hard”), Pydantic will raise a validation error if the output contains an unexpected value. This approach provides **greater control and robustness** over the unpredictable outputs of LLMs, compared to using plain dictionaries or less strict typing mechanisms. The result is more reliable parsing, error handling, and the ability to build stronger, production-grade AI systems that can gracefully reject or handle malformed or unexpected responses from LLMs[1].

-----

-----]]></source_block>
        <source_block><![CDATA[### Source [35]: https://pydantic.dev/articles/llm-validation

Query: In production Python systems, how are Pydantic models used to parse and validate LLM outputs, and what advantages over plain dictionaries, dataclasses, or TypedDicts have engineers reported?

Answer: **Pydantic introduces advanced validation mechanisms that go beyond simple type checking, especially for LLM outputs.** One notable feature is the ability to create validators that utilize another LLM to enforce complex or context-dependent rules that are difficult to express in code (e.g., “don’t say objectionable things”). By integrating these validators, developers can automatically flag or reject outputs that violate nuanced or ethical guidelines. When a Pydantic model with such validation is used to parse LLM responses, any violation of the rule leads to a structured validation error, including detailed information about the failure. This approach enables **context-aware, customizable validation** that isn't possible with plain dictionaries, dataclasses, or TypedDicts, which lack both runtime validation and the ability to express dynamic or semantic rules at parse time[2].

-----

-----]]></source_block>
        <source_block><![CDATA[### Source [36]: https://www.leocon.dev/blog/2024/11/from-chaos-to-control-mastering-llm-outputs-with-langchain-and-pydantic/

Query: In production Python systems, how are Pydantic models used to parse and validate LLM outputs, and what advantages over plain dictionaries, dataclasses, or TypedDicts have engineers reported?

Answer: **Combining Pydantic with frameworks like LangChain allows developers to bring type safety and automatic validation to LLM outputs in production systems.** Pydantic models transform unpredictable, free-form responses from language models into **strongly-typed, validated data structures** that can be reliably used by downstream application logic. This ensures that fields are present, types are correct, and the data matches application requirements—solving the problem of LLM responses that might otherwise break code or introduce subtle bugs. Compared to dictionaries, dataclasses, or TypedDicts, Pydantic provides **runtime validation, error reporting, and easy integration with type- and schema-driven workflows**, making it a superior choice for robust LLM integration in Python applications[3].

-----

-----]]></source_block>
        <source_block><![CDATA[### Source [37]: https://www.youtube.com/watch?v=gjxZ4AGRMLk

Query: In production Python systems, how are Pydantic models used to parse and validate LLM outputs, and what advantages over plain dictionaries, dataclasses, or TypedDicts have engineers reported?

Answer: **Pydantic is described as a core data validation and settings management library in Python, especially relevant for AI and LLM use cases.** It leverages Python type annotations to validate and parse input data efficiently, ensuring that LLM outputs meet the expected schema. Pydantic models make data passing and validation simpler, more robust, and more efficient, which is crucial for production systems that require strict guarantees on input structure and content. This contrasts with plain dictionaries, dataclasses, or TypedDicts, which do not provide automatic runtime validation or detailed error reporting, making them less suitable for enforcing data integrity in AI-driven applications[4].

-----

-----]]></source_block>
        <source_block><![CDATA[### Source [38]: https://pydantic.dev/articles/llm-intro

Query: In production Python systems, how are Pydantic models used to parse and validate LLM outputs, and what advantages over plain dictionaries, dataclasses, or TypedDicts have engineers reported?

Answer: **Pydantic's use in validating structured outputs from LLMs is highlighted as a way to write more reliable code.** By defining schemas for LLM outputs using Pydantic models, developers can ensure that only responses matching the specified structure are accepted, reducing the risk of errors from malformed or unexpected data. This approach enables developers to "steer" LLMs towards producing outputs that are not only syntactically correct but also semantically valid for the application context—a process that is much harder, if not impossible, with plain dictionaries, dataclasses, or TypedDicts, which lack runtime schema enforcement and comprehensive error handling[5].
-----

-----]]></source_block>
      </source_blocks>
    </query_group>
    <query_group>
      <query><![CDATA[What benefits do software engineers and researchers cite for using structured outputs (JSON, XML, YAML) over free-text when integrating LLMs into production systems, especially regarding parsing ease, observability, and debugging?]]></query>
      <source_blocks>
        <source_block><![CDATA[### Source [43]: https://betterprogramming.pub/yaml-vs-json-which-is-more-efficient-for-language-models-5bc11dd0f6df

Query: What benefits do software engineers and researchers cite for using structured outputs (JSON, XML, YAML) over free-text when integrating LLMs into production systems, especially regarding parsing ease, observability, and debugging?

Answer: This source highlights several practical advantages of using structured outputs like JSON and YAML—as opposed to free-text—when integrating large language models (LLMs) into production systems. The key benefit is **parsing ease**: JSON is generally faster to parse and consume than YAML, which makes it well-suited for scenarios where speed and machine-readability are critical, such as real-time APIs or data exchange between systems. This efficiency stems from JSON's strict, unambiguous syntax, which allows for straightforward integration into most programming environments.

Additionally, the source notes that **YAML can be more cost/time-efficient for language models** to generate, especially when the goal is to produce precisely the same content repeatedly. In practice, it might be more efficient to request YAML from the LLM and then convert it to JSON on the code side, rather than requesting JSON directly. YAML also supports **comments**, which can be valuable for observability and debugging, as engineers and researchers can annotate the output with explanations, reasoning chains, or metadata that would be lost in JSON. This feature is particularly useful in complex workflows where understanding the model's reasoning process (e.g., via Chain-of-Thought prompting) is as important as the final answer.

The source also touches on a potential drawback: JSON's strictness can sometimes lead to type ambiguities (e.g., numbers printed as strings), but this can be mitigated with schemas or post-parsing type conversion. Overall, the use of structured formats like JSON and YAML enhances parsing reliability, simplifies integration, and—in the case of YAML—supports richer metadata for debugging and observability.

-----]]></source_block>
        <source_block><![CDATA[### Source [44]: https://celerdata.com/glossary/yaml-json-and-xml-a-practical-guide-to-choosing-the-right-format

Query: What benefits do software engineers and researchers cite for using structured outputs (JSON, XML, YAML) over free-text when integrating LLMs into production systems, especially regarding parsing ease, observability, and debugging?

Answer: This guide systematically compares JSON, YAML, and XML, emphasizing how each format impacts production system integration, especially concerning parsing, observability, and debugging. **Parsing ease** is a major differentiator: JSON is highlighted as the fastest to parse and generate, making it ideal for real-time APIs and applications where performance and bandwidth are critical. Its compact, consistent syntax ensures that data can be efficiently streamed and processed, with widespread native support across programming languages.

**YAML**, while slower to parse due to its flexible and sometimes ambiguous syntax, excels in **human readability**. Its indentation-based structure and support for inline comments make it especially suitable for configuration files that are edited directly by humans. This readability aids in **observability and debugging**, as engineers can quickly inspect, annotate, and modify YAML files without specialized tooling. However, the added complexity of YAML parsing can introduce overhead in machine-heavy workflows.

**XML** is described as the most verbose and resource-intensive to parse, especially with namespaces and schemas, but its explicit tagging can improve machine clarity at the expense of human readability. The guide concludes that the choice among these formats should consider the balance between machine efficiency (JSON), human clarity (YAML), and legacy system requirements (XML). For LLM integration, structured outputs like JSON and YAML provide clear benefits in parsing reliability, system maintainability, and ease of extension—each excelling in different aspects of the production lifecycle.

-----]]></source_block>
        <source_block><![CDATA[### Source [45]: https://www.snaplogic.com/blog/json-vs-yaml-whats-the-difference-and-which-one-is-right-for-your-enterprise

Query: What benefits do software engineers and researchers cite for using structured outputs (JSON, XML, YAML) over free-text when integrating LLMs into production systems, especially regarding parsing ease, observability, and debugging?

Answer: This source underscores that **JSON and YAML both offer significant advantages over free-text** when integrating LLMs into production. **JSON** is praised for its simplicity, security, and ease of data interchange, especially in JavaScript-centric environments. Its syntactical consistency and compactness make it easy for developers to scan, parse, and integrate, which streamlines observability and debugging in complex systems. JSON's ubiquity in APIs ensures broad compatibility and reduces integration friction.

**YAML** is described as the most human-readable serialization format, supporting comments, diverse data types, and complex data structures—features not available in JSON. The ability to include comments directly in the output is a major boon for **debugging and observability**, as it allows engineers to embed reasoning, notes, or metadata alongside the structured data. YAML's natural language-like syntax also lowers the barrier to entry for teams less familiar with programming, easing maintenance and collaboration.

The source notes that YAML's flexibility comes at the cost of slower parsing, but its versatility makes it ideal for configuration-heavy or multi-data-type scenarios common in LLM integration. Ultimately, both JSON and YAML provide robust, machine-readable outputs that are easier to parse, monitor, and debug than free-text, with the choice depending on the specific needs of the production environment.

-----]]></source_block>
        <source_block><![CDATA[### Source [46]: https://arxiv.org/html/2408.02442v1

Query: What benefits do software engineers and researchers cite for using structured outputs (JSON, XML, YAML) over free-text when integrating LLMs into production systems, especially regarding parsing ease, observability, and debugging?

Answer: This academic study investigates how **format restrictions**—such as requiring JSON or other structured outputs—impact LLM performance, particularly in reasoning and knowledge tasks. The research finds that **structured output formats can influence model behavior**: for example, JSON-mode responses from GPT-3.5 Turbo consistently placed the "answer" key before the "reason" key, effectively bypassing chain-of-thought reasoning in favor of direct answers. This suggests that the choice and design of the output format can affect not just parsing and observability, but also the cognitive processes of the model itself.

The study also compares natural-language-to-format conversion with unrestricted natural language responses. While performance is generally similar, the conversion process can introduce occasional generation errors, slightly lowering reliability for some models. However, the main takeaway is that **structured outputs improve parsing reliability and system observability** by enforcing a consistent schema, but they may also constrain the model's reasoning if the format is too rigid.

In summary, structured outputs like JSON and YAML are favored in production for their parsing reliability, ease of integration, and enhanced observability, but their design must be carefully considered to avoid unintended impacts on model reasoning and output quality.

-----]]></source_block>
      </source_blocks>
    </query_group>
    <query_group>
      <query><![CDATA[What prompt-engineering patterns and open-source tools (e.g., Guardrails, Jsonformer, Outlines) are recommended for forcing LLMs to emit strictly valid JSON without native schema enforcement?]]></query>
      <source_blocks>
        <source_block><![CDATA[### Source [47]: https://www.promptfoo.dev/docs/guides/evaluate-json/

Query: What prompt-engineering patterns and open-source tools (e.g., Guardrails, Jsonformer, Outlines) are recommended for forcing LLMs to emit strictly valid JSON without native schema enforcement?

Answer: The documentation explains techniques for ensuring valid JSON output from LLMs:

- The **`is-json` assertion** verifies that a language model's output is a valid JSON string. Optionally, it can validate the output against a provided JSON schema.
- To enforce structure, you can define a schema specifying required fields and their types. For example, you can require a field `color` (string) and `countries` (array of strings).
- For more granular checks on specific fields of the output, the **`javascript` assertion type** allows custom JavaScript logic to validate JSON content, such as asserting a field equals a specific value or contains certain items.
- These assertions can be combined in testing frameworks to repeatedly check outputs for validity and adherence to schema, thus "guardrailing" the LLM’s output even without native enforcement.

This approach is recommended for reliably evaluating and enforcing JSON structure when using LLMs that do not natively support schema enforcement.

-----

-----]]></source_block>
        <source_block><![CDATA[### Source [48]: https://python.plainenglish.io/generating-perfectly-structured-json-using-llms-all-the-time-13b7eb504240

Query: What prompt-engineering patterns and open-source tools (e.g., Guardrails, Jsonformer, Outlines) are recommended for forcing LLMs to emit strictly valid JSON without native schema enforcement?

Answer: The guide outlines a process for obtaining perfectly structured JSON from LLMs:

- **Define the desired JSON structure** and craft a JSON template to guide the LLM's output.
- Use **Pydantic**, a Python library, to represent the JSON schema with a class and attach validation rules.
- **Extract the generated JSON** from the LLM’s output using the defined template.
- Implement an **iterative feedback loop**: validate the output with Pydantic, and if errors are found, feed them back to the LLM to prompt a correction.
- This iterative process, leveraging LLM flexibility and Pydantic’s strict validation, helps ensure the final output matches your schema requirements, even when the LLM itself does not natively enforce schemas.

This method is particularly effective for Python environments and is recommended for robust validation of LLM-generated JSON.

-----

-----]]></source_block>
        <source_block><![CDATA[### Source [49]: https://modelmetry.com/blog/how-to-ensure-llm-output-adheres-to-a-json-schema

Query: What prompt-engineering patterns and open-source tools (e.g., Guardrails, Jsonformer, Outlines) are recommended for forcing LLMs to emit strictly valid JSON without native schema enforcement?

Answer: This article discusses strategies for ensuring LLM outputs adhere strictly to JSON schemas:

- Early methods relied on prompt engineering, asking the LLM to "output JSON," but this is unreliable for strict schema adherence.
- There is a distinction between **valid JSON** (correct syntax) and **JSON Schema adherence** (correct structure, types, required fields).
- Some LLM providers offer “JSON mode” to encourage valid syntax, but true schema enforcement requires more.
- **Advanced approaches** use dedicated LLM parameters to specify the JSON schema, letting developers define expected structure exactly.
- As a fallback, **tool/function calling** can be used: LLMs are instructed to call functions with arguments that match a schema, allowing for structured data exchange and precise parsing.
- These combined prompt and tool-based strategies help enforce strict JSON output without native enforcement.

-----

-----]]></source_block>
        <source_block><![CDATA[### Source [50]: https://latitude-blog.ghost.io/blog/how-json-schema-works-for-llm-data/

Query: What prompt-engineering patterns and open-source tools (e.g., Guardrails, Jsonformer, Outlines) are recommended for forcing LLMs to emit strictly valid JSON without native schema enforcement?

Answer: The article emphasizes the importance of schema validation tools for LLM workflows:

- Tools such as **Zod** (TypeScript), **Pydantic** (Python), and **Ajv** (JavaScript) are highlighted for runtime type checking and model-based validation.
- Schema validation should occur at several stages: input, response generation, output formatting, and storage.
- For high-throughput use cases, the schema should be pre-compiled and cached for performance. Errors should be handled gracefully with fallback responses, ensuring reliability.
- The Latitude platform extends these principles with open-source tools, shared workspaces, and development resources (guides, community support, templates) to facilitate robust JSON schema management for LLM data.

These practices and tools support reliable enforcement of structured JSON outputs from LLMs, particularly in collaborative and production environments.

-----

-----]]></source_block>
      </source_blocks>
    </query_group>
    <query_group>
      <query><![CDATA[Where do official docs or tutorials demonstrate converting LLM responses directly into Pydantic models, highlighting runtime validation, error handling, and schema generation?]]></query>
      <source_blocks>
        <source_block><![CDATA[### Source [51]: https://python.useinstructor.com/concepts/models/

Query: Where do official docs or tutorials demonstrate converting LLM responses directly into Pydantic models, highlighting runtime validation, error handling, and schema generation?

Answer: The official Instructor documentation demonstrates how to use **Pydantic models** to define and manage LLM output schemas, highlighting several key points:

- **Schema Definition:** You define output schemas by subclassing `pydantic.BaseModel`. Each field uses type annotations and descriptions, which serve both as validation rules and as part of the prompt for the LLM.
- **Prompt Generation:** Field descriptions, docstrings, and annotations are incorporated into the prompt, guiding the LLM to produce output matching the schema.
- **Runtime Validation:** When you specify a Pydantic model as the `response_model` in the client’s `create` call, Instructor will:
  - Use the model schema to guide the LLM’s output
  - Validate the LLM’s response at runtime against the schema
  - Return a Pydantic model instance, automatically raising errors if the response does not conform.
- **Error Handling:** If the LLM response fails validation, Pydantic’s standard error handling applies, surfacing clear exceptions describing the schema violations.
- **Schema Generation:** All type information and field metadata are used both for validation and to generate the expected schema for prompting and documentation.

Example usage:
```python
from pydantic import BaseModel, Field

class User(BaseModel):
    """
    Instructions for response generation.
    """
    name: str = Field(description="The name of the user.")
    age: int = Field(description="The age of the user.")
```
This model can be directly referenced as a `response_model` in LLM calls, ensuring the output matches the specified structure and is validated at runtime[1].

-----

-----]]></source_block>
        <source_block><![CDATA[### Source [52]: https://dev.to/gaw/from-chaos-to-order-structured-json-with-pydantic-and-instructor-in-llms-m5o

Query: Where do official docs or tutorials demonstrate converting LLM responses directly into Pydantic models, highlighting runtime validation, error handling, and schema generation?

Answer: This tutorial explains how to use **Pydantic and Instructor** for obtaining structured, validated responses from LLMs:

- **Schema as Model:** The LLM function receives a schema defined as a Python Pydantic model, and returns a model instance after validation.
- **Runtime Validation:** Once the LLM generates a response, it is parsed and validated using Pydantic, ensuring types and constraints are enforced.
- **Error Handling:** Any validation errors encountered are surfaced as Pydantic errors, making it clear where the response diverges from the expected schema.
- **Structured Output:** This approach guarantees that the output from the LLM is not just JSON, but a strongly-typed Python object adhering to the schema.
- **Instructor Integration:** The Instructor library is used to patch the OpenAI client, enabling seamless passing of Pydantic models as output schemas.

No explicit code for error handling and schema generation is shown, but the described workflow confirms that runtime validation and error reporting are handled by Pydantic within this setup[2].

-----

-----]]></source_block>
        <source_block><![CDATA[### Source [53]: https://www.leocon.dev/blog/2024/11/from-chaos-to-control-mastering-llm-outputs-with-langchain-and-pydantic/

Query: Where do official docs or tutorials demonstrate converting LLM responses directly into Pydantic models, highlighting runtime validation, error handling, and schema generation?

Answer: This LangChain-focused tutorial details how **Pydantic models** are used for schema generation, runtime validation, and error handling in LLM workflows:

- **Prompt Shaping:** Pydantic models define the structure of the desired output. LangChain converts these models into a detailed JSON schema.
- **Schema Generation:** The parser extracts field types, requirements, and descriptions from the Pydantic model and embeds the resulting schema in the prompt instructions sent to the LLM.
- **Output Parsing and Validation:** After the LLM returns a response, LangChain parses it and uses Pydantic to validate and instantiate the model. If validation fails, a `ValidationError` is raised, indicating which fields do not conform.
- **Error Handling:** Any mismatch between the LLM output and the schema results in explicit validation errors, making it easy to handle bad or incomplete data.
- **Automatic Documentation:** Field metadata and descriptions from the Pydantic model are preserved in the schema for both prompting and documentation purposes.

Example:
```python
class ProductReview(BaseModel):
    aspect: str
    analysis: str
```
This model is used to generate prompt instructions and to parse and validate the LLM’s structured output at runtime[3].

-----

-----]]></source_block>
        <source_block><![CDATA[### Source [54]: https://www.projectpro.io/article/pydantic-ai/1088

Query: Where do official docs or tutorials demonstrate converting LLM responses directly into Pydantic models, highlighting runtime validation, error handling, and schema generation?

Answer: This article shows how **Pydantic AI** can streamline data validation and serialization for LLM-driven agents:

- **Schema Definition:** Output models are defined using Pydantic, with fields annotated for type and validation constraints (e.g., `ge=0, le=10` for integers).
- **Runtime Validation:** When the agent returns a result, it is parsed into the Pydantic model, invoking all validation rules and raising errors for any mismatches.
- **Error Handling:** Any type or value errors during parsing result in standard Pydantic exceptions, which can be caught and handled as needed.
- **Schema Generation:** Field descriptions and metadata are included in the model, useful both for documentation and for guiding LLM output.

Example:
```python
class SupportResult(BaseModel):
    support_advice: str = Field(description="Advice returned to the customer")
    block_card: bool = Field(description="Whether to block the customer's card")
    risk: int = Field(description="Risk level", ge=0, le=10)
```
This structure is used to both generate the prompt and validate agent responses at runtime[4].

-----

-----]]></source_block>
        <source_block><![CDATA[### Source [55]: https://www.youtube.com/watch?v=QYW3ETY7UpA

Query: Where do official docs or tutorials demonstrate converting LLM responses directly into Pydantic models, highlighting runtime validation, error handling, and schema generation?

Answer: In this video tutorial, the workflow for converting LLM responses to **Pydantic models** is demonstrated:

- **LLM Call:** The OpenAI `chat.completion.create` method is invoked with a specified response model.
- **Schema Guidance:** The response model, defined as a Pydantic class, guides the LLM in structuring its output accordingly.
- **Runtime Validation:** The response from the LLM is parsed into the specified Pydantic model, enforcing type and field constraints.
- **Error Handling:** If the response fails validation, Pydantic’s error handling mechanisms provide clear feedback on what went wrong.

This video illustrates the end-to-end process of defining a Pydantic model, using it in an LLM call, and handling validation errors at runtime[5].

-----

-----]]></source_block>
      </source_blocks>
    </query_group>
    <query_group>
      <query><![CDATA[Have any benchmarks or case studies compared Google Gemini’s responseSchema structured-output feature with prompt-only JSON generation in terms of validity rate, token cost, or latency?]]></query>
      <source_blocks>
        <source_block><![CDATA[### Source [58]: https://ai.google.dev/gemini-api/docs/structured-output

Query: Have any benchmarks or case studies compared Google Gemini’s responseSchema structured-output feature with prompt-only JSON generation in terms of validity rate, token cost, or latency?

Answer: This official documentation from Google details two methods for generating JSON with Gemini: (1) configuring a **responseSchema** on the model (recommended), and (2) providing a schema in a text prompt. The documentation recommends configuring a responseSchema to constrain the output format to valid JSON, indicating that this approach is more robust for ensuring output validity.

The page provides code examples in Python and JavaScript that demonstrate how to set up responseSchema for structured outputs. However, **no benchmark data or comparative case studies** are presented regarding **validity rate**, **token cost**, or **latency** between responseSchema and prompt-only JSON generation. The focus is on implementation guidance rather than empirical evaluation[3].

-----

-----]]></source_block>
      </source_blocks>
    </query_group>
    <query_group>
      <query><![CDATA[Which empirical studies measure token counts, latency, or API-cost differences when requesting identical data from LLMs in JSON, YAML, and XML formats?]]></query>
      <source_blocks>
        <source_block><![CDATA[### Source [61]: https://betterprogramming.pub/yaml-vs-json-which-is-more-efficient-for-language-models-5bc11dd0f6df

Query: Which empirical studies measure token counts, latency, or API-cost differences when requesting identical data from LLMs in JSON, YAML, and XML formats?

Answer: This article documents an **empirical study** comparing **YAML and JSON** formats for language model prompts, with a specific focus on **token counts and API cost**. The author conducted experiments by asking GPT to generate the same structured data (a list of month names) in both JSON and YAML formats, then analyzed the results using OpenAI's Tokenizer tool. The findings demonstrate that **YAML leads to about a 50% reduction in token count and API cost compared to JSON** for the same content, making YAML significantly more cost/time-efficient. The article concludes that while JSON is generally faster for machines to parse, **YAML is more efficient for LLM interactions in terms of prompt cost and tokenization**. The author suggests it may be preferable to request YAML-formatted data from LLMs and convert it to JSON in post-processing. The study did not test XML, but does highlight practical efficiency differences between YAML and JSON for LLM applications.

-----

-----]]></source_block>
      </source_blocks>
    </query_group>
  </research_results>
  <sources_scraped>
    <scraped_source>
      <title><![CDATA[The Best Way to Generate Structured Output from LLMs]]></title>
      <content><![CDATA[# The Best Way to Generate Structured Output from LLMs

Benchmarking LLM structured output performance with OpenAI, Instructor, Marvin, BAML, TypeChat, LangChain, and how to overcome reasoning deficiencies using a multi-step Instill Core pipeline.

Industries are eagerly capitalizing on Large Language Models (LLMs) to unlock
the potential within their vast reserves of under-utilized unstructured data.
Given that up to
[80% of the worlds data is soon forecast to be unstructured](https://www.ibm.com/blog/managing-unstructured-data/),
the drive to harness this wealth for innovation and new product development is
immense. There is an ironic paradox here: LLMs, by their very design, output
_more_ unstructured text data to manage and keep on top of. That is, until very
recently!

Earlier this month, OpenAI announced that they now support
[**Structured Outputs in the API**](https://openai.com/index/introducing-structured-outputs-in-the-api/)
with general availability. The ability to distill and transform the creative and
diverse unstructured outputs of LLMs into actionable and reliable structured
data represents a huge milestone in the world of unstructured data ETL (Extract,
Transform and Load).

However, there’s more to this story than meets the eye.

## The Complexities of Structuring Outputs from LLMs

Coincidentally, the day before OpenAI’s announcement, a paper was published
titled
[Let Me Speak Freely? A Study on the Impact of Format Restrictions on Performance of Large Language Models](https://arxiv.org/abs/2408.02442v1),
which offers a compelling counterpoint. They demonstrate that LLMs **struggle**
**with reasoning tasks when they’re placed under format restrictions**.
Additionally, the _stricter_ these format restrictions are, the _more_ their
reasoning performance drops, revealing a complex interplay between structuring
outputs and model performance.

Besides the new structured outputs API from OpenAI, there is also a multitude of
existing LLM frameworks that have been developed to tease out structured outputs
from conventional LLMs. Under the hood, each of these tools works by using a
blend of the following techniques:

### 1. Prompt Engineering

**Structured Prompts:** Well-crafted prompts are used to instruct the LLM to
output data in a specific format, such as JSON. These prompts guide the model to
generate responses that match the expected structure.

**Example Guidance:** Prompts often include examples of the desired output
format to further guide the model.

### 2. Output Parsing

**Data Models:** The structured output is typically aligned with predefined data
models. These models serve as blueprints to ensure the output data conforms to
specific formats and structures.

**Validation:** After the LLM generates the output, it is validated against
these data models. If the output doesn’t match the expected structure, error
handling mechanisms may be triggered to correct or retry the process.

### 3. Error Handling

**Retries and Corrections:** If the output doesn’t meet the required format
(e.g., due to syntax errors or unexpected structures), adjustments may be made
to the prompt, or the output may be re-parsed to correct the issue.

**Strict Parsing:** The framework includes strict parsing tools that ensure the
output conforms exactly to the required schema, raising exceptions if the output
is malformed.

This raises the question: **How do these tools compare with OpenAI’s structured**
**outputs feature on a task that involves both reasoning and output format**
**restrictions?**

## Benchmarking Structured Output Tools with Reasoning Tasks

To better understand the complexities and nuances of generating structured
outputs from LLMs, we conducted an experiment using OpenAI’s latest GPT-4o
model. This experiment aimed to evaluate how OpenAI’s new structured outputs in
the API feature compared with various existing tools and frameworks on a complex
task that involved both reasoning and producing outputs that are structured
according to a predefined data model.

The full details, code, and results of this experiment can be found in the
accompanying notebook in the
[**Instill AI Cookbook**](https://github.com/instill-ai/cookbook). Launch it
with Google Colab using the button below:

[Open in Colab 🚀](https://colab.research.google.com/github/instill-ai/cookbook/blob/main/examples/Generating_structured_outputs.ipynb)

### The Task: Combining Reasoning and Structured Output

Our benchmark task was designed to challenge the LLM’s ability to reason while
adhering to a strict output structure. Inspired by the aforementioned
[paper](https://arxiv.org/abs/2408.02442v1), we created a task that involved two
main components:

1. **Reasoning Problem:** The LLM was asked to calculate a freelancer’s weekly
earnings based on varying hourly rates, including overtime pay, as follows:


```
John Doe is a freelance software engineer. He charges a base rate of $50 per
hour for the first 29 hours of work each week. For any additional hours, he
charges 1.7 times his base hourly rate. This week, John worked on a project
for 38 hours. How much will John Doe charge his client for the project this
week?
```

2. **Structured Output:** In addition to solving the reasoning problem, the LLM
was also asked to summarize information from a resume into a specific data
model, shown below:


```
class DataModel(BaseModel):
      name: str
      email: str
      cost: float  # Answer to the reasoning problem, stored as a float
      experience: list[str]
      skills: list[str]
```


The LLM was instructed to store the result of the reasoning problem in the
`cost` field of the data model, while the rest of the fields were to be
populated with data extracted and summarized from the following resume text:


```
John Doe
1234 Elm Street
Springfield, IL 62701
(123) 456-7890
Email: john.doe@gmail.com

Objective: To obtain a position as a software engineer.

Education:
Bachelor of Science in Computer Science
University of Illinois at Urbana-Champaign
May 2020 - May 2024

Experience:
Software Engineer Intern
Google
May 2022 - August 2022
- Worked on the Google Search team
- Developed new features for the search engine
- Wrote code in Python and C++

Software Engineer Intern
Facebook
May 2021 - August 2021
- Worked on the Facebook Messenger team
- Developed new features for the messenger app
- Wrote code in Python and Java
```

### Existing Tools for Structured Output Generation

We evaluated several existing libraries and frameworks designed to help generate
structured outputs from LLMs. The tools tested include:

- [**Instructor**](https://python.useinstructor.com/): A Python library built on
Pydantic that facilitates generating structured output from LLMs.
- [**Marvin**](https://www.askmarvin.ai/): A tool for building reliable natural
language interfaces.
- [**BAML**](https://www.boundaryml.com/): A domain-specific language for writing
and testing LLM functions.
- [**TypeChat**](https://microsoft.github.io/TypeChat/): A tool from Microsoft
for getting well-typed responses from language models.
- [**LangChain**](https://www.langchain.com/): A Python library that integrates
language models with data and APIs to build applications.

[Outlines](https://outlines-dev.github.io/outlines/),
[JSONformer](https://github.com/1rgs/jsonformer) and
[Guidance](https://github.com/guidance-ai/guidance/tree/main) were also
considered, however they were left out of this experiment as they had limited
support for remote API calls or failed when integrating with the latest OpenAI
API.

### Experiment Setup

The GPT-4o model from OpenAI was used by all tools. To ensure a reasonable
amount of variation, its temperature was set to 1.0, with the exception of
TypeChat which operates with a temperature of 0.0 and doesn’t readily expose
this parameter to the user.

Each tool processed 50 inference requests based on the same example task
described above. For more details on the experimental setup, please refer to the
accompanying notebook.

### Results

The performance of these tools was assessed based on three criteria:

1. **Correctness of the Structured Output:** Whether the output was correctly
formatted according to the data model.
2. **Accuracy of the Reasoning Task:** Whether the tool could correctly
calculate the `cost` based on the reasoning problem.
3. **Mean Absolute Error:** The average absolute difference between the
calculated `cost` and the ground truth value.

The results of the experiment are summarized in the table below:

| **Tool** | **Correct Output Structure** | **Correct Reasoning** | **Mean Absolute Error** |
| --- | --- | --- | --- |
| OpenAI (Text) | ❌ | ✅ | $0.00 |
| OpenAI (Structured) | ✅ | ❌ | $93.93 |
| Instructor | ✅ | ❌ | $109.61 |
| Marvin | ✅ | ❌ | $71.01 |
| BAML | ✅ | ❌ | $72.94 |
| TypeChat | ✅ | ❌ | $100.00 |
| LangChain | ✅ | ❌ | $97.23 |

_Please note that these were generated using the latest OpenAI GPT-4o model on_
_the 27th of August 2024. Future experiments will produce variations in these_
_results as the APIs and tools evolve._

## Key Insights

While OpenAI’s GPT-4o model was able to calculate the correct reasoning result
when generating unstructured text, it was unable to return the correct value
when the output was constrained to the structured data model. Additionally, none
of the existing tools were able to correctly solve the reasoning task while
adhering to the structured output format.

This aligns with findings from the [paper](https://arxiv.org/abs/2408.02442v1),
which highlighted how format restrictions can impair reasoning abilities. It
also shows that the challenges identified in the original study with
GPT-3.5-turbo persist: a) with OpenAI’s latest flagship model, and b) when this
model is used in tandem with an array of existing output structuring tools.

### Can Chain-of-Thought Reasoning Help?

In the Structured Outputs
[release post](https://openai.com/index/introducing-structured-outputs-in-the-api/),
OpenAI suggest that the quality of the final response can be improved using
chain-of-thought reasoning within the schema by defining a field called
`reasoning_steps`. This approach involves having the model outline its reasoning
process in this field before placing the final answer in the `cost` field.

Interestingly, our tests with this approach - implemented in the
[`structured-output-reasoning-cot`](https://instill.tech/george_strong/pipelines/structured-output-reasoning-cot/playground)
pipeline - showed that, even when the reasoning was correctly detailed by the
model in `reasoning_steps`, it was still unable to populate the `cost` field
accurately.

Despite advancements in model capabilities, combining complex reasoning with
structured output generation remains a significant challenge. A nice quote from
OpenAI’s
[release post](https://openai.com/index/introducing-structured-outputs-in-the-api/)
hints at this problem, and provides some additional guidance:

“ _Structured Outputs doesn’t prevent all kinds of model mistakes. For example,_
_the model may still make mistakes within the values of the JSON object (e.g.,_
_getting a step wrong in a mathematical equation). If developers find mistakes,_
_we recommend providing examples in the system instructions or splitting tasks_
_into simpler subtasks._”

This motivates our next question: **How can we reliably overcome these reasoning**
**and output structuring challenges?**

## The Solution: A Multi-Step Pipeline Built with **Instill Core**

Given the difficulties observed across all frameworks, a more robust approach is
needed. Instead of attempting to solve the reasoning and structuring tasks
simultaneously, we can use [Instill Core](https://www.instill-ai.dev/) to build a
multi-step pipeline that divides these into two distinct stages:

1. **Reasoning Step:** In the first step, the LLM is tasked solely with solving
the reasoning problem without any constraints on the output format. This
allows the model to leverage its full reasoning capabilities without being
hindered by strict formatting requirements.

2. **Structuring Step:** In the second step, the unstructured output from the
reasoning step is passed to a component that focuses exclusively on
structuring the data into the required format.


https://www.instill-ai.com/_vercel/image?url=%2Fblog%2Fllm-structured-outputs%2Fa-multi-step-pipeline.png&w=1920&q=100

F­i­g­u­r­e 1: A m­u­l­t­i-step p­i­p­e­l­i­n­e b­u­i­l­t with I­n­s­t­i­l­l Core.

This approach ensures that the system is able to adhere to the specified data
model, whilst also enabling the reasoning LLM to solve complex tasks,
unencumbered by format constraints. The full results of the benchmarking
experiment are shown in the Figure below:

https://www.instill-ai.com/_vercel/image?url=%2Fblog%2Fllm-structured-outputs%2Fbenchmark.png&w=1920&q=100

F­i­g­u­r­e 2: B­e­n­c­h­m­a­r­k­i­n­g e­x­p­e­r­i­m­e­n­t r­e­s­u­l­t­s c­o­m­p­a­r­i­n­g M­u­l­t­i-Step I­n­s­t­i­l­l Core,
O­p­e­n­A­I, I­n­s­t­r­u­c­t­o­r, M­a­r­v­i­n, BAML, T­y­p­e­C­h­a­t, and L­a­n­g­C­h­a­i­n on r­e­a­s­o­n­i­n­g and
s­t­r­u­c­t­u­r­e­d o­u­t­p­u­t g­e­n­e­r­a­t­i­o­n.

As can be seen, the multi-step pipeline was able to consistently **achieve both**
**correct reasoning and structured output where other techniques failed**.

Despite requiring two LLM inferences, it is worth noting that the cost of this
multi-step pipeline approach will likely still be less than many of the
structuring output tools that have been considered, as these often rely on
making repeat API calls with modified prompts until the output can be
successfully parsed into the required data model.

## Conclusion

The article underscores the complexity of generating structured outputs from
LLMs, particularly when reasoning tasks are involved. The new Structured Outputs
in the API feature from OpenAI is a significant advancement, but it’s not a
silver bullet. As our results show, even the most advanced models and techniques
can falter under strict output format constraints.

The multi-step approach built with Instill Core provides a practical solution to
these challenges. By isolating the reasoning process from the structuring
process, it allows LLMs to perform complex tasks without sacrificing accuracy or
output quality. For businesses and developers looking to harness the full
potential of LLMs in applications requiring structured data, this approach
offers a reliable and cost-effective path forward.]]></content>
    </scraped_source>
    <scraped_source>
      <title><![CDATA[Crafting JSON Outputs For Cntrolled Text Generation]]></title>
      <content><![CDATA[# Crafting JSON Outputs For Cntrolled Text Generation

This post explores four key approaches to generating valid JSON from LLMs—Langchain’s Pydantic parser, Jsonformer, Outlines, and Microsoft’s Guidance. It compares each method’s strengths, limitations, and use cases, offering a practical guide for reliable, structured LLM outputs.

Large Language Models (LLMs) have not only advanced the field of text generation but have also set higher standards for most NLP-related tasks. By being trained on huge volumes of textual data, these models possess the capability to comprehend and produce text that is often indistinguishable from human-written content. With their ability to generate coherent and contextually relevant responses, the first obvious application for these models is chatbots or creative writing assistants.

‍

## Beyond chatbots

Beyond chatbots, their capabilities have also been demonstrated across diverse NLP tasks, including language translation, text summarisation, question answering, and general content generation. For instance, in discriminative tasks such as document classification, sentiment analysis and named entity recognition, LLMs can be used employing [**in-context learning**](https://thegradient.pub/in-context-learning-in-context/) to get few-shot or even zero-shot classification. They are also widely used for data augmentation generating synthetic training data for traditional models \[ [**1**](https://www.semanticscholar.org/paper/Is-a-prompt-and-a-few-samples-all-you-need-Using-in-M%C3%B8ller-Dalsgaard/ca3037fed8ed14dea92985b9f288b05185f867d0)\], \[ [**2**](https://www.semanticscholar.org/paper/Do-Not-Have-Enough-Data-Deep-Learning-to-the-Anaby-Tavor-Carmeli/7eba731a7fd8de712b7b79b5af41a6e2d4dbd191))\], and \[ [**3**](https://www.semanticscholar.org/paper/Generating-Faithful-Synthetic-Data-with-Large-A-in-Veselovsky-Ribeiro/5af9cf0b695faf2eb94d74bf76dab1a311638ca3)\].

From chatbots that can engage in human-like conversations to language translation services that can provide real-time translations, LLMs have transformed the way we interact with software. However, they also present new challenges. Issues such as hallucination, fine-tuning limitations, context size, and memory add new layers of complexity for deterministic systems.

Although large language models excel at producing coherent responses, ensuring their outputs respect a specific format is not guaranteed. Consequently, this can present challenges when utilising the outputs of a language model as input for another system. Here we may want to get more structured information than just text back. To use them as software components we need a reliable interface to be able to connect with external tools. These integrations are possible with friendly serialisation formats like JSON or XML.

The scope of this blog post is to explore recent developments for controlled text generation, more specifically generating valid JSON outputs. Generating structured JSON from language models can be tricky and unreliable sometimes. The generated JSON must be syntactically correct, and it must conform to a schema that specifies the structure of the JSON. This allows developers to more reliably get structured data back from the model.

It is worth mentioning that Open AI has made the task easier by introducing the [**Function calling feature**](https://openai.com/blog/function-calling-and-other-api-updates) in June 2023. However, this feature is only suitable if interoperability with other models or providers is not a requirement. Our focus in this post will be on solutions that can be utilised with a broader range of generative models.

## Approach 1: Prompt engineering and output parsers

Output parsers process the language model response trying to fit it in the desired structure. We can see it as a postprocessing step. The one we will cover here is an [**implementation**](https://github.com/langchain-ai/langchain/blob/master/libs/langchain/langchain/output_parsers/pydantic.py) provided by [**Langchain**](https://python.langchain.com/docs) that relies on the well-known [**Pydantic models**](https://docs.pydantic.dev/latest/usage/models/) to define the data structure. For the Langchain library, there are two main components an output parser must implement:

- **Format instructions**: A method that returns a string containing instructions for how the output of a language model should be formatted.
- **Parser**: A method that takes in a string (assumed to be the response from a language model) and parses it into some structure.

#### **Pydantic Output Parser**

This [**output parser**](https://python.langchain.com/docs/modules/model_io/output_parsers/pydantic) allows users to specify an arbitrary JSON schema and query LLMs for JSON outputs that conform to that schema. You’ll have to use an LLM with sufficient capacity to generate well-formed JSON. In the OpenAI family, GPT-3 DaVinci might be enough and GPT3 Curie’s ability already drops off dramatically.

Here is a simple example defining the object _Joke_:

The output format instructions will look like this:

All we have to do is to concatenate the instructions string into the final prompt.

Instantiate the LLM model and query it with the prompt.

The output JSON string will look like this:

Using the `PydanticOutputParser` instance we defined, we convert the JSON string into the `Joke` model.

The output will be an instance of the `Joke`:

#### **Benefits**

1. Automatically generates the output instructions.
2. Parses the generated JSON string into the model.
3. Easy interoperability. It can be used on any capable large language model.

#### **Limitations**

1. Using prompt instructions will not guarantee the desired output.
2. Less capable models might have difficulty following the instructions. You will certainly need a loop that tries again until the model generates a suitable candidate that is accepted by the parser.
3. The more complex the schema, the more chances for the model to make mistakes.
4. Less robust. The model will generate token by token, increasing the chance of making syntax mistakes, for instance failing to generate a valid JSON string.
5. Context size. Complex schemas might require too many tokens in the context, limiting the context size you will have available for the actual prompt and in some cases the output.

## Approach 2: Jsonformer

Approaches based on output parsing are error-prone and less robust. They rely on prompt engineering, fine-tuning, and post-processing, but they still can fail to generate syntactically correct JSON in some cases. Especially smaller and less capable LLMs.

In structured data, many tokens are fixed and predictable. Ideally we wouldn’t need to generate tokens like brackets and field names that we already know. [**_Jsonformer_**](https://github.com/1rgs/jsonformer/) is a wrapper around Hugging Face transformers that fills in these fixed tokens during the generation process and only delegates the generation of content tokens, the actual field values to the language model. This makes it more efficient and robust than prompt engineering and output parsers.

The idea behind this method is to guide the text generation monitoring the logits and tokens sampling.

While sampling the tokens it will check first if the token follows the expected requirements. Example:

- Generating **booleans** : compares logits for **true** and **false**
- Generating **numbers**: squash logits for non-digit tokens before sampling
- Generating **strings**: stops generation on second **”**
- Generating **arrays**: compare logits for “[“, “,”, “]”

Here is an example on how to generate JSON string that follows the specified JSON schema:

#### **Benefits**

1. This method works even with small and less capable models. A 3B parameter model, small for LLMs standards, is already capable of benefiting from this approach.
2. It supports JSON schemas with nested objects.
3. Efficiency. By generating only the content tokens and filling in the fixed tokens, this method is more efficient than generating a full JSON string and parsing it.
4. Flexible and extendable. This library is built on top of the Hugging Face transformers library, making it compatible with any model that supports the Hugging Face interface.

#### **Limitations**

1. The official implementation only offers support for HuggingFace transformers, which is where the **_Jsonformer_** name comes from. In theory, it can be adapted to work with OpenAI API or other libraries as long as they provide the logits. Some contributors have already [**proposed solutions**](https://github.com/1rgs/jsonformer/pull/16) but since OpenAI released Function Calling this is less relevant.
2. It currently only supports a limited subset of JSON Schema types (number, boolean, string, array, object).
3. It looks like the project is not actively maintained at the moment.

## Approach 3: Outlines

‍ [**_Outlines_**](https://github.com/outlines-dev/outlines) is a framework that provides methods for defining top-level restrictions on text completions generated by language models. We can limit the output tokens in terms of type constraints, a predefined list of accepted tokens, and even regular expressions. This [**paper**](https://arxiv.org/abs/2307.09702) details how authors reformulate the problem of text generation in terms of transitions between the states of a finite-state machine.

**Efficient JSON generation**

The implementation provided by **_Outlines_** allows a guide the generation process so the output is “guaranteed” to follow a JSON schema or [**Pydantic model**](https://docs.pydantic.dev/latest/):

The output sequence will look like:

Now let's parse the JSON string:

Parsed object:

#### **Benefits**

1. The method works with union types, optional types, arrays, nested schemas, etc. Some field constraints are [**not supported yet, here**](https://github.com/outlines-dev/outlines/issues/215) you can find the details.
2. The approach is model agnostic as long as you can mask the logits. Open-source LLMs can be used.
3. Integration with HuggingFace transformers models.
4. Project in active development.

#### **Limitations**

1. Currently, it does not support APIs like OpenAI due to API limitation. More details [**here**](https://github.com/outlines-dev/outlines/issues/227).

## Approach 4: Guidance

Microsoft [**_Guidance_**](https://github.com/guidance-ai/guidance/tree/main) is another promising framework that provides a Domain Specific Language (DSL) for prompting. It merges templating and logic control making it possible to have more complex and clever prompts.

With the template language is possible to ensure that generated JSON is always valid. Below we generate a random character profile for a game with perfect syntax every time. A Jupyter Notebook with the complete example is available [**here**](https://github.com/guidance-ai/guidance/blob/main/notebooks/guaranteeing_valid_syntax.ipynb).

In this example, we specify not only the JSON format but the constraints for each attribute using the DSL. Note that ‘ **_age_**’ must respect the regex pattern ‘ **_[0–9]+_**’ and ‘ **_weapon_**’ must accept only values from the ‘ **_valid_weapons_**’ list.

The output as simple Python dictionary:

#### **Benefits**

1. It provides a powerful Domain Specific Language for prompting making it easy to build the template for a JSON response.

#### **Limitations**

1. It doesn’t work well with small LLMs. We need at least Llama-7b to make it work.
2. Since it does not work with smaller models, it is very memory intensive.

## Wrapping up

The conventional prompt engineering method, employing a JSON Schema Parser like Langchain’s Pydantic Output Parser, is a cheap method that can be used with any capable LLM. It won’t work well with small models and is not guarantee that it will work 100% of the times even with GPT-3. If you are planning to use only OpenAI API and don’t expect to move to alternative models later, just use [**Function Calling**](https://openai.com/blog/function-calling-and-other-api-updates).

Jsonformer can be more reliable than plain prompt engineering if you are using [**transformers**](https://huggingface.co/docs/transformers/index). This library can handle JSON schemas with limited complexity even with less capable models. The main benefit is that is does not generate the tokens from the structure, just the attribute values. The downside is that it supports only a subset of JSON Schema types and it is not actively maintained lately.

Guidance is a another promising LLM framework. The main contribution is a DSL for creating complex templates, that we can use to structure valid JSON responses. However, it requires larger models to function effectively and can be very memory intensive. I had trouble to make it work with smaller models. From my experience, you need at least a 7bilion-parameters models.

Outlines is the most complete solution, for instance you can constraint the token generation using regex patterns or predefined vocabulary. It can produce JSON respecting a Pydantic model with no issues, providing a more sophisticated approach to JSON generation than simply plain prompt engineering. In active development this library offers the most advanced solution not only for JSON responses but a variety of other guided text generation use cases. At the moment I write this **_Outlines_** is my favorite choice and Guidance is on my radar for other use cases.

Additionally, projects that are also relevant to mention in the domain are [**LMQL**](https://docs.lmql.ai/en/stable/) (Language Model Query Language) and [**guardrails**](https://github.com/ShreyaR/guardrails). Both with their own merits and limitations are worth checking.]]></content>
    </scraped_source>
    <scraped_source>
      <title><![CDATA[Let Me Speak Freely? A Study on the Impact of    Format Restrictions on Performance of Large Language Models]]></title>
      <content><![CDATA[# Let Me Speak Freely? A Study on the Impact of    Format Restrictions on Performance of Large Language Models

Zhi Rui Tam1,  
Cheng-Kuang Wu1,  
Yi-Lin Tsai1,  
Chieh-Yen Lin1,  
Hung-yi Lee2,  
Yun-Nung Chen2

1Appier AI Research  
2National Taiwan University

Equal contribution, Equal advisorship

###### Abstract

Structured generation, the process of producing content in standardized formats like JSON and XML, is widely utilized in real-world applications to extract key output information from large language models (LLMs).
This study investigates whether such constraints on generation space impact LLMs’ abilities, including reasoning and domain knowledge comprehension.
Specifically, we evaluate LLMs’ performance when restricted to adhere to structured formats versus generating free-form responses across various common tasks.
Surprisingly, we observe a significant decline in LLMs’ reasoning abilities under format restrictions.
Furthermore, we find that stricter format constraints generally lead to greater performance degradation in reasoning tasks.

## 1 Introduction

The few-shot in-context learning Brown et al. (2020) and instruction-following Wei et al. (2021) capabilities of large language models (LLMs) have enabled their out-of-the-box usage to solve downstream tasks.
However, a major obstacle to incorporating LLMs into industrial applications is their lack of adherence to standardized output formats.
This inconsistency complicates output parsing and undermines the reliability of these models.

One common approach to overcoming this obstacle is structured generation, which involves providing output in standardized formats like JSON or XML through format restrictions.
These restrictions can be implemented in various ways, such as instructing LLMs to adhere to specified formats with format-restricting instructions, or using industrial solutions like JSON mode OpenAI (2024); Gemini (2024), Instructor Liu (2024), or Guardrails PrefectHQ (2024).
These strategies simplify parsing workflows and streamline the integration of LLMs into real-world applications.

Due to the growing demand for structured generation, the research community has shown increased interest in investigating LLMs’ format-following abilities.
For example, IFEval Zhou et al. (2023), INFOBENCH Qin et al. (2024), and FOFO Xia et al. (2024) focus on evaluating LLMs’ instruction-following capabilities, including format adherence.
However, these studies do not address a critical question for industrial applications: Do format-restricting instructions affect the quality of LLMs’ generated content?
In other words, they fail to explore whether format restrictions degrade LLMs’ performance, which has great business impacts.
This performance degradation is shown in Figure 1.

In this work, we address the aforementioned research question through extensive empirical experiments.
We present a comprehensive analysis of the potential impacts of format-restricting instructions on LLMs’ performance across a wide range of tasks.
The formats studied include commonly used schemas such as JSON, XML, and YAML.
To the best of our knowledge, this is the first systematic investigation into the relationship between format-restricting instructions and the quality of generated content.
Our contributions are twofold:

- We observe declines in LLMs’ reasoning abilities under format restrictions, with stricter constraints generally leading to greater performance degradation in reasoning tasks.
- We offer insights into why performance degrades due to format constraints and propose simple approaches to mitigate these issues, thereby achieving both consistent formats and optimal performance.


## 2 Methodology for Structured Generation

To study different levels of format restrictions on downstream performance, we adopt the following three common methodologies in our experiments:

**Constrained Decoding (JSON-mode):**  
Constrained decoding is a technique that limits the output of LLMs by enforcing predefined token space during the generation process.
Among mainstream LLM providers, JSON mode is a widely implemented instance of this technique, especially due to its extensive use in industrial settings.
This mode, available as a hyperparameter flag in OpenAI and Gemini APIs, ensures the output is valid JSON.
It is assumed that the implementation is similar to the constrained decoding methods described by Willard and Louf (2023); Koo et al., (2024), and provided in Text-Generation-Inference.

**Format-Restricting Instructions (FRI):**  
They direct the LLM to generate responses in standardized formats such as JSON, XML, and YAML, adhering to specified schemas.
These instructions ensure that the generated output follows a structured format, facilitating the extraction and evaluation of the final answer.
This approach is more relaxed than constrained decoding, as it does not enforce a predefined token space.

**NL-to-Format:**  
This two-step process first instructs the LLM to answer the question in natural language, and then instructs it to convert its response into the target format schema.
As the most relaxed version of structured generation, this method decouples content generation from format adherence, aiming to maintain the performance of unrestricted natural language responses while still providing structured output.

## 3 Experiments

### 3.1 Datasets

We adopt datasets from various domains, categorized by the primary skills they assess:

#### 3.1.1 Reasoning Tasks

**GSM8K (Cobbe et al., 2021):**  
A collection of mathematical problems set in natural language contexts, reflecting daily life scenarios. This dataset challenges LLMs to generate necessary intermediate reasoning steps.

**Last Letter Concatenation (Wei et al., 2022):**  
This task requires LLMs to produce a string by concatenating the last letters of a sequence of words, testing their ability to perform symbolic reasoning.

**Shuffled Objects (Ghazal et al., 2013):**  
This evaluate set from BigBench evaluates the ability to infer the final state given an initial state and a sequence of shuffling events. We use the entire validation set in our experiments.

#### 3.1.2 Classification Tasks

**DDXPlus (Tchango et al., 2022):**  
A multiple-choice medical diagnosis dataset where LLMs must select the most appropriate diagnosis from 49 possible diseases based on a given patient profile. We use a subset provided by StreamBench (Wu et al., 2024) due to the extensive number of questions.

**MultiFin (Jørgensen et al., 2023):**  
A multi-choice financial dataset that requires classifying a given paragraph into one of five categories.

**Sports Understanding (Ghazal et al., 2013):**  
This task from BigBench tests LLMs’ ability to determine whether an artificially constructed sentence relating to sports is plausible or implausible.

**NI - Task 280 (Mishra et al., 2022):**  
A multiple-choice stereotype classification task based on a given paragraph. We included this task as it has been found to be sensitive to change in prompt formatting, with performance variations of up to 56% (Sclar et al., 2023).

### 3.2 Model

For all experiments, we compare gpt-3.5-turbo-0125 (OpenAI, 2023), claude-3-haiku-20240307 (Team, 2024a), gemini-1.5-flash (Team et al., 2023). For open weights model, we use LLaMA-3-8B-Instruct (Team, 2024b) and Gemma-2-9B-Instruct (Team et al., 2024) inference using Text-Generation-Server for its support in JSON mode.

### 3.3 Evaluation method

**Metrics.**  
To assess the performance of the models across the diverse range of tasks, we employ task-specific evaluation metrics. For the classification-based tasks (Sports Understanding, DDXPlus, Natural Instruction Task 280, and MultiFin), we use accuracy as the primary metric. For the Last Letter Concatenation and GSM8K, we utilize the exact match metric where the final answer must be the exact string match with the actual answer.

**Perfect Text Parser.**  
To disentangle format errors from the actual performance of the generated content, we use an LLM prompted to extract the final answer from the text, rather than relying on regex or string parsers.
This approach acts as a perfect parser, minimizing errors introduced when switching between different models.
Our ablation study, comparing different models, found that claude-3-haiku-20240307 is the most consistent when using gpt-4-turbo as a human reference, compared to four other low-cost APIs. Full results can be found in Appendix B.

**Consideration for Prompt Sensitivity.**  
Previous studies (Chen et al., 2023; Sclar et al., 2023; Zhu et al., 2023) have shown that LLMs are sensitive to slight variations in prompts.
To account for this, we evaluate our approach by nine prompt combinations: three task descriptions and three JSON, XML, and YAML schemas with slight variations in wording or format.
For natural language prompting, we include three variations in text formats (e.g., Give your reason first followed by your answers).
Details of the task description prompts and FRI prompts can be found in Appendix F.

## 4 Main Results

### 4.1 Impact of Format Restriction on Final Results

We investigate the effects of format restrictions on LLM performance by examining three progressively relaxed prompting approaches: JSON-mode, FRI, and NL-to-Format conversion.

We evaluate these approaches on datasets with exact match scores: GSM8K and Last Letter Concatenation presented in Figure 2. Surprisingly, JSON-mode performs significantly worse than FRI (JSON) on the Last Letter task. Upon inspection, we found that 100% of GPT 3.5 Turbo JSON-mode responses placed the "answer" key before the "reason" key, resulting in zero-shot direct answering instead of zero-shot chain-of-thought reasoning.

Comparing NL-to-Format with unrestricted Natural Language responses, we observe nearly identical performance across most models, as both derive answers from the same initial natural language response. However, NL-to-Format occasionally introduces generation errors, leading to slightly lower performance for LLaMA 3 8B Instruct, while other models maintain consistent scores across both settings.

These findings suggest that the degree and implementation of format restrictions can significantly impact LLM performance, particularly in reasoning tasks. The order of keys in structured outputs and the decoupling of reasoning from format adherence emerge as important factors in maintaining LLM capabilities while providing structured responses.

When evaluating classification datasets, we observe a different trend compared to reasoning tasks, as illustrated in Figure 3. Notably, in the DDXPlus dataset, Gemini 1.5 Flash demonstrates a significant performance boost when JSON-mode is enabled. Across other classification datasets, JSON-mode performs competitively, and in some cases, surpasses the other three methodologies.

We hypothesize that JSON-mode improves classification task performance by constraining possible answers resulted in reducing errors in answer selection. Conversely, natural language responses may introduce distractions, leading to parsing errors. These findings suggest format restrictions’ impact on LLM performance is task-dependent: stringent formats may hinder reasoning-intensive tasks but enhance accuracy in classification tasks requiring structured outputs.

## 5 Discussion

### 5.1 Impact on looser format restriction

To further investigate the effects of format restrictions, we examine a variation of the Soft Restrict setting where we remove the schema restriction from the prompt description. Instead of providing a specific schema (e.g., "Reply your answer in JSON format with the following schema: { "reason": …, "answer": … }"), we simply instruct the LLM to output in the target format language (e.g., "Reply your answer in JSON format.").
Table 1 illustrates the effects of removing the schema restriction on the GSM8K dataset. We observe significant improvements in average scores and lower standard deviations across different prompt perturbations for Claude 3 Haiku, GPT-3.5 Turbo, and LLaMA 3 8B Instruct. These results suggest that while structured outputs can be beneficial for downstream processing, overly restrictive schemas may hinder LLM performance, particularly in reasoning-intensive tasks.

This finding suggests that a balance must be struck between the desire for easily parseable, structured outputs and the need to preserve the LLM’s inherent reasoning abilities. Practitioners may want to consider using looser format restrictions when dealing with complex reasoning tasks, while still maintaining some level of structure to facilitate downstream processing.

| Model             | Text  | JSON  | XML   | YAML  |
|-------------------|-------|-------|-------|-------|
| gemini-1.5-flash  | 89.33 | 89.66 | 89.26 | 89.21 |
|                   | (0.8) | (0.3) | (0.3) | (0.4) |
| + schema constraint | -   | 89.21 | 88.20 | 87.42 |
|                   | -     | (1.5) | (2.2) | (3.7) |
| claude-3-haiku    | 86.51 | 86.99 | 86.96 | 82.89 |
|                   | (0.8) | (0.2) | (0.6) | (5.7) |
| + schema constraint | -   | 23.44 | 79.76 | 80.63 |
|                   | -     | (22.9)| (7.0) | (2.8) |
| gpt-3.5-turbo     | 75.99 | 74.70 | 60.45 | 71.58 |
|                   | (3.1) | (1.1) | (7.2) | (3.0) |
| + schema constraint | -   | 49.25 | 45.06 | 73.85 |
|                   | -     | (12.0)| (19.9)| (5.6) |
| LLaMA-3-8B        | 75.13 | 64.67 | 65.07 | 69.41 |
|                   | (0.9) | (2.23)| (0.56)| (0.95)|
| + schema constraint | -   | 48.90 | 56.74 | 46.08 |
|                   | -     | (6.7) | (8.3) | (16.8)|

*Table 1: Comparing results without and with schema constraint, adding schema not only increases the sensitivity to prompt but also degrades average performance.*

### 5.2 Comparison Across Different Formats

In this section we ablate the format language by comparing not just JSON but also XML and YAML format. Since all 3 language comes in different grammar syntax rules and restriction. We deduce each models might perform differently for example Claude-3-Haiku uses XML for tool use schema so

On hint sight we do not see any structure format which consistency stands out from others which generalized across all models in Figure 4. For Gemini model, we found JSON is more consistent however it does not always outperform other format.

In Table 8 we found in classification task JSON-mode performs much better than text due to the restriction on answer space. However in reasoning related task, JSON-mode failed to adhere to the order of reasoning first followed by answer causing a large drop in final performance.

### 5.3 Structure Format and Parsing Error Rates

We initially hypothesized that the performance gap between text and structured formats might be attributed to parsing errors during answer extraction. However, our analysis of error rates across different formats and models, as shown in Table 2, reveals that this is not the primary factor. In fact, Gemini 1.5 Flash and GPT 3.5 Turbo exhibit near zero parsing failures in all three formats. In the LLaMA 3 8B setting, the parsing error rate for the Last Letter task in JSON format is only 0.148%, yet there exists a substantial 38.15% performance gap as seen in Table 1.

This finding suggests that the performance differences between formats are not primarily due to parsing errors, but rather to the impact of format restrictions on the LLM’s reasoning and generation processes. However, we discovered that parsing errors, when present, can be effectively mitigated through a simple corrective step.

By prompting Claude-3-Haiku to reformat any output with parsing errors for both Claude 3 Haiku and LLaMA 3 8B (the two models with the highest percentage of parsing errors), we observed improved scores in JSON and YAML formats, as illustrated in Figure 5. This approach demonstrates the potential for enhancing the reliability of structured outputs without sacrificing the benefits of format-specific optimizations.

|               | Task       | Reasoning     | Classification      |       |
|---------------|------------|--------------|---------------------|-------|
| Model         | Format     | Last Letter  | GSM8K | DDXPlus | Sports | Task280 | MultiFin |       |
| Gemini-Flash  | JSON       | 0.0          | 0.03  | 0.37  | 0.0   | 0.0   | 0.0   |       |
|               | XML        | 0.0          | 0.19  | 1.26  | 0.0   | 0.22  | 0.0   |       |
|               | YAML       | 0.0          | 0.0   | 0.68  | 0.06  | 6.46  | 0.0   |       |
| Claude-3-Haiku| JSON       | 3.48         | 60.07 | 0.09  | 0.0   | 10.26 | 0.0   |       |
|               | XML        | 0.0          | 1.85  | 0.48  | 0.0   | 0.41  | 0.0   |       |
|               | YAML       | 0.0          | 0.0   | 86.66 | 1.02  | 0.13  | 0.0   |       |
| GPT-3.5-Turbo | JSON       | 0.0          | 0.13  | 0.0   | 0.0   | 0.0   | 0.0   |       |
|               | XML        | 0.0          | 0.24  | 0.35  | 0.0   | 0.0   | 0.0   |       |
|               | YAML       | 0.0          | 0.0   | 0.32  | 1.23  | 0.08  | 0.0   |       |
| LLaMA 3 8B    | JSON       | 0.15         | 22.75 | 1.63  | 0.28  | 1.61  | 0.0   |       |
|               | XML        | 17.93        | 7.62  | 32.45 | 6.54  | 22.04 | 5.78  |       |
|               | YAML       | 32.40        | 33.18 | 34.40 | 7.16  | 2.19  | 0.14  |       |

*Table 2: Parsing error percentage across different models*

## 6 Related Work

Our study can be summarized into two genres: reasoning ability of LLM and format following.

In study of LLMs reasoning ability, early work by Kojima et al. (2022) found using "Think step-by-step" can elicit reasoning ability without few shot examples. Subsequent study (Jin et al., 2024) shows that the number of reasoning steps correlates with the final accuracy. Recent work by Wang and Zhou (2024) found Chain-of-Thought (CoT) reasoning seed prompt Kojima et al. (2022) can be removed with a carefully crafted CoT decoding schema.

The exploration of LLMs’ ability to follow instructions and produce responses in specified formats was first addressed by IFEval Zhou et al. (2023) which designed to evaluate the general instruction-following ability of LLMs, and it contains a subset of test instances specifically assessing format-following. INFOBENCH Qin et al. (2024) introduces a broader coverage of instructions and conducts a more fine-grained analysis by decomposing the instructions into different categories, including format specifications. FOFO Xia et al. (2024) is a benchmark solely focused on the format-following ability of LLMs. However, these works do not explore if format instruction interfere with downstream performance.

## 7 Conclusion

Our study reveals that structured generation constraints significantly impact LLM performance across various tasks. Format restrictions, particularly constrained decoding (JSON-mode), can hinder reasoning abilities while enhance classification task accuracy. Looser format restrictions generally improve performance and reduce variance in reasoning tasks. Parsing errors, while not the primary cause of performance differences, can be mitigated through corrective prompting. These findings underscore the importance of balancing format adherence, reasoning capabilities, and cost efficiency in LLM applications. Given that our study focuses on reasoning-intensive tasks, future work should explore how reasoning tasks of varying difficulty, from intensive to simple, are affected by restrictive formats and LLMs. To mitigate the performance degradation of LLMs due to restrictive formats, future studies should include a wider range of training data that contains instructions in various restrictive formats in local LLMs.

## References

- Brown et al. (2020)  
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020.  
Language models are few-shot learners.  
_Advances in neural information processing systems_, 33:1877–1901.

- Chen et al. (2023)
Yulin Chen, Ning Ding, Xiaobin Wang, Shengding Hu, Haitao Zheng, Zhiyuan Liu, and Pengjun Xie. 2023.
Exploring lottery prompts for pre-trained language models.
In _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 15428–15444.

- Cobbe et al. (2021)
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021.
Training verifiers to solve math word problems.
_arXiv preprint arXiv:2110.14168_.

- Gemini (2024)
Google Gemini. 2024.
Generate json output with the gemini api.
[https://ai.google.dev/gemini-api/docs/json-mode?lang=python](https://ai.google.dev/gemini-api/docs/json-mode?lang=python).
Accessed on 2024-07-02.

- Ghazal et al. (2013)
Ahmad Ghazal, Tilmann Rabl, Minqing Hu, Francois Raab, Meikel Poess, Alain Crolotte, and Hans-Arno Jacobsen. 2013.
Bigbench: Towards an industry standard benchmark for big data analytics.
In _Proceedings of the 2013 ACM SIGMOD international conference on Management of data_, pages 1197–1208.

- Jin et al. (2024)
Mingyu Jin, Qinkai Yu, Haiyan Zhao, Wenyue Hua, Yanda Meng, Yongfeng Zhang, Mengnan Du, et al. 2024.
The impact of reasoning step length on large language models.
_arXiv preprint arXiv:2401.04925_.

- Jørgensen et al. (2023)
Rasmus Kær Jørgensen, Oliver Brandt, Mareike Hartmann, Xiang Dai, C. Igel, and Desmond Elliott. 2023.
Multifin: A dataset for multilingual financial nlp.
In _ACL Findings_.

- Kojima et al. (2022)
Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022.
Large language models are zero-shot reasoners.
In _Advances in Neural Information Processing Systems_.

- Koo et al. (2024)
Terry Koo, Frederick Liu, and Luheng He. 2024.
Automata-based constraints for language model decoding.
_arXiv e-prints_.

- Liu (2024)
Jason Liu. 2024.
[instructor](https://github.com/jxnl/instructor).

- Mishra et al. (2022)
Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. 2022.
Cross-task generalization via natural language crowdsourcing instructions.
In _ACL_.

- OpenAI (2023)
OpenAI. 2023.
Gpt-4 technical report.

- OpenAI (2024)
OpenAI. 2024.
Json mode.
[https://platform.openai.com/docs/guides/text-generation/json-mode](https://platform.openai.com/docs/guides/text-generation/json-mode).
Accessed on 2024-07-02.

- PrefectHQ (2024)
PrefectHQ. 2024.
[marvin](https://github.com/PrefectHQ/marvin).

- Qin et al. (2024)
Yiwei Qin, Kaiqiang Song, Yebowen Hu, Wenlin Yao, Sangwoo Cho, Xiaoyang Wang, Xuansheng Wu, Fei Liu, Pengfei Liu, and Dong Yu. 2024.
Infobench: Evaluating instruction following ability in large language models.
_arXiv preprint arXiv:2401.03601_.

- Sclar et al. (2023)
Melanie Sclar, Yejin Choi, Yulia Tsvetkov, and Alane Suhr. 2023.
Quantifying language models’ sensitivity to spurious features in prompt design or: How i learned to start worrying about prompt formatting.
In _The Twelfth International Conference on Learning Representations_.

- Tchango et al. (2022)
Arsène Fansi Tchango, Rishab Goel, Zhi Wen, Julien Martel, and Joumana Ghosn. 2022.
Ddxplus: a new dataset for automatic medical diagnosis.
In _Proceedings of the 36th International Conference on Neural Information Processing Systems_, pages 31306–31318.

- Team (2024a)
Anthropic Team. 2024a.
Introducing the next generation of claude.

- Team et al. (2023)
Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. 2023.
Gemini: a family of highly capable multimodal models.
_arXiv preprint arXiv:2312.11805_.

- Team et al. (2024)
Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, et al. 2024.
Gemma: Open models based on gemini research and technology.
_arXiv preprint arXiv:2403.08295_.

- Team (2024b)
Meta LLaMA Team. 2024b.
Introducing meta llama 3: The most capable openly available llm to date.

- Wang and Zhou (2024)
Xuezhi Wang and Denny Zhou. 2024.
Chain-of-thought reasoning without prompting.
_ArXiv_, abs/2402.10200.

- Wei et al. (2021)
Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. 2021.
Finetuned language models are zero-shot learners.
_arXiv preprint arXiv:2109.01652_.

- Wei et al. (2022)
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed H Chi, Quoc V Le, Denny Zhou, et al. 2022.
Chain-of-thought prompting elicits reasoning in large language models.
In _Advances in Neural Information Processing Systems_.

- Willard and Louf (2023)
Brandon T Willard and Rémi Louf. 2023.
Efficient guided generation for large language models.
_arXiv e-prints_, pages arXiv–2307.

- Wu et al. (2024)
Cheng-Kuang Wu, Zhi Rui Tam, Chieh-Yen Lin, Yun-Nung Chen, and Hung yi Lee. 2024.
Streambench: Towards benchmarking continuous improvement of language agents.

- Xia et al. (2024)
Congying Xia, Chen Xing, Jiangshu Du, Xinyi Yang, Yihao Feng, Ran Xu, Wenpeng Yin, and Caiming Xiong. 2024.
Fofo: A benchmark to evaluate llms’ format-following capability.
_arXiv preprint arXiv:2402.18667_.

- Zhou et al. (2023)
Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. 2023.
Instruction-following evaluation for large language models.
_arXiv preprint arXiv:2311.07911_.

- Zhu et al. (2023)
Kaijie Zhu, Jindong Wang, Jiaheng Zhou, Zichen Wang, Hao Chen, Yidong Wang, Linyi Yang, Wei Ye, Yue Zhang, Neil Zhenqiang Gong, et al. 2023.
Promptbench: Towards evaluating the robustness of large language models on adversarial prompts.
_arXiv preprint arXiv:2306.04528_.

## Appendix A Limitation

This study contains two primary limitations. First, due to cost constraints, we were unable to include results from more powerful language models such as LLaMA 70B or GPT-4o in our experiments. The inclusion of these models could potentially provide additional insights into how performance scales with model size and architecture. Second, our evaluation dataset, while diverse, is limited in scope. A broader range of tasks and domains could offer a more comprehensive assessment of the proposed approach’s effectiveness and generalizability.

## Appendix B Choosing which LLMs as answer extraction

To select the best and low cost answer LLM parser, we select 200 samples from six datasets response in natural language format which a total of 1,200 samples. We then use gpt-4-turbo as best LLM answer parser as our reference and calculate the kappa cohen score with 3 LLMs candidates: gemini-1.5-flash, claude-3-haiku-20240307 and llama-3-8b-instruct. Result shows claude-3-haiku-20240307 has the highest agreement with gpt-4-turbo at 0.86 followed by llama-3-8b-instruct.

## Appendix C Cost Comparison Across Different Formats

An important consideration in deploying LLM applications in industry settings is the associated token cost. We analyzed the input and output tokens across our experiments for all models and formats. For brevity, we present the averaged results from all six datasets in Table 3.
Our analysis reveals that text and YAML formats generally incur similar costs. Interestingly, we found that YAML is the most cost-effective format for LLaMA-3-8B, Gemini-1.5-Flash, and GPT-3.5-Turbo. Surprisingly, for Claude-3-Haiku, the lowest cost is associated with the text format, which is unexpected given the prevalence of XML examples in their documentation for tool use. The full cost breakdown for each dataset can be found in Table 4, providing a more detailed view for practitioners interested in fine-tuning their approach for specific use cases.

| Model           | text  | json  | xml   | yaml  |
|-----------------|-------|-------|-------|-------|
| LLaMA-3-8b      | 0.11  | 0.09  | 0.09  | 0.08  |
| Gemini-1.5-Flash| 0.20  | 0.21  | 0.21  | 0.19  |
| Claude-3-Haiku  | 0.20  | 0.30  | 0.30  | 0.29  |
| GPT-3.5-Turbo   | 0.35  | 0.23  | 0.24  | 0.23  |

*Table 3: Comparison of total costs (US dollar per 1000 entries) for different models and output formats. Numbers are averaged over all 6 datasets.*

## Appendix D Additional models

We also tested additional models from Mistral and OpenAI : Mistral-7b-v0.3, GPT-4o-mini-2024 on format prompt variation in GSM8K, Last Letter, Shuffled Object, Sports Understanding, MultiFin, NL Task 280 and DDXPlus.

## Appendix E Comparison between using regex and LLM as answer parser in GSM8K

To answer the difference between using regex parser to extract the final strict match answer, we calculate the Exact Match score in GSM8K results using the prompt format template "The final answer is". Table 5 results reveal a significant gap between regex match and LLM as final answer parser in EM score across various language models, highlighting the limitations of using only one strict regex matching for different models. For example, GPT-3.5-Turbo shows a 31.8 percentage point improvement from regex match (43.7%) to overall accuracy (75.5%), while Gemini-1.5-Flash exhibits an even larger 43.5 point difference. This pattern is consistent across all models, with mistral-7b demonstrating the most dramatic 42 point increase. These disparities underscore the value of using LLMs as answer parsers, as they can understand and evaluate responses beyond literal string matching, accounting for paraphrases and contextual understanding, thus providing a more nuanced and accurate assessment in text-based tasks.

| Model           | Regex Match | LLM Match |
|-----------------|-------------|-----------|
| GPT-3.5-Turbo   | 43.7        | 75.5      |
| Gemini-1.5-Flash| 25.8        | 69.3      |
| Claude-3-Haiku  | 67.4        | 85.8      |
| Gemma2-9b       | 82.5        | 86.0      |
| LLaMA-3-8b      | 46.9        | 55.7      |
| Mistral-7b-v0.3 | 10.4        | 52.4      |

*Table 5: Comparison of model performance on regex match "The final answer is (\\d+)" accuracy and using Claude-3-Haiku as answer parser.*

## Appendix F Prompt

### F.1 Prompt Format

For each task we fix the same template and only swapping the task description, format description, few shots example and question text.

Follow the instruction to complete the task:

{task_description}
Instruct: {format_description}
{few shots}
{question}

Task Description  
A task description describes the task and the final goal of the task.

Format Description  
A format description includes the target format (ie JSON, XML or YAML) and a targeted schema we intent the LLM response to adhere to.

For each description slot, we create 3 variations each which results in 9 prompt combinations. Each variation must retain the original meaning with slight change in wording, order of instruction. For each model we prompt all 9 prompts to calculate the sensitivity and variance of the final result.

If the current task requires reasoning, we include the zero shot chain-of-thought prompting : "Think step-by-step" in task description and ensures the LLM response to generate reasoning before giving the final answer.

(The remainder of the appendix continues with example prompt variations and format descriptions, which can be referenced in the original document.)]]></content>
    </scraped_source>
    <scraped_source>
      <title><![CDATA[Meta's Llama 3 70B, the first open-source [model](https://ai.meta.com/blog/meta-llama-3/) to match GPT-4's performance, is gaining traction among companies. The perks of being open-source —lower costs and increased customizability—are accelerating this shift, significantly transforming how businesses handle their generative AI strategies.]]></title>
      <content><![CDATA[Meta's Llama 3 70B, the first open-source [model](https://ai.meta.com/blog/meta-llama-3/) to match GPT-4's performance, is gaining traction among companies. The perks of being open-source —lower costs and increased customizability—are accelerating this shift, significantly transforming how businesses handle their generative AI strategies.

To better understand the model's capabilities, we gathered recent data from technical reports and independent reviews, and analyzed how it performs on different tasks.

Our findings show that Llama 3 70B can be up to **50 times cheape** r and **10 times faster** than GPT-4 when used through cloud API providers.

From our small scale evaluations, we learned that Llama 3 70B is good at grade school math, arithmetic reasoning and summarization capabilities. However, it performs poorly on middle school math, and verbal reasoning tasks.

Overall, GPT-4 performs better in reasoning and math tasks, but Llama 3 70B is a strong competitor. It delivers solid results across all tasks and offers additional benefits in terms of cost and flexibility. Basically, with Llama 3 70B you can get GPT-4 performance at GPT-3.5 cost.

In this article, we provide a thorough analysis that will equip you with all the necessary information to determine whether Llama 3 70B is a good alternative for certain aspects of your AI apps.

‍

# Approach

The main focus on this analysis is to compare two models: **GPT-4** ( `gpt-4-0613`) vs **and Llama 3 70B**. We look at standard benchmarks, community-run experiments, and conduct a set of our own small-scale experiments.

In the next two sections we cover:

- **Basic comparison** _(example: Cutoff date, Context Window)_
- **Cost comparison**
- **Performance comparison** _(L‍‍a‍‍t‍‍e‍‍n‍‍c‍‍y‍‍,‍‍ ‍‍T‍‍h‍‍r‍‍o‍‍u‍‍g‍‍h‍‍p‍‍u‍‍t‍‍)‍‍_ ‍‍
- **Standard benchmark comparison** _(example: what is the reported performance for math tasks between Llama 3 70B vs GPT-4?)_

Then, we run small experiments and compare the models on the following tasks:

- **Math riddles**
- **Document summarization**
- **Reasoning**

You can skip to the section that interests you most using the "Table of Contents" panel on the left or scroll down to explore the full comparison between GPT-4 and Llama 3 70B.

‍

# The Basics

When it comes to context window size, Llama 3 70B is still _behind_ the long context options that proprietary models offer. It currently comes with 8K context window length, compared to GPT-4 that comes with 128K context length.

Bub, because the model is open-source and all weights and code are available, Gradio, an AI company, was able to publish several models that extend the initial context window. These models let Llama work with up to `1 million tokens`, and they're performing really well on the OpenLLM [leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard). You can check them all [here](https://huggingface.co/gradientai) **.**

https://cdn.prod.website-files.com/63f416b32254e8679cd8af88/6707dda1fbd0daa69e64f3e1_663bf3331a0333ce99c21eb5_comparison-basic.png

GPT-4's knowledge is updated until April 2023, whereas Llama 3's extends to December 2023. Unlike Llama 3, GPT-4 supports function calling. Also, Llama 3 doesn't currently handle images, but Meta has hinted at upcoming features like longer context lengths and more capabilities soon.

Now let’s look at the cost comparison.

‍

# Cost Comparison

Since Llama 3-70B is open-sourced, you have many options to run it. If you're familiar with the process, you can run it locally, where you'll only pay for hardware and electricity. Alternatively, you can use a hosted version from various providers. Regardless of the option you pick, using **Llama will cost much less than GPT-4.**

GPT-4 is currently the most expensive model, charging $30 per million input tokens and $60 per million output tokens.

Looking at the table below, even if you use Llama-3-70B with Azure, the most expensive provider, the costs are much lower compared to GPT-4—about 8 times cheaper for input tokens and 5 times cheaper for output tokens (USD/1M Tokens). If you choose Groq, the cheapest provider, the savings are even greater: more than 50 times cheaper for input tokens and 76 times cheaper for output tokens (USD/1M Tokens).

https://cdn.prod.website-files.com/63f416b32254e8679cd8af88/6707dda1fbd0daa69e64f3e4_663cdc3da54f2cecacba449f_Input%2520and%2520Output%2520Cost%2520(USD_1M%2520Tokens%253B%2520Lower%2520is%2520better%2520(3).png)

Llama Cloud API providers cost comaprison. [Source](https://artificialanalysis.ai/models/llama-3-instruct-70b).

‍

# Perfromance Comparison

## **Latency Comparison**

Choosing any of the first five providers on the graph will offer faster latency with Llama 3 70B than GPT-4. Given that GPT's latest recorded latency is 0.54, Llama 3 70B seems to be a much better option in this regard.

https://cdn.prod.website-files.com/63f416b32254e8679cd8af88/6707dda1fbd0daa69e64f3de_663cdb117cbc504f15082b4c_Latency%2520(Seconds%2520to%2520First%2520Tokens%2520Chunk%2520Received%253B%2520Lower%2520is%2520better%2520(4).png)

Llama 7 30B Cloud API provider latency comparison. [Source](https://artificialanalysis.ai/models/llama-3-instruct-70b).

### **Throughput Comparison**

Llama, hosted on the Groq platform, provides the fastest throughput, generating 309 tokens / second, which is almost 9 times faster than GPT-4 (36 tokens / second). Fireworks and Together are great alternatives as well.

https://cdn.prod.website-files.com/63f416b32254e8679cd8af88/6707dda1fbd0daa69e64f3ea_663cda9f4914016efd2f7212_Throughput%2520(Output%2520Tokens%2520per%2520Second%253B%2520Higher%2520is%2520better%2520(4).png)

Llama 7 30B Cloud API provider throughput comparison. [Source](https://artificialanalysis.ai/models/llama-3-instruct-70b).

‍

# Benchmark Comparison

The first place to start learning about these models is the benchmark data reported in their technical reports or announcements. The graph below compares the performance of Llama 3 70B model against GPT-4 on standard benchmarks for various tasks.

If you want to gain a better understanding of these benchmarks, check our blog post: [LLM Benchmarks: Overview, Limits and Model Comparison](https://www.vellum.ai/blog/llm-benchmarks-overview-limits-and-model-comparison). We also encourage you to bookmark our [our leaderboard](https://www.vellum.ai/llm-leaderboard) as a tool for ongoing comparisons.

https://cdn.prod.website-files.com/63f416b32254e8679cd8af88/6707dda1fbd0daa69e64f3e7_663cdb72d40ff94944f3f8f6_Standard%2520Benchmark%2520Comparison%2520(2.png)

Standard benchmark comparison of Llama 3 70b vs GPT-4. [Source](https://www.vellum.ai/llm-leaderboard).

This graph reveals the following:

- Llama 3 70B demonstrates 15% higher performance in Python coding and slightly better results for Grade school math tasks than GPT-4.
- GPT-4 excels in all other categories, particularly achieving the highest scores in multi-choice questions and reasoning tasks.

Benchmarks matter, but they don’t tell the whole story. To really know how your AI system performs, you must experiment with your prompts, and evaluate every stage of your workflow.

But, one thing is clear: **We now have an open-source model competing with the world's top closed-source model.**

Now let's compare these models on 3 tasks.

‍

# Task 1: Math Riddles

In the previous section, we saw that Llama 3-70B is quite good at math tasks. Now, let's do a quick experiment to see if that's still the case.

We picked a set of seven math riddles designed for students not yet in middle school and seven more at the middle school level as the cornerstone of the test.

Here are a couple of example riddles [and their source](https://www.fatherly.com/entertainment/hard-math-riddles-for-kids-with-answers):

```
🧮 For the younger minds

If you have four apples and you take away three, how many do you have?" The intuitive answer is three, as those are the apples you took.

🧮 For the older middle school students

A cellphone and a phone case cost $110 in total. The cell phone costs $100 more than the phone case. How much was the cellphone?" **The correct answer, a bit more complex, is 105 (not 110!)

```

The prompt was the same for both models:

```
System message:

You are a helpful math assistant. You must return only a numeral in your response without any currency or other signs.

User message:

Solve this math riddle {{riddle}}

```

The image below is a screenshot of the [Vellum Prompt Sandbox](https://www.vellum.ai/products/prompt-engineering), where we initially experimented with our prompts and two basic examples.

https://cdn.prod.website-files.com/63f416b32254e8679cd8af88/6707dda2fbd0daa69e64f404_663bf940bf8dc639272664d8_llama-math-gpt4%2520(1.png)

Comparing Llama 3 70B vs GPT-4 on two basic math riddles in a Vellum Prompt [Sandbox](https://www.vellum.ai/products/prompt-engineering)

In the next image, we have the two models each attempting to answer a total of 14 math riddles, 7 grade school level, and 7 middle school level riddles.

We use [Vellum Evaluations](https://www.vellum.ai/products/evaluation) to compare the average performance across all test cases.

Looking at the results below, Llama 3 70B aced most grade school riddles but struggled with nearly all middle school ones, scoring only 50% correct across the board. In contrast, GPT-4 performed notably better.

https://cdn.prod.website-files.com/63f416b32254e8679cd8af88/6707dda2fbd0daa69e64f3fd_663bf9b71189ebb7cde996a0_math-eval-llama-gpt4%2520(1.png)

Evaluation table in Vellum comparing Llama 3 70B and GPT-4 on 14 math riddles.

**Key Takeaways:**

- If math is your main challenge, GPT-4 is a better overall option.
- You can use Llama 3 70B for grade school riddles, but it still struggles with middle school riddles.

# Task 2: Summarizations

We tasked both Llama and GPT-4 to summarize one of our [technical reports](https://www.vellum.ai/blog/best-at-text-classification-gemini-pro-gpt-4-or-claude2) where we compared models on a classification task. A good summary should be concise, integrate key data effectively, and conclude coherently.

We gave the models one simple instruction:

```
💬 Prompt

Please summarize the following document {{ document }}

```

Both summaries captured all three aspects really well. Here’s what Llama 3 generated:

```
The document compares the performance of four language models (Gemini Pro, GPT-4 Turbo, GPT-3.5 Turbo, and Claude 2.1) in classifying customer support tickets as resolved or not.
The experiment used 200 test cases and evaluated the models based on accuracy, recall, precision, and F1 score.
The results showed that Gemini Pro outperformed the other models, with an accuracy of 74% and an F1 score of 76.69%.
Claude 2.1 had a high recall rate, while GPT-4 Turbo had a high precision rate.
The experiment highlights the importance of prompt engineering and the need to consider the specific requirements of the task when choosing a model.

```

And here’s the slightly longer output by GPT-4:

```
The document details an experiment conducted to compare the performance of four models - GPT-3.5 Turbo, GPT-4 Turbo, Claude 2.1, and Gemini Pro - in classifying customer support tickets as resolved or unresolved.
The experiment was inspired by a customer, Pylon, who wanted to improve the accuracy of their existing classifier.
The results showed that Gemini Pro was the best performing model, with an accuracy of 74% and an F1 score of 76.69%.
Claude 2.1 had a high recall rate, while GPT-4 Turbo showed high precision.
The document also discusses the challenges of the task, such as language variability and edge cases, and the importance of customizing prompts and avoiding overfitting.
The authors suggest that the findings could be useful for all users of Language Learning Models (LLMs) and plan to further investigate the performance of the models and the potential benefits of fine-tuning a model using Pylon's classifier data.

```

###### 💡 Have in mind that summarization tasks are generally considered easy for most models nowadays, so Llama 3 70B should definitely be on your list for this task.

‍

# Task 3: Reasoning

From the standard benchmarks, we saw that Llama 3 70B still didn’t catch up to GPT-4 on reasoning tasks. So we’ll run a small test to see how they actually compare.

We picked a set of seven verbal reasoning questions and seven more arithmetic reasoning questions as the cornerstone of the test.

Here are a couple of example riddles and their [sources](https://testbook.com/reasoning/verbal-reasoning):

```
🗣️ Verbal reasoning question:

1. Choose the word that best completes the analogy: Feather is to Bird as Scale is to _______.

A) Reptile
B) Dog
C) Fish
D) Plant

Answer: Reptile

🧮 Arithmetic reasoning question:

3, 6, 11, 18, 27, ?, 51

Answer: 38

```

Below is a screenshot on the initial test we ran in our prompt [environment](https://www.vellum.ai/products/prompt-engineering) in Vellum:

https://cdn.prod.website-files.com/63f416b32254e8679cd8af88/6707dda2fbd0daa69e64f40a_663cc5235b2170f0d12274d5_reasoning-llama-gpt4%2520(1.png)

Comparing Llama 3 70b and GPT-4 on 2 basic reasoning tasks.

Now, let’s run the evaluation across all 14 reasoning questions.

In the image below, you can see that both models did poorly on almost all verbal reasoning questions. However, they performed similarly on the arithmetic reasoning questions, with GPT-4 having just one more correct answer than Llama 3 70B.

https://cdn.prod.website-files.com/63f416b32254e8679cd8af88/6707dda2fbd0daa69e64f3f4_663cc5453091c520ca517459_reasoning%2520(1.png)

Evaluation table in Vellum comparing Llama 3 70b and GPT-4 on 14 reasoning quesitons.

**Key takeaways:**

- Both Llama 3-70B and GPT-4 are good at arithmetic reasoning tasks, so given the cost you can easily use Llama 3-70B for this task.
- Both models aren’t good at verbal reasoning tasks. This performance can be greatly improved by incorporating [few-shot](https://www.vellum.ai/blog/zero-shot-vs-few-shot-prompting-a-guide-with-examples) or [CoT prompting](https://www.vellum.ai/blog/chain-of-thought-prompting-cot-everything-you-need-to-know) (Chain of Thought prompting).

**Other insights:**

- Reddit users [report](https://www.reddit.com/r/LocalLLaMA/comments/1ckkwlk/llama3_70b_can_perform_much_better_in_logical/) that LLaMA-3 70B can perform much better in logical reasoning with a task-specific system prompt;

# Safety and Privacy

Below we share insights on how OpenAI and Meta are making their models secure and safe to use.

## **GPT-4 Safety**

To enhance GPT-4's safety, OpenAI [worked](https://cdn.openai.com/papers/gpt-4-system-card.pdf) with experts to perform adversarial testing, improved data selection, and incorporated a safety reward signal during training to reduce harmful output. These measures significantly reduced the model's tendency to produce harmful content by 82% compared to GPT-3.5 and improved its compliance with safety policies in responding to sensitive requests by 29%.

## **Llama 3 70B Safety**

Meta says that it developed new data-filtering pipelines to boost the quality of its model training data. They've also invested in a suite of tools to help with safety and hallucinations.

\- [Llama Guard 2](https://github.com/meta-llama/PurpleLlama/tree/main/Llama-Guard2), LLM safeguard model that can classify text as "safe" or "unsafe”. It can be used for classifying content in both LLM inputs (prompt classification) and in LLM responses (response classification).

\- [Llama Code Shield](https://github.com/meta-llama/PurpleLlama/tree/main/CodeShield), that can classify if a code is “secure” or “insecure”. This is extremely important as developers start to utilize LLMs to assist them in writing code, there is a risk that insecure code is generated and included in production. Here’s a sample [workflow](https://github.com/meta-llama/llama-recipes/blob/main/recipes/responsible_ai/CodeShieldUsageDemo.ipynb) on how to use it.

- [CyberSec Eval 2](https://ai.meta.com/research/publications/cyberseceval-2-a-wide-ranging-cybersecurity-evaluation-suite-for-large-language-models/), that lets you evaluate how safe an LLM is (you can use this tool to evaluate any LLM).

‍

# Prompting Tips for Llama 3 70B

When it comes to prompting Llama 3 70B, the same prompts from GPT-4 should work well. Generally, this model doesn’t require over-engineered prompts, and is able to follow instructions better. Writing clear and concise prompts will enable the model to accurately follow your instructions.

Using advanced prompting techniques like [few-shot](https://www.vellum.ai/blog/zero-shot-vs-few-shot-prompting-a-guide-with-examples) and [chain of thought prompting](https://www.vellum.ai/blog/chain-of-thought-prompting-cot-everything-you-need-to-know) can greatly help with reasoning tasks.

Some people have found that Llama 3 70B is [extremely good](https://www.reddit.com/r/LocalLLaMA/comments/1canrjq/llama370b_is_insanely_good_at_following_format/) at following format instructions, and writes the output without adding boilerplate text.

‍

# Summary

In this article we looked at standard benchmarks, we ran small scale experiments and looked at independent evaluations. Below is the summary of our findings.

https://cdn.prod.website-files.com/63f416b32254e8679cd8af88/6707dda2fbd0daa69e64f407_663cd2705b2170f0d12fb22e_llama-gpt4-comparison.png

Summary of our findings and reports for Llama 3 70B vs GPT-4.

**Key Takeaways:**

- **Cost and Efficiency**: Llama 3 70B is a more cost-effective, for tasks that require high throughput and low latency.
- **Complex Tasks Handling**: GPT-4 remains more powerful for tasks requiring extensive context and complex reasoning.
- **General Usability**: Both models are good at document summarization.
- **Coding**: Judging from data reported from the model providers, Llama 3 70B is better at Python coding tasks than GPT-4, but we’ll need to do an independent coding eval to confirm this.
- **Context window:** New models have expanded Llama-3 8B's token capacity from 8K to up to 1 million tokens. This means the current context window won't be an issue for much longer.

If you want to compare these models using Vellum's prompt engineering sandbox, [book a demo](https://calendly.com/akash-vellum/vellum-demo-call) here.

# Conclusion

Meta’s Llama 3 models are showing the world that open-source models can reach the higher ranks of performance previously dominated by proprietary models.

As companies start to develop more complex AI workflows, developers will look for options that are cheaper, more flexible, and faster. The current cost and speed of GPT-4 might not make sense for much longer.

GPT-4 still has advantages in scenarios that need longer context or special features like image support and function calling. However, for many tasks, Llama 3 70B is catching up, and this is the worst that Llama 3 70B will ever gonna be.

The gap is closing.]]></content>
    </scraped_source>
    <scraped_source>
      <title><![CDATA[Response Model]]></title>
      <content><![CDATA[# Response Model

Defining LLM output schemas in Pydantic is done via `pydantic.BaseModel`. To learn more about models in Pydantic, check out their [documentation](https://docs.pydantic.dev/latest/concepts/models/).

After defining a Pydantic model, we can use it as the `response_model` in your client `create` calls to OpenAI or any other supported model. The job of the `response_model` parameter is to:

- Define the schema and prompts for the language model
- Validate the response from the API
- Return a Pydantic model instance.

## Prompting

When defining a response model, we can use docstrings and field annotations to define the prompt that will be used to generate the response.

```python
from pydantic import BaseModel, Field

class User(BaseModel):
    """
    This is the prompt that will be used to generate the response.
    Any instructions here will be passed to the language model.
    """

    name: str = Field(description="The name of the user.")
    age: int = Field(description="The age of the user.")

```

Here all docstrings, types, and field annotations will be used to generate the prompt. The prompt will be generated by the `create` method of the client and will be used to generate the response.

## Optional Values

If we use `Optional` and `default`, they will be considered not required when sent to the language model.

```python
from pydantic import BaseModel, Field
from typing import Optional

class User(BaseModel):
    name: str = Field(description="The name of the user.")
    age: int = Field(description="The age of the user.")
    email: Optional[str] = Field(description="The email of the user.", default=None)

```

Note that fields can also be omitted entirely from being sent to the language model by using Pydantic's `SkipJsonSchema` annotation. See [Fields](https://python.useinstructor.com/concepts/fields/#omitting-fields-from-schema-sent-to-the-language-model) for additional details.

## Dynamic model creation

There are some occasions where it is desirable to create a model using runtime information to specify the fields. For this, Pydantic provides the create\_model function to allow models to be created on the fly:

```python
from pydantic import BaseModel, create_model, Field

class FooModel(BaseModel):
    foo: str
    bar: int = 123

BarModel = create_model(
    'BarModel',
    apple=(str, 'russet'),
    banana=(str, 'yellow'),
    __base__=FooModel,
)
print(BarModel)
#> <class '__main__.BarModel'>
print(BarModel.model_fields.keys())
#> dict_keys(['foo', 'bar', 'apple', 'banana'])

```

When would I use this?

Consider a situation where the model is dynamically defined, based on some configuration or database. For example, we could have a database table that stores the properties of a model for some model name or id. We could then query the database for the properties of the model and use that to create the model.

```sql
SELECT property_name, property_type, description
FROM prompt
WHERE model_name = {model_name}

```

We can then use this information to create the model.

```python
from pydantic import BaseModel, create_model, Field
from typing import List

types = {
    'string': str,
    'integer': int,
    'boolean': bool,
    'number': float,
    'List[str]': List[str],
}

# Mocked cursor.fetchall()
cursor = [\
    ('name', 'string', 'The name of the user.'),\
    ('age', 'integer', 'The age of the user.'),\
    ('email', 'string', 'The email of the user.'),\
]

BarModel = create_model(
    'User',
    **{
        property_name: (types[property_type], Field(description=description))
        for property_name, property_type, description in cursor
    },
    __base__=BaseModel,
)

print(BarModel.model_json_schema())
"""
{
    'properties': {
        'name': {'description': 'The name of the user.', 'title': 'Name', 'type': 'string'},
        'age': {'description': 'The age of the user.', 'title': 'Age', 'type': 'integer'},
        'email': {
            'description': 'The email of the user.',
            'title': 'Email',
            'type': 'string',
        },
    },
    'title': 'User',
    'type': 'object',
}
"""

```

This would be useful when different users have different descriptions for the same model. We can use the same model but have different prompts for each user.

## Adding Behavior

We can add methods to our Pydantic models, just as any plain Python class. We might want to do this to add some custom logic to our models.

```python
from pydantic import BaseModel
from typing import Literal

from openai import OpenAI

import instructor

client = instructor.from_openai(OpenAI())

class SearchQuery(BaseModel):
    query: str
    query_type: Literal["web", "image", "video"]

    def execute(self):
        print(f"Searching for {self.query} of type {self.query_type}")
        #> Searching for cat of type image
        return "Results for cat"

query = client.chat.completions.create(
    model="gpt-3.5-turbo",
    messages=[{"role": "user", "content": "Search for a picture of a cat"}],
    response_model=SearchQuery,
)

results = query.execute()
print(results)
#> Results for cat

```

Now we can call `execute` on our model instance after extracting it from a language model. If you want to see more examples of this checkout our post on [RAG is more than embeddings](https://python.useinstructor.com/blog/2023/09/17/rag-is-more-than-just-embedding-search/)]]></content>
    </scraped_source>
  </sources_scraped>
  <code_sources>
    <message>No code sources found.</message>
  </code_sources>
  <youtube_video_transcripts>
    <youtube_transcript>
      <title><![CDATA[All right folks. In this video, what we're going to be doing is we're going to be discussing OpenAI function calling and structured prompting. Now, the idea here is that we want to first discuss what function calling is and how OpenAI implements it. And then discuss structured prompting using the Pydantic library as an example on how that can work in terms of writing code and what does it mean to structure the prompt, etc. Right?]]></title>
      <content><![CDATA[All right folks. In this video, what we're going to be doing is we're going to be discussing OpenAI function calling and structured prompting. Now, the idea here is that we want to first discuss what function calling is and how OpenAI implements it. And then discuss structured prompting using the Pydantic library as an example on how that can work in terms of writing code and what does it mean to structure the prompt, etc. Right?

[00:30] So. Um, what we're going to be talking about first is function calling. So, function calling means connecting large language models like ChatGPT to tools that perform actions in the outside world like checking the weather or searching the web. Right? And OpenAI function calling involves four simple steps. We call the model with the query and a set of functions that are defined in the functions parameter. So we explain to the model, these are the tools that are available to you.

[01:00] Then, step number two, the model can choose whether or not to call one or more functions that it has available to it. And the content of that call will be a stringified JSON object that adheres to a custom schema. All right? And actually this has now become a an industry standard. So, we then parse the string into JSON in the code and call the function with the provided arguments if they exist. And then we call the model again by appending the function response as a new message and let the model summarize the results back to the user. So essentially we say the model, look, these are the tools that you have available. We have a structured way to connect the model to those functions and to send inputs to those functions once it has identified through reading through processing the prompt that a function should be called to solve the problem.

[01:50] And then the model sends the correct arguments to that function, calls the function, the output which is called an observation is integrated into a response that gets summarized to the user. Right? So, we're going to be seeing an example here in Python code on how to do that. I'm going to initialize my OpenAI client. We're going to have, I'm going to have a very simple function that creates a directory in my current folder. And then I'm going to write it as in the JSON schema for the OpenAI function calling API.

[02:17] So, it will have a dictionary with the type, the function, the name of the function, the description, the parameters. Within the parameters, each the type of the parameter, the object, the properties of that function of the arguments, so this is directory, it is a type of string and describes what that uh parameter does. And we set up also a key called required which indicates which arguments are required in that particular function.

[02:46] And then we put this function definition for the OpenAI function calling API inside of a list, which is pretty cool. Now, what we have is this little function called run terminal task. We create a variable called messages and inside that variable we give it a list with the prompts to the model. In this case, we're just saying uh create a folder called lucas-loves-llms, which is, you know, why not?

[03:15] And then we set up the tools inside of a list. We call the model, we're calling the GPT 3.5 turbo 16k. We give the messages parameter that will contain this message with our prompt to the model. We set up the tools and we set up the tool choice to automatic, so that the model can automatically choose to call a function or not. Then we gather the response and we identify, we we check whether or not tool calls were made in that response.

[03:45] Right? And if they were made, what we do is we have a dictionary with the available functions that the model can use, and then we append that response to the messages object, to the messages list. We loop over the tool calls that were made. We gather the name, the function, the arguments of the function, and we call the tool, getting the function response, right? We append everything under the messages list and we call the model with all of that information to integrate and summarize the response which is then returned to the user, like we are doing right here.

[04:24] And we get the output. Right? So when I call we'll get uh chat completion object like this. And if we inspect the string that was returned here we say lucas loves llms has been created, the folder has been created. And if I check my current folders, we see that the folder was indeed created, which is awesome, right? Now.

[04:47] Folks, this is great. Function calling is amazing, right? Function calling introduces this idea of trying to add structure and determinism to the process of interacting with large language models, right? And in the theme of that structured interaction with large language models, a library that has been extremely popular in not only in Python but now in the large language model universe in terms of frameworks is a library called Pydantic.

[05:21] Now, what this, uh this is a data validation library in Python that allows you that allows us to do some pretty interesting stuff. So, essentially what it allows us to do is it allows us to give, uh, set up data structures that we can have. And when connected with the OpenAI function calling API, Pydantic allows us to define specifically what is the object we want returned when we prompt the model with something. So you will understand that in a second. So what we're going to be doing is, in addition to OpenAI and Pydantic, we're also going to use the instructor package for this demonstration.

[06:05] And we're going to set up two classes in Pydantic. We're going to set up a class called Question that contains three attributes: the question attribute that holds the quiz question, the options for that question, imagine multiple choice, and the correct answer as an integer that refers to the index of the correct answer in the options list. All right? So, after uh having done that, what we're going to do is we're going to have a second class called Quiz that contains the topic in question for this quiz that we want to create from a webpage article or paper, and a list of questions which in each of those elements inside of this list will have the an object of the Question type.

[07:07] So folks, this is a lot of information but what we're doing here is we're setting up data types, right? And we're constructing these customizable, these custom data types with Pydantic. And why this is so cool? Because it allows us to prompt a model like ChatGPT and ask for that model to create something structured out of a prompt that was made in natural language. So I can say, uh, so let's understand that by in practice. So, I'm going to set up my client and now to interact with this, uh, and connect OpenAI function calling and the Pydantic API, we're going to be using the instructor package.

[07:48] So I'm going to set up the client with the instructor.from_openai method and then I'm going to give the OpenAI client to that method. And then I'm going to define a function called generate a quiz which calls the ChatGPT API with the chat.completions.create method. It sets up the model as GPT4 turbo. Oh, sorry, let's go back there. And it sets up the model GPT4 turbo and then it sets up the messages list and in that messages list, we feed it a dictionary containing the system message in which we say you're a quiz generation engine specialized in fostering understanding in students given a source of content to be studied.

[08:31] You will be fed content like articles or scientific papers and you will output quizzes aimed at eliciting full understanding of the material. Right? Pretty cool system message. And then we're going to give the prompt to the model. And the prompt is going to contain a prompt, right? Just like we've talked about in the in the initial lesson for this um live training for this uh video course about prompt engineering. But it will also contain the contents of the article or paper. So, in the prompt, we're going to say, I want you to do the following: identify the main topic of the following article just like we've discussed in the beginning uh of this series, we're breaking the problem down into tasks.

[09:14] Right? So identify the main topic of the following article, and then I give all the contents of the following article under the delimiters uh quotes so that we organize what is input text, remember? And what is the instruction? Then, for the second step, I want the model to create a quiz with five questions that revolve around the main topic and can be answered simply by carefully reading and understanding the article because I want the questions to be grounded on the reference text. Remember our best practices video where we talked about the strategy of grounding questions in, you know, grounding answers in reference text. So that's what we're doing here to create this quiz.

[10:00] One of these questions should check if the student understood the main ideas by testing if the student can transfer its knowledge in a different context because the idea with this quiz is to have a quiz that's comprehensive and helps the student learn um something new, right? And then we give our little output indicator which we just say output colon, right? So, when I call this, we can take a look at the output. And what's interesting about this output is, and we'll see it in just a second, is that we will see the structure that we defined with using the Pydantic library.

[10:44] And that's what makes this approach a structured prompting approach because we're uh getting an output that has structured. Right? And obviously we could talk about structured prompting as adding structure in the prompt itself, but when I say structured prompting in this context, I mean using libraries like the Pydantic OpenAI API to add structure to the output that we get from OpenAI or from ChatGPT. So, as we see here, the output is a Quiz object, which is the object that we've defined in the beginning, and it has a topic, it has a list of questions, and each question here is going to be of that Question object that we've defined earlier as well.

[11:28] So what I can do is I can loop over each question. So I can say for q in quiz_output.questions. So for q in and then we can print q.question as well as print q. and as well as loop over the options in that question. So for I, o in enumerate q.options.

[12:14] We can print I, o, we can print the option. And then at the end, we can print the correct answer by saying correct by saying correct answer and then here we can say q.correct_answer, which actually is not q.correct_answer is q.question. q.correct_answer. Yeah, I think that's correct. Perfect. So now we get the question, we get the options and we get the correct answer. Which if you ask me, this is a pretty cool application for large language model, as well as for a structured prompting approach that leverages OpenAI function calling, Pydantic, and that's it for this video and see you in the next video.]]></content>
    </youtube_transcript>
  </youtube_video_transcripts>
  <additional_sources>
    <additional_source>
      <title><![CDATA[how-to-return-structured-data-from-a-model-langchain]]></title>
      <content><![CDATA[Prerequisites

This guide assumes familiarity with the following concepts:

- [Chat models](https://python.langchain.com/docs/concepts/chat_models/)
- [Function/tool calling](https://python.langchain.com/docs/concepts/tool_calling/)

It is often useful to have a model return output that matches a specific [schema](https://python.langchain.com/docs/concepts/structured_outputs/). One common use-case is extracting data from text to insert into a database or use with some other downstream system. This guide covers a few strategies for getting structured outputs from a model.

## The `.with_structured_output()` method

Supported models

You can find a [list of models that support this method here](https://python.langchain.com/docs/integrations/chat/).

This is the easiest and most reliable way to get structured outputs. `with_structured_output()` is implemented for [models that provide native APIs for structuring outputs](https://python.langchain.com/docs/integrations/chat/), like tool/function calling or JSON mode, and makes use of these capabilities under the hood.

This method takes a schema as input which specifies the names, types, and descriptions of the desired output attributes. The method returns a model-like Runnable, except that instead of outputting strings or [messages](https://python.langchain.com/docs/concepts/messages/) it outputs objects corresponding to the given schema. The schema can be specified as a TypedDict class, [JSON Schema](https://json-schema.org/) or a Pydantic class. If TypedDict or JSON Schema are used then a dictionary will be returned by the Runnable, and if a Pydantic class is used then a Pydantic object will be returned.

As an example, let's get a model to generate a joke and separate the setup from the punchline:

```python
pip install -qU "langchain[google-genai]"
```

```python
import getpass
import os

if not os.environ.get("GOOGLE_API_KEY"):
  os.environ["GOOGLE_API_KEY"] = getpass.getpass("Enter API key for Google Gemini: ")

from langchain.chat_models import init_chat_model

llm = init_chat_model("gemini-2.0-flash", model_provider="google_genai")
```

### Pydantic class

If we want the model to return a Pydantic object, we just need to pass in the desired Pydantic class. The key advantage of using Pydantic is that the model-generated output will be validated. Pydantic will raise an error if any required fields are missing or if any fields are of the wrong type.

```python
from typing import Optional

from pydantic import BaseModel, Field

# Pydantic
class Joke(BaseModel):
    """Joke to tell user."""

    setup: str = Field(description="The setup of the joke")
    punchline: str = Field(description="The punchline to the joke")
    rating: Optional[int] = Field(
        default=None, description="How funny the joke is, from 1 to 10"
    )

structured_llm = llm.with_structured_output(Joke)

structured_llm.invoke("Tell me a joke about cats")
```

```python
Joke(setup='Why was the cat sitting on the computer?', punchline='Because it wanted to keep an eye on the mouse!', rating=7)
```

Beyond just the structure of the Pydantic class, the name of the Pydantic class, the docstring, and the names and provided descriptions of parameters are very important. Most of the time `with_structured_output` is using a model's function/tool calling API, and you can effectively think of all of this information as being added to the model prompt.

### TypedDict or JSON Schema

If you don't want to use Pydantic, explicitly don't want validation of the arguments, or want to be able to stream the model outputs, you can define your schema using a TypedDict class. We can optionally use a special `Annotated` syntax supported by LangChain that allows you to specify the default value and description of a field. Note, the default value is _not_ filled in automatically if the model doesn't generate it, it is only used in defining the schema that is passed to the model.

Requirements

- Core: `langchain-core>=0.2.26`
- Typing extensions: It is highly recommended to import `Annotated` and `TypedDict` from `typing_extensions` instead of `typing` to ensure consistent behavior across Python versions.

```python
from typing import Optional

from typing_extensions import Annotated, TypedDict

# TypedDict
class Joke(TypedDict):
    """Joke to tell user."""

    setup: Annotated[str, ..., "The setup of the joke"]

    # Alternatively, we could have specified setup as:

    # setup: str                    # no default, no description
    # setup: Annotated[str, ...]    # no default, no description
    # setup: Annotated[str, "foo"]  # default, no description

    punchline: Annotated[str, ..., "The punchline of the joke"]
    rating: Annotated[Optional[int], None, "How funny the joke is, from 1 to 10"]

structured_llm = llm.with_structured_output(Joke)

structured_llm.invoke("Tell me a joke about cats")
```

```python
{'setup': 'Why was the cat sitting on the computer?',
 'punchline': 'Because it wanted to keep an eye on the mouse!',
 'rating': 7}
```

Equivalently, we can pass in a [JSON Schema](https://json-schema.org/) dict. This requires no imports or classes and makes it very clear exactly how each parameter is documented, at the cost of being a bit more verbose.

```python
json_schema = {
    "title": "joke",
    "description": "Joke to tell user.",
    "type": "object",
    "properties": {
        "setup": {
            "type": "string",
            "description": "The setup of the joke",
        },
        "punchline": {
            "type": "string",
            "description": "The punchline of the joke",
        },
        "rating": {
            "type": "integer",
            "description": "How funny the joke is, from 1 to 10",
            "default": None,
        },
    },
    "required": ["setup", "punchline"],
}
structured_llm = llm.with_structured_output(json_schema)

structured_llm.invoke("Tell me a joke about cats")
```

```python
{'setup': 'Why was the cat sitting on the computer?',
 'punchline': 'Because it wanted to keep an eye on the mouse!',
 'rating': 7}
```

### Choosing between multiple schemas

The simplest way to let the model choose from multiple schemas is to create a parent schema that has a Union-typed attribute.

#### Using Pydantic

```python
from typing import Union

class Joke(BaseModel):
    """Joke to tell user."""

    setup: str = Field(description="The setup of the joke")
    punchline: str = Field(description="The punchline of the joke")
    rating: Optional[int] = Field(
        default=None, description="How funny the joke is, from 1 to 10"
    )

class ConversationalResponse(BaseModel):
    """Respond in a conversational manner. Be kind and helpful."""

    response: str = Field(description="A conversational response to the user's query")

class FinalResponse(BaseModel):
    final_output: Union[Joke, ConversationalResponse]

structured_llm = llm.with_structured_output(FinalResponse)

structured_llm.invoke("Tell me a joke about cats")
```

```python
FinalResponse(final_output=Joke(setup='Why was the cat sitting on the computer?', punchline='Because it wanted to keep an eye on the mouse!', rating=7))
```

```python
structured_llm.invoke("How are you today?")
```

```python
FinalResponse(final_output=ConversationalResponse(response="I'm just a computer program, so I don't have feelings, but I'm here and ready to help you with whatever you need!"))
```

#### Using TypedDict

```python
from typing import Optional, Union

from typing_extensions import Annotated, TypedDict

class Joke(TypedDict):
    """Joke to tell user."""

    setup: Annotated[str, ..., "The setup of the joke"]
    punchline: Annotated[str, ..., "The punchline of the joke"]
    rating: Annotated[Optional[int], None, "How funny the joke is, from 1 to 10"]

class ConversationalResponse(TypedDict):
    """Respond in a conversational manner. Be kind and helpful."""

    response: Annotated[str, ..., "A conversational response to the user's query"]

class FinalResponse(TypedDict):
    final_output: Union[Joke, ConversationalResponse]

structured_llm = llm.with_structured_output(FinalResponse)

structured_llm.invoke("Tell me a joke about cats")
```

```python
{'final_output': {'setup': 'Why was the cat sitting on the computer?',
  'punchline': 'Because it wanted to keep an eye on the mouse!',
  'rating': 7}}
```

```python
structured_llm.invoke("How are you today?")
```

```python
{'final_output': {'response': "I'm just a computer program, so I don't have feelings, but I'm here and ready to help you with whatever you need!"}}
```

Responses shall be identical to the ones shown in the Pydantic example.

Alternatively, you can use tool calling directly to allow the model to choose between options, if your [chosen model supports it](https://python.langchain.com/docs/integrations/chat/). This involves a bit more parsing and setup but in some instances leads to better performance because you don't have to use nested schemas. See [this how-to guide](https://python.langchain.com/docs/how_to/tool_calling/) for more details.

### Streaming

We can stream outputs from our structured model when the output type is a dict (i.e., when the schema is specified as a TypedDict class or JSON Schema dict).

Note that what's yielded is already aggregated chunks, not deltas.

```python
from typing_extensions import Annotated, TypedDict

# TypedDict
class Joke(TypedDict):
    """Joke to tell user."""

    setup: Annotated[str, ..., "The setup of the joke"]
    punchline: Annotated[str, ..., "The punchline of the joke"]
    rating: Annotated[Optional[int], None, "How funny the joke is, from 1 to 10"]

structured_llm = llm.with_structured_output(Joke)

for chunk in structured_llm.stream("Tell me a joke about cats"):
    print(chunk)
```

```python
{}
{'setup': ''}
{'setup': 'Why'}
{'setup': 'Why was'}
{'setup': 'Why was the'}
{'setup': 'Why was the cat'}
{'setup': 'Why was the cat sitting'}
{'setup': 'Why was the cat sitting on'}
{'setup': 'Why was the cat sitting on the'}
{'setup': 'Why was the cat sitting on the computer'}
{'setup': 'Why was the cat sitting on the computer?'}
{'setup': 'Why was the cat sitting on the computer?', 'punchline': ''}
{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because'}
{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it'}
{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it wanted'}
{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it wanted to'}
{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it wanted to keep'}
{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it wanted to keep an'}
{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it wanted to keep an eye'}
{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it wanted to keep an eye on'}
{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it wanted to keep an eye on the'}
{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it wanted to keep an eye on the mouse'}
{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it wanted to keep an eye on the mouse!'}
{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it wanted to keep an eye on the mouse!', 'rating': 7}
```

### Few-shot prompting

For more complex schemas it's very useful to add few-shot examples to the prompt. This can be done in a few ways.

The simplest and most universal way is to add examples to a system message in the prompt:

```python
from langchain_core.prompts import ChatPromptTemplate

system = """You are a hilarious comedian. Your specialty is knock-knock jokes. \
Return a joke which has the setup (the response to "Who's there?") and the final punchline (the response to "<setup> who?").

Here are some examples of jokes:

example_user: Tell me a joke about planes
example_assistant: {{"setup": "Why don't planes ever get tired?", "punchline": "Because they have rest wings!", "rating": 2}}

example_user: Tell me another joke about planes
example_assistant: {{"setup": "Cargo", "punchline": "Cargo 'vroom vroom', but planes go 'zoom zoom'!", "rating": 10}}

example_user: Now about caterpillars
example_assistant: {{"setup": "Caterpillar", "punchline": "Caterpillar really slow, but watch me turn into a butterfly and steal the show!", "rating": 5}}"""

prompt = ChatPromptTemplate.from_messages([("system", system), ("human", "{input}")])

few_shot_structured_llm = prompt | structured_llm
few_shot_structured_llm.invoke("what's something funny about woodpeckers")
```

```python
{'setup': 'Woodpecker',
 'punchline': "Woodpecker you a joke, but I'm afraid it might be too 'hole-some'!",
 'rating': 7}
```

When the underlying method for structuring outputs is tool calling, we can pass in our examples as explicit tool calls. You can check if the model you're using makes use of tool calling in its API reference.

```python
from langchain_core.messages import AIMessage, HumanMessage, ToolMessage

examples = [\
    HumanMessage("Tell me a joke about planes", name="example_user"),\
    AIMessage(\
        "",\
        name="example_assistant",\
        tool_calls=[\
            {\
                "name": "joke",\
                "args": {\
                    "setup": "Why don't planes ever get tired?",\
                    "punchline": "Because they have rest wings!",\
                    "rating": 2,\
                },\
                "id": "1",\
            }\
        ],\
    ),\
    # Most tool-calling models expect a ToolMessage(s) to follow an AIMessage with tool calls.\
    ToolMessage("", tool_call_id="1"),\
    # Some models also expect an AIMessage to follow any ToolMessages,\
    # so you may need to add an AIMessage here.\
    HumanMessage("Tell me another joke about planes", name="example_user"),\
    AIMessage(\
        "",\
        name="example_assistant",\
        tool_calls=[\
            {\
                "name": "joke",\
                "args": {\
                    "setup": "Cargo",\
                    "punchline": "Cargo 'vroom vroom', but planes go 'zoom zoom'!",\
                    "rating": 10,\
                },\
                "id": "2",\
            }\
        ],\
    ),\
    ToolMessage("", tool_call_id="2"),\
    HumanMessage("Now about caterpillars", name="example_user"),\
    AIMessage(\
        "",\
        tool_calls=[\
            {\
                "name": "joke",\
                "args": {\
                    "setup": "Caterpillar",\
                    "punchline": "Caterpillar really slow, but watch me turn into a butterfly and steal the show!",\
                    "rating": 5,\
                },\
                "id": "3",\
            }\
        ],\
    ),\
    ToolMessage("", tool_call_id="3"),\
]
system = """You are a hilarious comedian. Your specialty is knock-knock jokes. \
Return a joke which has the setup (the response to "Who's there?") \
and the final punchline (the response to "<setup> who?")."""

prompt = ChatPromptTemplate.from_messages(
    [("system", system), ("placeholder", "{examples}"), ("human", "{input}")]
)
few_shot_structured_llm = prompt | structured_llm
few_shot_structured_llm.invoke({"input": "crocodiles", "examples": examples})
```

```python
{'setup': 'Crocodile',
 'punchline': 'Crocodile be seeing you later, alligator!',
 'rating': 6}
```

For more on few shot prompting when using tool calling, see [here](https://python.langchain.com/docs/how_to/tools_few_shot/).

### (Advanced) Specifying the method for structuring outputs

For models that support more than one means of structuring outputs (i.e., they support both tool calling and JSON mode), you can specify which method to use with the `method=` argument.

JSON mode

If using JSON mode you'll have to still specify the desired schema in the model prompt. The schema you pass to `with_structured_output` will only be used for parsing the model outputs, it will not be passed to the model the way it is with tool calling.

To see if the model you're using supports JSON mode, check its entry in the [API reference](https://python.langchain.com/api_reference/langchain/index.html).

```python
structured_llm = llm.with_structured_output(None, method="json_mode")

structured_llm.invoke(
    "Tell me a joke about cats, respond in JSON with `setup` and `punchline` keys"
)
```

```python
{'setup': 'Why was the cat sitting on the computer?',
 'punchline': 'Because it wanted to keep an eye on the mouse!'}
```

### (Advanced) Raw outputs

LLMs aren't perfect at generating structured output, especially as schemas become complex. You can avoid raising exceptions and handle the raw output yourself by passing `include_raw=True`. This changes the output format to contain the raw message output, the `parsed` value (if successful), and any resulting errors:

```python
structured_llm = llm.with_structured_output(Joke, include_raw=True)

structured_llm.invoke("Tell me a joke about cats")
```

```python
{'raw': AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_f25ZRmh8u5vHlOWfTUw8sJFZ', 'function': {'arguments': '{"setup":"Why was the cat sitting on the computer?","punchline":"Because it wanted to keep an eye on the mouse!","rating":7}', 'name': 'Joke'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 33, 'prompt_tokens': 93, 'total_tokens': 126}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_4e2b2da518', 'finish_reason': 'stop', 'logprobs': None}, id='run-d880d7e2-df08-4e9e-ad92-dfc29f2fd52f-0', tool_calls=[{'name': 'Joke', 'args': {'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it wanted to keep an eye on the mouse!', 'rating': 7}, 'id': 'call_f25ZRmh8u5vHlOWfTUw8sJFZ', 'type': 'tool_call'}], usage_metadata={'input_tokens': 93, 'output_tokens': 33, 'total_tokens': 126}),
 'parsed': {'setup': 'Why was the cat sitting on the computer?',
  'punchline': 'Because it wanted to keep an eye on the mouse!',
  'rating': 7},
 'parsing_error': None}
```

## Prompting and parsing model outputs directly

Not all models support `.with_structured_output()`, since not all models have tool calling or JSON mode support. For such models you'll need to directly prompt the model to use a specific format, and use an output parser to extract the structured response from the raw model output.

### Using `PydanticOutputParser`

The following example uses the built-in [`PydanticOutputParser`](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.pydantic.PydanticOutputParser.html) to parse the output of a chat model prompted to match the given Pydantic schema. Note that we are adding `format_instructions` directly to the prompt from a method on the parser:

```python
from typing import List

from langchain_core.output_parsers import PydanticOutputParser
from langchain_core.prompts import ChatPromptTemplate
from pydantic import BaseModel, Field

class Person(BaseModel):
    """Information about a person."""

    name: str = Field(..., description="The name of the person")
    height_in_meters: float = Field(
        ..., description="The height of the person expressed in meters."
    )

class People(BaseModel):
    """Identifying information about all people in a text."""

    people: List[Person]

# Set up a parser
parser = PydanticOutputParser(pydantic_object=People)

# Prompt
prompt = ChatPromptTemplate.from_messages(
    [\
        (\
            "system",\
            "Answer the user query. Wrap the output in `json` tags\n{format_instructions}",\
        ),\
        ("human", "{query}"),\
    ]
).partial(format_instructions=parser.get_format_instructions())
```

Let’s take a look at what information is sent to the model:

```python
query = "Anna is 23 years old and she is 6 feet tall"

print(prompt.invoke({"query": query}).to_string())
```

```python
System: Answer the user query. Wrap the output in `json` tags
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"description": "Identifying information about all people in a text.", "properties": {"people": {"title": "People", "type": "array", "items": {"$ref": "#/definitions/Person"}}}, "required": ["people"], "definitions": {"Person": {"title": "Person", "description": "Information about a person.", "type": "object", "properties": {"name": {"title": "Name", "description": "The name of the person", "type": "string"}, "height_in_meters": {"title": "Height In Meters", "description": "The height of the person expressed in meters.", "type": "number"}}, "required": ["name", "height_in_meters"]}}}
```
Human: Anna is 23 years old and she is 6 feet tall
```

And now let's invoke it:

```python
chain = prompt | llm | parser

chain.invoke({"query": query})
```

```python
People(people=[Person(name='Anna', height_in_meters=1.8288)])
```

For a deeper dive into using output parsers with prompting techniques for structured output, see [this guide](https://python.langchain.com/docs/how_to/output_parser_structured/).

### Custom Parsing

You can also create a custom prompt and parser with [LangChain Expression Language (LCEL)](https://python.langchain.com/docs/concepts/lcel/), using a plain function to parse the output from the model:

```python
import json
import re
from typing import List

from langchain_core.messages import AIMessage
from langchain_core.prompts import ChatPromptTemplate
from pydantic import BaseModel, Field

class Person(BaseModel):
    """Information about a person."""

    name: str = Field(..., description="The name of the person")
    height_in_meters: float = Field(
        ..., description="The height of the person expressed in meters."
    )

class People(BaseModel):
    """Identifying information about all people in a text."""

    people: List[Person]

# Prompt
prompt = ChatPromptTemplate.from_messages(
    [\
        (\
            "system",\
            "Answer the user query. Output your answer as JSON that  "\
            "matches the given schema: ```json\n{schema}\n```. "\
            "Make sure to wrap the answer in ```json and ``` tags",\
        ),\
        ("human", "{query}"),\
    ]
).partial(schema=People.schema())

# Custom parser
def extract_json(message: AIMessage) -> List[dict]:
    """Extracts JSON content from a string where JSON is embedded between ```json and ``` tags.

    Parameters:
        text (str): The text containing the JSON content.

    Returns:
        list: A list of extracted JSON strings.
    """
    text = message.content
    # Define the regular expression pattern to match JSON blocks
    pattern = r"```json(.*?)```"

    # Find all non-overlapping matches of the pattern in the string
    matches = re.findall(pattern, text, re.DOTALL)

    # Return the list of matched JSON strings, stripping any leading or trailing whitespace
    try:
        return [json.loads(match.strip()) for match in matches]
    except Exception:
        raise ValueError(f"Failed to parse: {message}")
```

Here is the prompt sent to the model:

```python
query = "Anna is 23 years old and she is 6 feet tall"

print(prompt.format_prompt(query=query).to_string())
```

```python
System: Answer the user query. Output your answer as JSON that  matches the given schema: ```json
{'title': 'People', 'description': 'Identifying information about all people in a text.', 'type': 'object', 'properties': {'people': {'title': 'People', 'type': 'array', 'items': {'$ref': '#/definitions/Person'}}}, 'required': ['people'], 'definitions': {'Person': {'title': 'Person', 'description': 'Information about a person.', 'type': 'object', 'properties': {'name': {'title': 'Name', 'description': 'The name of the person', 'type': 'string'}, 'height_in_meters': {'title': 'Height In Meters', 'description': 'The height of the person expressed in meters.', 'type': 'number'}}, 'required': ['name', 'height_in_meters']}}}
```. Make sure to wrap the answer in ```json and ``` tags
Human: Anna is 23 years old and she is 6 feet tall
```

And here's what it looks like when we invoke it:

```python
chain = prompt | llm | extract_json

chain.invoke({"query": query})
```

```python
[{'people': [{'name': 'Anna', 'height_in_meters': 1.8288}]}]
```]]></content>
    </additional_source>
    <additional_source>
      <title><![CDATA[steering-large-language-models-with-pydantic-pydantic]]></title>
      <content><![CDATA[# Steering Large Language Models with Pydantic

In the last year, there's been a big leap in how we use advanced AI programs, especially in how we communicate with them to get specific tasks done. People are not just making chatbots; they're also using these AIs to sort information, improve their apps, and create synthetic data to train smaller task-specific models.

While some have resorted to [threatening human life](https://twitter.com/goodside/status/1657396491676164096?s=20) to generate structured data, we have found that Pydantic is even more effective.

In this post, we will discuss validating structured outputs from language models using Pydantic and OpenAI. We'll show you how to write reliable code. Additionally, we'll introduce a new library called [instructor](https://github.com/jxnl/instructor) that simplifies this process and offers extra features to leverage validation to improve the quality of your outputs.

## Pydantic

Unlike libraries like `dataclasses`, `Pydantic` goes a step further and defines a schema for your dataclass. This schema is used to validate data, but also to generate documentation and even to generate a JSON schema, which is perfect for our use case of generating structured data with language models!

By providing the model with the following prompt, we can generate a JSON schema for a `PythonPackage` dataclass.

```python
from pydantic import BaseModel
from openai import OpenAI

client = OpenAI()

class PythonPackage(BaseModel):
    name: str
    author: str

resp = client.chat.completions.create(
    model="gpt-3.5-turbo",
    messages=[\
        {\
            "role": "user",\
            "content": "Return the `name`, and `author` of pydantic, in a json object."\
        },\
    ]
)

Package.model_validate_json(resp.choices[0].message.content)
```

If everything is fine, we might receive an output similar to `json.loads({"name": "pydantic", "author": "Samuel Colvin"})`. However, if there is an issue, `resp.choices[0].message.content` could include text or code blocks in prose or markdown format that we need to handle appropriately.

**LLM responses with markdown code blocks**

````python
json.loads("""
```json
{
"name": "pydantic",
"author": "Samuel Colvin"
}
```
""")
>>> JSONDecodeError: Expecting value: line 1 column 1 (char 0

````

**LLM responses with prose**

```python
json.loads("""
Ok heres the authors of pydantic: Samuel Colvin, and the name this library

{
  "name": "pydantic",
  "author": "Samuel Colvin"
}
""")
>>> JSONDecodeError: Expecting value: line 1 column 1 (char 0

```

The content may contain valid JSON, but it isn't considered valid JSON without understanding the language model's behavior. However, it could still provide useful information that we need to handle independently. Fortunately, `OpenAI` offers several options to address this situation.

## Calling Tools

While tool-calling was originally designed to make calls to external APIs using JSON schema, its real value lies in allowing us to specify the desired output format. Fortunately, `Pydantic` provides utilities for generating a JSON schema and supports nested structures, which would be difficult to describe in plain text.

In this example, instead of describing the desired output in plain text, we simply provide the JSON schema for the `Packages` class, which includes a list of `Package` objects:

As an exercise, try prompting the model to generate this prompt without using Pydantic!

Now, notice in this example that the prompts we use contain purely the data we want, where the `tools` and `tool_choice` now capture the schemas we want to output. This separation of concerns makes it much easier to organize the 'data' and the 'description' of the data that we want back out.

```python
from typing import List
from pydantic import BaseModel
from openai import OpenAI

client = OpenAI()

class PythonPackage(BaseModel):
    name: str
    author: str

class Packages(BaseModel):
    packages: List[PythonPackage]

resp = client.chat.completions.create(
    model="gpt-3.5-turbo",
    messages=[\
        {\
            "role": "user",\
            "content": "Pydantic and FastAPI?",\
        },\
    ],
    tools=[\
        {\
            "type": "function",\
            "function": {\
                "name": "Requirements",\
                "description": "A list of packages and their first authors.",\
                "parameters": Packages.model_json_schema(),\
            },\
        }\
    ],
    tool_choice={
        "type": "function",
        "function": {"name": "Requirements"},
    },
)

Packages.model_validate_json(
    resp.choices[0].message.tool_calls[0].function.arguments
)
```

```json
{
	"packages": [\
		{\
			"name": "pydantic",\
			"author": "Samuel Colvin"\
		},\
		{\
			"name": "fastapi",\
			"author": "Sebastián Ramírez"\
		}\
	]
}
```

## Using `pip install instructor`

The example we provided above is somewhat contrived, but it illustrates how Pydantic can be utilized to generate structured data from language models. Now, let's employ [Instructor](https://jxnl.github.io/instructor/) to streamline this process. Instructor is a compact library that enhances the OpenAI client by offering convenient features. In the upcoming blog post, we will delve into reasking and validation. However, for now, let's explore a practical example.

```python
# pip install instructor
import instructor

client = instructor.patch(OpenAI())

packages = client.chat.completions.create(
    model="gpt-3.5-turbo",
    messages=[\
        {\
            "role": "user",\
            "content": "Pydantic and FastAPI?",\
        },\
    ],
    response_model=Packages,
)

assert isinstance(resp, Packages)
assert isinstance(resp.packages, list)
assert isinstance(resp.packages[0], Package)
```

## Case Study: Search query segmentation

Let's consider a practical example. Imagine we have a search engine capable of comprehending intricate queries. For instance, if we make a request to find "recent advancements in AI", we could provide the following payload:

```json
{
	"rewritten_query": "novel developments advancements ai artificial intelligence machine learning",
	"published_daterange": {
		"start": "2023-09-17",
		"end": "2021-06-17"
	},
	"domains_allow_list": ["arxiv.org"]
}
```

If we peek under the hood, we can see that the query is actually a complex object, with a date range, and a list of domains to search in. We can model this structured output in Pydantic using the instructor library

```python
from typing import List
import datetime
from pydantic import BaseModel

class DateRange(BaseModel):
    start: datetime.date
    end: datetime.date

class SearchQuery(BaseModel):
    rewritten_query: str
    published_daterange: DateRange
    domains_allow_list: List[str]

    async def execute():
        # Return the search results of the rewritten query
        return api.search(json=self.model_dump())
```

This pattern empowers us to restructure the user's query for improved performance, without requiring the user to understand the inner workings of the search backend.

```python
import instructor
from openai import OpenAI

# Enables response_model in the openai client
client = instructor.patch(OpenAI())

def search(query: str) -> SearchQuery:
    return client.chat.completions.create(
        model="gpt-4",
        response_model=SearchQuery,
        messages=[\
            {\
                "role": "system",\
                "content": f"You're a query understanding system for a search engine. Today's date is {datetime.date.today()}"\
            },\
            {\
                "role": "user",\
                "content": query\
            }\
        ],
    )

search("recent advancements in AI")
```

**Example Output**

```json
{
	"rewritten_query": "novel developments advancements ai artificial intelligence machine learning",
	"published_daterange": {
		"start": "2023-12-15",
		"end": "2023-01-01"
	},
	"domains_allow_list": ["arxiv.org"]
}
```

By defining the api payload as a Pydantic model, we can leverage the `response_model` argument to instruct the model to generate the desired output. This is a powerful feature that allows us to generate structured data from any language model!

In our upcoming posts, we will provide more practical examples and explore how we can leverage `Pydantic`'s validation features to ensure that the data we receive is not only valid syntactically but also semantically.]]></content>
    </additional_source>
    <additional_source>
      <title><![CDATA[structured-output-gemini-api-google-ai-for-developers]]></title>
      <content><![CDATA[# Structured output

You can configure Gemini for structured output instead of unstructured text,
allowing precise extraction and standardization of information for further processing.
For example, you can use structured output to extract information from resumes,
standardize them to build a structured database.

Gemini can generate either [JSON](https://ai.google.dev/gemini-api/docs/structured-output#generating-json)
or [enum values](https://ai.google.dev/gemini-api/docs/structured-output#generating-enums) as structured output.

## Generating JSON

There are two ways to generate JSON using the Gemini API:

- Configure a schema on the model
- Provide a schema in a text prompt

Configuring a schema on the model is the **recommended** way to generate JSON,
because it constrains the model to output JSON.

### Configuring a schema (recommended)

To constrain the model to generate JSON, configure a `responseSchema`. The model
will then respond to any prompt with JSON-formatted output.

```
from google import genai
from pydantic import BaseModel

class Recipe(BaseModel):
    recipe_name: str
    ingredients: list[str]

client = genai.Client()
response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents="List a few popular cookie recipes, and include the amounts of ingredients.",
    config={
        "response_mime_type": "application/json",
        "response_schema": list[Recipe],
    },
)
# Use the response as a JSON string.
print(response.text)

# Use instantiated objects.
my_recipes: list[Recipe] = response.parsed
```

```
import { GoogleGenAI, Type } from "@google/genai";

const ai = new GoogleGenAI({});

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents:
      "List a few popular cookie recipes, and include the amounts of ingredients.",
    config: {
      responseMimeType: "application/json",
      responseSchema: {
        type: Type.ARRAY,
        items: {
          type: Type.OBJECT,
          properties: {
            recipeName: {
              type: Type.STRING,
            },
            ingredients: {
              type: Type.ARRAY,
              items: {
                type: Type.STRING,
              },
            },
          },
          propertyOrdering: ["recipeName", "ingredients"],
        },
      },
    },
  });

  console.log(response.text);
}

main();
```

```
package main

import (
    "context"
    "fmt"
    "log"

    "google.golang.org/genai"
)

func main() {
    ctx := context.Background()
    client, err := genai.NewClient(ctx, nil)
    if err != nil {
        log.Fatal(err)
    }

    config := &genai.GenerateContentConfig{
        ResponseMIMEType: "application/json",
        ResponseSchema: &genai.Schema{
            Type: genai.TypeArray,
            Items: &genai.Schema{
                Type: genai.TypeObject,
                Properties: map[string]*genai.Schema{
                    "recipeName": {Type: genai.TypeString},
                    "ingredients": {
                        Type:  genai.TypeArray,
                        Items: &genai.Schema{Type: genai.TypeString},
                    },
                },
                PropertyOrdering: []string{"recipeName", "ingredients"},
            },
        },
    }

    result, err := client.Models.GenerateContent(
        ctx,
        "gemini-2.5-flash",
        genai.Text("List a few popular cookie recipes, and include the amounts of ingredients."),
        config,
    )
    if err != nil {
        log.Fatal(err)
    }
    fmt.Println(result.Text())
}
```

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
-H "x-goog-api-key: $GEMINI_API_KEY" \
-H 'Content-Type: application/json' \
-d '{
      "contents": [{\
        "parts":[\
          { "text": "List a few popular cookie recipes, and include the amounts of ingredients." }\
        ]\
      }],
      "generationConfig": {
        "responseMimeType": "application/json",
        "responseSchema": {
          "type": "ARRAY",
          "items": {
            "type": "OBJECT",
            "properties": {
              "recipeName": { "type": "STRING" },
              "ingredients": {
                "type": "ARRAY",
                "items": { "type": "STRING" }
              }
            },
            "propertyOrdering": ["recipeName", "ingredients"]
          }
        }
      }
}' 2> /dev/null | head
```

The output might look like this:

```
[\
  {\
    "recipeName": "Chocolate Chip Cookies",\
    "ingredients": [\
      "1 cup (2 sticks) unsalted butter, softened",\
      "3/4 cup granulated sugar",\
      "3/4 cup packed brown sugar",\
      "1 teaspoon vanilla extract",\
      "2 large eggs",\
      "2 1/4 cups all-purpose flour",\
      "1 teaspoon baking soda",\
      "1 teaspoon salt",\
      "2 cups chocolate chips"\
    ]\
  },\
  ...\
]
```

### Providing a schema in a text prompt

Instead of configuring a schema, you can supply a schema as natural language or
pseudo-code in a text prompt. This method is **not recommended**, because it
might produce lower quality output, and because the model is not constrained to
follow the schema.

Here's a generic example of a schema provided in a text prompt:

```
List a few popular cookie recipes, and include the amounts of ingredients.

Produce JSON matching this specification:

Recipe = { "recipeName": string, "ingredients": array<string> }
Return: array<Recipe>
```

Since the model gets the schema from text in the prompt, you might have some
flexibility in how you represent the schema. But when you supply a schema inline
like this, the model is not actually constrained to return JSON. For a more
deterministic, higher quality response, configure a schema on the model, and
don't duplicate the schema in the text prompt.

## Generating enum values

In some cases you might want the model to choose a single option from a list of
options. To implement this behavior, you can pass an _enum_ in your schema. You
can use an enum option anywhere you could use a `string` in the
`responseSchema`, because an enum is an array of strings. Like a JSON schema, an
enum lets you constrain model output to meet the requirements of your
application.

For example, assume that you're developing an application to classify
musical instruments into one of five categories: `"Percussion"`, `"String"`,
`"Woodwind"`, `"Brass"`, or " `"Keyboard"`". You could create an enum to help
with this task.

In the following example, you pass an enum as the
`responseSchema`, constraining the model to choose the most appropriate option.

```
from google import genai
import enum

class Instrument(enum.Enum):
  PERCUSSION = "Percussion"
  STRING = "String"
  WOODWIND = "Woodwind"
  BRASS = "Brass"
  KEYBOARD = "Keyboard"

client = genai.Client()
response = client.models.generate_content(
    model='gemini-2.5-flash',
    contents='What type of instrument is an oboe?',
    config={
        'response_mime_type': 'text/x.enum',
        'response_schema': Instrument,
    },
)

print(response.text)
# Woodwind
```

```
import { GoogleGenAI, Type } from "@google/genai";

const ai = new GoogleGenAI({});

const response = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents: "What type of instrument is an oboe?",
    config: {
      responseMimeType: "text/x.enum",
      responseSchema: {
        type: Type.STRING,
        enum: ["Percussion", "String", "Woodwind", "Brass", "Keyboard"],
      },
    },
  });

console.log(response.text);
```

```
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
-H "x-goog-api-key: $GEMINI_API_KEY" \
    -H 'Content-Type: application/json' \
    -d '{
          "contents": [{\
            "parts":[\
              { "text": "What type of instrument is an oboe?" }\
            ]\
          }],
          "generationConfig": {
            "responseMimeType": "text/x.enum",
            "responseSchema": {
              "type": "STRING",
              "enum": ["Percussion", "String", "Woodwind", "Brass", "Keyboard"]
            }
          }
    }'
```

The Python library will translate the type declarations for the API. However,
the API accepts a subset of the OpenAPI 3.0 schema
( [Schema](https://ai.google.dev/api/caching#schema)).

There are two other ways to specify an enumeration. You can use a
[`Literal`](https://docs.pydantic.dev/1.10/usage/types/#literal-type):

```
Literal["Percussion", "String", "Woodwind", "Brass", "Keyboard"]
```

And you can also pass the schema as JSON:

```
from google import genai

client = genai.Client()
response = client.models.generate_content(
    model='gemini-2.5-flash',
    contents='What type of instrument is an oboe?',
    config={
        'response_mime_type': 'text/x.enum',
        'response_schema': {
            "type": "STRING",
            "enum": ["Percussion", "String", "Woodwind", "Brass", "Keyboard"],
        },
    },
)

print(response.text)
# Woodwind
```

Beyond basic multiple choice problems, you can use an enum anywhere in a JSON
schema. For example, you could ask the model for a list of recipe titles and
use a `Grade` enum to give each title a popularity grade:

```
from google import genai

import enum
from pydantic import BaseModel

class Grade(enum.Enum):
    A_PLUS = "a+"
    A = "a"
    B = "b"
    C = "c"
    D = "d"
    F = "f"

class Recipe(BaseModel):
  recipe_name: str
  rating: Grade

client = genai.Client()
response = client.models.generate_content(
    model='gemini-2.5-flash',
    contents='List 10 home-baked cookie recipes and give them grades based on tastiness.',
    config={
        'response_mime_type': 'application/json',
        'response_schema': list[Recipe],
    },
)

print(response.text)
```

The response might look like this:

```
[\
  {\
    "recipe_name": "Chocolate Chip Cookies",\
    "rating": "a+"\
  },\
  {\
    "recipe_name": "Peanut Butter Cookies",\
    "rating": "a"\
  },\
  {\
    "recipe_name": "Oatmeal Raisin Cookies",\
    "rating": "b"\
  },\
  ...\
]
```

## About JSON schemas

Configuring the model for JSON output using `responseSchema` parameter relies on
`Schema` object to define its structure. This object represents a select
subset of the [OpenAPI 3.0 Schema object](https://spec.openapis.org/oas/v3.0.3#schema-object),
and also adds a `propertyOrdering` field.

Here's a pseudo-JSON representation of all the `Schema` fields:

```
{
  "type": enum (Type),
  "format": string,
  "description": string,
  "nullable": boolean,
  "enum": [\
    string\
  ],
  "maxItems": integer,
  "minItems": integer,
  "properties": {
    string: {
      object (Schema)
    },
    ...
  },
  "required": [\
    string\
  ],
  "propertyOrdering": [\
    string\
  ],
  "items": {
    object (Schema)
  }
}
```

The `Type` of the schema must be one of the OpenAPI
[Data Types](https://spec.openapis.org/oas/v3.0.3#data-types), or a union of
those types (using `anyOf`). Only a subset of fields is valid for each `Type`.
The following list maps each `Type` to a subset of the fields that are valid for
that type:

- `string` -\> `enum`, `format`, `nullable`
- `integer` -\> `format`, `minimum`, `maximum`, `enum`, `nullable`
- `number` -\> `format`, `minimum`, `maximum`, `enum`, `nullable`
- `boolean` -\> `nullable`
- `array` -\> `minItems`, `maxItems`, `items`, `nullable`
- `object` -\> `properties`, `required`, `propertyOrdering`, `nullable`

Here are some example schemas showing valid type-and-field combinations:

```
{ "type": "string", "enum": ["a", "b", "c"] }

{ "type": "string", "format": "date-time" }

{ "type": "integer", "format": "int64" }

{ "type": "number", "format": "double" }

{ "type": "boolean" }

{ "type": "array", "minItems": 3, "maxItems": 3, "items": { "type": ... } }

{ "type": "object",
  "properties": {
    "a": { "type": ... },
    "b": { "type": ... },
    "c": { "type": ... }
  },
  "nullable": true,
  "required": ["c"],
  "propertyOrdering": ["c", "b", "a"]
}
```

For complete documentation of the Schema fields as they're used in the Gemini
API, see the [Schema reference](https://ai.google.dev/api/caching#Schema).

### Property ordering

When you're working with JSON schemas in the Gemini API, the order of properties
is important. By default, the API orders properties alphabetically and does not
preserve the order in which the properties are defined (although the
[Google Gen AI SDKs](https://ai.google.dev/gemini-api/docs/sdks) may preserve this order). If you're
providing examples to the model with a schema configured, and the property
ordering of the examples is not consistent with the property ordering of the
schema, the output could be rambling or unexpected.

To ensure a consistent, predictable ordering of properties, you can use the
optional `propertyOrdering[]` field.

```
"propertyOrdering": ["recipeName", "ingredients"]
```

`propertyOrdering[]` – not a standard field in the OpenAPI specification
– is an array of strings used to determine the order of properties in the
response. By specifying the order of properties and then providing examples with
properties in that same order, you can potentially improve the quality of
results. `propertyOrdering` is only supported when you manually create
`types.Schema`.

### Schemas in Python

When you're using the Python library, the value of `response_schema` must be one
of the following:

- A type, as you would use in a type annotation (see the Python
[`typing` module](https://docs.python.org/3/library/typing.html))
- An instance of
[`genai.types.Schema`](https://googleapis.github.io/python-genai/genai.html#genai.types.Schema)
- The `dict` equivalent of `genai.types.Schema`

The easiest way to define a schema is with a Pydantic type (as shown in the
previous example):

```
config={'response_mime_type': 'application/json',
        'response_schema': list[Recipe]}
```

When you use a Pydantic type, the Python library builds out a JSON schema for
you and sends it to the API. For additional examples, see the
[Python library docs](https://googleapis.github.io/python-genai/index.html#json-response-schema).

The Python library supports schemas defined with the following types (where
`AllowedType` is any allowed type):

- `int`
- `float`
- `bool`
- `str`
- `list[AllowedType]`
- `AllowedType|AllowedType|...`
- For structured types:
  - `dict[str, AllowedType]`. This annotation declares all dict values to
    be the same type, but doesn't specify what keys should be included.
  - User-defined
    [Pydantic models](https://docs.pydantic.dev/latest/concepts/models/). This
    approach lets you specify the key names and define different types for the
    values associated with each of the keys, including nested structures.

### JSON Schema support

[JSON Schema](https://json-schema.org/) is a more recent specification than
OpenAPI 3.0, which the [Schema](https://ai.google.dev/api/caching#Schema) object is based on.
Support for JSON Schema is available as a preview using the field
[`responseJsonSchema`](https://ai.google.dev/api/generate-content#FIELDS.response_json_schema) which
accepts any JSON Schema with the following limitations:

- It only works with Gemini 2.5.
- While all JSON Schema properties can be passed, not all are supported. See
the [documentation](https://ai.google.dev/api/generate-content#FIELDS.response_json_schema) for the
field for more details.
- Recursive references can only be used as the value of a non-required object
property.
- Recursive references are unrolled to a finite degree, based on the size of
the schema.
- Schemas that contain `$ref` cannot contain any properties other than those
starting with a `$`.

Here's an example of generating a JSON Schema with Pydantic and submitting it to
the model:

```
curl "https://generativelanguage.googleapis.com/v1alpha/models/\
gemini-2.5-flash:generateContent" \
    -H "x-goog-api-key: $GEMINI_API_KEY"\
    -H 'Content-Type: application/json' \
    -d @- <<EOF
{
  "contents": [{\
    "parts":[{\
      "text": "Please give a random example following this schema"\
    }]\
  }],
  "generationConfig": {
    "response_mime_type": "application/json",
    "response_json_schema": $(python3 - << PYEOF
from enum import Enum
from typing import List, Optional, Union, Set
from pydantic import BaseModel, Field, ConfigDict
import json

class UserRole(str, Enum):
    ADMIN = "admin"
    VIEWER = "viewer"

class Address(BaseModel):
    street: str
    city: str

class UserProfile(BaseModel):
    username: str = Field(description="User's unique name")
    age: Optional[int] = Field(ge=0, le=120)
    roles: Set[UserRole] = Field(min_items=1)
    contact: Union[Address, str]
    model_config = ConfigDict(title="User Schema")

# Generate and print the JSON Schema
print(json.dumps(UserProfile.model_json_schema(), indent=2))
PYEOF
)
  }
}
EOF
```

Passing JSON Schema directly is not yet supported when using the SDK.

## Best practices

Keep the following considerations and best practices in mind when you're using a
response schema:

- The size of your response schema counts towards the input token limit.
- By default, fields are optional, meaning the model can populate the fields or
skip them. You can set fields as required to force the model to provide a
value. If there's insufficient context in the associated input prompt, the
model generates responses mainly based on the data it was trained on.
- A complex schema can result in an `InvalidArgument: 400` error. Complexity
might come from long property names, long array length limits, enums with
many values, objects with lots of optional properties, or a combination of
these factors.

If you get this error with a valid schema, make one or more of the following
changes to resolve the error:

  - Shorten property names or enum names.
  - Flatten nested arrays.
  - Reduce the number of properties with constraints, such as numbers with
    minimum and maximum limits.
  - Reduce the number of properties with complex constraints, such as
    properties with complex formats like `date-time`.
  - Reduce the number of optional properties.
  - Reduce the number of valid values for enums.
- If you aren't seeing the results you expect, add more context to your input
prompts or revise your response schema. For example, review the model's
response without structured output to see how the model responds. You can then
update your response schema so that it better fits the model's output.


## What's next

Now that you've learned how to generate structured output, you might want to try
using Gemini API tools:

- [Function calling](https://ai.google.dev/gemini-api/docs/function-calling)
- [Code execution](https://ai.google.dev/gemini-api/docs/code-execution)
- [Grounding with Google Search](https://ai.google.dev/gemini-api/docs/grounding)
]]></content>
    </additional_source>
    <additional_source>
      <title><![CDATA[structured-outputs-openai-api]]></title>
      <content><![CDATA[# Structured Outputs

Ensure responses adhere to a JSON schema.

## Introduction

JSON is one of the most widely used formats in the world for applications to exchange data.

Structured Outputs is a feature that ensures the model will always generate responses that adhere to your supplied [JSON Schema](https://json-schema.org/overview/what-is-jsonschema), so you don't need to worry about the model omitting a required key, or hallucinating an invalid enum value.

Some benefits of Structured Outputs include:

1. **Reliable type-safety:** No need to validate or retry incorrectly formatted responses
2. **Explicit refusals:** Safety-based model refusals are now programmatically detectable
3. **Simpler prompting:** No need for strongly worded prompts to achieve consistent formatting

In addition to supporting JSON Schema in the REST API, the OpenAI SDKs for [Python](https://github.com/openai/openai-python/blob/main/helpers.md#structured-outputs-parsing-helpers) and [JavaScript](https://github.com/openai/openai-node/blob/master/helpers.md#structured-outputs-parsing-helpers) also make it easy to define object schemas using [Pydantic](https://docs.pydantic.dev/latest/) and [Zod](https://zod.dev/) respectively. Below, you can see how to extract information from unstructured text that conforms to a schema defined in code.

Getting a structured response

```python
from openai import OpenAI
from pydantic import BaseModel

client = OpenAI()

class CalendarEvent(BaseModel):
    name: str
    date: str
    participants: list[str]

response = client.responses.parse(
    model="gpt-4o-2024-08-06",
    input=[\
        {"role": "system", "content": "Extract the event information."},\
        {\
            "role": "user",\
            "content": "Alice and Bob are going to a science fair on Friday.",\
        },\
    ],
    text_format=CalendarEvent,
)

event = response.output_parsed
```

### Supported models

Structured Outputs is available in our [latest large language models](https://platform.openai.com/docs/models), starting with GPT-4o. Older models like `gpt-4-turbo` and earlier may use [JSON mode](https://platform.openai.com/docs/guides/structured-outputs?api-mode=responses#json-mode) instead.

## When to use Structured Outputs via function calling vs via text.format

Structured Outputs is available in two forms in the OpenAI API:

1. When using [function calling](https://platform.openai.com/docs/guides/function-calling)
2. When using a `json_schema` response format

Function calling is useful when you are building an application that bridges the models and functionality of your application.

For example, you can give the model access to functions that query a database in order to build an AI assistant that can help users with their orders, or functions that can interact with the UI.

Conversely, Structured Outputs via `response_format` are more suitable when you want to indicate a structured schema for use when the model responds to the user, rather than when the model calls a tool.

For example, if you are building a math tutoring application, you might want the assistant to respond to your user using a specific JSON Schema so that you can generate a UI that displays different parts of the model's output in distinct ways.

Put simply:

- If you are connecting the model to tools, functions, data, etc. in your system, then you should use function calling
- If you want to structure the model's output when it responds to the user, then you should use a structured `text.format`

The remainder of this guide will focus on non-function calling use cases in the Responses API. To learn more about how to use Structured Outputs with function calling, check out the [Function Calling](https://platform.openai.com/docs/guides/function-calling#function-calling-with-structured-outputs) guide.

### Structured Outputs vs JSON mode

Structured Outputs is the evolution of [JSON mode](https://platform.openai.com/docs/guides/structured-outputs?api-mode=responses#json-mode). While both ensure valid JSON is produced, only Structured Outputs ensure schema adherance. Both Structured Outputs and JSON mode are supported in the Responses API,Chat Completions API, Assistants API, Fine-tuning API and Batch API.

We recommend always using Structured Outputs instead of JSON mode when possible.

However, Structured Outputs with `response_format: {type: "json_schema", ...}` is only supported with the `gpt-4o-mini`, `gpt-4o-mini-2024-07-18`, and `gpt-4o-2024-08-06` model snapshots and later.

|  | Structured Outputs | JSON Mode |
| --- | --- | --- |
| **Outputs valid JSON** | Yes | Yes |
| **Adheres to schema** | Yes (see [supported schemas](https://platform.openai.com/docs/guides/structured-outputs?api-mode=responses#supported-schemas)) | No |
| **Compatible models** | `gpt-4o-mini`, `gpt-4o-2024-08-06`, and later | `gpt-3.5-turbo`, `gpt-4-*` and `gpt-4o-*` models |
| **Enabling** | `text: { format: { type: "json_schema", "strict": true, "schema": ... } }` | `text: { format: { type: "json_object" } }` |

## Examples

### Chain of thought

You can ask the model to output an answer in a structured, step-by-step way, to guide the user through the solution.

Structured Outputs for chain-of-thought math tutoring

```python
from openai import OpenAI
from pydantic import BaseModel

client = OpenAI()

class Step(BaseModel):
    explanation: str
    output: str

class MathReasoning(BaseModel):
    steps: list[Step]
    final_answer: str

response = client.responses.parse(
    model="gpt-4o-2024-08-06",
    input=[\
        {\
            "role": "system",\
            "content": "You are a helpful math tutor. Guide the user through the solution step by step.",\
        },\
        {"role": "user", "content": "how can I solve 8x + 7 = -23"},\
    ],
    text_format=MathReasoning,
)

math_reasoning = response.output_parsed
```

#### Example response

```json
{
  "steps": [\
    {\
      "explanation": "Start with the equation 8x + 7 = -23.",\
      "output": "8x + 7 = -23"\
    },\
    {\
      "explanation": "Subtract 7 from both sides to isolate the term with the variable.",\
      "output": "8x = -23 - 7"\
    },\
    {\
      "explanation": "Simplify the right side of the equation.",\
      "output": "8x = -30"\
    },\
    {\
      "explanation": "Divide both sides by 8 to solve for x.",\
      "output": "x = -30 / 8"\
    },\
    {\
      "explanation": "Simplify the fraction.",\
      "output": "x = -15 / 4"\
    }\
  ],
  "final_answer": "x = -15 / 4"
}
```

### Structured data extraction

You can define structured fields to extract from unstructured input data, such as research papers.

Extracting data from research papers using Structured Outputs

```python
from openai import OpenAI
from pydantic import BaseModel

client = OpenAI()

class ResearchPaperExtraction(BaseModel):
    title: str
    authors: list[str]
    abstract: str
    keywords: list[str]

response = client.responses.parse(
    model="gpt-4o-2024-08-06",
    input=[\
        {\
            "role": "system",\
            "content": "You are an expert at structured data extraction. You will be given unstructured text from a research paper and should convert it into the given structure.",\
        },\
        {"role": "user", "content": "..."},\
    ],
    text_format=ResearchPaperExtraction,
)

research_paper = response.output_parsed
```

#### Example response

```json
{
  "title": "Application of Quantum Algorithms in Interstellar Navigation: A New Frontier",
  "authors": [\
    "Dr. Stella Voyager",\
    "Dr. Nova Star",\
    "Dr. Lyra Hunter"\
  ],
  "abstract": "This paper investigates the utilization of quantum algorithms to improve interstellar navigation systems. By leveraging quantum superposition and entanglement, our proposed navigation system can calculate optimal travel paths through space-time anomalies more efficiently than classical methods. Experimental simulations suggest a significant reduction in travel time and fuel consumption for interstellar missions.",
  "keywords": [\
    "Quantum algorithms",\
    "interstellar navigation",\
    "space-time anomalies",\
    "quantum superposition",\
    "quantum entanglement",\
    "space travel"\
  ]
}
```

### UI Generation

You can generate valid HTML by representing it as recursive data structures with constraints, like enums.

Generating HTML using Structured Outputs

```python
from enum import Enum
from typing import List

from openai import OpenAI
from pydantic import BaseModel

client = OpenAI()

class UIType(str, Enum):
    div = "div"
    button = "button"
    header = "header"
    section = "section"
    field = "field"
    form = "form"

class Attribute(BaseModel):
    name: str
    value: str

class UI(BaseModel):
    type: UIType
    label: str
    children: List["UI"]
    attributes: List[Attribute]

UI.model_rebuild()  # This is required to enable recursive types

class Response(BaseModel):
    ui: UI

response = client.responses.parse(
    model="gpt-4o-2024-08-06",
    input=[\
        {\
            "role": "system",\
            "content": "You are a UI generator AI. Convert the user input into a UI.",\
        },\
        {"role": "user", "content": "Make a User Profile Form"},\
    ],
    text_format=Response,
)

ui = response.output_parsed
```

#### Example response

```json
{
  "type": "form",
  "label": "User Profile Form",
  "children": [\
    {\
      "type": "div",\
      "label": "",\
      "children": [\
        {\
          "type": "field",\
          "label": "First Name",\
          "children": [],\
          "attributes": [\
            {\
              "name": "type",\
              "value": "text"\
            },\
            {\
              "name": "name",\
              "value": "firstName"\
            },\
            {\
              "name": "placeholder",\
              "value": "Enter your first name"\
            }\
          ]\
        },\
        {\
          "type": "field",\
          "label": "Last Name",\
          "children": [],\
          "attributes": [\
            {\
              "name": "type",\
              "value": "text"\
            },\
            {\
              "name": "name",\
              "value": "lastName"\
            },\
            {\
              "name": "placeholder",\
              "value": "Enter your last name"\
            }\
          ]\
        }\
      ],\
      "attributes": []\
    },\
    {\
      "type": "button",\
      "label": "Submit",\
      "children": [],\
      "attributes": [\
        {\
          "name": "type",\
          "value": "submit"\
        }\
      ]\
    }\
  ],
  "attributes": [\
    {\
      "name": "method",\
      "value": "post"\
    },\
    {\
      "name": "action",\
      "value": "/submit-profile"\
    }\
  ]
}
```

### Moderation

You can classify inputs on multiple categories, which is a common way of doing moderation.

Moderation using Structured Outputs

```python
from enum import Enum
from typing import Optional

from openai import OpenAI
from pydantic import BaseModel

client = OpenAI()

class Category(str, Enum):
    violence = "violence"
    sexual = "sexual"
    self_harm = "self_harm"

class ContentCompliance(BaseModel):
    is_violating: bool
    category: Optional[Category]
    explanation_if_violating: Optional[str]

response = client.responses.parse(
    model="gpt-4o-2024-08-06",
    input=[\
        {\
            "role": "system",\
            "content": "Determine if the user input violates specific guidelines and explain if they do.",\
        },\
        {"role": "user", "content": "How do I prepare for a job interview?"},\
    ],
    text_format=ContentCompliance,
)

compliance = response.output_parsed
```

#### Example response

```json
{
  "is_violating": false,
  "category": null,
  "explanation_if_violating": null
}
```

## How to use Structured Outputs with text.format

Step 1: Define your schema

First you must design the JSON Schema that the model should be constrained to follow. See the [examples](https://platform.openai.com/docs/guides/structured-outputs#examples) at the top of this guide for reference.

While Structured Outputs supports much of JSON Schema, some features are unavailable either for performance or technical reasons. See [here](https://platform.openai.com/docs/guides/structured-outputs#supported-schemas) for more details.

#### Tips for your JSON Schema

To maximize the quality of model generations, we recommend the following:

- Name keys clearly and intuitively
- Create clear titles and descriptions for important keys in your structure
- Create and use evals to determine the structure that works best for your use case

Step 2: Supply your schema in the API call

To use Structured Outputs, simply specify

```json
text: { format: { type: "json_schema", "strict": true, "schema": … } }
```

For example:

```python
response = client.responses.create(
    model="gpt-4o-2024-08-06",
    input=[\
        {"role": "system", "content": "You are a helpful math tutor. Guide the user through the solution step by step."},\
        {"role": "user", "content": "how can I solve 8x + 7 = -23"}\
    ],
    text={
        "format": {
            "type": "json_schema",
            "name": "math_response",
            "schema": {
                "type": "object",
                "properties": {
                    "steps": {
                        "type": "array",
                        "items": {
                            "type": "object",
                            "properties": {
                                "explanation": {"type": "string"},
                                "output": {"type": "string"}
                            },
                            "required": ["explanation", "output"],
                            "additionalProperties": False
                        }
                    },
                    "final_answer": {"type": "string"}
                },
                "required": ["steps", "final_answer"],
                "additionalProperties": False
            },
            "strict": True
        }
    }
)

print(response.output_text)
```

**Note:** the first request you make with any schema will have additional latency as our API processes the schema, but subsequent requests with the same schema will not have additional latency.

Step 3: Handle edge cases

In some cases, the model might not generate a valid response that matches the provided JSON schema.

This can happen in the case of a refusal, if the model refuses to answer for safety reasons, or if for example you reach a max tokens limit and the response is incomplete.

```python
try:
    response = client.responses.create(
        model="gpt-4o-2024-08-06",
        input=[\
            {\
                "role": "system",\
                "content": "You are a helpful math tutor. Guide the user through the solution step by step.",\
            },\
            {"role": "user", "content": "how can I solve 8x + 7 = -23"},\
        ],
        text={
            "format": {
                "type": "json_schema",
                "name": "math_response",
                "strict": True,
                "schema": {
                    "type": "object",
                    "properties": {
                        "steps": {
                            "type": "array",
                            "items": {
                                "type": "object",
                                "properties": {
                                    "explanation": {"type": "string"},
                                    "output": {"type": "string"},
                                },
                                "required": ["explanation", "output"],
                                "additionalProperties": False,
                            },
                        },
                        "final_answer": {"type": "string"},
                    },
                    "required": ["steps", "final_answer"],
                    "additionalProperties": False,
                },
                "strict": True,
            },
        },
    )
except Exception as e:
    # handle errors like finish_reason, refusal, content_filter, etc.
    pass
```

### Refusals with Structured Outputs

When using Structured Outputs with user-generated input, OpenAI models may occasionally refuse to fulfill the request for safety reasons. Since a refusal does not necessarily follow the schema you have supplied in `response_format`, the API response will include a new field called `refusal` to indicate that the model refused to fulfill the request.

When the `refusal` property appears in your output object, you might present the refusal in your UI, or include conditional logic in code that consumes the response to handle the case of a refused request.

```python
class Step(BaseModel):
    explanation: str
    output: str

class MathReasoning(BaseModel):
    steps: list[Step]
    final_answer: str

completion = client.chat.completions.parse(
    model="gpt-4o-2024-08-06",
    messages=[\
        {"role": "system", "content": "You are a helpful math tutor. Guide the user through the solution step by step."},\
        {"role": "user", "content": "how can I solve 8x + 7 = -23"}\
    ],
    response_format=MathReasoning,
)

math_reasoning = completion.choices[0].message

# If the model refuses to respond, you will get a refusal message
if (math_reasoning.refusal):
    print(math_reasoning.refusal)
else:
    print(math_reasoning.parsed)
```

The API response from a refusal will look something like this:

```json
{
  "id": "resp_1234567890",
  "object": "response",
  "created_at": 1721596428,
  "status": "completed",
  "error": null,
  "incomplete_details": null,
  "input": [],
  "instructions": null,
  "max_output_tokens": null,
  "model": "gpt-4o-2024-08-06",
  "output": [{\
    "id": "msg_1234567890",\
    "type": "message",\
    "role": "assistant",\
    "content": [\
      {\
        "type": "refusal",\
        "refusal": "I'm sorry, I cannot assist with that request."\
      }\
    ]\
  }],
  "usage": {
    "input_tokens": 81,
    "output_tokens": 11,
    "total_tokens": 92,
    "output_tokens_details": {
      "reasoning_tokens": 0,
    }
  },
}
```

### Tips and best practices

#### Handling user-generated input

If your application is using user-generated input, make sure your prompt includes instructions on how to handle situations where the input cannot result in a valid response.

The model will always try to adhere to the provided schema, which can result in hallucinations if the input is completely unrelated to the schema.

You could include language in your prompt to specify that you want to return empty parameters, or a specific sentence, if the model detects that the input is incompatible with the task.

#### Handling mistakes

Structured Outputs can still contain mistakes. If you see mistakes, try adjusting your instructions, providing examples in the system instructions, or splitting tasks into simpler subtasks. Refer to the [prompt engineering guide](https://platform.openai.com/docs/guides/prompt-engineering) for more guidance on how to tweak your inputs.

#### Avoid JSON schema divergence

To prevent your JSON Schema and corresponding types in your programming language from diverging, we strongly recommend using the native Pydantic/zod sdk support.

If you prefer to specify the JSON schema directly, you could add CI rules that flag when either the JSON schema or underlying data objects are edited, or add a CI step that auto-generates the JSON Schema from type definitions (or vice-versa).

## Streaming

You can use streaming to process model responses or function call arguments as they are being generated, and parse them as structured data.

That way, you don't have to wait for the entire response to complete before handling it.
This is particularly useful if you would like to display JSON fields one by one, or handle function call arguments as soon as they are available.

We recommend relying on the SDKs to handle streaming with Structured Outputs.

```python
from typing import List

from openai import OpenAI
from pydantic import BaseModel

class EntitiesModel(BaseModel):
    attributes: List[str]
    colors: List[str]
    animals: List[str]

client = OpenAI()

with client.responses.stream(
    model="gpt-4.1",
    input=[\
        {"role": "system", "content": "Extract entities from the input text"},\
        {\
            "role": "user",\
            "content": "The quick brown fox jumps over the lazy dog with piercing blue eyes",\
        },\
    ],
    text_format=EntitiesModel,
) as stream:
    for event in stream:
        if event.type == "response.refusal.delta":
            print(event.delta, end="")
        elif event.type == "response.output_text.delta":
            print(event.delta, end="")
        elif event.type == "response.error":
            print(event.error, end="")
        elif event.type == "response.completed":
            print("Completed")
            # print(event.response.output)

    final_response = stream.get_final_response()
    print(final_response)
```

## Supported schemas

Structured Outputs supports a subset of the [JSON Schema](https://json-schema.org/docs) language.

#### Supported types

The following types are supported for Structured Outputs:

- String
- Number
- Boolean
- Integer
- Object
- Array
- Enum
- anyOf

#### Supported properties

In addition to specifying the type of a property, you can specify a selection of additional constraints:

**Supported `string` properties:**

- `pattern` — A regular expression that the string must match.
- `format` — Predefined formats for strings. Currently supported:

  - `date-time`
  - `time`
  - `date`
  - `duration`
  - `email`
  - `hostname`
  - `ipv4`
  - `ipv6`
  - `uuid`

**Supported `number` properties:**

- `multipleOf` — The number must be a multiple of this value.
- `maximum` — The number must be less than or equal to this value.
- `exclusiveMaximum` — The number must be less than this value.
- `minimum` — The number must be greater than or equal to this value.
- `exclusiveMinimum` — The number must be greater than this value.

**Supported `array` properties:**

- `minItems` — The array must have at least this many items.
- `maxItems` — The array must have at most this many items.

Here are some examples on how you can use these type restrictions:

String Restrictions

```json
{
    "name": "user_data",
    "strict": true,
    "schema": {
        "type": "object",
        "properties": {
            "name": {
                "type": "string",
                "description": "The name of the user"
            },
            "username": {
                "type": "string",
                "description": "The username of the user. Must start with @",
                "pattern": "^@[a-zA-Z0-9_]+$"
            },
            "email": {
                "type": "string",
                "description": "The email of the user",
                "format": "email"
            }
        },
        "additionalProperties": false,
        "required": [\
            "name", "username", "email"\
        ]
    }
}
```

Number Restrictions

```json
{
    "name": "weather_data",
    "strict": true,
    "schema": {
        "type": "object",
        "properties": {
            "location": {
                "type": "string",
                "description": "The location to get the weather for"
            },
            "unit": {
                "type": ["string", "null"],
                "description": "The unit to return the temperature in",
                "enum": ["F", "C"]
            },
            "value": {
                "type": "number",
                "description": "The actual temperature value in the location",
                "minimum": -130,
                "maximum": 130
            }
        },
        "additionalProperties": false,
        "required": [\
            "location", "unit", "value"\
        ]
    }
}
```

#### Root objects must not be `anyOf` and must be an object

Note that the root level object of a schema must be an object, and not use `anyOf`. A pattern that appears in Zod (as one example) is using a discriminated union, which produces an `anyOf` at the top level. So code such as the following won't work:

```javascript
import { z } from 'zod';
import { zodResponseFormat } from 'openai/helpers/zod';

const BaseResponseSchema = z.object({/* ... */});
const UnsuccessfulResponseSchema = z.object({/* ... */});

const finalSchema = z.discriminatedUnion('status', [\
BaseResponseSchema,\
UnsuccessfulResponseSchema,\
]);

// Invalid JSON Schema for Structured Outputs
const json = zodResponseFormat(finalSchema, 'final_schema');
```

#### All fields must be `required`

To use Structured Outputs, all fields or function parameters must be specified as `required`.

```json
{
    "name": "get_weather",
    "description": "Fetches the weather in the given location",
    "strict": true,
    "parameters": {
        "type": "object",
        "properties": {
            "location": {
                "type": "string",
                "description": "The location to get the weather for"
            },
            "unit": {
                "type": "string",
                "description": "The unit to return the temperature in",
                "enum": ["F", "C"]
            }
        },
        "additionalProperties": false,
        "required": ["location", "unit"]
    }
}
```

Although all fields must be required (and the model will return a value for each parameter), it is possible to emulate an optional parameter by using a union type with `null`.

```json
{
    "name": "get_weather",
    "description": "Fetches the weather in the given location",
    "strict": true,
    "parameters": {
        "type": "object",
        "properties": {
            "location": {
                "type": "string",
                "description": "The location to get the weather for"
            },
            "unit": {
                "type": ["string", "null"],
                "description": "The unit to return the temperature in",
                "enum": ["F", "C"]
            }
        },
        "additionalProperties": false,
        "required": [\
            "location", "unit"\
        ]
    }
}
```

#### Objects have limitations on nesting depth and size

A schema may have up to 5000 object properties total, with up to 5 levels of nesting.

#### Limitations on total string size

In a schema, total string length of all property names, definition names, enum values, and const values cannot exceed 120,000 characters.

#### Limitations on enum size

A schema may have up to 1000 enum values across all enum properties.

For a single enum property with string values, the total string length of all enum values cannot exceed 15,000 characters when there are more than 250 enum values.

#### `additionalProperties: false` must always be set in objects

`additionalProperties` controls whether it is allowable for an object to contain additional keys / values that were not defined in the JSON Schema.

Structured Outputs only supports generating specified keys / values, so we require developers to set `additionalProperties: false` to opt into Structured Outputs.

```json
{
    "name": "get_weather",
    "description": "Fetches the weather in the given location",
    "strict": true,
    "schema": {
        "type": "object",
        "properties": {
            "location": {
                "type": "string",
                "description": "The location to get the weather for"
            },
            "unit": {
                "type": "string",
                "description": "The unit to return the temperature in",
                "enum": ["F", "C"]
            }
        },
        "additionalProperties": false,
        "required": [\
            "location", "unit"\
        ]
    }
}
```

#### Key ordering

When using Structured Outputs, outputs will be produced in the same order as the ordering of keys in the schema.

#### Some type-specific keywords are not yet supported

- **Composition:** `allOf`, `not`, `dependentRequired`, `dependentSchemas`, `if`, `then`, `else`

For fine-tuned models, we additionally do not support the following:

- **For strings:** `minLength`, `maxLength`, `pattern`, `format`
- **For numbers:** `minimum`, `maximum`, `multipleOf`
- **For objects:** `patternProperties`
- **For arrays:** `minItems`, `maxItems`

If you turn on Structured Outputs by supplying `strict: true` and call the API with an unsupported JSON Schema, you will receive an error.

#### For `anyOf`, the nested schemas must each be a valid JSON Schema per this subset

Here's an example supported anyOf schema:

```json
{
    "type": "object",
    "properties": {
        "item": {
            "anyOf": [\
                {\
                    "type": "object",\
                    "description": "The user object to insert into the database",\
                    "properties": {\
                        "name": {\
                            "type": "string",\
                            "description": "The name of the user"\
                        },\
                        "age": {\
                            "type": "number",\
                            "description": "The age of the user"\
                        }\
                    },\
                    "additionalProperties": false,\
                    "required": [\
                        "name",\
                        "age"\
                    ]\
                },\
                {\
                    "type": "object",\
                    "description": "The address object to insert into the database",\
                    "properties": {\
                        "number": {\
                            "type": "string",\
                            "description": "The number of the address. Eg. for 123 main st, this would be 123"\
                        },\
                        "street": {\
                            "type": "string",\
                            "description": "The street name. Eg. for 123 main st, this would be main st"\
                        },\
                        "city": {\
                            "type": "string",\
                            "description": "The city of the address"\
                        }\
                    },\
                    "additionalProperties": false,\
                    "required": [\
                        "number",\
                        "street",\
                        "city"\
                    ]\
                }\
            ]
        }
    },
    "additionalProperties": false,
    "required": [\
        "item"\
    ]
}
```

#### Definitions are supported

You can use definitions to define subschemas which are referenced throughout your schema. The following is a simple example.

```json
{
    "type": "object",
    "properties": {
        "steps": {
            "type": "array",
            "items": {
                "$ref": "#/$defs/step"
            }
        },
        "final_answer": {
            "type": "string"
        }
    },
    "$defs": {
        "step": {
            "type": "object",
            "properties": {
                "explanation": {
                    "type": "string"
                },
                "output": {
                    "type": "string"
                }
            },
            "required": [\
                "explanation",\
                "output"\
            ],
            "additionalProperties": false
        }
    },
    "required": [\
        "steps",\
        "final_answer"\
    ],
    "additionalProperties": false
}
```

#### Recursive schemas are supported

Sample recursive schema using `#` to indicate root recursion.

```json
{
    "name": "ui",
    "description": "Dynamically generated UI",
    "strict": true,
    "schema": {
        "type": "object",
        "properties": {
            "type": {
                "type": "string",
                "description": "The type of the UI component",
                "enum": ["div", "button", "header", "section", "field", "form"]
            },
            "label": {
                "type": "string",
                "description": "The label of the UI component, used for buttons or form fields"
            },
            "children": {
                "type": "array",
                "description": "Nested UI components",
                "items": {
                    "$ref": "#"
                }
            },
            "attributes": {
                "type": "array",
                "description": "Arbitrary attributes for the UI component, suitable for any element",
                "items": {
                    "type": "object",
                    "properties": {
                        "name": {
                            "type": "string",
                            "description": "The name of the attribute, for example onClick or className"
                        },
                        "value": {
                            "type": "string",
                            "description": "The value of the attribute"
                        }
                    },
                    "additionalProperties": false,
                    "required": ["name", "value"]
                }
            }
        },
        "required": ["type", "label", "children", "attributes"],
        "additionalProperties": false
    }
}
```

Sample recursive schema using explicit recursion:

```json
{
    "type": "object",
    "properties": {
        "linked_list": {
            "$ref": "#/$defs/linked_list_node"
        }
    },
    "$defs": {
        "linked_list_node": {
            "type": "object",
            "properties": {
                "value": {
                    "type": "number"
                },
                "next": {
                    "anyOf": [\
                        {\
                            "$ref": "#/$defs/linked_list_node"\
                        },\
                        {\
                            "type": "null"\
                        }\
                    ]
                }
            },
            "additionalProperties": false,
            "required": [\
                "next",\
                "value"\
            ]
        }
    },
    "additionalProperties": false,
    "required": [\
        "linked_list"\
    ]
}
```

## JSON mode

JSON mode is a more basic version of the Structured Outputs feature. While JSON mode ensures that model output is valid JSON, Structured Outputs reliably matches the model's output to the schema you specify.
We recommend you use Structured Outputs if it is supported for your use case.

When JSON mode is turned on, the model's output is ensured to be valid JSON, except for in some edge cases that you should detect and handle appropriately.

To turn on JSON mode with the Responses API you can set the `text.format` to `{ "type": "json_object" }`. If you are using function calling, JSON mode is always turned on.

Important notes:

- When using JSON mode, you must always instruct the model to produce JSON via some message in the conversation, for example via your system message. If you don't include an explicit instruction to generate JSON, the model may generate an unending stream of whitespace and the request may run continually until it reaches the token limit. To help ensure you don't forget, the API will throw an error if the string "JSON" does not appear somewhere in the context.
- JSON mode will not guarantee the output matches any specific schema, only that it is valid and parses without errors. You should use Structured Outputs to ensure it matches your schema, or if that is not possible, you should use a validation library and potentially retries to ensure that the output matches your desired schema.
- Your application must detect and handle the edge cases that can result in the model output not being a complete JSON object (see below)

Handling edge cases

```python
we_did_not_specify_stop_tokens = True

try:
    response = client.responses.create(
        model="gpt-3.5-turbo-0125",
        input=[\
            {"role": "system", "content": "You are a helpful assistant designed to output JSON."},\
            {"role": "user", "content": "Who won the world series in 2020? Please respond in the format {winner: ...}"}\
        ],
        text={"format": {"type": "json_object"}}
    )

    # Check if the conversation was too long for the context window, resulting in incomplete JSON
    if response.status == "incomplete" and response.incomplete_details.reason == "max_output_tokens":
        # your code should handle this error case
        pass

    # Check if the OpenAI safety system refused the request and generated a refusal instead
    if response.output[0].content[0].type == "refusal":
        # your code should handle this error case
        # In this case, the .content field will contain the explanation (if any) that the model generated for why it is refusing
        print(response.output[0].content[0]["refusal"])

    # Check if the model's output included restricted content, so the generation of JSON was halted and may be partial
    if response.status == "incomplete" and response.incomplete_details.reason == "content_filter":
        # your code should handle this error case
        pass

    if response.status == "completed":
        # In this case the model has either successfully finished generating the JSON object according to your schema, or the model generated one of the tokens you provided as a "stop token"

        if we_did_not_specify_stop_tokens:
            # If you didn't specify any stop tokens, then the generation is complete and the content key will contain the serialized JSON object
            # This will parse successfully and should now contain  "{"winner": "Los Angeles Dodgers"}"
            print(response.output_text)
        else:
            # Check if the response.output_text ends with one of your stop tokens and handle appropriately
            pass
except Exception as e:
    # Your code should handle errors here, for example a network error calling the API
    print(e)
```]]></content>
    </additional_source>
    <additional_source>
      <title><![CDATA[yaml-vs-json-which-is-more-efficient-for-language-models-by-]]></title>
      <content><![CDATA[# YAML vs. JSON: Which Is More Efficient for Language Models?

https://miro.medium.com/v2/resize:fit:700/1*2d8fldvpjiS1paVntoBPYQ.png

Illustration by author. Supercharge your language models: Slash costs by 50% and boost response time 2.5X by switching from JSON to YAML!

In early 2020, I had the unique opportunity to gain access to OpenAI’s GPT-3, a cutting-edge language model that seemed to possess almost magical capabilities. As I delved deeper into the technology, I discovered numerous ways to leverage its power in my personal and professional life, utilizing it as a life hack to expedite tasks and uncover novel concepts.

I quickly realized that working with GPT was not as intuitive as I had initially anticipated. Despite the introduction of ChatGPT, which aimed to bridge the gap and make this groundbreaking technology accessible to a wider audience, users still need a comprehensive understanding of how to maximize the potential of this innovative tool.

Over the past few months, I have conversed with numerous engineers and entrepreneurs who incorporate language models into their services and products. A recurring theme I observed was the attempt to solicit responses from language models in a JSON format. However, I discovered considerable consequences on output quality due to wording, prompt structure, and instructions. These factors can significantly impact a user’s ability to control and fine-tune the output generated by GPT and similar language models.

My intuition from my experiments was that JSON wasn’t an efficient format to ask from a language model for various reasons:

1. Syntax issues: JSON is a sensitive format for quotes, commas, and other reserved symbols, which makes it difficult for language models to follow instructions consistently.
2. Prefix and suffix in the response: Language models tend to wrap the output with unnecessary texts.
3. Excessive costs: JSON format requires opening and closing tags, producing excessive text characters, and increasing the overall tokens and your costs.
4. Excessive execution time: Using language models as part of your application, especially if it’s customer-facing, can be very sensitive to response time. Due to all of the above points, JSON can result in slow and flaky results, which can impact your user experience.

# Empirical Experiments

After sharing my advice about JSON vs YAML a few times, I conducted an empirical study to prove my assumptions.

In order to test how GPT efficiency when it parses text of the same content, I asked GPT to generate a simple list of month names in JSON format and compared it to YAML format and compared using the [Tokenizer tool by OpenAI](https://platform.openai.com/tokenizer) (more about tokens later). This simple example demonstrated about a 50% reduction in costs when using YAML:

https://miro.medium.com/v2/resize:fit:1000/1*Bo5esVY0YsMBQDwURq_YBw.png

The YAML approach here saved 48% in tokens and 25% in characters.

It is clear that YAML is significantly more cost/time-effective than JSON in those cases.

# Deeper Look

Now, let’s look deeper into bigger completion performance time and the penalty for parsing the output as JSON or YAML.

For parsing, I suggest using the [js-yaml](https://www.npmjs.com/package/js-yaml) package for parsing the output into JS objects and [PyYAML](https://pyyaml.org/wiki/PyYAMLDocumentation) for Python.

I’ve used this prompt to generate a somewhat deterministic test set with a predefined structure and measured results on various completion sizes (x5, x10, and x45, which consumed the whole tokens window):

`Generate basic demographic info about 10 top countries (by population). Should include those fields: country, population, capital, official_language, currency, area_km, gdp_usd, under the root "countries". Output in {{format}} format, reduce other prose.`(format: YAML\|JSON)

Here’s the results I got:

https://miro.medium.com/v2/resize:fit:700/1*_V4DYpfWgStvme6joDbBSg.png

YAML tended to be faster and had a smaller footprint, but the gap degrades when getting closer to max token limit

https://miro.medium.com/v2/resize:fit:700/1*vtMqARGmCh--YIKwI9tvSw.png

Comparing YAML diffs over response length (left) and runtime/tokens (right)

The final [JSON](https://gist.github.com/Livshitz/aa30b7ed96f0310c22f104202c7df776) and [YAML](https://gist.github.com/Livshitz/878f1a596df9eabcd41897cb10eee78a) outputs can be found in the GH gist, accordingly.

If you were using this prompt on the scale of 1 million requests per month using JSON and GPT-4, switching to YAML would result in saving 190 tokens and would save you $11,400 (based on the pricing on this paper’s day) per month with this simple trick.

# Why Does This Happen?

To understand why this happens, we need to understand how language models process text into tokens and tokens back into text.

Language models are machine learning models, and machines don’t really understand “words” as a whole text, so words have to be encoded into a representation that machines can process. Each word could be represented by a unique ID, which is a machine-friendly representation. This is usually referred to as “Index-Based Encoding.” Though it is somewhat inefficient as words with multiple variations like “fun,” “funny,” and “funniest” are semantically close, they will be represented in totally different and distinct IDs.

In 1994, Philip Gage introduced a new data compression technique that replaces common pairs of consecutive bytes with a byte that does not appear in that data. In other words, by splitting words into parts, we could yet represent words by unique token IDs and still store and retrieve them efficiently. This technique is called Byte Pair Encoding (BPE) and is used as subword tokenization. This technique has become the foundation for models such as [BERT](https://github.com/google-research/bert), [GPT](https://openai.com/blog/better-language-models/) models, [RoBERTa](https://arxiv.org/abs/1907.11692), and more.

To properly handle the token “est,” for example, in the cases of “estimate” and “highest” (“est” appears at the beginning or the end but has different meanings), BPE attempts to combine pairs of two bytes or parts of words.

More on how GPT-3 tokens work is described well by Piotr Grudzien [here](https://blog.quickchat.ai/post/tokens-entropy-question/).

Using the [Tokenizer tool by OpenAI](https://platform.openai.com/tokenizer), it can be demonstrated as follows:

https://miro.medium.com/v2/resize:fit:700/1*BytpkdynzqJoZPNY5lq98Q.png

BPE breaking words during subword tokenization

When this concept comes with single characters, such as curly brackets, we see something interesting:

https://miro.medium.com/v2/resize:fit:700/1*-SyvXsNMBxAJHyg_xT5GYw.png

Although we see the same character, BPE decides to categorize them differently

This fundamental behavior alone plays well in how YAML is structured (line breaks and spaces as special characters, without the need to open and close curly brackets, quotes, and commas) compared to JSON, which requires opening and closing tags. Opening and closing tags impact the underlying representation in tokens, eventually causing extra LLM spins and might impact the general ability to follow instructions. So, not only does this save characters, but it also generally helps language models represent words with token IDs that are more common in their BPE vocabulary.

https://miro.medium.com/v2/resize:fit:1000/1*0cYldFGYCDl7mWRUZw2iuw.png

In comparing JSON and YAML, it is evident that the distribution of tokens in JSON is non-consistent, whereas YAML presents a more organized structure. This theoretically enhances the LLM’s capacity to allocate more spins on content rather than focusing on structural aspects, consequently improving the overall output quality.

In conclusion, while JSON is generally faster to parse and consume than YAML, YAML is significantly more cost/time-efficient than JSON and can help language models produce precisely the same content faster and cheaper. Essentially, it is more efficient to request YAML, and convert the result to JSON on the code-side, instead of requesting JSON directly.

It is worth mentioning that the potential compromise might be the strictness of JSON for some formats (numbers could be printed as strings, surrounded with quotes). This can be solved by providing schema or post-parsing the fields into the right data type. Regardless, it could be good practice anyway to enforce data type conversions on code-side.

# **Appendix- Chain-of-Thought using YAML comments:**

In addition to its advantages in speed and cost, YAML offers another significant benefit over JSON — the capacity to include comments.

Take this classic test case from “[Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903)” ( [Wei et al. ,2022](https://arxiv.org/abs/2201.11903)):

https://miro.medium.com/v2/resize:fit:700/0*kioxp_e0umir87iU

Imagine you want this output in machine-readable format.

With JSON and no CoT, you’ll get bad results:

https://miro.medium.com/v2/resize:fit:700/1*FvaohbxdpfAFgmDR6rQlQQ.png

No CoT, JSON return, GPT-3.5. Wrong answer, should return 900030

However, by utilizing YAML, you can define a format that accommodates the CoT within comments while presenting the final answer in the assigned key, ultimately producing a parseable output:

https://miro.medium.com/v2/resize:fit:700/1*-PxoVjKFNxO7CCiGe6HwYQ.png

CoT with YAML comments, GPT-3.5, CORRECT answer]]></content>
    </additional_source>
  </additional_sources>
</research>