{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30357c43",
   "metadata": {},
   "source": [
    "# Lesson 18: Research Loop with Query Generation and Human-in-the-Loop Feedback\n",
    "\n",
    "In this lesson, we'll dive deep into the research loop that forms the heart of our research agent's intelligence. We'll explore how the agent generates relevant search queries based on content gathered in previous lessons and uses Perplexity to expand its knowledge base. We'll see also how to integrate human feedback into the research workflow. This creates a powerful human-in-the-loop system that allows users to guide the research direction while leveraging the agent's analytical capabilities.\n",
    "\n",
    "Learning Objectives:\n",
    "- Learn how to generate contextual research queries using LLMs and structured outputs\n",
    "- Understand how to integrate external research services like Perplexity for comprehensive web search\n",
    "- Implement human-in-the-loop feedback mechanisms in agentic workflows\n",
    "- Explore the iterative design process behind building effective AI research agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4667a701",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "First, we define some standard Magic Python commands to autoreload Python packages whenever they change:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4ee337a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2aa0e9",
   "metadata": {},
   "source": [
    "### Set Up Python Environment\n",
    "\n",
    "To set up your Python virtual environment using `uv` and load it into the Notebook, follow the step-by-step instructions from the `Course Admin` lesson from the beginning of the course.\n",
    "\n",
    "**TL/DR:** Be sure the correct kernel pointing to your `uv` virtual environment is selected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7dd2654",
   "metadata": {},
   "source": [
    "### Configure Required APIs\n",
    "\n",
    "To run this lesson, you'll need several API keys configured:\n",
    "\n",
    "1. **Gemini API Key**, `GOOGLE_API_KEY` variable: Get your key from [Google AI Studio](https://aistudio.google.com/app/apikey).\n",
    "2. **Perplexity API Key**, `PPLX_API_KEY` variable: Get your key from [Perplexity](https://www.perplexity.ai/settings/api).\n",
    "3. **Firecrawl API Key**, `FIRECRAWL_API_KEY` variable: Get your key from [Firecrawl](https://firecrawl.dev/). They have a free tier that allows you to scrape 500 pages.\n",
    "4. **Perplexity API Key**, `PPLX_API_KEY` variable: Get your key from [Perplexity](https://www.perplexity.ai/settings/api). This is required for the research loop functionality that performs web searches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d2c790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment variables loaded from `/Users/fabio/Desktop/course-ai-agents/.env`\n",
      "Environment variables loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "from utils import env\n",
    "\n",
    "env.load(required_env_vars=[\"GOOGLE_API_KEY\", \"PPLX_API_KEY\", \"FIRECRAWL_API_KEY\", \"PPLX_API_KEY\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109cda3b",
   "metadata": {},
   "source": [
    "### Import Key Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc058045",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply() # Allow nested async usage in notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586002c9",
   "metadata": {},
   "source": [
    "## 2. Understanding the Research Loop\n",
    "\n",
    "The research loop is where our agent becomes truly intelligent. After gathering initial content from guidelines (as we saw in previous lessons), the agent enters a 3-round research cycle designed to fill knowledge gaps and expand understanding.\n",
    "\n",
    "From the MCP prompt workflow, here's how the research loop works. We'll see its implementation in the next sections.\n",
    "\n",
    "```markdown\n",
    "3. Repeat the following research loop for 3 rounds:\n",
    "\n",
    "    3.1. Run the \"generate_next_queries\" tool to analyze the ARTICLE_GUIDELINE_FILE, the already-scraped guideline\n",
    "    URLs, and the existing PERPLEXITY_RESULTS_FILE. The tool identifies knowledge gaps, proposes new web-search\n",
    "    questions, and writes them - together with a short justification for each - to the NEXT_QUERIES_FILE within\n",
    "    NOVA_FOLDER.\n",
    "\n",
    "    3.2. Run the \"run_perplexity_research\" tool with the new queries. This tool executes the queries with\n",
    "    Perplexity and appends the results to the PERPLEXITY_RESULTS_FILE within NOVA_FOLDER.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6d2d65",
   "metadata": {},
   "source": [
    "## 3. The Philosophy: Plugging Into Specialized Services\n",
    "\n",
    "Before diving into the implementation, it's important to understand our architectural philosophy. Similar to our approach with web scraping using Firecrawl, we're leveraging Perplexity for web search rather than building our own solution.\n",
    "\n",
    "**Why use external services like Perplexity?**\n",
    "\n",
    "Perplexity is an AI-powered search engine that combines the capabilities of large language models with real-time web search.\n",
    "\n",
    "When there's a general problem faced by many in the industry, it's often more efficient to plug into dedicated tools rather than building every element yourself, similarly to what we did in the previous lesson with Firecrawl for scraping. Companies like Perplexity make LLM-based web search their entire business, investing heavily in:\n",
    "\n",
    "- Comprehensive source coverage across the web\n",
    "- Real-time information retrieval\n",
    "- Advanced ranking and relevance algorithms\n",
    "- Handling of dynamic content and paywalls\n",
    "- Rate limiting and API reliability\n",
    "\n",
    "This allows us to focus on the unique elements of our research agent: the intelligent query generation, human feedback integration, and workflow orchestration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296b9e71",
   "metadata": {},
   "source": [
    "## 4. Query Generation: The Brain of the Research Loop\n",
    "\n",
    "The `generate_next_queries` tool is where the magic happens. It analyzes all available context and intelligently identifies knowledge gaps to fill."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7fcdd2",
   "metadata": {},
   "source": [
    "### 4.1 Understanding the Implementation\n",
    "\n",
    "Let's examine the core implementation:\n",
    "\n",
    "Source: _mcp_server/src/tools/generate_next_queries_tool.py_\n",
    "\n",
    "```python\n",
    "async def generate_next_queries_tool(research_directory: str, n_queries: int = 5) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Generate candidate web-search queries for the next research round.\n",
    "\n",
    "    Analyzes the article guidelines, already-scraped content, and existing Perplexity\n",
    "    results to identify knowledge gaps and propose new web-search questions.\n",
    "    Each query includes a rationale explaining why it's important for the article.\n",
    "    Results are saved to next_queries.md in the research directory.\n",
    "    \"\"\"\n",
    "    # Convert to Path object\n",
    "    research_path = Path(research_directory)\n",
    "    nova_path = research_path / NOVA_FOLDER\n",
    "\n",
    "    # Gather context from the research folder\n",
    "    guidelines_path = research_path / ARTICLE_GUIDELINE_FILE\n",
    "    results_path = nova_path / PERPLEXITY_RESULTS_FILE\n",
    "    urls_from_guidelines_dir = nova_path / URLS_FROM_GUIDELINES_FOLDER\n",
    "\n",
    "    article_guidelines = read_file_safe(guidelines_path)\n",
    "    past_research = read_file_safe(results_path)\n",
    "\n",
    "    # Collect all scraped content for context\n",
    "    scraped_ctx_parts: List[str] = []\n",
    "    if urls_from_guidelines_dir.exists():\n",
    "        for md_file in sorted(urls_from_guidelines_dir.glob(f\"*{MARKDOWN_EXTENSION}\")):\n",
    "            scraped_ctx_parts.append(md_file.read_text(encoding=\"utf-8\"))\n",
    "    scraped_ctx_str = \"\\n\\n\".join(scraped_ctx_parts)\n",
    "\n",
    "    # Generate queries using LLM\n",
    "    queries_and_reasons = await generate_queries_with_reasons(\n",
    "        article_guidelines, past_research, scraped_ctx_str, n_queries=n_queries\n",
    "    )\n",
    "\n",
    "    # Write to next_queries.md (overwrite)\n",
    "    next_q_path = nova_path / NEXT_QUERIES_FILE\n",
    "    write_queries_to_file(next_q_path, queries_and_reasons)\n",
    "\n",
    "    return {\n",
    "        \"status\": \"success\",\n",
    "        \"queries_count\": len(queries_and_reasons),\n",
    "        \"queries\": queries_and_reasons,\n",
    "        \"output_path\": str(next_q_path.resolve()),\n",
    "        \"message\": f\"Successfully generated {len(queries_and_reasons)} candidate queries...\"\n",
    "    }\n",
    "```\n",
    "\n",
    "The tool gathers three types of context:\n",
    "\n",
    "1. **Article Guidelines** (`article_guidelines`): The original requirements and scope read from the `ARTICLE_GUIDELINE_FILE`. This provides the foundational understanding of what the article should cover.\n",
    "\n",
    "2. **Past Research** (`past_research`): Previous Perplexity results read from the `PERPLEXITY_RESULTS_FILE`. This ensures the LLM doesn't generate duplicate queries and can identify gaps in existing research.\n",
    "\n",
    "3. **Scraped Content** (`scraped_ctx_str`): All content from guideline URLs (web pages, GitHub repos, YouTube transcripts) concatenated together. This provides comprehensive background context that helps the LLM understand what information is already available.\n",
    "\n",
    "The LLM analyzes all three contexts simultaneously. It uses the article guidelines to understand the target scope, reviews past research to see what's already been covered, and examines scraped content to understand the existing knowledge base. This comprehensive analysis enables it to generate queries that specifically target knowledge gaps. For example, if the article guidelines mention \"error handling in AI agents\" but neither past research nor scraped content covers this topic adequately, the LLM will prioritize generating queries about error handling strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4108b36",
   "metadata": {},
   "source": [
    "### 4.2 The LLM-Powered Query Generation\n",
    "\n",
    "The actual query generation happens in the `generate_queries_with_reasons` function:\n",
    "\n",
    "Source: _mcp_server/src/app/generate_queries_handler.py_\n",
    "\n",
    "```python\n",
    "async def generate_queries_with_reasons(\n",
    "    article_guidelines: str,\n",
    "    past_research: str,\n",
    "    scraped_ctx: str,\n",
    "    n_queries: int = 5,\n",
    ") -> List[Tuple[str, str]]:\n",
    "    \"\"\"Return a list of tuples (query, reason).\"\"\"\n",
    "\n",
    "    prompt = PROMPT_GENERATE_QUERIES_AND_REASONS.format(\n",
    "        n_queries=n_queries,\n",
    "        article_guidelines=article_guidelines or \"<none>\",\n",
    "        past_research=past_research or \"<none>\",\n",
    "        scraped_ctx=scraped_ctx or \"<none>\",\n",
    "    )\n",
    "\n",
    "    chat_llm = get_chat_model(settings.query_generation_model, GeneratedQueries)\n",
    "    response = await chat_llm.ainvoke(prompt)\n",
    "\n",
    "    queries_and_reasons = [(item.question, item.reason) for item in response.queries]\n",
    "    return queries_and_reasons[:n_queries]\n",
    "```\n",
    "\n",
    "This is the prompt used for query generation. It can potentially have contexts of ~100k tokens or more:\n",
    "\n",
    "Source: _mcp_server/src/config/prompts.py_\n",
    "\n",
    "```python\n",
    "PROMPT_GENERATE_QUERIES_AND_REASONS = \"\"\"\n",
    "You are a research assistant helping to craft an article.\n",
    "\n",
    "Your task: propose {n_queries} diverse, insightful web-search questions\n",
    "that, taken **as a group**, will collect authoritative sources for the\n",
    "article **and** provide a short explanation of why each question is\n",
    "important.\n",
    "\n",
    "<article_guidelines>\n",
    "{article_guidelines}\n",
    "</article_guidelines>\n",
    "\n",
    "<past_research>\n",
    "{past_research}\n",
    "</past_research>\n",
    "\n",
    "<scraped_context>\n",
    "{scraped_ctx}\n",
    "</scraped_context>\n",
    "\n",
    "Guidelines for the set of queries:\n",
    "â€¢ Give priority to sections/topics from the article guidelines that\n",
    "  currently lack supporting sources in <past_research> and\n",
    "  <scraped_context>.\n",
    "â€¢ Cover any remaining major sections to ensure balanced coverage.\n",
    "â€¢ Avoid duplication; each query should target a distinct aspect.\n",
    "â€¢ The web search queries should be natural language questions, not just keywords.\n",
    "\"\"\".strip()\n",
    "```\n",
    "\n",
    "This prompt explicitly instructs the LLM to identify missing information and ensures queries cover different aspects rather than duplicating.\n",
    "\n",
    "Last, the `generate_queries_with_reasons` function uses Pydantic models to ensure consistent, structured responses:\n",
    "\n",
    "Source: _mcp_server/src/models/query_models.py_\n",
    "\n",
    "```python\n",
    "class QueryAndReason(BaseModel):\n",
    "    \"\"\"A single web-search query and the reason for it.\"\"\"\n",
    "\n",
    "    question: str = Field(description=\"The web-search question to research.\")\n",
    "    reason: str = Field(description=\"The reason why this question is important for the research.\")\n",
    "\n",
    "class GeneratedQueries(BaseModel):\n",
    "    \"\"\"A list of generated web-search queries and their reasons.\"\"\"\n",
    "\n",
    "    queries: List[QueryAndReason] = Field(description=\"A list of web-search queries and their reasons.\")\n",
    "```\n",
    "\n",
    "This structured approach ensures the LLM returns exactly what we need: queries paired with clear justifications. These justifications have multiple purposes:\n",
    "1. They help the LLM understand why each query is important for the research, similarly to how chain of thought prompts work.\n",
    "2. They help us understand the LLM's thought process when generating queries, which is useful for debugging and improving the prompt. It's also useful for the human in the loop for providing useful feedback to the agent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d0051d",
   "metadata": {},
   "source": [
    "Let's test the `generate_queries_with_reasons` function to see how it works in practice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b769655",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fabio/Desktop/course-ai-agents/.venv/lib/python3.12/site-packages/pydantic/_internal/_fields.py:198: UserWarning: Field name \"schema\" in \"FewShotExampleStructuredOutputCompliance\" shadows an attribute in parent \"BaseModel\"\n",
      "  warnings.warn(\n",
      "/Users/fabio/Desktop/course-ai-agents/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What are the most common security vulnerabilities when giving LLMs access to external tools via function calling, and what are the established mitigation strategies?\n",
      "Reason: This question directly targets the 'Security considerations' section, a critical topic for production systems that is not covered in the existing research. It seeks to find authoritative sources on risks and specific, actionable solutions.\n",
      "---\n",
      "Query: How can latency be minimized and token usage be optimized when using chained or parallel function calls with large language models?\n",
      "Reason: This addresses the 'Performance optimization' section. It focuses on advanced, practical challenges like speed and cost-efficiency, especially in complex scenarios that go beyond single, basic function calls.\n",
      "---\n",
      "Query: What are robust error handling patterns for LLM function calling when external API calls fail, return unexpected data, or the LLM hallucinates a function call?\n",
      "Reason: This covers the 'Error handling strategies' section. It aims to find expert advice on building resilient systems, which is a cornerstone of production-readiness and a key theme for an 'advanced' article.\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-23 10:07:20.276\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36mlogging\u001b[0m:\u001b[36mcallHandlers\u001b[0m:\u001b[36m1762\u001b[0m | Research folder does not exist: /path/to/research_folder\n",
      "\u001b[32m2025-09-23 10:07:20.467\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlogging\u001b[0m:\u001b[36mcallHandlers\u001b[0m:\u001b[36m1762\u001b[0m | Exception in execute request:\n",
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 5\u001b[39m\n",
      "\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Update this path to your actual sample research folder\u001b[39;00m\n",
      "\u001b[32m      4\u001b[39m research_folder = \u001b[33m\"\u001b[39m\u001b[33m/path/to/research_folder\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m result = \u001b[38;5;28;01mawait\u001b[39;00m generate_next_queries_tool(research_directory=research_folder, n_queries=\u001b[32m3\u001b[39m)\n",
      "\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(result)\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/course-ai-agents/lessons/research_agent_part_2/mcp_server/src/tools/generate_next_queries_tool.py:76\u001b[39m, in \u001b[36mgenerate_next_queries_tool\u001b[39m\u001b[34m(research_directory, n_queries)\u001b[39m\n",
      "\u001b[32m     73\u001b[39m nova_path = research_path / NOVA_FOLDER\n",
      "\u001b[32m     75\u001b[39m \u001b[38;5;66;03m# Validate folders and files\u001b[39;00m\n",
      "\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m \u001b[43mvalidate_research_folder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresearch_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m     78\u001b[39m \u001b[38;5;66;03m# Create NOVA_FOLDER directory if it doesn't exist\u001b[39;00m\n",
      "\u001b[32m     79\u001b[39m nova_path.mkdir(parents=\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/course-ai-agents/lessons/research_agent_part_2/mcp_server/src/utils/file_utils.py:38\u001b[39m, in \u001b[36mvalidate_research_folder\u001b[39m\u001b[34m(research_path)\u001b[39m\n",
      "\u001b[32m     36\u001b[39m     msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mResearch folder does not exist: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresearch_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n",
      "\u001b[32m     37\u001b[39m     logger.error(msg)\n",
      "\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n",
      "\u001b[32m     40\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m research_path.is_dir():\n",
      "\u001b[32m     41\u001b[39m     msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPath is not a directory: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresearch_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n",
      "\n",
      "\u001b[31mValueError\u001b[39m: Research folder does not exist: /path/to/research_folder\n",
      "\u001b[32m2025-09-23 10:07:55.201\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlogging\u001b[0m:\u001b[36mcallHandlers\u001b[0m:\u001b[36m1762\u001b[0m | HTTP Request: POST https://api.perplexity.ai/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2025-09-23 10:08:02.384\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlogging\u001b[0m:\u001b[36mcallHandlers\u001b[0m:\u001b[36m1762\u001b[0m | HTTP Request: POST https://api.perplexity.ai/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2025-09-23 10:08:23.489\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mlogging\u001b[0m:\u001b[36mcallHandlers\u001b[0m:\u001b[36m1762\u001b[0m | File not found: /Users/fabio/Desktop/course-ai-agents/lessons/research_agent_part_2/data/sample_research_folder/.nova/perplexity_results.md\n",
      "\u001b[32m2025-09-23 10:09:03.700\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlogging\u001b[0m:\u001b[36mcallHandlers\u001b[0m:\u001b[36m1762\u001b[0m | HTTP Request: POST https://api.perplexity.ai/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2025-09-23 10:09:04.625\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlogging\u001b[0m:\u001b[36mcallHandlers\u001b[0m:\u001b[36m1762\u001b[0m | HTTP Request: POST https://api.perplexity.ai/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2025-09-23 10:09:29.683\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlogging\u001b[0m:\u001b[36mcallHandlers\u001b[0m:\u001b[36m1762\u001b[0m | ðŸ“Š Opik monitoring disabled (missing configuration)\n",
      "\u001b[32m2025-09-23 10:09:29.701\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlogging\u001b[0m:\u001b[36mcallHandlers\u001b[0m:\u001b[36m1762\u001b[0m | ðŸš€ Starting MCP client with in-memory transport...\n",
      "\u001b[32m2025-09-23 10:09:29.722\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlogging\u001b[0m:\u001b[36mcallHandlers\u001b[0m:\u001b[36m1762\u001b[0m | Processing request of type ListToolsRequest\n",
      "\u001b[32m2025-09-23 10:09:29.723\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlogging\u001b[0m:\u001b[36mcallHandlers\u001b[0m:\u001b[36m1762\u001b[0m | Processing request of type ListResourcesRequest\n",
      "\u001b[32m2025-09-23 10:09:29.724\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlogging\u001b[0m:\u001b[36mcallHandlers\u001b[0m:\u001b[36m1762\u001b[0m | Processing request of type ListPromptsRequest\n",
      "\u001b[32m2025-09-23 10:09:40.553\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlogging\u001b[0m:\u001b[36mcallHandlers\u001b[0m:\u001b[36m1762\u001b[0m | Processing request of type GetPromptRequest\n",
      "\u001b[32m2025-09-23 10:10:26.110\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlogging\u001b[0m:\u001b[36mcallHandlers\u001b[0m:\u001b[36m1762\u001b[0m | Processing request of type CallToolRequest\n",
      "\u001b[32m2025-09-23 10:10:26.119\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlogging\u001b[0m:\u001b[36mcallHandlers\u001b[0m:\u001b[36m1762\u001b[0m | Processing request of type ListToolsRequest\n",
      "\u001b[32m2025-09-23 10:10:30.206\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlogging\u001b[0m:\u001b[36mcallHandlers\u001b[0m:\u001b[36m1762\u001b[0m | Processing request of type CallToolRequest\n",
      "\u001b[32m2025-09-23 10:10:34.706\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlogging\u001b[0m:\u001b[36mcallHandlers\u001b[0m:\u001b[36m1762\u001b[0m | Processing request of type CallToolRequest\n",
      "\u001b[32m2025-09-23 10:12:00.787\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlogging\u001b[0m:\u001b[36mcallHandlers\u001b[0m:\u001b[36m1762\u001b[0m | Processing request of type CallToolRequest\n",
      "\u001b[32m2025-09-23 10:12:00.790\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mgitingest.entrypoint\u001b[0m:\u001b[36mingest_async\u001b[0m:\u001b[36m89\u001b[0m | Starting ingestion process | {\"source\":\"https://github.com/openai/openai-cookbook/blob/main/examples/gpt-5/gpt-5_prompting_guide.ipynb\"}\n",
      "\u001b[32m2025-09-23 10:12:00.790\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mgitingest.entrypoint\u001b[0m:\u001b[36mingest_async\u001b[0m:\u001b[36m98\u001b[0m | Parsing remote repository | {\"source\":\"https://github.com/openai/openai-cookbook/blob/main/examples/gpt-5/gpt-5_prompting_guide.ipynb\"}\n",
      "\u001b[32m2025-09-23 10:12:02.255\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mgitingest.clone\u001b[0m:\u001b[36mclone_repo\u001b[0m:\u001b[36m56\u001b[0m | Starting git clone operation | {\"url\":\"https://github.com/openai/openai-cookbook\",\"local_path\":\"/var/folders/6w/zxb4hbqn3p722sx3s_r_dms40000gn/T/gitingest/8638d1b3-9c17-4ddf-a92b-f2a08d756b51/openai-openai-cookbook\",\"partial_clone\":true,\"subpath\":\"/examples/gpt-5/gpt-5_prompting_guide.ipynb\",\"branch\":\"main\",\"tag\":null,\"commit\":\"5eedb60904522b9b52fc89f840c93b3a66f6d9ee\",\"include_submodules\":false}\n",
      "\u001b[32m2025-09-23 10:12:02.740\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mgitingest.clone\u001b[0m:\u001b[36mclone_repo\u001b[0m:\u001b[36m97\u001b[0m | Executing git clone command | {\"command\":\"git -c http.https://github.com/.extraheader=Authorization: Basic eC1vYXV0aC1iYXNpYzpnaHBfcGFqZHA1S2E0ZVRnOGg3bjM5ZFhvQWtCTFJQaWlPMmYyNk1G clone --single-branch --no-checkout --depth=1 --filter=blob:none --sparse https://github.com/openai/openai-cookbook <url> /var/folders/6w/zxb4hbqn3p722sx3s_r_dms40000gn/T/gitingest/8638d1b3-9c17-4ddf-a92b-f2a08d756b51/openai-openai-cookbook\"}\n",
      "\u001b[32m2025-09-23 10:12:03.507\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mgitingest.clone\u001b[0m:\u001b[36mclone_repo\u001b[0m:\u001b[36m99\u001b[0m | Git clone completed successfully\n",
      "\u001b[32m2025-09-23 10:12:03.508\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mgitingest.clone\u001b[0m:\u001b[36mclone_repo\u001b[0m:\u001b[36m103\u001b[0m | Setting up partial clone for subpath | {\"subpath\":\"/examples/gpt-5/gpt-5_prompting_guide.ipynb\"}\n",
      "\u001b[32m2025-09-23 10:12:04.292\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mgitingest.clone\u001b[0m:\u001b[36mclone_repo\u001b[0m:\u001b[36m114\u001b[0m | Checking out commit | {\"commit\":\"5eedb60904522b9b52fc89f840c93b3a66f6d9ee\"}\n",
      "\u001b[32m2025-09-23 10:12:06.281\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mgitingest.clone\u001b[0m:\u001b[36mclone_repo\u001b[0m:\u001b[36m123\u001b[0m | Git clone operation completed successfully | {\"local_path\":\"/var/folders/6w/zxb4hbqn3p722sx3s_r_dms40000gn/T/gitingest/8638d1b3-9c17-4ddf-a92b-f2a08d756b51/openai-openai-cookbook\"}\n",
      "\u001b[32m2025-09-23 10:12:06.282\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mgitingest.entrypoint\u001b[0m:\u001b[36mingest_async\u001b[0m:\u001b[36m132\u001b[0m | Repository cloned, starting file processing\n",
      "\u001b[32m2025-09-23 10:12:06.286\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mgitingest.entrypoint\u001b[0m:\u001b[36mingest_async\u001b[0m:\u001b[36m140\u001b[0m | Processing files and generating output\n",
      "\u001b[32m2025-09-23 10:12:06.286\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mgitingest.ingestion\u001b[0m:\u001b[36mingest_query\u001b[0m:\u001b[36m44\u001b[0m | Starting file ingestion | {\"slug\":\"openai-openai-cookbook\",\"subpath\":\"/examples/gpt-5/gpt-5_prompting_guide.ipynb\",\"local_path\":\"/var/folders/6w/zxb4hbqn3p722sx3s_r_dms40000gn/T/gitingest/8638d1b3-9c17-4ddf-a92b-f2a08d756b51/openai-openai-cookbook\",\"max_file_size\":10485760}\n",
      "\u001b[32m2025-09-23 10:12:06.286\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mgitingest.ingestion\u001b[0m:\u001b[36mingest_query\u001b[0m:\u001b[36m64\u001b[0m | Processing single file | {\"file_path\":\"/var/folders/6w/zxb4hbqn3p722sx3s_r_dms40000gn/T/gitingest/8638d1b3-9c17-4ddf-a92b-f2a08d756b51/openai-openai-cookbook/examples/gpt-5/gpt-5_prompting_guide.ipynb\"}\n",
      "\u001b[32m2025-09-23 10:12:06.288\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mgitingest.ingestion\u001b[0m:\u001b[36mingest_query\u001b[0m:\u001b[36m87\u001b[0m | Single file processing completed | {\"file_name\":\"gpt-5_prompting_guide.ipynb\",\"file_size\":53799}\n",
      "\u001b[32m2025-09-23 10:12:06.462\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mgitingest.entrypoint\u001b[0m:\u001b[36mingest_async\u001b[0m:\u001b[36m147\u001b[0m | Ingestion completed successfully\n",
      "\u001b[32m2025-09-23 10:12:10.121\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlogging\u001b[0m:\u001b[36mcallHandlers\u001b[0m:\u001b[36m1762\u001b[0m | Processing request of type CallToolRequest\n",
      "\u001b[32m2025-09-23 10:12:10.192\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlogging\u001b[0m:\u001b[36mcallHandlers\u001b[0m:\u001b[36m1762\u001b[0m | â³ Processing transcription request for https://www.youtube.com/watch?v=h8gMhXYAv1k, it may take a while.\n",
      "\u001b[32m2025-09-23 10:12:10.192\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlogging\u001b[0m:\u001b[36mcallHandlers\u001b[0m:\u001b[36m1762\u001b[0m | AFC is enabled with max remote calls: 10.\n",
      "\u001b[32m2025-09-23 10:12:48.672\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlogging\u001b[0m:\u001b[36mcallHandlers\u001b[0m:\u001b[36m1762\u001b[0m | âœ… Transcription for https://www.youtube.com/watch?v=h8gMhXYAv1k finished in 38.48 seconds.\n",
      "\u001b[32m2025-09-23 10:12:53.286\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlogging\u001b[0m:\u001b[36mcallHandlers\u001b[0m:\u001b[36m1762\u001b[0m | Processing request of type CallToolRequest\n",
      "\u001b[32m2025-09-23 10:12:53.289\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mlogging\u001b[0m:\u001b[36mcallHandlers\u001b[0m:\u001b[36m1762\u001b[0m | File not found: /Users/fabio/Desktop/course-ai-agents/lessons/research_agent_part_2/data/sample_research_folder/.nova/perplexity_results.md\n",
      "\u001b[32m2025-09-23 10:16:40.678\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlogging\u001b[0m:\u001b[36mcallHandlers\u001b[0m:\u001b[36m1762\u001b[0m | Processing request of type CallToolRequest\n",
      "\u001b[32m2025-09-23 10:17:10.443\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlogging\u001b[0m:\u001b[36mcallHandlers\u001b[0m:\u001b[36m1762\u001b[0m | Processing request of type CallToolRequest\n",
      "\u001b[32m2025-09-23 10:17:47.189\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlogging\u001b[0m:\u001b[36mcallHandlers\u001b[0m:\u001b[36m1762\u001b[0m | ðŸ‘‹ Terminating application...\n"
     ]
    }
   ],
   "source": [
    "from research_agent_part_2.mcp_server.src.app.generate_queries_handler import generate_queries_with_reasons\n",
    "\n",
    "# Example inputs (simplified for demonstration)\n",
    "article_guidelines = '''\n",
    "# Article: Advanced Function Calling with LLMs\n",
    "\n",
    "## Sections to cover:\n",
    "1. Error handling strategies\n",
    "2. Performance optimization\n",
    "3. Security considerations\n",
    "4. Best practices for production\n",
    "'''\n",
    "\n",
    "past_research = '''\n",
    "### Source [1]: https://example.com/basic-function-calling\n",
    "Query: What is function calling in LLMs?\n",
    "Answer: Function calling allows LLMs to invoke external tools and APIs...\n",
    "\n",
    "### Source [2]: https://example.com/simple-examples  \n",
    "Query: How to implement basic function calling?\n",
    "Answer: Basic implementation involves defining function schemas...\n",
    "'''\n",
    "\n",
    "scraped_ctx = '''\n",
    "# Function Calling Documentation\n",
    "This guide covers the fundamentals of function calling...\n",
    "[Basic examples and simple use cases already covered]\n",
    "'''\n",
    "\n",
    "# Generate queries based on this context\n",
    "queries_and_reasons = await generate_queries_with_reasons(\n",
    "    article_guidelines=article_guidelines,\n",
    "    past_research=past_research,\n",
    "    scraped_ctx=scraped_ctx,\n",
    "    n_queries=3\n",
    ")\n",
    "\n",
    "for query, reason in queries_and_reasons:\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"Reason: {reason}\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8beaf929",
   "metadata": {},
   "source": [
    "**Understanding the Output**\n",
    "\n",
    "When you run this code, you'll see output similar to this example:\n",
    "\n",
    "```\n",
    "Query: What are the most common security vulnerabilities when implementing LLM function calling and how can they be mitigated?\n",
    "Reason: This question directly targets the 'Security considerations' section of the article, which is not covered by existing research...\n",
    "\n",
    "Query: What are effective strategies for optimizing the performance and reducing latency of function calling in large language models?\n",
    "Reason: This question addresses the 'Performance optimization' section. The existing research covers basic implementation...\n",
    "\n",
    "Query: What are the best practices for error handling and ensuring reliability in production-level LLM function calling systems?\n",
    "Reason: This question is designed to gather information for the 'Error handling strategies' and 'Best practices for production' sections...\n",
    "```\n",
    "\n",
    "Observe the following:\n",
    "\n",
    "1. **Gap Analysis**: Notice how the LLM identified that the existing research only covers \"basic function calling\" and \"simple examples\", then generated queries specifically targeting the missing advanced topics (security, performance, error handling).\n",
    "\n",
    "2. **Section Mapping**: Each generated query directly maps to sections mentioned in the article guidelines that lack coverage. The LLM demonstrates sophisticated understanding by connecting article requirements to knowledge gaps.\n",
    "\n",
    "3. **Query Quality**: The queries are well-crafted, specific, and actionable. Instead of generic questions like \"What is security?\", it asks targeted questions like \"What are the most common security vulnerabilities when implementing LLM function calling?\"\n",
    "\n",
    "4. **Reasoning Transparency**: Each query comes with clear reasoning explaining why it's important and which article section it addresses. This transparency helps users understand the LLM's decision-making process.\n",
    "\n",
    "5. **Progressive Complexity**: The LLM naturally progresses from basic concepts (already covered) to advanced topics (security, performance, production practices), showing an understanding of knowledge hierarchies.\n",
    "\n",
    "This intelligent gap analysis is what makes the research loop so powerful - it doesn't just generate random queries, but strategically identifies exactly what information is missing to complete the article's objectives.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9972ef33",
   "metadata": {},
   "source": [
    "## 5. Perplexity Integration\n",
    "\n",
    "Once we have our queries, we need to execute them efficiently. The `run_perplexity_research_tool` tool handles this integration with concurrent execution and structured outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c778a8",
   "metadata": {},
   "source": [
    "### 5.1 The Research Tool Implementation\n",
    "\n",
    "The `run_perplexity_research_tool` is the orchestrator that manages the entire Perplexity research process:\n",
    "\n",
    "Source: _mcp_server/src/tools/run_perplexity_research_tool.py_\n",
    "\n",
    "```python\n",
    "async def run_perplexity_research_tool(research_directory: str, queries: List[str]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Run Perplexity research queries for the research folder.\n",
    "\n",
    "    Executes the provided queries using Perplexity's Sonar-Pro model and appends\n",
    "    the results to perplexity_results.md in the research directory. Each query\n",
    "    result includes the answer and source citations.\n",
    "    \"\"\"\n",
    "    research_path = Path(research_directory)\n",
    "    nova_path = research_path / NOVA_FOLDER\n",
    "    results_path = nova_path / PERPLEXITY_RESULTS_FILE\n",
    "\n",
    "    if not queries:\n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"message\": \"No queries provided â€“ nothing to do.\",\n",
    "            \"queries_processed\": 0,\n",
    "            \"sources_added\": 0,\n",
    "        }\n",
    "\n",
    "    # Execute all queries concurrently\n",
    "    tasks = [run_perplexity_search(query) for query in queries]\n",
    "    search_results = await asyncio.gather(*tasks)\n",
    "\n",
    "    # Process and append search results to file\n",
    "    total_sources = append_search_results_to_file(results_path, queries, search_results)\n",
    "\n",
    "    return {\n",
    "        \"status\": \"success\",\n",
    "        \"queries_processed\": len(queries),\n",
    "        \"sources_added\": total_sources,\n",
    "        \"output_path\": str(results_path.resolve()),\n",
    "        \"message\": f\"Successfully completed Perplexity research round...\"\n",
    "    }\n",
    "```\n",
    "\n",
    "**How this function works:**\n",
    "\n",
    "1. **Path Setup**: It establishes the file paths for storing results, specifically the `PERPLEXITY_RESULTS_FILE` in the `.nova` folder.\n",
    "\n",
    "2. **Concurrent Execution of Perplexity Searches**: The key performance optimization is concurrent execution. Instead of running queries sequentially (which would be slow), it creates a list of tasks using `[run_perplexity_search(query) for query in queries]` and executes them all at once with `asyncio.gather(*tasks)`.\n",
    "\n",
    "3. **Result Processing**: The `append_search_results_to_file` function processes all the search results and appends them to the persistent results file, maintaining unique source IDs across multiple research rounds.\n",
    "\n",
    "4. **Cumulative Storage**: Results from each round are appended (not overwritten) to the same file, building up a comprehensive research database over the 3 rounds.\n",
    "\n",
    "This design ensures fast execution while maintaining a complete audit trail of all research conducted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b17e9a",
   "metadata": {},
   "source": [
    "### 5.2 Structured Perplexity Responses\n",
    "\n",
    "The core Perplexity integration uses structured outputs to ensure consistent, parseable results. This is important: we're not just getting raw text responses, but structured data that our system can reliably process.\n",
    "\n",
    "Source: _mcp_server/src/app/perplexity_handler.py_\n",
    "\n",
    "```python\n",
    "class SourceAnswer(BaseModel):\n",
    "    \"\"\"A single source answer with URL and content.\"\"\"\n",
    "\n",
    "    url: str = Field(description=\"The URL of the source\")\n",
    "    answer: str = Field(description=\"The detailed answer extracted from that source\")\n",
    "\n",
    "class PerplexityResponse(BaseModel):\n",
    "    \"\"\"Structured response from Perplexity search containing multiple sources.\"\"\"\n",
    "\n",
    "    sources: List[SourceAnswer] = Field(description=\"List of sources with their answers\")\n",
    "\n",
    "async def run_perplexity_search(query: str) -> Tuple[str, Dict[int, str], Dict[int, str]]:\n",
    "    \"\"\"Run a Perplexity Sonar-Pro search and return full answer + parsed sections.\"\"\"\n",
    "\n",
    "    # run perplexity search with structured output\n",
    "    llm = get_chat_model(\"perplexity\", PerplexityResponse)\n",
    "\n",
    "    prompt = PROMPT_WEB_SEARCH.format(query=query)\n",
    "    logger.debug(f\"Searching web for: {query} â€¦\")\n",
    "\n",
    "    response = await llm.ainvoke(prompt)\n",
    "\n",
    "    # Convert structured response to the expected format\n",
    "    answer_by_source = {}\n",
    "    citations = {}\n",
    "    for i, source in enumerate(response.sources, 1):\n",
    "        answer_by_source[i] = source.answer\n",
    "        citations[i] = source.url\n",
    "\n",
    "    # Create a readable full answer string for compatibility\n",
    "    full_answer_lines = []\n",
    "    for i, source in enumerate(response.sources, 1):\n",
    "        full_answer_lines.append(f\"### [{i}]: {source.url}\")\n",
    "        full_answer_lines.append(source.answer)\n",
    "        full_answer_lines.append(\"\")\n",
    "    full_answer = \"\\n\".join(full_answer_lines)\n",
    "\n",
    "    return full_answer, answer_by_source, citations\n",
    "```\n",
    "\n",
    "When you run a Perplexity search as above, you get a structured response like this:\n",
    "\n",
    "```python\n",
    "# Example of what run_perplexity_search returns for a query about \"LLM function calling best practices\"\n",
    "\n",
    "full_answer = '''### [1]: https://openai.com/docs/guides/function-calling\n",
    "Function calling allows models to connect to external tools and APIs...\n",
    "\n",
    "### [2]: https://docs.anthropic.com/claude/docs/tool-use\n",
    "Claude can use tools to perform actions beyond text generation...\n",
    "\n",
    "### [3]: https://ai.google.dev/gemini-api/docs/function-calling\n",
    "Gemini models support function calling for structured interactions...\n",
    "'''\n",
    "\n",
    "answer_by_source = {\n",
    "    1: \"Function calling allows models to connect to external tools and APIs. Best practices include defining clear schemas, handling errors gracefully, and validating inputs...\",\n",
    "    2: \"Claude can use tools to perform actions beyond text generation. Key considerations include security, rate limiting, and proper error handling...\",\n",
    "    3: \"Gemini models support function calling for structured interactions. Important aspects include schema design, response validation, and performance optimization...\"\n",
    "}\n",
    "\n",
    "citations = {\n",
    "    1: \"https://openai.com/docs/guides/function-calling\",\n",
    "    2: \"https://docs.anthropic.com/claude/docs/tool-use\", \n",
    "    3: \"https://ai.google.dev/gemini-api/docs/function-calling\"\n",
    "}\n",
    "```\n",
    "\n",
    "This structured format ensures that:\n",
    "- **Each source is clearly separated** and can be processed independently\n",
    "- **URLs are preserved** for citation and potential full scraping later\n",
    "- **Content is substantial** (up to 300 words per source) for comprehensive coverage\n",
    "- **Data is parseable** by downstream processing tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1194632a",
   "metadata": {},
   "source": [
    "### 5.3 The Perplexity Search Prompt\n",
    "\n",
    "The prompt used for Perplexity searches is designed to extract maximum value:\n",
    "\n",
    "```python\n",
    "PROMPT_WEB_SEARCH = \"\"\"\n",
    "Question: {query}\n",
    "\n",
    "Provide a detailed answer to the question above.\n",
    "The answer should be organized into source sections, where each source section\n",
    "contains all the information coming from a single source.\n",
    "Never use multiple source citations in the same source section. A source section should refer to a single source.\n",
    "Focus on the official sources and avoid personal opinions.\n",
    "For each source, write as much information as possible coming from the source\n",
    "and that is relevant to the question (at most 300 words).\n",
    "\n",
    "Return a list of objects, where each object represents a source and has the following fields:\n",
    "- url: The URL of the source\n",
    "- answer: The detailed answer extracted from that source\n",
    "\"\"\".strip()\n",
    "```\n",
    "\n",
    "This prompt ensures that each source is clearly identified and separated, that up to 300 words per source for detailed information, and that the output is in a consistent format for parsing and storage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b85e9e",
   "metadata": {},
   "source": [
    "## 6 Testing the tools\n",
    "\n",
    "### 6.1 Testing the Query Generation Tool\n",
    "\n",
    "Let's test the query generation tool programmatically to understand its output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4bd1ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'status': 'success', 'queries_count': 3, 'queries': [('What are the fundamental limitations of large language models that necessitate the use of external tools?', 'This question addresses the core \"why\" behind function calling, directly supporting the \"Understanding why agents need tools\" section of the article. The provided text explains *how* to use tools with a specific API, but this query will find sources that explain the foundational reasons, such as the inability to access real-time data, perform precise calculations, or interact with private APIs.'), ('How do the function calling or tool use APIs of major LLM providers like OpenAI, Gemini, and Anthropic compare in terms of features and implementation?', \"The supplied context is exclusively about the Gemini API. To create a balanced and authoritative article, it's crucial to compare this with other major platforms. This query will uncover differences in syntax, capabilities (like parallel vs. sequential calling), and terminology, providing a comprehensive market overview for the reader.\"), ('What are the established design patterns and best practices for building robust, multi-tool LLM agents, particularly regarding error handling and security?', \"The provided documentation covers simple tool-use cases but doesn't delve into the complexities of real-world agentic systems. This query targets advanced, practical knowledge on topics like the ReAct framework, managing conversational state, handling tool failures gracefully, and ensuring the secure execution of actions, which is critical for developers building production-level applications.\")], 'output_path': '/Users/fabio/Desktop/course-ai-agents/lessons/research_agent_part_2/data/sample_research_folder/.nova/next_queries.md', 'message': 'Successfully generated 3 candidate queries for research folder \\'/Users/fabio/Desktop/course-ai-agents/lessons/research_agent_part_2/data/sample_research_folder\\'. Queries and reasons saved to: .nova/next_queries.md\\n\\nGenerated Queries:\\n\\n1. What are the fundamental limitations of large language models that necessitate the use of external tools?\\nReason: This question addresses the core \"why\" behind function calling, directly supporting the \"Understanding why agents need tools\" section of the article. The provided text explains *how* to use tools with a specific API, but this query will find sources that explain the foundational reasons, such as the inability to access real-time data, perform precise calculations, or interact with private APIs.\\n\\n2. How do the function calling or tool use APIs of major LLM providers like OpenAI, Gemini, and Anthropic compare in terms of features and implementation?\\nReason: The supplied context is exclusively about the Gemini API. To create a balanced and authoritative article, it\\'s crucial to compare this with other major platforms. This query will uncover differences in syntax, capabilities (like parallel vs. sequential calling), and terminology, providing a comprehensive market overview for the reader.\\n\\n3. What are the established design patterns and best practices for building robust, multi-tool LLM agents, particularly regarding error handling and security?\\nReason: The provided documentation covers simple tool-use cases but doesn\\'t delve into the complexities of real-world agentic systems. This query targets advanced, practical knowledge on topics like the ReAct framework, managing conversational state, handling tool failures gracefully, and ensuring the secure execution of actions, which is critical for developers building production-level applications.'}\n"
     ]
    }
   ],
   "source": [
    "from research_agent_part_2.mcp_server.src.tools import generate_next_queries_tool\n",
    "\n",
    "# Update this path to your actual sample research folder\n",
    "research_folder = \"/path/to/research_folder\"\n",
    "result = await generate_next_queries_tool(research_directory=research_folder, n_queries=3)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f2638e",
   "metadata": {},
   "source": [
    "The output will show a structured summary like:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"status\": \"success\",\n",
    "  \"queries_count\": 3,\n",
    "  \"queries\": [\n",
    "    (\"What are the latest best practices for implementing function calling with LLMs?\", \"This query addresses implementation details that may not be fully covered in the basic documentation from the guidelines.\"),\n",
    "    (\"How do different LLM providers handle function calling differently?\", \"Understanding provider-specific approaches will help create more comprehensive guidance.\"),\n",
    "    (\"What are common pitfalls and debugging techniques for function calling?\", \"Practical troubleshooting information is essential for developers implementing these systems.\")\n",
    "  ],\n",
    "  \"output_path\": \"/path/to/research_folder/.nova/next_queries.md\",\n",
    "  \"message\": \"Successfully generated 3 candidate queries for research folder...\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a938e27",
   "metadata": {},
   "source": [
    "### 6.2 Testing the Perplexity Research Tool\n",
    "\n",
    "Now let's test the Perplexity research functionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39b31851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool call output:\n",
      "{'status': 'success', 'queries_processed': 2, 'sources_added': 7, 'output_path': '/Users/fabio/Desktop/course-ai-agents/lessons/research_agent_part_2/data/sample_research_folder/.nova/perplexity_results.md', 'message': \"Successfully completed Perplexity research round for research folder '/Users/fabio/Desktop/course-ai-agents/lessons/research_agent_part_2/data/sample_research_folder'. Processed 2 queries and added 7 source sections to perplexity_results.md\"}\n",
      "\n",
      "Content of the resulting file:\n",
      "### Source [1]: https://arxiv.org/html/2505.20192v1\n",
      "\n",
      "Query: What are the latest developments in LLM function calling?\n",
      "\n",
      "Answer: The paper introduces **FunReason**, a novel framework designed to enhance large language modelsâ€™ (LLMs) function calling capabilities. FunReason addresses the challenge of combining detailed reasoning with accurate function executionâ€”a known limitation of prior approachesâ€”by employing an automated data refinement strategy and a Self-Refinement Multiscale Loss (SRML) technique. This approach allows LLMs to generate high-quality training examples, focusing on three aspects: query parseability, reasoning coherence, and function call precision. The SRML dynamically balances reasoning and function call accuracy during training, overcoming the typical trade-off between these factors. The authors report that FunReason achieves performance on par with GPT-4o and effectively mitigates catastrophic forgetting during fine-tuning. The work positions function calling as central for LLM practical utility, highlighting a shift from prompt engineering to more data-driven and fine-tuned approaches, including reinforcement learning (RL)-based optimization for task success. The availability of code and datasets is emphasized, supporting reproducibility and practical adoption.\n",
      "\n",
      "-----\n",
      "\n",
      "### Source [2]: https://aclanthology.org/2025.naacl-industry.9.pdf\n",
      "\n",
      "Query: What are the latest developments in LLM function calling?\n",
      "\n",
      "Answer: This research explores strategies to enhance function-calling capabilities in LLMs, particularly for autonomous agents. Key advances include: (1) using instruction-following data to improve both function-calling accuracy and the ability to detect relevant functions, (2) experimenting with different prompt formats, including the integration of function descriptions and a novel Decision Token for conditional prompts, (3) blending function-calling and instruction-following data in training, (4) employing chain-of-thought reasoning to clarify the steps leading to a function call, and (5) introducing a translation pipeline to overcome multilingual challenges in function calling. Results show that combining instruction-following and function-calling data yields better performance than training on either type alone, and chain-of-thought reasoning further enhances interpretability and accuracy, especially in complex or multilingual scenarios.\n",
      "\n",
      "-----\n",
      "\n",
      "### Source [3]: https://arxiv.org/html/2509.18076v1\n",
      "\n",
      "Query: What are the latest developments in LLM function calling?\n",
      "\n",
      "Answer: This source presents a structured, template-based reasoning framework for improving LLM function calling. The methodology relies on two components: prompting strategies, which embed a reasoning template directly into the prompt, and fine-tuning strategies, which use a curated data pipeline to train the model with explicit reasoning chains. The key innovation is to jointly model the reasoning process and the function call, making the modelâ€™s decision process more transparent and interpretable. Experimental results demonstrate that this approach reduces tool-use errors and achieves 3â€“12% relative improvements over strong baselines across diverse benchmarks. The paper also provides explicit reasoning templates, a synthetic dataset construction method, and empirical evidence that both structured prompting and supervised fine-tuning significantly boost function call accuracy and interpretability. The authors suggest future work on extending these techniques to broader decision-making tasks.\n",
      "\n",
      "-----\n",
      "\n",
      "### Source [4]: https://github.com/Applied-Machine-Learning-Lab/Awesome-Function-Callings\n",
      "\n",
      "Query: What are the latest developments in LLM function calling?\n",
      "\n",
      "Answer: This GitHub repository serves as a comprehensive index and review of recent developments in LLM function calling. It covers concepts such as the function calling pipeline (including pre-call, call, and post-call stages), sample construction and fine-tuning methodologies, deployment and inference strategies, and evaluation frameworks. The repository emphasizes the importance of reproducibility and open-source implementations. It also refers to an associated survey paper (Wang et al., 2025) that details industrial practices, challenges, and future directions for LLM function calling. The resource is intended to provide practitioners with a broad view of the ecosystem, highlighting practical methodologies, benchmarks, and current research trends in optimizing LLMs for accurate, reliable, and scalable function calling in real-world applications.\n",
      "\n",
      "-----\n",
      "\n",
      "### Source [5]: https://synergetics.ai/solving-ai-agent-errors-for-better-performance/\n",
      "\n",
      "Query: How do you handle errors in AI agent tool execution?\n",
      "\n",
      "Answer: Diagnosing and handling AI agent tool execution errors begins with careful observation and structured troubleshooting. The process starts by spotting inconsistenciesâ€”tracking when and how mistakes happen, noting patterns related to specific inputs or system updates. Next, practitioners run small, controlled tests to isolate the problematic component or function, making single-variable changes and tracking outcomes. Reviewing system and communication logs is another critical step, as these can reveal missed messages, misinterpreted commands, or unexecuted steps. Once the root cause is determined, improvements may involve refreshing outdated or biased data, tightening test coverage (including both normal and edge cases), and ensuring robust communication protocols, especially in multi-agent systems. These steps collectively increase the reliability and accuracy of AI agents, reducing the occurrence and impact of errors during tool execution.\n",
      "\n",
      "-----\n",
      "\n",
      "### Source [6]: https://galileo.ai/blog/prevent-ai-agent-failure\n",
      "\n",
      "Query: How do you handle errors in AI agent tool execution?\n",
      "\n",
      "Answer: Effective error handling in AI agent tool execution requires designing for resilience and transparency. Architectures should include circuit breakers to prevent small failures from escalating into system-wide outages. If a subsystem fails (like knowledge retrieval), the agent should degrade gracefully, for example, by switching to fallback responses rather than crashing. Clear error taxonomies help distinguish between temporary and fundamental issues, enabling tailored responsesâ€”such as retries with exponential backoff for transient network errors, and clear, actionable user messages for persistent problems. Transparency with users is emphasized: rather than vague codes, the agent should communicate the issue and possible next steps. Challenging or ambiguous cases should be routed to human experts, with these interactions captured as training data for future improvements. Ongoing monitoring and proactive data collection help identify and address recurring gaps, while ensemble model strategies can broaden the agent's ability to handle diverse scenarios.\n",
      "\n",
      "-----\n",
      "\n",
      "### Source [7]: https://stytch.com/blog/if-an-ai-agent-cant-figure-out-how-your-api-works-neither-can-your-users\n",
      "\n",
      "Query: How do you handle errors in AI agent tool execution?\n",
      "\n",
      "Answer: AI agent tool execution errors are often surfaced through detailed, context-rich logs and explicit error messages. When an agent encounters an error (such as a missing required field in an API call), clear error messages and comprehensive documentation enable rapid iteration and correction. Reviewing the agent's step-by-step log of actions and observations helps developers pinpoint exactly where the process broke down. For robust error handling, systems should provide precise error codes and detailed explanations (e.g., distinguishing between 'token missing' and 'token invalid'), and ideally link errors directly to relevant documentation. This approach not only accelerates agent (and developer) troubleshooting but also highlights documentation or design deficiencies that need remediation. Teams are increasingly focused on 'Agent Experience' (AX), ensuring that the fundamental resourcesâ€”clear docs, consistent patterns, and explicit errorsâ€”are in place to support both human and AI users.\n",
      "\n",
      "-----\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from research_agent_part_2.mcp_server.src.tools import run_perplexity_research_tool\n",
    "\n",
    "# Example queries to test\n",
    "test_queries = [\n",
    "    \"What are the latest developments in LLM function calling?\",\n",
    "    \"How do you handle errors in AI agent tool execution?\"\n",
    "]\n",
    "\n",
    "result = await run_perplexity_research_tool(\n",
    "    research_directory=research_folder, \n",
    "    queries=test_queries\n",
    ")\n",
    "print(\"Tool call output:\")\n",
    "print(result)\n",
    "print()\n",
    "\n",
    "print(\"Content of the resulting file:\")\n",
    "with open(result[\"output_path\"], \"r\") as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7ec518",
   "metadata": {},
   "source": [
    "## 7. Human-in-the-Loop\n",
    "\n",
    "One of the most powerful aspects of our research agent is its ability to integrate human feedback directly into the workflow. This transforms the agent from a fully automated system into a collaborative research partner."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f280fea",
   "metadata": {},
   "source": [
    "### 7.1 How Human Feedback Works\n",
    "\n",
    "The agent can be instructed to pause after generating queries and ask for human approval before proceeding. This is accomplished by modifying the workflow instructions when triggering the MCP prompt.\n",
    "\n",
    "When the user starts the research workflow, they can specify modifications like:\n",
    "- \"Ask for my feedback after generating each set of queries\"\n",
    "- \"Show me the proposed queries and wait for my approval before running them\"\n",
    "- \"Let me select which queries to run from the generated list\"\n",
    "\n",
    "Let's see how to run the complete research agent with human-in-the-loop feedback. We'll start the MCP client and demonstrate how to request user feedback integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d788c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ› ï¸  Available tools: 11\n",
      "ðŸ“š Available resources: 2\n",
      "ðŸ’¬ Available prompts: 1\n",
      "\n",
      "Available Commands: /tools, /resources, /prompts, /prompt/<name>, /resource/<uri>, /model-thinking-switch, /quit\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95mðŸ¤” LLM's Thoughts:\u001b[0m\n",
      "\u001b[35m**Ready to Begin the Research Workflow!**\n",
      "\n",
      "Okay, so I'm geared up and ready to dive into this research project. The user has given me the go-ahead, so let's get rolling. First things first, I need to outline the workflow. I'll make sure the user understands what I'm about to do, so they know what to expect.\n",
      "\n",
      "Here's the plan, in a nutshell:\n",
      "\n",
      "1.  **Setting the Stage (Setup):**  I'll start by taking a look at the `ARTICLE_GUIDELINE_FILE` to grab any URLs and local files that are relevant. It's like collecting my starting ingredients.\n",
      "\n",
      "2.  **Processing the Resources (In Parallel):**  This is the bulk of the action.  I'll be working in parallel to speed things up. I'll be copying the local files, scraping and cleaning content from those websites, processing GitHub links (summarizing and collecting the content), and even transcribing YouTube videos. Efficiency is key!\n",
      "\n",
      "3.  **The Research Loop (3 Rounds):** Time to start generating new web search queries. Based on what I've learned, I'll generate new queries and use Perplexity to get the latest info. This is my \"knowledge loop,\" and I'll do this three times to get a broader view.\n",
      "\n",
      "4.  **Filtering Perplexity Results:** Now it's time to refine those results. I'll need to sift through all of the Perplexity results and pick the best sources. I'm looking for trustworthiness, authority, and real relevance to the topic. I will weed out anything questionable.\n",
      "\n",
      "5.  **Finding Full Sources:** I'll pick the best 5 sources for the full scrape. These are the \"keepers.\"  Once I have them, I'll scrape all of the content of the selected sources.\n",
      "\n",
      "6.  **Writing the Research Document:**  Finally, I'll synthesize everything into a complete `RESEARCH_MD_FILE`. This is where all the data, research, and insights come together. This will be the output!\n",
      "\n",
      "Now, before I jump into all of that, I need to get the `research_directory` from the user. And, I need to know if they want me to start at a specific step or add any feedback. This is important to ensure I'm on the same page and that I'm delivering exactly what they're looking for. Once I've got that, I'm ready to kick things off at step 1.3: `extract_guidelines_urls`. I'm excited to see where this leads!\u001b[0m\n",
      "\n",
      "\u001b[37mðŸ’¬ LLM Response: \u001b[0mHere's a breakdown of the research workflow steps:\n",
      "\n",
      "1.  **Setup**: Extracts URLs (GitHub, YouTube, other web links) and local file references from your `ARTICLE_GUIDELINE_FILE`.\n",
      "2.  **Process Extracted Resources**: Simultaneously processes the extracted resources:\n",
      "    *   Copies referenced local files to a dedicated folder.\n",
      "    *   Scrapes and cleans content from other web URLs.\n",
      "    *   Processes GitHub URLs to create summaries and content.\n",
      "    *   Transcribes YouTube videos.\n",
      "3.  **Research Loop**: Repeats for 3 rounds:\n",
      "    *   Generates new web-search queries to fill knowledge gaps.\n",
      "    *   Executes these queries using Perplexity and appends the results.\n",
      "4.  **Filter Perplexity Results**: Evaluates Perplexity sources for quality (trustworthiness, authority, relevance) and selects the best ones.\n",
      "5.  **Identify and Scrape Full Sources**: Selects a few high-value sources from the filtered Perplexity results for a full content scrape, then performs the scraping.\n",
      "6.  **Write Final Research File**: Compiles all gathered research data (filtered Perplexity results, scraped guideline sources, full research sources) into a comprehensive `RESEARCH_MD_FILE`.\n",
      "\n",
      "Please provide the path to your research directory. Also, let me know if you'd like to start from a specific step or add any feedback to certain steps.\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95mðŸ¤” LLM's Thoughts:\u001b[0m\n",
      "\u001b[35m**Execution Plan for Research Workflow**\n",
      "\n",
      "Okay, here's what I'm thinking. The user wants to run a research workflow, and I've already explained the general steps. They've given me the research directory: `/Users/fabio/Desktop/course-ai-agents/lessons/research_agent_part_2/data/sample_research_folder`. Now, I need to translate their instructions into actionable steps.\n",
      "\n",
      "They've requested a couple of specific modifications. First, they want feedback after each set of queries is generated in the research loop (Step 3). That means I need to pause, show them the queries, and get their approval *before* running Perplexity. Second, and this is crucial, they want me to stop after Step 3. No further processing.\n",
      "\n",
      "So, here's how I see it breaking down, considering these modifications:\n",
      "\n",
      "**Step 1: Initial Setup**\n",
      "\n",
      "*   I'll start with Step 1.3: running `default_api.extract_guidelines_urls` using the provided research directory.\n",
      "\n",
      "**Step 2: Parallel Resource Processing**\n",
      "\n",
      "*   Next, I need to execute Step 2.1 ( `default_api.process_local_files`), 2.2 ( `default_api.scrape_and_clean_other_urls`), 2.3 ( `default_api.process_github_urls`), and 2.4 ( `default_api.transcribe_youtube_urls`). The important thing is that these need to be done *in parallel*. All tool calls should be made in a single turn.\n",
      "\n",
      "**Step 3: Iterative Research Loop (Important!)**\n",
      "\n",
      "*   This is the meat of the modified workflow: The 3 round research loop.\n",
      "    *   **3.1: Query Generation:** Call `default_api.generate_next_queries` to create the next set of search queries.\n",
      "    *   **User Interaction:** Crucially, I have to pause here. I'll display the generated queries to the user. I need to handle a few scenarios here, since the request is for feedback:\n",
      "        *   If the user approves the queries, I will call step 3.2.\n",
      "        *   If they want modifications, I need to run that query or those queries.\n",
      "        *   If the user declines, I'll need to figure out how to continue.\n",
      "    *   **3.2: Perplexity Research**: If the user approves or modifies the queries, I'll call `default_api.run_perplexity_research` with those approved or modified queries.\n",
      "    *   This whole process (3.1 and 3.2) will repeat for three rounds.\n",
      "\n",
      "**Termination**\n",
      "\n",
      "*   After completing the three iterations of Step 3, *I must stop*. I will inform the user that the workflow has been completed up to Step 3.\n",
      "\n",
      "**Error Handling is critical**\n",
      "\n",
      "I need to be vigilant about error checking. For each tool call, I'll check the 'status' field in the output. If it's not \"success\" or if a processing count is zero when it should be non-zero, I'll immediately halt, report the problem to the user, and ask for guidance.\n",
      "\n",
      "Alright, the plan is set. I'll begin now with Step 1.3, running the URL extraction tool, and then report back on the results. From there, I'll move on to the parallel resource processing, and then the critical iterative research loop with the user feedback at each step.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[36mðŸ”§ Function Call (Tool):\u001b[0m\n",
      "\u001b[36m  Tool: \u001b[0m\u001b[1m\u001b[36mextract_guidelines_urls\u001b[0m\n",
      "\u001b[36m  Arguments: \u001b[0m\u001b[36m{\n",
      "  \"research_directory\": \"/Users/fabio/Desktop/course-ai-agents/lessons/research_agent_part_2/data/sample_research_folder\"\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[36mâš¡ Executing tool 'extract_guidelines_urls' via MCP server...\u001b[0m\n",
      "\u001b[36mâœ… Tool execution successful!\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[95mðŸ¤” LLM's Thoughts:\u001b[0m\n",
      "\u001b[35m**Navigating the Research Workflow**\n",
      "\n",
      "Okay, so I've received the research directory and a clear set of modifications to how I operate. The `extract_guidelines_urls` tool already ran smoothly, which is great. Now, I need to focus on the parallel steps â€“ 2.1 to 2.4 â€“ and get those underway. That's the first order of business.\n",
      "\n",
      "Once those are complete, I'll dive into the core research loop, step 3. This is where things get interesting because the user wants a more interactive approach here. They want to see the queries I'm generating *before* I run them with Perplexity. I need to pause after generating each set of queries, display them, and wait for the \"go ahead\" before proceeding. This is smart, and I'll make sure to integrate that feedback loop seamlessly. Also, they want me to stop after the completion of step 3, so I will ensure that's built in as well.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[36mðŸ”§ Function Call (Tool):\u001b[0m\n",
      "\u001b[36m  Tool: \u001b[0m\u001b[1m\u001b[36mprocess_local_files\u001b[0m\n",
      "\u001b[36m  Arguments: \u001b[0m\u001b[36m{\n",
      "  \"research_directory\": \"/Users/fabio/Desktop/course-ai-agents/lessons/research_agent_part_2/data/sample_research_folder\"\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[36mâš¡ Executing tool 'process_local_files' via MCP server...\u001b[0m\n",
      "\u001b[36mâœ… Tool execution successful!\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[95mðŸ¤” LLM's Thoughts:\u001b[0m\n",
      "\u001b[35m**Current Workflow & Next Steps**\n",
      "\n",
      "Okay, so I've received the research directory and workflow modifications.  Essentially, the user wants me to be more interactive and conservative with my Perplexity queries. They want feedback after *each* set of queries is generated (step 3.1) and a manual approval step before I actually run them (step 3.2).  Also, they've cut the workflow short, stopping after step 3. No problem, I can handle that.\n",
      "\n",
      "I've already handled steps 1.3 (URL extraction) and 2.1 (local file processing). Now I need to move onto the parallel processing portion of step 2. Specifically, I'm thinking I'll kick off these three sub-processes simultaneously:\n",
      "\n",
      "*   **2.2 `scrape_and_clean_other_urls`**: This should be relatively straightforward.\n",
      "*   **2.3 `process_github_urls`**: This one might be a little more involved, but I've worked with GitHub data before.\n",
      "*   **2.4 `transcribe_youtube_urls`**:  Always interesting to work with video data.\n",
      "\n",
      "I'm going to start these three tasks in parallel now and wait for them to complete before moving to step 3, where I'll be asking for feedback. Let's get this rolling.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[36mðŸ”§ Function Call (Tool):\u001b[0m\n",
      "\u001b[36m  Tool: \u001b[0m\u001b[1m\u001b[36mscrape_and_clean_other_urls\u001b[0m\n",
      "\u001b[36m  Arguments: \u001b[0m\u001b[36m{\n",
      "  \"research_directory\": \"/Users/fabio/Desktop/course-ai-agents/lessons/research_agent_part_2/data/sample_research_folder\"\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[36mâš¡ Executing tool 'scrape_and_clean_other_urls' via MCP server...\u001b[0m\n",
      "\u001b[36mâœ… Tool execution successful!\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1758615120.793915 3155870 fork_posix.cc:71] Other threads are currently calling into gRPC, skipping fork() handlers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95mðŸ¤” LLM's Thoughts:\u001b[0m\n",
      "\u001b[35m**Navigating the Research Workflow**\n",
      "\n",
      "Okay, here's the deal. I've successfully completed the initial steps of this research project. I've extracted the guidelines' URLs (Step 1.3), checked for local files (there were none, Step 2.1), and scraped & cleaned other URLs (Step 2.2). Now, I need to execute Steps 2.3 and 2.4 concurrently - that's the next logical move, and it aligns perfectly with the requested workflow.\n",
      "\n",
      "The real shift happens at Step 3, the research loop. The user wants me to be more interactive here. I have to pause after generating each set of research queries and get approval before running them through Perplexity. That's a key modification, and I'll need to implement that specifically.\n",
      "\n",
      "Just to be clear, the research directory is `/Users/fabio/Desktop/course-ai-agents/lessons/research_agent_part_2/data/sample_research_folder`, and my task is to halt the process after Step 3. No need to move on to steps 4, 5, or 6. That's the objective. I'm ready to tackle 2.3 and 2.4, and then jump into the modified research loop.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[36mðŸ”§ Function Call (Tool):\u001b[0m\n",
      "\u001b[36m  Tool: \u001b[0m\u001b[1m\u001b[36mprocess_github_urls\u001b[0m\n",
      "\u001b[36m  Arguments: \u001b[0m\u001b[36m{\n",
      "  \"research_directory\": \"/Users/fabio/Desktop/course-ai-agents/lessons/research_agent_part_2/data/sample_research_folder\"\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[36mâš¡ Executing tool 'process_github_urls' via MCP server...\u001b[0m\n",
      "\u001b[36mâœ… Tool execution successful!\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[95mðŸ¤” LLM's Thoughts:\u001b[0m\n",
      "\u001b[35m**Okay, Here's My Plan**\n",
      "\n",
      "Alright, I'm at the point where I'm ready to dive back in. I've got the setup done and the resources are prepped and ready, thanks to the parallel processing. Now, I need to get started with the YouTube transcription part - that's step 2.4. Once that's done, I'll be in the heart of the research loop.\n",
      "\n",
      "The key thing is, the user wants a more interactive approach this time. They want to provide feedback and approve the direction of the research. So, after I generate the queries in the research loop (step 3.1), I need to *immediately* loop back and present those queries for their input. No running ahead! Then, *only* after getting their green light (step 3.2), will I actually send those queries over to Perplexity. It's a much more controlled, deliberate process.\n",
      "\n",
      "I'm aiming to go through this query-feedback-approval cycle three times. Then, I need to remember to *stop* the whole workflow right after that research loop is complete (step 3). No extra steps, just exactly what's been requested. Let's get these YouTube URLs transcribed and get moving.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[36mðŸ”§ Function Call (Tool):\u001b[0m\n",
      "\u001b[36m  Tool: \u001b[0m\u001b[1m\u001b[36mtranscribe_youtube_urls\u001b[0m\n",
      "\u001b[36m  Arguments: \u001b[0m\u001b[36m{\n",
      "  \"research_directory\": \"/Users/fabio/Desktop/course-ai-agents/lessons/research_agent_part_2/data/sample_research_folder\"\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[36mâš¡ Executing tool 'transcribe_youtube_urls' via MCP server...\u001b[0m\n",
      "\u001b[36mâœ… Tool execution successful!\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[95mðŸ¤” LLM's Thoughts:\u001b[0m\n",
      "\u001b[35m**My Research Workflow - Iteration 1**\n",
      "\n",
      "Okay, so the user has laid out the plan clearly. I've successfully completed the initial steps: gathering guidelines, processing local and web-based resources, and transcribing YouTube content. Now, the real work begins â€“ the research loop.\n",
      "\n",
      "The goal is to refine my search queries iteratively. This means I'll generate a set of queries, present them to the user for their expert feedback, get their approval (or modifications), and *then* run them against Perplexity. This loop will run for three cycles.\n",
      "\n",
      "I'm starting the first round now. I'll call `generate_next_queries` to create the initial set. Then, I'll pause, show the user these proposed queries, and patiently wait for their input. This is crucial for ensuring the research stays focused and relevant. If the user approves, I'll execute the `run_perplexity_research` tool. If not, I'll go back to the user and re-iterate with them.  Let's see what the first set of queries looks like.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[36mðŸ”§ Function Call (Tool):\u001b[0m\n",
      "\u001b[36m  Tool: \u001b[0m\u001b[1m\u001b[36mgenerate_next_queries\u001b[0m\n",
      "\u001b[36m  Arguments: \u001b[0m\u001b[36m{\n",
      "  \"research_directory\": \"/Users/fabio/Desktop/course-ai-agents/lessons/research_agent_part_2/data/sample_research_folder\"\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[36mâš¡ Executing tool 'generate_next_queries' via MCP server...\u001b[0m\n",
      "\u001b[36mâœ… Tool execution successful!\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[95mðŸ¤” LLM's Thoughts:\u001b[0m\n",
      "\u001b[35m**Okay, Here's Where We Stand**\n",
      "\n",
      "Alright, I've got the lay of the land here. The user's made some specific requests to modify the research loop, and I need to make sure I'm adhering to those.  Specifically, they want me to pause after generating the queries, *show* them to the user, and get the green light *before* running Perplexity. They've also indicated the research should end after step three of the process.\n",
      "\n",
      "Currently, I've completed all of step one (extracting guideline URLs), step two (processing local files, scraping and cleaning other URLs, processing GitHub URLs, and transcribing YouTube URLs), and I'm on the first iteration of step three (generating the next round of queries). The tool `generate_next_queries` has just run, and I have its output â€“ the generated queries.\n",
      "\n",
      "So, here's the plan: I need to hold right here. I need to present these queries to the user.  They need to see what I've come up with. And, crucially, I need their approval *before* I even think about sending those queries to Perplexity for the initial research run. That's the only way to ensure I'm staying true to the user's modifications.\u001b[0m\n",
      "\n",
      "\u001b[37mðŸ’¬ LLM Response: \u001b[0mOkay, I have extracted the URLs, processed local files (none found), scraped other URLs, processed GitHub URLs, and transcribed YouTube URLs.\n",
      "\n",
      "I have also generated the first set of queries for the research loop. Here they are:\n",
      "\n",
      "1.  **What are the fundamental limitations of large language models that tool use and function calling aim to solve?**\n",
      "    *   **Reason**: This foundational question addresses the \"why\" behind tool use, directly supporting the \"Understanding why agents need tools\" section. It seeks sources that explain core LLM weaknesses, such as knowledge cutoffs, inability to interact with the real world, and mathematical errors, providing essential context for the entire article.\n",
      "\n",
      "2.  **How do the function calling implementations of Google's Gemini, OpenAI's GPT, and Anthropic's Claude compare in terms of features, reliability, and developer experience?**\n",
      "    *   **Reason**: The provided context details the Gemini and OpenAI APIs but doesn't compare them or include other major players. This question seeks a comparative analysis, which is crucial for an authoritative article, helping readers understand the trade-offs and nuances of different ecosystems.\n",
      "\n",
      "3.  **What are the primary security risks and mitigation strategies when building LLM agents with external tool-calling capabilities?**\n",
      "    *   **Reason**: The provided documentation focuses on \"how\" to implement function calling but lacks depth on production-level challenges. This question targets the critical, practical aspects of security and reliability, ensuring the article provides value beyond a basic tutorial.\n",
      "\n",
      "4.  **What are the trade-offs of using a framework like LangChain or LlamaIndex for tool calling versus implementing it directly with native LLM APIs?**\n",
      "    *   **Reason**: Developers often choose between using a framework or the native API. This question explores the ecosystem around tool use, seeking sources that compare the pros and cons of different development approaches (e.g., ease-of-use vs. control and performance), which is a vital consideration for the article's audience.\n",
      "\n",
      "5.  **What is the role of function calling in the development of more autonomous AI agents and the concept of \"action models\"?**\n",
      "    *   **Reason**: This forward-looking question helps frame the conclusion of the article. It aims to find sources that connect the specific feature of function calling to the broader, more advanced vision of autonomous agents, positioning the topic within the future trajectory of AI.\n",
      "\n",
      "Do you approve these queries to be run with Perplexity? If not, please suggest any modifications or new queries.\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95mðŸ¤” LLM's Thoughts:\u001b[0m\n",
      "\u001b[35m**My Current Thinking**\n",
      "\n",
      "Okay, so the user has finally given me the green light on the first set of queries. My next step is straightforward: I need to execute `run_perplexity_research` using those carefully refined queries. That's the primary task for this cycle.\n",
      "\n",
      "Once that's done, I'll switch gears and generate the next batch of queries for the second research round. It's all according to the established workflow, so I know exactly what to do.\n",
      "\n",
      "After generating the second round of queries, I'll essentially hit a pause button. I'll need to wait for the user to review the new set and provide feedback before I can proceed any further. I'm hoping the user's input will help me refine the direction of the research and ensure we're on the right track.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[36mðŸ”§ Function Call (Tool):\u001b[0m\n",
      "\u001b[36m  Tool: \u001b[0m\u001b[1m\u001b[36mrun_perplexity_research\u001b[0m\n",
      "\u001b[36m  Arguments: \u001b[0m\u001b[36m{\n",
      "  \"queries\": [\n",
      "    \"What are the fundamental limitations of large language models that tool use and function calling aim to solve?\",\n",
      "    \"What are the primary security risks and mitigation strategies when building LLM agents with external tool-calling capabilities?\",\n",
      "    \"What is the role of function calling in the development of more autonomous AI agents and the concept of \\\"action models\\\"?\"\n",
      "  ],\n",
      "  \"research_directory\": \"/Users/fabio/Desktop/course-ai-agents/lessons/research_agent_part_2/data/sample_research_folder\"\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[36mâš¡ Executing tool 'run_perplexity_research' via MCP server...\u001b[0m\n",
      "\u001b[36mâœ… Tool execution successful!\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[95mðŸ¤” LLM's Thoughts:\u001b[0m\n",
      "\u001b[35m**Second Round Query Generation - Let's Keep This Momentum!**\n",
      "\n",
      "Okay, so the user gave the green light on some of those initial query suggestions. Fantastic! That `run_perplexity_research` function did its thing, and it was a success. Now it's time to keep the research engine humming. We're moving into round two of this three-round query generation process. That means it's back to the drawing board for me, crafting a fresh batch of queries that build upon what we've already learned. Once I've got that next set of queries ready, I'll loop back and request the user's feedback again. Consistency is key, and I want to make sure we're always on the right track with these. Time to dive back in!\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[36mðŸ”§ Function Call (Tool):\u001b[0m\n",
      "\u001b[36m  Tool: \u001b[0m\u001b[1m\u001b[36mgenerate_next_queries\u001b[0m\n",
      "\u001b[36m  Arguments: \u001b[0m\u001b[36m{\n",
      "  \"research_directory\": \"/Users/fabio/Desktop/course-ai-agents/lessons/research_agent_part_2/data/sample_research_folder\"\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[36mâš¡ Executing tool 'generate_next_queries' via MCP server...\u001b[0m\n",
      "\u001b[36mâœ… Tool execution successful!\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[95mðŸ¤” LLM's Thoughts:\u001b[0m\n",
      "\u001b[35m**Perplexity Research - Round Two**\n",
      "\n",
      "Okay, so the initial Perplexity run is done. Queries 1, 3, and 5 got the green light from the user, and I've already run them.  `run_perplexity_research` is complete, and then I triggered `generate_next_queries` to kickstart the next round. Now, according to the revised workflow, I need to present these *newly generated* queries to the user for approval. This is the second loop, the second iteration. Time to get those queries in front of the user!\u001b[0m\n",
      "\n",
      "\u001b[37mðŸ’¬ LLM Response: \u001b[0mI have executed the Perplexity research with your selected queries and generated a new set of queries for the next round.\n",
      "\n",
      "Here are the queries for the second round:\n",
      "\n",
      "1.  **What are the common architectural patterns and best practices for designing and managing multi-tool AI agents?**\n",
      "    *   **Reason**: The provided documentation explains the mechanics of a single tool call but doesn't cover the architectural strategies needed to build robust agents with multiple, potentially interacting, tools. This question targets sources that discuss agent design, tool selection, and workflow management.\n",
      "\n",
      "2.  **What are the state-of-the-art benchmarks and evaluation frameworks for assessing the reliability and accuracy of tool-calling in large language models?**\n",
      "    *   **Reason**: To move beyond basic implementation, the article needs to address how developers can measure and compare the tool-calling performance of different models. This research is essential for providing evidence-based recommendations on model selection and fine-tuning.\n",
      "\n",
      "3.  **How do popular agent-building frameworks like LangChain, LlamaIndex, or Microsoft's AutoGen abstract and simplify the implementation of multi-step, tool-calling workflows compared to using base model APIs?**\n",
      "    *   **Reason**: Many developers use higher-level frameworks rather than calling base APIs directly. Understanding the abstractions, benefits, and trade-offs of these frameworks is essential for a practical guide aimed at developers building real-world applications.\n",
      "\n",
      "4.  **What are the best practices for designing the user experience (UX) around AI agents that use tools, especially concerning error handling, transparency, and user confirmation?**\n",
      "    *   **Reason**: An agent's utility is not just about technical capability but also about how it interacts with users. This question seeks sources on the crucial topic of UX design for tool-using AI, covering how to manage failures, communicate actions, and maintain user trust.\n",
      "\n",
      "5.  **What are the most promising alternatives or enhancements to standard function calling, such as the ReAct (Reasoning and Acting) framework or adaptive tool retrieval?**\n",
      "    *   **Reason**: Standard function calling is just one mechanism for agentic behavior. Exploring advanced frameworks that intertwine reasoning and action, or dynamically select tools, provides a more complete picture of the field and future directions, adding significant depth to the article.\n",
      "\n",
      "Do you approve these queries to be run with Perplexity for the second research round? Please let me know if you have any modifications or new queries.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from research_agent_part_2.mcp_client.src.client import main as client_main\n",
    "import sys\n",
    "\n",
    "async def run_client():\n",
    "    _argv_backup = sys.argv[:]\n",
    "    sys.argv = [\"client\"]\n",
    "    try:\n",
    "        await client_main()\n",
    "    finally:\n",
    "        sys.argv = _argv_backup\n",
    "\n",
    "# Start client with in-memory server \n",
    "await run_client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9769de82",
   "metadata": {},
   "source": [
    "Once the client is running, try these commands in sequence:\n",
    "\n",
    "1. **Start the workflow with feedback**: `/prompt/full_research_instructions_prompt`\n",
    "\n",
    "2. **Request human feedback integration**: When the agent asks for the research directory and workflow modifications, respond with:\n",
    "   ```\n",
    "   The research folder is /path/to/your/research_folder. Please modify the workflow to ask for my feedback after generating each set of queries in the research loop. Show me the proposed queries and wait for my approval before running them with Perplexity. Run up to step 3 of the workflow and then stop there, don't run the rest of the workflow from step 4 onwards.\n",
    "   ```\n",
    "   Otherwise, if you've run steps 1 and 2 of the workflow already, you can request the following:\n",
    "   ```\n",
    "   The research folder is /path/to/your/research_folder. Please modify the workflow to ask for my feedback after generating each set of queries in the research loop. Show me the proposed queries and wait for my approval before running them with Perplexity. Run only step 3 of the workflow and then stop there, don't run the rest of the workflow from step 4 onwards. Don't run steps 1 and 2 of the workflow.\n",
    "   ```\n",
    "3. **Observe the agent behavior**: The agent will:\n",
    "   - Run the initial data ingestion (steps 1-2)\n",
    "   - Generate the first set of research queries\n",
    "   - **Pause and show you the queries**\n",
    "   - Wait for your feedback before proceeding, allowing you to approve, modify, or reject queries\n",
    "\n",
    "4. **Provide feedback**: You can respond with:\n",
    "   - \"Approve all queries\" to proceed as planned\n",
    "   - \"Only run queries 1, 3, and 5\" to select specific queries\n",
    "   - \"Replace query 2 with: [your custom query]\" to modify the research direction\n",
    "\n",
    "5. **Continue the loop**: The agent will repeat this process for each of the 3 research rounds. You can stop the loop by responding with `/quit\"`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
