{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 30: Building LLM Judge Metrics for AI Evals\n",
    "\n",
    "In this lesson, we'll build LLM judge metrics to evaluate the quality of articles generated by Brown, the writing workflow.\n",
    "\n",
    "**Learning Objectives:**\n",
    "\n",
    "- Understand how to build LLM judges that evaluate articles at the section level using binary metrics\n",
    "- Learn the `FollowsGTMetricLLMJudge` metric that compares generated articles to ground truth\n",
    "- Learn the `UserIntentMetricLLMJudge` metric that checks guideline adherence and research anchoring\n",
    "- Use the evaluation dataset created in Lesson 28 to run experiments\n",
    "- Execute end-to-end AI evals using Opik and the Brown agent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [!NOTE]\n",
    "> ðŸ’¡ Remember that you can also run `brown` as a standalone Python package by going to `lessons/writing_workflow/` and following the instructions from there. We have a script at `lessons/writing_workflow/scripts/brown_run_eval.py` that you can use to run evaluations as well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "First, we define some standard Magic Python commands to autoreload Python packages whenever they change:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up Python Environment\n",
    "\n",
    "To set up your Python virtual environment using `uv` and load it into the Notebook, follow the step-by-step instructions from the `Course Admin` lesson from the beginning of the course.\n",
    "\n",
    "**TL/DR:** Be sure the correct kernel pointing to your `uv` virtual environment is selected.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Gemini and Opik API's\n",
    "\n",
    "To configure the Gemini and Opik API, follow the step-by-step instructions in the `Course Admin` lesson.\n",
    "\n",
    "Here is a quick checklist of what you need to run this notebook:\n",
    "\n",
    "1.  Get your key from [Google AI Studio](https://aistudio.google.com/app/api-keys).\n",
    "2.  Get your key from [Opik](https://www.comet.com/site/products/opik/).\n",
    "3.  From the root of your project, run: `cp .env.example .env` \n",
    "4.  Within the `.env` file, fill in the `GOOGLE_API_KEY` and `OPIK_API_KEY` variables.\n",
    "\n",
    "Now, the code below will load the keys from the `.env` file:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment variables loaded from `/Users/pauliusztin/Documents/01_projects/TAI/agentic-ai-engineering-course/.env`\n",
      "Environment variables loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "from utils import env\n",
    "\n",
    "env.load(required_env_vars=[\"GOOGLE_API_KEY\", \"OPIK_API_KEY\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Key Packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "from utils import pretty_print\n",
    "\n",
    "nest_asyncio.apply()  # Allow nested async usage in notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Required Files\n",
    "\n",
    "First, let's download Brown's configs folder:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "!rm -rf configs\n",
    "!curl -L -o configs.zip https://raw.githubusercontent.com/iusztinpaul/agentic-ai-engineering-course-data/main/data/configs.zip\n",
    "!unzip configs.zip\n",
    "!rm -rf configs.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's download Brown's inputs folder containing profiles, examples, and test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "!rm -rf inputs\n",
    "!curl -L -o inputs.zip https://raw.githubusercontent.com/iusztinpaul/agentic-ai-engineering-course-data/main/data/inputs.zip\n",
    "!unzip inputs.zip\n",
    "!rm -rf inputs.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ultimately, we will download Brown's outputs folder that contains pre-generated articles used as a cache to speed things up and keep costs low:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "!rm -rf outputs\n",
    "!curl -L -o outputs.zip https://raw.githubusercontent.com/iusztinpaul/agentic-ai-engineering-course-data/main/data/outputs.zip\n",
    "!unzip outputs.zip\n",
    "!rm -rf outputs.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify what we downloaded:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "article.md                  notebook.ipynb\n",
      "article_guideline.md        notebook_guideline.md\n",
      "article_guideline_notes.md  \u001b[1m\u001b[36moutputs\u001b[m\u001b[m/\n",
      "\u001b[1m\u001b[36mconfigs\u001b[m\u001b[m/                    research.md\n",
      "\u001b[1m\u001b[36minputs\u001b[m\u001b[m/\n"
     ]
    }
   ],
   "source": [
    "%ls\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, create constants for all the directories of interest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configs directory exists: True\n",
      "Inputs directory exists: True\n",
      "Outputs directory exists: True\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "CONFIGS_DIR = Path(\"configs\")\n",
    "INPUTS_DIR = Path(\"inputs\")\n",
    "OUTPUTS_DIR = Path(\"outputs\")\n",
    "\n",
    "print(f\"Configs directory exists: {CONFIGS_DIR.exists()}\")\n",
    "print(f\"Inputs directory exists: {INPUTS_DIR.exists()}\")\n",
    "print(f\"Outputs directory exists: {OUTPUTS_DIR.exists()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample directory exists: True\n"
     ]
    }
   ],
   "source": [
    "SAMPLE_DIR = Path(\"inputs/tests/01_sample_small\")\n",
    "\n",
    "print(f\"Sample directory exists: {SAMPLE_DIR.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Understanding Our LLM Judge Metrics\n",
    "\n",
    "Before diving into the code, let's understand what LLM judges we will build and how they work with our evaluation dataset.\n",
    "\n",
    "### The Evaluation Dataset Structure\n",
    "\n",
    "Remember that in Lesson 28, we created an evaluation dataset where each sample contains:\n",
    "- **Article Guideline**: The user intent describing how the article should look\n",
    "- **Research**: The source material the article should be based on  \n",
    "- **Expected Article (Ground Truth)**: The reference article to compare against\n",
    "\n",
    "The key insight is that **the generated article is computed on the fly**. Every time we make changes to our AI app (Brown), we regenerate the articles and recompute the metrics. This is why everything else is staticâ€”we only vary the generated outputs and system parameters during optimization.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/iusztinpaul/agentic-ai-engineering-course-data/main/images/30_ai_evals_offline_metrics_dataset_samples.png\" alt=\"30_ai_evals_offline_metrics_dataset_samples\" height=600/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Two LLM Judge Metrics\n",
    "\n",
    "We will build two complementary LLM judge metrics:\n",
    "\n",
    "1. **FollowsGT (Follows Ground Truth)**: Compares the **generated article** with the **expected article** across three dimensions:\n",
    "   - **Content**: Does the generated section cover the same topics and ideas?\n",
    "   - **Flow**: Does it follow the same order of ideas and transitions?\n",
    "   - **Structure**: Does it use the same formatting patterns (headers, lists, code blocks)?\n",
    "\n",
    "2. **UserIntent**: Compares the **generated article** with the **input** (article guideline and research) across two dimensions:\n",
    "   - **Guideline Adherence**: Does the generated article follow the specific requirements from the article guideline?\n",
    "   - **Research Anchoring**: Is the content based solely on the provided research, or does it hallucinate?\n",
    "\n",
    "### Section-Level Binary Metrics\n",
    "\n",
    "Our LLM judges use **binary metrics scoped at the section level** instead of the whole article level. For each article, we compute all the metrics from above (content, flow, structure, guideline adherencem research anchoring) for each section individually.\n",
    "\n",
    "**Why this approach?**\n",
    "- **More signal**: Binary metrics at the section level give us granular insights into what went wrong\n",
    "- **Easier calibration**: It's easier to align binary (0/1) judgments with human expectations\n",
    "- **Independence**: For all our metrics, sections are independent units\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/iusztinpaul/agentic-ai-engineering-course-data/main/images/30_ai_evals_offline_metrics_section_level.png\" alt=\"30_ai_evals_offline_metrics_section_level\" height=600/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset Splits for LLM Judge Development\n",
    "\n",
    "We split our dataset strategically to develop and validate our LLM judges:\n",
    "\n",
    "- **Training Split (Few-Shot Examples)**: Used as few-shot examples to \"train\" the LLM judges. These manually labeled examples teach the judge how to score correctly.\n",
    "- **Validation Split**: Used to align the LLM judges with human expectations. We evaluate the judges themselves on this split.\n",
    "- **Testing Split**: Used to compute the final metrics on unseen data.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/iusztinpaul/agentic-ai-engineering-course-data/main/images/30_ai_evals_offline_metrics_dataset_splits.png\" alt=\"30_ai_evals_offline_metrics_dataset_splits\" height=600/>\n",
    "\n",
    "Remember that in the previous lesson, where we constructed the AI evals dataset, Lessons 4 and 7 are marked as `is_few_shot_example=True`, meaning we will use these two lessons as few-shot examples to \"train\" our LLM judges.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. The FollowsGT Metric: Comparing Against Ground Truth\n",
    "\n",
    "Let's dive into the `FollowsGTMetricLLMJudge`, our first LLM judge metric. We'll examine the base abstractions used across all the metrics and then the specific implementation.\n",
    "\n",
    "### 4.1 The BrownBaseMetric Abstraction\n",
    "\n",
    "All our LLM judge metrics inherit from `BrownBaseMetric`, which provides common functionality for model initialization, structured output parsing, and scoring interfaces.\n",
    "\n",
    "Source: `brown.evals.metrics.base`\n",
    "\n",
    "```python\n",
    "class BrownBaseMetric(base_metric.BaseMetric, Generic[FewShotExamplesT, StructuredOutputTypeT], abc.ABC):\n",
    "    \"\"\"Abstract base class for Brown evaluation metrics that use LLMs for structured scoring.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: SupportedModels,\n",
    "        name: str,\n",
    "        structured_output_type: type[StructuredOutputTypeT],\n",
    "        few_shot_examples: FewShotExamplesT,\n",
    "        model_config: ModelConfig,\n",
    "        track: bool = True,\n",
    "        project_name: str | None = None,\n",
    "    ) -> None:\n",
    "        super().__init__(\n",
    "            name=name,\n",
    "            track=track,\n",
    "            project_name=project_name,\n",
    "        )\n",
    "\n",
    "        self.model = model\n",
    "        self.structured_output_type = structured_output_type\n",
    "        self.model_config = model_config\n",
    "        self.few_shot_examples = few_shot_examples\n",
    "\n",
    "        if self.model == SupportedModels.FAKE_MODEL:\n",
    "            assert self.model_config and self.model_config.mocked_response is not None\n",
    "\n",
    "    def init_model(self) -> Runnable:\n",
    "        \"\"\"Initialize the language model with structured output capabilities.\"\"\"\n",
    "        model_instance = get_model(self.model, self.model_config)\n",
    "        model_instance = model_instance.with_structured_output(self.structured_output_type)\n",
    "        return model_instance\n",
    "\n",
    "    def score(self, *args: Any, **kwargs: Any) -> score_result.ScoreResult | list[score_result.ScoreResult]:\n",
    "        \"\"\"Calculate evaluation scores synchronously by wrapping the async implementation.\"\"\"\n",
    "        return a.asyncio_run(self.ascore(*args, **kwargs))\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    async def ascore(self, *args: Any, **kwargs: Any) -> score_result.ScoreResult | list[score_result.ScoreResult]:\n",
    "        \"\"\"Abstract async method for implementing metric-specific evaluation logic.\"\"\"\n",
    "        pass\n",
    "```\n",
    "\n",
    "**Key Features:**\n",
    "- **Generic Type Parameters**: `FewShotExamplesT` for few-shot examples and `StructuredOutputTypeT` for the LLM response format\n",
    "- **Model Initialization**: `init_model()` creates a fresh model instance with structured output for each evaluation\n",
    "- **Sync/Async Support**: `score()` wraps the async `ascore()` method for synchronous usage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 The FollowsGTMetricLLMJudge Implementation\n",
    "\n",
    "Now let's look at the concrete implementation:\n",
    "\n",
    "Source: `brown.evals.metrics.follows_gt.metric`\n",
    "\n",
    "```python\n",
    "class FollowsGTMetricLLMJudge(BrownBaseMetric):\n",
    "    \"\"\"A metric that evaluates the quality of article content across multiple sections and dimensions.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: SupportedModels = SupportedModels.GOOGLE_GEMINI_25_FLASH,\n",
    "        name: str = \"follows_gt\",\n",
    "        model_config: ModelConfig | None = None,\n",
    "        track: bool = True,\n",
    "        project_name: str | None = None,\n",
    "    ) -> None:\n",
    "        model_config = model_config or ModelConfig(\n",
    "            temperature=0.0, thinking_budget=1024 * 4, include_thoughts=False, max_retries=3\n",
    "        )\n",
    "        super().__init__(\n",
    "            model=model,\n",
    "            name=name,\n",
    "            structured_output_type=FollowsGTArticleScores,\n",
    "            few_shot_examples=prompts.DEFAULT_FEW_SHOT_EXAMPLES,\n",
    "            model_config=model_config,\n",
    "            track=track,\n",
    "            project_name=project_name,\n",
    "        )\n",
    "\n",
    "    async def ascore(\n",
    "        self,\n",
    "        output: str,\n",
    "        expected_output: str,\n",
    "        **ignored_kwargs: Any,\n",
    "    ) -> score_result.ScoreResult | list[score_result.ScoreResult]:\n",
    "        # Initialize the model client at the function level to avoid coroutine reuse issues\n",
    "        model_client = self.init_model()\n",
    "\n",
    "        llm_query = prompts.get_eval_prompt(\n",
    "            output=output,\n",
    "            expected_output=expected_output,\n",
    "            few_shot_examples=self.few_shot_examples,\n",
    "        )\n",
    "        article_response = cast(\n",
    "            FollowsGTArticleScores,\n",
    "            await model_client.ainvoke([{\"role\": \"user\", \"content\": llm_query}]),\n",
    "        )\n",
    "\n",
    "        if not article_response:\n",
    "            raise ValueError(\"Model failed to return a structured response.\")\n",
    "\n",
    "        return article_response.to_score_result(self.name)\n",
    "```\n",
    "\n",
    "**Key Points:**\n",
    "- Uses `FollowsGTArticleScores` as the structured output type\n",
    "- Takes `output` (generated article) and `expected_output` (ground truth) as inputs\n",
    "- Returns a list of `ScoreResult` objects, one per dimension (content, flow, structure)\n",
    "\n",
    "Still, the real magic happens in how we define our `FollowsGTArticleScores` scoring entities and within the prompt. Let's start with the structured output entities. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 The Score Types Hierarchy\n",
    "\n",
    "The structured output uses a hierarchy of Pydantic models:\n",
    "\n",
    "Source: `brown.evals.metrics.follows_gt.types`\n",
    "\n",
    "1. **FollowsGTArticleScores** - Top-level container for all section scores:\n",
    "\n",
    "```python\n",
    "class FollowsGTArticleScores(pydantic.BaseModel):\n",
    "    \"\"\"Article-level scores for the FollowsGT evaluation metric.\"\"\"\n",
    "\n",
    "    sections: list[FollowsGTSectionScores]\n",
    "\n",
    "    def to_score_result(self, prefix: str) -> list[score_result.ScoreResult]:\n",
    "        \"\"\"Convert the evaluation results to ScoreResult objects with dimension-wise scoring.\"\"\"\n",
    "        return aggregate_section_scores_to_results(self.sections, prefix)\n",
    "```\n",
    "\n",
    "2. **FollowsGTSectionScores** - Scores for a single section:\n",
    "\n",
    "```python\n",
    "class FollowsGTSectionScores(pydantic.BaseModel):\n",
    "    \"\"\"Section evaluation with scores across all FollowsGT dimensions.\"\"\"\n",
    "\n",
    "    title: str = pydantic.Field(description=\"The title of the section being evaluated.\")\n",
    "    scores: FollowsGTCriterionScores = pydantic.Field(description=\"The scores of the section.\")\n",
    "```\n",
    "\n",
    "3. **FollowsGTCriterionScores** - The three dimension scores:\n",
    "\n",
    "```python\n",
    "class FollowsGTCriterionScores(CriteriaScores):\n",
    "    \"\"\"Represents scores for all three evaluation dimensions of a section.\"\"\"\n",
    "\n",
    "    content: CriterionScore\n",
    "    flow: CriterionScore\n",
    "    structure: CriterionScore\n",
    "```\n",
    "\n",
    "The only custom abstractions appear within the `CriteriaScores` Pydantic model. Let's see how that works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Base Score Classes\n",
    "\n",
    "Source: `brown.evals.metrics.base`\n",
    "\n",
    "1. **CriterionScore** - We first define a single binary score with it's reasoning trace:\n",
    "\n",
    "```python\n",
    "class CriterionScore(pydantic.BaseModel):\n",
    "    \"\"\"Base model for a single score representing a specific evaluation dimension.\"\"\"\n",
    "\n",
    "    score: Annotated[int, Ge(0), Le(1)] = pydantic.Field(description=\"Binary score of the section.\")\n",
    "    reason: str = pydantic.Field(description=\"The reason for the given score.\")\n",
    "```\n",
    "\n",
    "2. **CriteriaScores** - Then we define a base class that aggregates multiple scores within a single unit and knows how to translate it to context for LLMs. We used this to define the `FollowsGTCriterionScores` from above, which are used for each section of one article:\n",
    "\n",
    "```python\n",
    "class CriteriaScores(pydantic.BaseModel):\n",
    "    \"\"\"Abstract base class for scores across multiple evaluation dimensions.\"\"\"\n",
    "\n",
    "    def to_context(self) -> str:\n",
    "        \"\"\"Convert the scores to a formatted XML string for use as context in prompts.\"\"\"\n",
    "        scores_fields = self.__class__.model_fields\n",
    "        scores_xml = \"\"\n",
    "        for field_name in scores_fields.keys():\n",
    "            field_score = getattr(self, field_name)\n",
    "            scores_xml += f\"\"\"    <{field_name}>\n",
    "        <score>{field_score.score}</score>\n",
    "        <reason>{field_score.reason}</reason>\n",
    "    </{field_name}>\n",
    "\"\"\"\n",
    "        return scores_xml\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 The Aggregation Function\n",
    "\n",
    "The `aggregate_section_scores_to_results` function is the key to converting section-level scores to final metrics:\n",
    "\n",
    "Source: `brown.evals.metrics.base`\n",
    "\n",
    "```python\n",
    "def aggregate_section_scores_to_results(\n",
    "    section_scores: list,\n",
    "    prefix: str,\n",
    ") -> list[score_result.ScoreResult]:\n",
    "    \"\"\"Convert section-level evaluation results to aggregated ScoreResult objects.\"\"\"\n",
    "    if not section_scores:\n",
    "        return []\n",
    "\n",
    "    # Automatically infer dimensions from the first section's scores class\n",
    "    scores_class = type(section_scores[0].scores)\n",
    "    scores_fields = scores_class.model_fields\n",
    "    aggregated_scores: dict[str, dict[str, list[int] | str]] = {\n",
    "        field_name: {\"scores\": [], \"reason\": \"\"}\n",
    "        for field_name in scores_fields.keys()\n",
    "    }\n",
    "\n",
    "    for section in section_scores:\n",
    "        for dimension in aggregated_scores.keys():\n",
    "            dimension_score = getattr(section.scores, dimension)\n",
    "            aggregated_scores[dimension][\"scores\"].append(dimension_score.score)\n",
    "            aggregated_scores[dimension][\"reason\"] += f\"{section.title}:\\n\"\n",
    "            aggregated_scores[dimension][\"reason\"] += f\"**{dimension_score.score}:** {dimension_score.reason}\\n\\n\"\n",
    "\n",
    "    results: list[score_result.ScoreResult] = []\n",
    "    for dimension, scores_data in aggregated_scores.items():\n",
    "        scores_list = scores_data[\"scores\"]\n",
    "        aggregated_score = CriterionAggregatedScore(\n",
    "            name=f\"{prefix}_{dimension}\",\n",
    "            score=sum(scores_list) / len(scores_list),\n",
    "            reason=str(scores_data[\"reason\"]),\n",
    "        )\n",
    "        results.append(aggregated_score.to_score_result())\n",
    "\n",
    "    return results\n",
    "```\n",
    "\n",
    "**How it works:**\n",
    "1. **Infer dimensions**: Automatically detects score dimensions (content, flow, structure) from the Pydantic model\n",
    "2. **Collect scores**: Gathers binary scores and reasons from each section\n",
    "3. **Compute averages**: Calculates the mean score per dimension across all sections. As each is a binary score (0 or 1), the average will result in a number within the `[0, 1]` interval.\n",
    "4. **Build reasons**: Concatenates section-level reasons for debugging\n",
    "5. **Return ScoreResults**: Produces one `ScoreResult` per dimension (e.g., `follows_gt_content: 0.75`, `follows_gt_flow: 0.6`)\n",
    "\n",
    "We used this function within the `FollowsGTArticleScores` class to transform the list of scores for each section into a single aggregate score for the entire article.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Few-Shot Examples Classes\n",
    "\n",
    "The few-shot examples teach the LLM judge how to score correctly. \n",
    "\n",
    "Remember how we split the dataset between training, validation and testing? With these classes we model the training split passed to the LLM judge within the system prompt.\n",
    "\n",
    "Source: `brown.evals.metrics.follows_gt.types`\n",
    "\n",
    "```python\n",
    "class FollowsGTMetricFewShotExample(BaseExample):\n",
    "    \"\"\"Represents a single example for the follows_gt evaluation.\"\"\"\n",
    "\n",
    "    output: str\n",
    "    expected_output: str\n",
    "    scores: FollowsGTArticleScores\n",
    "\n",
    "    @classmethod\n",
    "    def from_markdown(\n",
    "        cls, output_file: Path, expected_output_file: Path, scores: FollowsGTArticleScores\n",
    "    ) -> \"FollowsGTMetricFewShotExample\":\n",
    "        output = output_file.read_text()\n",
    "        expected_output = expected_output_file.read_text()\n",
    "        return cls(output=output, expected_output=expected_output, scores=scores)\n",
    "\n",
    "    def to_context(self) -> str:\n",
    "        return f\"\"\"\n",
    "<output>\n",
    "{self.output}\n",
    "</output>\n",
    "<expected_output>\n",
    "{self.expected_output}\n",
    "</expected_output>\n",
    "{self.scores.to_context()}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class FollowsGTMetricFewShotExamples(BaseFewShotExamples[FollowsGTMetricFewShotExample]):\n",
    "    \"\"\"Collection of few-shot examples for the FollowsGT evaluation metric.\"\"\"\n",
    "    pass\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the base classes that define the common interface and functionality of the few-shot examples:\n",
    "\n",
    "Source: `brown.evals.metrics.base`\n",
    "\n",
    "```python\n",
    "class BaseExample(pydantic.BaseModel, Generic[ExampleT]):\n",
    "    \"\"\"Base class for examples used in evaluation metrics.\"\"\"\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def to_context(self) -> str:\n",
    "        \"\"\"Convert the example to a formatted string for use as context in prompts.\"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class BaseFewShotExamples(pydantic.BaseModel, Generic[ExampleT]):\n",
    "    \"\"\"Base class for few-shot examples collections.\"\"\"\n",
    "\n",
    "    examples: list[ExampleT]\n",
    "\n",
    "    def to_context(self) -> str:\n",
    "        examples = \"\\n\\n\".join(\n",
    "            [f\"<example_{i + 1}>\\n\\t{example.to_context()}\\n</example_{i + 1}>\\n\" \n",
    "             for i, example in enumerate(self.examples)]\n",
    "        )\n",
    "        return examples\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7 The System Prompt\n",
    "\n",
    "The system prompt is carefully engineered to guide the LLM judge in scoring each section of the article using our metrics: content, flow, and structure. \n",
    "\n",
    "Here is the prompt divided by key sections:\n",
    "\n",
    "Source: `brown.evals.metrics.follows_gt.prompts`\n",
    "\n",
    "**1. Introduction:** - Sets the role and general task:\n",
    "```python\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are an expert in Natural Language Processing (NLP) evaluation metrics, specifically trained to \n",
    "assess answer quality in responses provided by large language models (LLMs). \n",
    "\n",
    "Your task is to evaluate the quality of a generated article by another LLM relative to \n",
    "an expected article output across multiple criteria: content, flow, structure, and mechanics.\n",
    "...\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "**2. Scoring Instructions** - Section-level binary scoring across the content, flow and structure dimensions. Note how detailed we defined each step of the LLM judge on how to use the expected and generated outputs, plus how to reason about the output business metrics:\n",
    "```python\n",
    "\"\"\"\n",
    "...\n",
    "## INSTRUCTIONS \n",
    "\n",
    "1. You must analyze the given expected article (<expected_output>) and generated article (<generated_output>) \n",
    "to determine the most relevant evaluation.\n",
    "2. Since the generated output is an answer from another LLM, you must use the expected output as the reference \n",
    "standard to compare and evaluate the quality of the generated output.\n",
    "3. Both the generated and expected outputs are in Markdown format.\n",
    "4. Instead of comparing the outputs as a whole, you will divide the outputs into sections and compare each section \n",
    "individually. \n",
    "5. You will always use the expected output as the reference point to extract the sections of interest during the\n",
    "evaluation. If there is no perfect match between the expected and generated section names, first try to infer\n",
    "the corresponding section based on the similarity of section names and their respective content. If you conclude that \n",
    "the expected output contains a section that the generated output lacks, you will assign a score of 0 to the missing \n",
    "section in the generated output.\n",
    "6. Sections are divided by H2 headers, marked as \"##\" in Markdown. You will use these headers as \n",
    "separators. Anything between two H2 headers constitutes a section. The only valid exception to this rule is the first \n",
    "section, the introduction, which sometimes appears between the title and the first H2 header. You will never include \n",
    "the title or subtitle as part of the first section.\n",
    "7. When comparing each individual section of the expected output to the generated output, you will assign a binary \n",
    "score for multiple criteria: 0 or 1, where 0 indicates a non-match and 1 indicates a perfect match. Each criterion\n",
    "is completely independent of the others, meaning that a score of 0 in one criterion does not affect the score of \n",
    "another criterion.\n",
    "8. You must compute binary scores for each section based on the following criteria:\n",
    "  1. **Content:** Evaluate whether the generated section covers the same content as the expected section:\n",
    "    - Focus only on evaluating that the substance of the content is the same between the expected and generated section. By content, we \n",
    "    mean core subjects, topics, research, ideas, key points or arguments. For example, if both sections discuss the\n",
    "    fundamentals of RAG, it's valid. But, if the expected section discusses advanced RAG topics, while the generated section\n",
    "    discusses basic RAG topics, it's invalid.\n",
    "    - In this criterion, we are not interested in the order, structure, layout, or any other aspect related to the flow of\n",
    "    ideas, structure or mechanics. For example, if there are missing or additional ideas discussed it's still valid, as\n",
    "    long as the substance of the content between the expected and generated section is the same.\n",
    "  2. **Flow:** Evaluate whether the generated section follows the same order of ideas as the expected section, such as \n",
    "  the flow of:\n",
    "    - Main ideas covered starting with the beginning, until the end of the section. With special emphasis on the beginning and end of the \n",
    "    section as they reflect the transition between the previous and next sections\n",
    "    - Internal transitions between the main points within the section. We expect a smooth flow of ideas, \n",
    "    without any abrupt jumps or breaks.\n",
    "    - Placement of notes, images, tables, code blocks, or any other media elements within the generated section, \n",
    "    relative to the expected section. \n",
    "    - We don't expect a perfect one on one match between the paragraphs and sentences between the expected and generated section.\n",
    "    However, we expect the same ideas and concepts to be discussed in the same way, order, and storyline.\n",
    "    - Assign a score of 0 if anything is missing from the generated section relative to the expected section, such as missing\n",
    "    topics, ideas, or media.\n",
    "    - Assign a score of 0 if anything is additionally added to the generated section relative to the expected section, such \n",
    "    as additional topics, ideas or media elements.\n",
    "    - Accepted differences between the expected and generated section:\n",
    "        - Mismatching media numbering is accepted. For example, if in the expected section we have a figure with the number 3 and\n",
    "        in the generated section we have a figure with the number 4, it's valid. It will be invalid, only if the figure would be\n",
    "        missing altogether.\n",
    "        - Mismatching or missing emojis. For example, if the excepted section has a ðŸ’¡ emoji, while the generated section has \n",
    "        a ðŸ”‘ emoji, it's valid. Also, if the emoji is missing altogether from the generated section, it's valid.\n",
    "        - Mismatching source reference numbers. For example, if the expected section referes a source with the number 3, \n",
    "        while the generated section referes a source with the number 7, it's valid. It will be invalid, only if the generated\n",
    "        section misses the source altogether. \n",
    "        - Different placement of the source in the generated section. For example, if the expected section has the source\n",
    "        at the end of a sentence within the paragraph, while the generated section has it at the end of the paragraph, it's valid. \n",
    "        It will be invalid, only if the generated section would be missing altogether.\n",
    "        - Mismatching number of source references. For example, if the expected section has 3 source references, while the \n",
    "        generated section has 2 source references, it's valid. It will be invalid, only if the generated section would have \n",
    "        misses the references altogether. For example if the expected section has 3 source references, while the generated \n",
    "        section has 0 source references, it's invalid.\n",
    "        - Having reference numbers in the generated section, while having none in the expected section. For example, if the \n",
    "        expected section has 0 reference numbers, while the generated section has 3 reference number, it's valid. It will be \n",
    "        invalid, only the other way around, where the expected section has 3 reference numbers, while the generated section has 0.\n",
    "  3. **Structure:** Evaluate whether the generated section follows the same structure as the expected section. By \n",
    "  structure, we mean:\n",
    "    - H3/H4/H5/H6 sub-heading structure and formatting\n",
    "    - Mismatches in headers formatting and presence. For example, if the expected section doesn't has a header,\n",
    "    while the generated section has one, it's invalid. It's valid only if there is a one on one match between the headers\n",
    "    formatting and presence.\n",
    "    - Use of bulleted lists, numbered lists, callouts, notes, or other layout elements\n",
    "    - Division of the section when guiding readers through code blocks or diagrams\n",
    "    - Formatting of notes and code blocks\n",
    "    - Use of bolding, italicizing, quotes, backticks, or other formatting elements\n",
    "    - Formatting of citation references across sentences\n",
    "    - Formatting of images, tables, and Mermaid diagrams and their corresponding citations. If they are missing from\n",
    "    the generated section, we consider it valid for this criterion, as we are interested ONLY in formatting, which we\n",
    "    cannot verify when elements are absent. For this criterion, missing elements from the generated sections are \n",
    "    considered valid. They are invalid only if present in both sections but formatted differently.\n",
    "    - Number formatting conventions\n",
    "9. Along with the binary scores, you will provide a brief and concise explanation containing the reasoning behind \n",
    "the score for each criterion. The score will be used to debug and monitor the evaluation process. Therefore, it is\n",
    "important to provide thorough reasoning for the score. Since we provide binary scores, the reasoning should always \n",
    "contain what is good and what is problematic about the generated section, regardless of the score. For example, if the \n",
    "score is 0, the reasoning should also contain what is good about the generated section, such as \"both sections \n",
    "follow the same flow of ideas,\" and what is problematic, such as \"the generated section contains an additional \n",
    "paragraph on AI Evals that is not present in the expected section.\"\n",
    "10. Important rules when comparing the content of sections:\n",
    "    - Focus on substance, not superficial formatting differences\n",
    "    - When comparing **media**, you only care about the placement of the media, not the content of the media. \n",
    "    Since media can take many forms such as Mermaid diagrams, tables, images, or URLs, you will completely ignore the \n",
    "    content of the media and only check whether the media is present in the correct place in the section, has \n",
    "    the appropriate citation, and proper numbering.\n",
    "...\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "**3. Chain of Thought** - Helping the LLM reason across all the intructions:\n",
    "```python\n",
    "\"\"\"\n",
    "...\n",
    "## CHAIN OF THOUGHT\n",
    "\n",
    "**Understanding Input:**\n",
    "1.1. Read, understand, and compare each section of the expected output and generated output.\n",
    "1.2. Since we want to compute scores for each section of the expected output Markdown file, split the expected output \n",
    "into sections using the H2 headers as separators.\n",
    "\n",
    "**Splitting into Sections:**\n",
    "2.1. Using the expected output as the reference point, compare each section of the expected and generated \n",
    "outputs individually and assign a binary score of 0 or 1, where 0 indicates a mismatch and 1 indicates a perfect match.\n",
    "2.2. Always use the expected output as the reference point to extract the sections of interest. \n",
    "2.3. When computing the score for an individual section, you will iterate through each section of the expected output, \n",
    "find its associated section in the generated output, and compute the score in isolation, ignoring all other sections.\n",
    "\n",
    "**Assigning Scores to Each Section:**\n",
    "3.1. Based on all sections of the expected output, assign a binary score of either 0 or 1 \n",
    "for all evaluation criteria listed in the instructions:\n",
    "    - **1:** The generated section matches the expected section perfectly on the given criterion.\n",
    "    - **0:** The generated section does not match the expected section on the given criterion.\n",
    "3.2. Justify why you assigned a score of 0 or 1 with a brief explanation that highlights the reasoning behind the score\n",
    "based on the given criterion.\n",
    "...\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "**4. What to Avoid** - as a good practice to highlight key reasoning steps:\n",
    "```python\n",
    "\"\"\"\n",
    "...\n",
    "## WHAT TO AVOID\n",
    "\n",
    "- Do not provide scores using the generated output as the reference point to divide into sections. You must always \n",
    "use the expected output as the reference point to divide into sections.\n",
    "- Do not let other sections influence the score of a section. The score of each section must be determined in complete \n",
    "isolation from any other section.\n",
    "- Do not overlap requirements between different criteria. For example, in the content criterion, as are not interested in the flow \n",
    "of ideas, if the ideas are in different order, or something is missing or additional, it's still valid. However, that is an important\n",
    "aspect of the flow criterion, which will be invalid.\n",
    "...\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "**5. Few-shot examples and input placeholders:**\n",
    "```python\n",
    "\"\"\"\n",
    "...\n",
    "## FEW-SHOT EXAMPLES\n",
    "\n",
    "Here are few-shot examples demonstrating how to compute the scores for each section and criterion:\n",
    "<few-shot-examples>\n",
    "{examples}\n",
    "</few-shot-examples>\n",
    "\n",
    "## INPUTS\n",
    "\n",
    "<generated_output>\n",
    "{output}\n",
    "</generated_output>\n",
    "\n",
    "<expected_output>\n",
    "{expected_output}\n",
    "</expected_output>\n",
    "...\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "**6. Final anchoring to the requested task:**\n",
    "```python\n",
    "\"\"\"\n",
    "...\n",
    "Think through your answer step by step, and provide the requested evaluation.\n",
    "\"\"\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.8 Plugging the Few-Shot Examples Into the System Prompt\n",
    "\n",
    "Now that we looked over how we model the few-shot examples entities and understood the system prompt, it's time to glue them together.\n",
    "\n",
    "To do that, we have to define the few-shot examples that we manually created by:\n",
    "1. Generating articles based on the article guideline and research inputs.\n",
    "2. Adding noise to the generated article to create variation at the section level. Remember that in reality each section acts as it's own independent example. \n",
    "3. Manually labeling each section along each metric dimension.\n",
    "\n",
    "Here's how we strctured the few-shot examples using the `FollowsGTMetricFewShotExamples`, `FollowsGTMetricFewShotExample` and `FollowsGTArticleScores` entity classes.\n",
    "\n",
    "Source: `brown.evals.metrics.follows_gt.prompts`\n",
    "\n",
    "```python\n",
    "EXAMPLES_DIR = Path(__file__).parent / \"examples\"\n",
    "EXAMPLES_DIR = Path(__file__).parent / \"examples\"\n",
    "DEFAULT_FEW_SHOT_EXAMPLES = FollowsGTMetricFewShotExamples(\n",
    "    examples=[\n",
    "        FollowsGTMetricFewShotExample.from_markdown(\n",
    "            output_file=EXAMPLES_DIR / \"04_structured_outputs\" / \"article_generated.md\",\n",
    "            expected_output_file=EXAMPLES_DIR / \"04_structured_outputs\" / \"article_ground_truth.md\",\n",
    "            scores=FollowsGTArticleScores(\n",
    "                sections=[\n",
    "                    FollowsGTSectionScores(\n",
    "                        title=\"Introduction\",\n",
    "                        scores=FollowsGTCriterionScores(\n",
    "                            content=CriterionScore(\n",
    "                                score=1,\n",
    "                                reason=(\n",
    "                                    \"Both sections cover the same core subjects and ideas, discussing the purpose \"\n",
    "                                    \"of structured outputs as a bridge between LLMs and traditional applications.\"\n",
    "                                ),\n",
    "                            ),\n",
    "                            flow=CriterionScore(\n",
    "                                score=0,\n",
    "                                reason=(\n",
    "                                    \"The generated section lacks the first sentence, which is used as a smooth \"\n",
    "                                    \"transition into the article. Also, it misses the diagram present in the \"\n",
    "                                    \"expected output, labeled as Figure 1.\"\n",
    "                                ),\n",
    "                            ),\n",
    "                            structure=CriterionScore(\n",
    "                                score=1,\n",
    "                                reason=\"Both sections use the same paragraph length patterns.\",\n",
    "                            ),\n",
    "                        ),\n",
    "                    ),\n",
    "                    FollowsGTSectionScores(\n",
    "                        title=\"Why Structured Outputs Are Critical\",\n",
    "                        scores=FollowsGTCriterionScores(\n",
    "                            content=CriterionScore(\n",
    "                                score=0,\n",
    "                                reason=(\n",
    "                                    \"Both sections cover the same reasons why structured outputs are critical, \"\n",
    "                                    \"including ease of parsing, data validation with Pydantic, and common use \"\n",
    "                                    \"cases. Still, the generated section has a section on GraphRAG, which is not \"\n",
    "                                    \"related to the specific topic of the section.\"\n",
    "                                ),\n",
    "                            ),\n",
    "                            flow=CriterionScore(\n",
    "                                score=0,\n",
    "                                reason=(\n",
    "                                    \"Follows a similar logical flow, starting with the importance, detailing \"\n",
    "                                    \"benefits, discussing use cases, and concluding with a diagram. Still, the \"\n",
    "                                    \"generated section contains an additional paragraph on GraphRAG, which \"\n",
    "                                    \"doesn't fit with the expected flow. Additionally, it omits the last sentence, \"\n",
    "                                    \"which is necessary for a smooth transition to the next section.\"\n",
    "                                ),\n",
    "                            ),\n",
    "                            structure=CriterionScore(\n",
    "                                score=1,\n",
    "                                reason=(\n",
    "                                    \"Both sections use the same paragraph length patterns and have the same usage \"\n",
    "                                    \"pattern for backticks and citation references across sentences. Also, the figures \"\n",
    "                                    \"and their corresponding citations use the same formatting rules.\"\n",
    "                                ),\n",
    "                            ),\n",
    "                        ),\n",
    "                    ),\n",
    "                    FollowsGTSectionScores(\n",
    "                        title=\"Implementing Structured Outputs From Scratch Using JSON\",\n",
    "                        scores=FollowsGTCriterionScores(\n",
    "                            content=CriterionScore(\n",
    "                                score=1,\n",
    "                                reason=(\n",
    "                                    \"Both sections provide a step-by-step guide on implementing structured outputs \"\n",
    "                                    \"using JSON from scratch, covering client setup, document definition, prompt \"\n",
    "                                    \"crafting, and parsing.\"\n",
    "                                ),\n",
    "                            ),\n",
    "                            flow=CriterionScore(\n",
    "                                score=0,\n",
    "                                reason=\"The generated section omits the Note callout box present in the expected output.\",\n",
    "                            ),\n",
    "                            structure=CriterionScore(\n",
    "                                score=0,\n",
    "                                reason=(\n",
    "                                    \"The generated section incorrectly formats the JSON code block under point 4), \"\n",
    "                                    \"where it misses the closing ```. Also, in the last section, where it outputs \"\n",
    "                                    \"the final JSON structure, it doesn't enclose the JSON into Python backticks \"\n",
    "                                    \"as expected: ```python <content> ```\"\n",
    "                                ),\n",
    "                            ),\n",
    "                        ),\n",
    "                    ),\n",
    "                    ... # The rest of the sections \n",
    "                    FollowsGTSectionScores(\n",
    "                        title=\"Structured Outputs Are Everywhere\",\n",
    "                        scores=FollowsGTCriterionScores(\n",
    "                            content=CriterionScore(\n",
    "                                score=0,\n",
    "                                reason=(\n",
    "                                    \"Both sections serve as a conclusion, summarizing the importance of structured \"\n",
    "                                    \"outputs as a fundamental pattern. Still, the generated section misses the last \"\n",
    "                                    \"paragraph that presents how structured outputs fit in the course and the AI \"\n",
    "                                    \"Engineering field, which is critical for the conclusion.\"\n",
    "                                ),\n",
    "                            ),\n",
    "                            flow=CriterionScore(\n",
    "                                score=0,\n",
    "                                reason=(\n",
    "                                    \"Both sections follow a similar flow, summarizing the key takeaway. Still, the \"\n",
    "                                    \"generated section misses the last paragraph on looking ahead to future lessons \"\n",
    "                                    \"in the course.\"\n",
    "                                ),\n",
    "                            ),\n",
    "                            structure=CriterionScore(\n",
    "                                score=0,\n",
    "                                reason=(\n",
    "                                    \"Both sections use the same paragraph length patterns. Still, the number \"\n",
    "                                    \"formatting of the citation reference from the first paragraph misses the \"\n",
    "                                    \"square brackets.\"\n",
    "                                ),\n",
    "                            ),\n",
    "                        ),\n",
    "                    ),\n",
    "                    FollowsGTSectionScores(\n",
    "                        title=\"References\",\n",
    "                        scores=FollowsGTCriterionScores(\n",
    "                            content=CriterionScore(\n",
    "                                score=1,\n",
    "                                reason=\"Both sections contain a list of references, similar in purpose.\",\n",
    "                            ),\n",
    "                            flow=CriterionScore(\n",
    "                                score=1,\n",
    "                                reason=\"Both sections follow the same flow for referencing the sources, as a numbered list from 1 to n.\",\n",
    "                            ),\n",
    "                            structure=CriterionScore(\n",
    "                                score=1,\n",
    "                                reason=(\n",
    "                                    \"Both sections use the same pattern to structure the references, as a bulleted \"\n",
    "                                    \"list, where each element is structured as [<reference_number>] \"\n",
    "                                    \"[<reference_name>](<reference_url>)\"\n",
    "                                ),\n",
    "                            ),\n",
    "                        ),\n",
    "                    ),\n",
    "                ]\n",
    "            ),\n",
    "        ),\n",
    "        FollowsGTMetricFewShotExample.from_markdown(\n",
    "            output_file=EXAMPLES_DIR / \"07_reasoning_planning\" / \"article_generated.md\",\n",
    "            expected_output_file=EXAMPLES_DIR / \"07_reasoning_planning\" / \"article_ground_truth.md\",\n",
    "            scores=FollowsGTArticleScores(\n",
    "                sections=[\n",
    "                    FollowsGTSectionScores(\n",
    "                        title=\"Introduction\",\n",
    "                        scores=FollowsGTCriterionScores(\n",
    "                            content=CriterionScore(\n",
    "                                score=1,\n",
    "                                reason=(\n",
    "                                    \"Covers the same core subjects and ideas, discussing the limitations of standard \"\n",
    "                                    \"LLMs and the need for planning and reasoning in AI agents.\"\n",
    "                                ),\n",
    "                            ),\n",
    "                            flow=CriterionScore(\n",
    "                                score=0,\n",
    "                                reason=(\n",
    "                                    \"Both sections set the scene of the lesson, dicussing the 'why' behind the need for planning and \"\n",
    "                                    \"reasoning in AI agents. However, the generated introduction omits the sentences that talk about \"\n",
    "                                    \"the previous lessons and anchor the lesson within the course.\"\n",
    "                                ),\n",
    "                            ),\n",
    "                            structure=CriterionScore(\n",
    "                                score=0,\n",
    "                                reason=(\n",
    "                                    \"The generated output uses an H2 header 'Why Your Agent Needs to Think Before It Acts' \"\n",
    "                                    \"as a title for the introduction, while the expected section doesn't.\"\n",
    "                                ),\n",
    "                            ),\n",
    "                        ),\n",
    "                    ),\n",
    "                    FollowsGTSectionScores(\n",
    "                        title=\"What a Non-Reasoning Model Does And Why It Fails on Complex Tasks\",\n",
    "                        scores=FollowsGTCriterionScores(\n",
    "                            content=CriterionScore(\n",
    "                                score=1,\n",
    "                                reason=(\n",
    "                                    \"Accurately covers the core subject of why non-reasoning models fail on complex \"\n",
    "                                    \"tasks, using the same 'Technical Research Assistant Agent' example and discussing \"\n",
    "                                    \"similar failure points.\"\n",
    "                                ),\n",
    "                            ),\n",
    "                            flow=CriterionScore(\n",
    "                                score=1,\n",
    "                                reason=(\n",
    "                                    \"Follows a similar order of ideas, starting with the example, explaining the failure, \"\n",
    "                                    \"and then discussing the need for reasoning.\"\n",
    "                                ),\n",
    "                            ),\n",
    "                            structure=CriterionScore(\n",
    "                                score=1,\n",
    "                                reason=(\n",
    "                                    \"Both sections have similar paragraph length patterns and use of images and their \"\n",
    "                                    \"corresponding citations.\"\n",
    "                                ),\n",
    "                            ),\n",
    "                        ),\n",
    "                    ),\n",
    "                    ... # The rest of the sections\n",
    "                    FollowsGTSectionScores(\n",
    "                        title=\"ReAct in Depth: The Loop of Thought, Action, and Observation\",\n",
    "                        scores=FollowsGTCriterionScores(\n",
    "                            content=CriterionScore(\n",
    "                                score=1,\n",
    "                                reason=(\n",
    "                                    \"The two sections provide the same detailed explanation of the ReAct framework, \"\n",
    "                                    \"its iterative loop, and a step-by-step example using the research assistant agent.\"\n",
    "                                ),\n",
    "                            ),\n",
    "                            flow=CriterionScore(\n",
    "                                score=0,\n",
    "                                reason=(\n",
    "                                    \"Both sections begin with the same flow, introducing ReAct, explaining its loop \"\n",
    "                                    \"and presenting the diagram. The generated section has some additional reference numbers, which \"\n",
    "                                    \"is correct. Still, the generated sections wrote the primary advantages and disadvantages of \"\n",
    "                                    \"ReAct section before the hands-on example, instead of after it, as expected.\"\n",
    "                                ),\n",
    "                            ),\n",
    "                            structure=CriterionScore(\n",
    "                                score=0,\n",
    "                                reason=(\n",
    "                                    \"The generated section employs a similar strategy to format the diagram's \"\n",
    "                                    \"citation, references. However, in the expected section, the list is formatted as a \"\n",
    "                                    \"numbered list, while in the generated section, it's formatted as a bulleted list. Also, the \"\n",
    "                                    \"generated section added backquotes around the text from Action 1, 2, 3, and 4, while the \"\n",
    "                                    \"expected section does not.\"\n",
    "                                ),\n",
    "                            ),\n",
    "                        ),\n",
    "                    ),\n",
    "                    FollowsGTSectionScores(\n",
    "                        title=\"Plan-and-Execute in Depth: Structure and Predictability\",\n",
    "                        scores=FollowsGTCriterionScores(\n",
    "                            content=CriterionScore(\n",
    "                                score=1,\n",
    "                                reason=(\n",
    "                                    \"Accurately explains the Plan-and-Execute pattern, its two phases \"\n",
    "                                    \"(Planning and Execution), and its benefits for predictable tasks.\"\n",
    "                                ),\n",
    "                            ),\n",
    "                            flow=CriterionScore(\n",
    "                                score=0,\n",
    "                                reason=(\n",
    "                                    \"Both sections follow a similar logical flow, introducing the pattern, \"\n",
    "                                    \"explaining its efficiency and then detailing the planning and execution \"\n",
    "                                    \"phases with an example. The issue is that the Plan-and-Execute pattern \"\n",
    "                                    \"diagram was expected before digging into the **Planning Phase** section, and \"\n",
    "                                    \"instead, it's placed within the numbered list of the **Planning Phase** section.\"\n",
    "                                ),\n",
    "                            ),\n",
    "                            structure=CriterionScore(\n",
    "                                score=0,\n",
    "                                reason=(\n",
    "                                    \"The generated section employs a similar strategy to format the diagram's \"\n",
    "                                    \"citation, number formatting, references and the bulleted list. Still, it formats the planning \"\n",
    "                                    \"and execution phases as bolded text instead of as H3 headers.\"\n",
    "                                ),\n",
    "                            ),\n",
    "                        ),\n",
    "                    ),\n",
    "                    ... # The rest of the sections\n",
    "                    FollowsGTSectionScores(\n",
    "                        title='Reasoning Models: How LLMs\\' \"Reasoning and Planning\" are Being Internalized in LLMs',\n",
    "                        scores=FollowsGTCriterionScores(\n",
    "                            content=CriterionScore(\n",
    "                                score=0,\n",
    "                                reason=\"The generated section is completely empty.\",\n",
    "                            ),\n",
    "                            flow=CriterionScore(\n",
    "                                score=0,\n",
    "                                reason=\"The generated section is completely empty.\",\n",
    "                            ),\n",
    "                            structure=CriterionScore(\n",
    "                                score=0,\n",
    "                                reason=\"The generated section is completely empty.\",\n",
    "                            ),\n",
    "                        ),\n",
    "                    ),\n",
    "                    FollowsGTSectionScores(\n",
    "                        title=\"Conclusion\",\n",
    "                        scores=FollowsGTCriterionScores(\n",
    "                            content=CriterionScore(\n",
    "                                score=1,\n",
    "                                reason=(\n",
    "                                    \"In both sections, the conclusion summarizes the key takeaways of the article, \"\n",
    "                                    \"including the importance of planning and reasoning, and the two foundational \"\n",
    "                                    \"patterns (ReAct and Plan-and-Execute).\"\n",
    "                                ),\n",
    "                            ),\n",
    "                            flow=CriterionScore(\n",
    "                                score=1,\n",
    "                                reason=(\n",
    "                                    \"Follows a similar flow, reiterating the main points within the lesson \"\n",
    "                                    \"and setting the scene for future lessons.\"\n",
    "                                ),\n",
    "                            ),\n",
    "                            structure=CriterionScore(\n",
    "                                score=1,\n",
    "                                reason=\"Both sections have similar paragraph length, number formatting, and citation patterns.\",\n",
    "                            ),\n",
    "                        ),\n",
    "                    ),\n",
    "                    FollowsGTSectionScores(\n",
    "                        title=\"References\",\n",
    "                        scores=FollowsGTCriterionScores(\n",
    "                            content=CriterionScore(\n",
    "                                score=1,\n",
    "                                reason=\"Both sections contain a list of citations, similar in purpose.\",\n",
    "                            ),\n",
    "                            flow=CriterionScore(\n",
    "                                score=1,\n",
    "                                reason=\"Both sections follow the same flow for referencing the sources, as a numbered list from 1 to n.\",\n",
    "                            ),\n",
    "                            structure=CriterionScore(\n",
    "                                score=0,\n",
    "                                reason=(\n",
    "                                    \"Both sections use a bulleted list to enumerate the citations, \"\n",
    "                                    \"but the use of parentheses is not the same. The generated article outputs the references as \"\n",
    "                                    \"`- [<number>] <reference_name>(<url>), ` instead of `- [[<number>]](<url>) <article_name>`.\"\n",
    "                                ),\n",
    "                            ),\n",
    "                        ),\n",
    "                    ),\n",
    "                ]\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "```\n",
    "\n",
    "Let's look at more details about the few-shot examples:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of few-shot examples: 2\n",
      "\n",
      "Example 1:\n",
      "  - Output length: 20,182 characters\n",
      "  - Expected output length: 24,788 characters\n",
      "  - Number of sections scored: 7\n",
      "  - Section titles: ['Introduction', 'Why Structured Outputs Are Critical', 'Implementing Structured Outputs From Scratch Using JSON', 'Implementing Structured Outputs From Scratch Using Pydantic', 'Implementing Structured Outputs Using Gemini and Pydantic', 'Structured Outputs Are Everywhere', 'References']\n",
      "\n",
      "Example 2:\n",
      "  - Output length: 26,000 characters\n",
      "  - Expected output length: 33,942 characters\n",
      "  - Number of sections scored: 11\n",
      "  - Section titles: ['Introduction', 'What a Non-Reasoning Model Does And Why It Fails on Complex Tasks', 'Teaching Models to \"Think\": Chain-of-Thought and Its Limits', 'Separating Planning from Answering: Foundations of ReAct and Plan-and-Execute', 'ReAct in Depth: The Loop of Thought, Action, and Observation', 'Plan-and-Execute in Depth: Structure and Predictability', 'Pros and Cons: ReAct vs. Plan-and-Execute', 'Deep Research AI Assistant Systems', 'Reasoning Models: How LLMs\\' \"Reasoning and Planning\" are Being Internalized in LLMs', 'Conclusion', 'References']\n"
     ]
    }
   ],
   "source": [
    "from brown.evals.metrics.follows_gt.prompts import DEFAULT_FEW_SHOT_EXAMPLES as FOLLOWS_GT_FEW_SHOT_EXAMPLES\n",
    "\n",
    "print(f\"Number of few-shot examples: {len(FOLLOWS_GT_FEW_SHOT_EXAMPLES.examples)}\")\n",
    "for i, example in enumerate(FOLLOWS_GT_FEW_SHOT_EXAMPLES.examples):\n",
    "    print(f\"\\nExample {i + 1}:\")\n",
    "    print(f\"  - Output length: {len(example.output):,} characters\")\n",
    "    print(f\"  - Expected output length: {len(example.expected_output):,} characters\")\n",
    "    print(f\"  - Number of sections scored: {len(example.scores.sections)}\")\n",
    "    print(f\"  - Section titles: {[s.title for s in example.scores.sections]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you want to look at the full definition of the few-shot examples go to the [brown.evals.metrics.follows_gt.prompts.py](../writing_workflow/src/brown/evals/metrics/follows_gt/prompts.py) file from the Brown project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.9 The get_eval_prompt Function\n",
    "\n",
    "To actually use the system prompt from above, together with the few-shot examples we defined, we defined a `get_eval_prompt` function that assembles the final prompt. This function is used within the `FollowsGTMetricLLMJudge` metric class we defined at the beginning, right before calling the LLM:\n",
    "\n",
    "Source: `brown.evals.metrics.follows_gt.prompts`\n",
    "\n",
    "```python\n",
    "def get_eval_prompt(\n",
    "    output: str,\n",
    "    expected_output: str,\n",
    "    few_shot_examples: FollowsGTMetricFewShotExamples,\n",
    ") -> str:\n",
    "    \"\"\"Generate the evaluation prompt for the follows_gt metric.\"\"\"\n",
    "\n",
    "    return SYSTEM_PROMPT.format(\n",
    "        examples=few_shot_examples.to_context(),\n",
    "        output=output,\n",
    "        expected_output=expected_output,\n",
    "    )\n",
    "```\n",
    "\n",
    "This function takes the generated output, expected output, and few-shot examples, and formats them into the complete system prompt for the LLM judge.\n",
    "\n",
    "Here is how the system prompt looks like:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m-------------------------------- FollowsGT LLM Judge System Prompt --------------------------------\u001b[0m\n",
      "   You are an expert in Natural Language Processing (NLP) evaluation metrics, specifically trained to \n",
      "assess answer quality in responses provided by large language models (LLMs). \n",
      "\n",
      "Your task is to evaluate the quality of a generated article by another LLM relative to \n",
      "an expected article output across multiple criteria: content, flow, structure, and mechanics.\n",
      "\n",
      "## INSTRUCTIONS \n",
      "\n",
      "1. You must analyze the given expected article (<expected_output>) and generated article (<generated_output>) \n",
      "to determine the most relevant evaluation.\n",
      "2. Since the generated output is an answer from another LLM, you must use the expected output as the reference \n",
      "standard to compare and evaluate the quality of the generated output.\n",
      "3. Both the generated and expected outputs are in Markdown format.\n",
      "4. Instead of comparing the outputs as a whole, you will divide the outputs into sections and compare each section \n",
      "individually. \n",
      "5. You will always use the expected output as the reference point to extract the sections of interest during the\n",
      "evaluation. If there is no perfect match between the expected and generated section names, first try to infer\n",
      "the corresponding section based on the similarity of section names and their respective content. If you conclude that \n",
      "the expected output contains a section that the generated output lacks, you will assign a score of 0 to the missing \n",
      "section in the generated output.\n",
      "6. Sections are divided by H2 headers, marked as \"##\" in Markdown. You will use these headers as \n",
      "separators. Anything between two H2 headers constitutes a section. The only valid exception to this rule is the first \n",
      "section, the introduction, which sometimes appears between the title and the first H2 header. You will never include \n",
      "the title or subtitle as part of the first section.\n",
      "7. When comparing each individual section of the expected output to the generated output, you will assign a binary \n",
      "score for multiple criteria: 0 or 1, where 0 indicates a non-match and 1 indicates a perfect match. Each criterion\n",
      "is completely independent of the others, meaning that a score of 0 in one criterion does not affect the score of \n",
      "another criterion.\n",
      "8. You must compute binary scores for each section based on the following criteria:\n",
      "  1. **Content:** Evaluate whether the generated section covers the same content as the expected section:\n",
      "    - Focus only on evaluating that the substance of the content is the same between the expected and generated section. By content, we \n",
      "    mean core subjects, topics, research, ideas, key points or arguments. For example, if both sections discuss the\n",
      "    fundamentals of RAG, it's valid. But, if the expected section discusses advanced RAG topics, while the generated section\n",
      "    discusses basic RAG topics, it's invalid.\n",
      "    - In this criterion, we are not interested in the order, structure, layout, or any other aspect related to the flow of\n",
      "    ideas, structure or mechanics. For example, if there are missing or additional ideas discussed it's still valid, as\n",
      "    long as the substance of the content between the expected and generated section is the same.\n",
      "  2. **Flow:** Evaluate whether the generated section follows the same order of ideas as the expected section, such as \n",
      "  the flow of:\n",
      "    - Main ideas covered starting with the beginning, until the end of the section. With special emphasis on the beginning and end of the \n",
      "    section as they reflect the transition between the previous and next sections\n",
      "    - Internal transitions between the main points within the section. We expect a smooth flow of ideas, \n",
      "    without any abrupt jumps or breaks.\n",
      "    - Placement of notes, images, tables, code blocks, or any other media elements within the generated section, \n",
      "    relative to the expected section. \n",
      "    - We don't expect a perfect one on one match between the paragraphs and sentences between the expected and generated section.\n",
      "    However, we expect the same ideas and concepts to be discussed in the same way, order, and storyline.\n",
      "    - Assign a score of 0 if anything is missing from the generated section relative to the expected section, such as missing\n",
      "    topics, ideas, or media.\n",
      "    - Assign a score of 0 if anything is additionally added to the generated section relative to the expected section, such \n",
      "    as additional topics, ideas or media elements.\n",
      "    - Accepted differences between the expected and generated section:\n",
      "        - Mismatching media numbering is accepted. For example, if in the expected section we have a figure with the number 3 and\n",
      "        in the generated section we have a figure with the number 4, it's valid. It will be invalid, only if the figure would be\n",
      "        missing altogether.\n",
      "        - Mismatching or missing emojis. For example, if the excepted section has a ðŸ’¡ emoji, while the generated section has \n",
      "        a ðŸ”‘ emoji, it's valid. Also, if the emoji is missing altogether from the generated section, it's valid.\n",
      "        - Mismatching source reference numbers. For example, if the expected section referes a source with the number 3, \n",
      "        while the generated section referes a source with the number 7, it's valid. It will be invalid, only if the generated\n",
      "        section misses the source altogether. \n",
      "        - Different placement of the source in the generated section. For example, if the expected section has the source\n",
      "        at the end of a sentence within the paragraph, while the generated section has it at the end of the paragraph, it's valid. \n",
      "        It will be invalid, only if the generated section would be missing altogether.\n",
      "        - Mismatching number of source references. For example, if the expected section has 3 source references, while the \n",
      "        generated section has 2 source references, it's valid. It will be invalid, only if the generated section would have \n",
      "        misses the references altogether. For example if the expected section has 3 source references, while the generated \n",
      "        section has 0 source references, it's invalid.\n",
      "        - Having reference numbers in the generated section, while having none in the expected section. For example, if the \n",
      "        expected section has 0 reference numbers, while the generated section has 3 reference number, it's valid. It will be \n",
      "        invalid, only the other way around, where the expected section has 3 reference numbers, while the generated section has 0.\n",
      "  3. **Structure:** Evaluate whether the generated section follows the same structure as the expected section. By \n",
      "  structure, we mean:\n",
      "    - H3/H4/H5/H6 sub-heading structure and formatting\n",
      "    - Mismatches in headers formatting and presence. For example, if the expected section doesn't has a header,\n",
      "    while the generated section has one, it's invalid. It's valid only if there is a one on one match between the headers\n",
      "    formatting and presence.\n",
      "    - Use of bulleted lists, numbered lists, callouts, notes, or other layout elements\n",
      "    - Division of the section when guiding readers through code blocks or diagrams\n",
      "    - Formatting of notes and code blocks\n",
      "    - Use of bolding, italicizing, quotes, backticks, or other formatting elements\n",
      "    - Formatting of citation references across sentences\n",
      "    - Formatting of images, tables, and Mermaid diagrams and their corresponding citations. If they are missing from\n",
      "    the generated section, we consider it valid for this criterion, as we are interested ONLY in formatting, which we\n",
      "    cannot verify when elements are absent. For this criterion, missing elements from the generated sections are \n",
      "    considered valid. They are invalid only if present in both sections but formatted differently.\n",
      "    - Number formatting conventions\n",
      "9. Along with the binary scores, you will provide a brief and concise explanation containing the reasoning behind \n",
      "the score for each criterion. The score will be used to debug and monitor the evaluation process. Therefore, it is\n",
      "important to provide thorough reasoning for the score. Since we provide binary scores, the reasoning should always \n",
      "contain what is good and what is problematic about the generated section, regardless of the score. For example, if the \n",
      "score is 0, the reasoning should also contain what is good about the generated section, such as \"both sections \n",
      "follow the same flow of ideas,\" and what is problematic, such as \"the generated section contains an additional \n",
      "paragraph on AI Evals that is not present in the expected section.\"\n",
      "10. Important rules when comparing the content of sections:\n",
      "    - Focus on substance, not superficial formatting differences\n",
      "    - When comparing **media**, you only care about the placement of the media, not the content of the media. \n",
      "    Since media can take many forms such as Mermaid diagrams, tables, images, or URLs, you will completely ignore the \n",
      "    content of the media and only check whether the media is present in the correct place in the section, has \n",
      "    the appropriate citation, and proper numbering.\n",
      "\n",
      "\n",
      "## CHAIN OF THOUGHT\n",
      "\n",
      "**Understanding Input:**\n",
      "1.1. Read, understand, and compare each section of the expected output and generated output.\n",
      "1.2. Since we want to compute scores for each section of the expected output Markdown file, split the expected output \n",
      "into sections using the H2 headers as separators.\n",
      "\n",
      "**Splitting into Sections:**\n",
      "2.1. Using the expected output as the reference point, compare each section of the expected and generated \n",
      "outputs individually and assign a binary score of 0 or 1, where 0 indicates a mismatch and 1 indicates a perfect match.\n",
      "2.2. Always use the expected output as the reference point to extract the sections of interest. \n",
      "2.3. When computing the score for an individual section, you will iterate through each section of the expected output, \n",
      "find its associated section in the generated output, and compute the score in isolation, ignoring all other sections.\n",
      "\n",
      "**Assigning Scores to Each Section:**\n",
      "3.1. Based on all sections of the expected output, assign a binary score of either 0 or 1 \n",
      "for all evaluation criteria listed in the instructions:\n",
      "    - **1:** The generated section matches the expected section perfectly on the given criterion.\n",
      "    - **0:** The generated section does not match the expected section on the given criterion.\n",
      "3.2. Justify why you assigned a score of 0 or 1 with a brief explanation that highlights the reasoning behind the score\n",
      "based on the given criterion.\n",
      "\n",
      "## WHAT TO AVOID\n",
      "\n",
      "- Do not provide scores using the generated output as the reference point to divide into sections. You must always \n",
      "use the expected output as the reference point to divide into sections.\n",
      "- Do not let other sections influence the score of a section. The score of each section must be determined in complete \n",
      "isolation from any other section.\n",
      "- Do not overlap requirements between different criteria. For example, in the content criterion, as are not interested in the flow \n",
      "of ideas, if the ideas are in different order, or something is missing or additional, it's still valid. However, that is an important\n",
      "aspect of the flow criterion, which will be invalid.\n",
      "\n",
      "## FEW-SHOT EXAMPLES\n",
      "\n",
      "Here are few-shot examples demonstrating how to compute the scores for each section and criterion:\n",
      "<few-shot-examples>\n",
      "<example_1>\n",
      "\t\n",
      "<output>\n",
      "# Lesson 4: Structured Outputs\n",
      "\n",
      "We will start with a critical component for building production-grade AI systems: structured outputs. This concept is the bridge between the probabilistic, flexible world of Large Language Models (LLMs)â€”what some call Software 3.0â€”and the deterministic, rigid world of traditional Python code, or Software 1.0.\n",
      "\n",
      "LLMs generate text, but our applications need dataâ€”objects, lists, and validated fields. Structured outputs provide a reliable contract that forces the modelâ€™s free-form text into a predictable format. Mastering this is not just a nice-to-have, it is essential for any AI Engineer who wants to build systems that are reliable, testable, and maintainable. Without it, you are just building fragile applications on a foundation of hope.\n",
      "\n",
      "## Why Structured Outputs Are Critical\n",
      "\n",
      "Before we write any code, it is important to understand why forcing an LLM to return structured data is a non-negotiable best practice. At its core, it is about control and reliabi...\n",
      "</output>\n",
      "<expected_output>\n",
      "# Lesson 4: Structured Outputs\n",
      "\n",
      "In our previous lessons, we laid the groundwork for AI Engineering. We explored the AI agent landscape, looked at the difference between rule-based LLM workflows and autonomous AI agents, and covered context engineering: the art of feeding the right information to an LLM. In this lesson, we will tackle a fundamental challenge: getting structured and reliable informationÂ *out*Â of an LLM.\n",
      "\n",
      "![Figure 1: The goal of structured outputs is to fill in the gap between Software 3.0 (LLM workflows & AI Agents) and Software 1.0 (Traditional Applications). **Side note:** Software 2.0 is referred to as the Machine Learning/Deep Learning world.](Lesson%204%20Structured%20Outputs%20247f9b6f4270802194a3d7cad6e516e9/image.png)\n",
      "\n",
      "Figure 1: The goal of structured outputs is to fill in the gap between Software 3.0 (LLM workflows & AI Agents) and Software 1.0 (Traditional Applications). **Side note:** Software 2.0 is referred to as the Machine Learning/Deep Learning world.\n",
      "\n",
      "LL...\n",
      "</expected_output>\n",
      "<article_scores>\n",
      "\t\n",
      "<section_scores>\n",
      "    <section_title>Introduction</section_title>\n",
      "    <content>\n",
      "        <score>1</score>\n",
      "        <reason>Both sections cover the same core subjects and ideas, discussing the purpose of structured outputs as a bridge between LLMs and traditional applications.</reason>\n",
      "    </content>\n",
      "    <flow>\n",
      "        <score>0</score>\n",
      "        <reason>The generated section lacks the first sentence, which is used as a smooth transition into the article. Also, it misses the diagram present in the expected output, labeled as Figure 1.</reason>\n",
      "    </flow>\n",
      "    <structure>\n",
      "        <score>1</score>\n",
      "        <reason>Both sections use the same paragraph length patterns.</reason>\n",
      "    </structure>\n",
      "</section_scores>\n",
      "\n",
      "\t\n",
      "<section_scores>\n",
      "    <section_title>Why Structured Outputs Are Critical</section_title>\n",
      "    <content>\n",
      "        <score>0</score>\n",
      "        <reason>Both sections cover the same reasons why structured outputs are critical, including ease of parsing, data validation with Pydantic, and common use cases. Still, the generated section has a section on GraphRAG, which is not related to the specific topic of the section.</reason>\n",
      "    </content>\n",
      "    <flow>\n",
      "        <score>0</score>\n",
      "        <reason>Follows a similar logical flow, starting with the importance, detailing benefits, discussing use cases, and concluding with a diagram. Still, the generated section contains an additional paragraph on GraphRAG, which doesn't fit with the expected flow. Additionally, it omits the last sentence, which is necessary for a smooth transition to the next section.</reason>\n",
      "    </flow>\n",
      "    <structure>\n",
      "        <score>1</score>\n",
      "        <reason>Both sections use the same paragraph length patterns and have the same usage pattern for backticks and citation references across sentences. Also, the figures and their corresponding citations use the same formatting rules.</reason>\n",
      "    </structure>\n",
      "</section_scores>\n",
      "\n",
      "\t\n",
      "<section_scores>\n",
      "    <section_title>Implementing Structured Outputs From Scratch Using JSON</section_title>\n",
      "    <content>\n",
      "        <score>1</score>\n",
      "        <reason>Both sections provide a step-by-step guide on implementing structured outputs using JSON from scratch, covering client setup, document definition, prompt crafting, and parsing.</reason>\n",
      "    </content>\n",
      "    <flow>\n",
      "        <score>0</score>\n",
      "        <reason>The generated section omits the Note callout box present in the expected output.</reason>\n",
      "    </flow>\n",
      "    <structure>\n",
      "        <score>0</score>\n",
      "        <reason>The generated section incorrectly formats the JSON code block under point 4), where it misses the closing ```. Also, in the last section, where it outputs the final JSON structure, it doesn't enclose the JSON into Python backticks as expected: ```python <content> ```</reason>\n",
      "    </structure>\n",
      "</section_scores>\n",
      "\n",
      "\t\n",
      "<section_scores>\n",
      "    <section_title>Implementing Structured Outputs From Scratch Using Pydantic</section_title>\n",
      "    <content>\n",
      "        <score>0</score>\n",
      "        <reason>Both sections accurately explain the benefits of Pydantic for structured outputs, demonstrate defining models, generating schemas, and validating responses, and compare it with other Python types. Still, the generated section uses different code examples, using a RedditThread Pydantic Python class instead of the expected DocumentMetadata class.</reason>\n",
      "    </content>\n",
      "    <flow>\n",
      "        <score>1</score>\n",
      "        <reason>Even if the sections use different code examples, from the point of view of the flow of ideas, both sections follow a similar logical flow, introducing Pydantic, demonstrating its implementation through steps with code, and concluding with a comparison to other data validation methods.</reason>\n",
      "    </flow>\n",
      "    <structure>\n",
      "        <score>1</score>\n",
      "        <reason>Both sections maintain similar introductory paragraphs, numbered steps with code blocks, and a concluding comparison. The formatting of the Python code and JSON blocks is the same. Also, the use of backticks and formatting of citation references is the same.</reason>\n",
      "    </structure>\n",
      "</section_scores>\n",
      "\n",
      "\t\n",
      "<section_scores>\n",
      "    <section_title>Implementing Structured Outputs Using Gemini and Pydantic</section_title>\n",
      "    <content>\n",
      "        <score>0</score>\n",
      "        <reason>Both sections accurately describe the native implementation of structured outputs using the Gemini API and Pydantic. Still, the generated section lacks some key code block examples (points 3 and 4 on calling the Gemini API), which are necessary to fully illustrate the concept.</reason>\n",
      "    </content>\n",
      "    <flow>\n",
      "        <score>0</score>\n",
      "        <reason>Both sections follow a similar logical flow, introducing native API support and then demonstrating its implementation through numbered steps with code and outputs. Still, the generated section misses the first sentence used to make the transition from the previous sentence, and also it misses points 3) and 4) from the code walkthrough numbered list.</reason>\n",
      "    </flow>\n",
      "    <structure>\n",
      "        <score>0</score>\n",
      "        <reason>In both sections, the use of citation references and backticks is the same. Also, the structure of the introductory paragraph, division of code blocks and conclusion follow the same pattern. Still, the generated section uses a bulleted list to divide the code blocks instead of a numbered list as expected.</reason>\n",
      "    </structure>\n",
      "</section_scores>\n",
      "\n",
      "\t\n",
      "<section_scores>\n",
      "    <section_title>Structured Outputs Are Everywhere</section_title>\n",
      "    <content>\n",
      "        <score>0</score>\n",
      "        <reason>Both sections serve as a conclusion, summarizing the importance of structured outputs as a fundamental pattern. Still, the generated section misses the last paragraph that presents how structured outputs fit in the course and the AI Engineering field, which is critical for the conclusion.</reason>\n",
      "    </content>\n",
      "    <flow>\n",
      "        <score>0</score>\n",
      "        <reason>Both sections follow a similar flow, summarizing the key takeaway. Still, the generated section misses the last paragraph on looking ahead to future lessons in the course.</reason>\n",
      "    </flow>\n",
      "    <structure>\n",
      "        <score>0</score>\n",
      "        <reason>Both sections use the same paragraph length patterns. Still, the number formatting of the citation reference from the first paragraph misses the square brackets.</reason>\n",
      "    </structure>\n",
      "</section_scores>\n",
      "\n",
      "\t\n",
      "<section_scores>\n",
      "    <section_title>References</section_title>\n",
      "    <content>\n",
      "        <score>1</score>\n",
      "        <reason>Both sections contain a list of references, similar in purpose.</reason>\n",
      "    </content>\n",
      "    <flow>\n",
      "        <score>1</score>\n",
      "        <reason>Both sections follow the same flow for referencing the sources, as a numbered list from 1 to n.</reason>\n",
      "    </flow>\n",
      "    <structure>\n",
      "        <score>1</score>\n",
      "        <reason>Both sections use the same pattern to structure the references, as a bulleted list, where each element is structured as [<reference_number>] [<reference_name>](<reference_url>)</reason>\n",
      "    </structure>\n",
      "</section_scores>\n",
      "\n",
      "\n",
      "</article_scores>\n",
      "\n",
      "</example_1>\n",
      "\n",
      "</few-shot-examples>\n",
      "\n",
      "## INPUTS\n",
      "\n",
      "<generated_output>\n",
      "Dummy output article\n",
      "</generated_output>\n",
      "\n",
      "<expected_output>\n",
      "Dummy expected output article\n",
      "</expected_output>\n",
      "\n",
      "Think through your answer step by step, and provide the requested evaluation.\n",
      "\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from brown.evals.metrics.follows_gt.prompts import DEFAULT_FEW_SHOT_EXAMPLES, get_eval_prompt\n",
    "\n",
    "# Crop the few-shot examples to 1 example and cropped dummy output and expected output to\n",
    "# easily see the prompt structure\n",
    "cropped_few_shot_examples = DEFAULT_FEW_SHOT_EXAMPLES.model_copy(deep=True)\n",
    "cropped_few_shot_examples.examples = cropped_few_shot_examples.examples[:1]\n",
    "cropped_few_shot_examples.examples[0].output = f\"{cropped_few_shot_examples.examples[0].output[:1000]}...\"\n",
    "cropped_few_shot_examples.examples[\n",
    "    0\n",
    "].expected_output = f\"{cropped_few_shot_examples.examples[0].expected_output[:1000]}...\"\n",
    "\n",
    "pretty_print.wrapped(\n",
    "    get_eval_prompt(\n",
    "        output=\"Dummy output article\",\n",
    "        expected_output=\"Dummy expected output article\",\n",
    "        few_shot_examples=cropped_few_shot_examples,\n",
    "    ),\n",
    "    title=\"FollowsGT LLM Judge System Prompt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this is how the few-shot examples data structure looks like when we map it to context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m------------------------------ FollowsGT LLM Judge Few-Shot Examples ------------------------------\u001b[0m\n",
      "  <example_1>\n",
      "\t\n",
      "<output>\n",
      "# Lesson 4: Structured Outputs\n",
      "\n",
      "We will start with a critical component for building production-grade AI systems: structured outputs. This concept is the bridge between the probabilistic, flexible world of Large Language Models (LLMs)â€”what some call Software 3.0â€”and the deterministic, rigid world of traditional Python code, or Software 1.0.\n",
      "\n",
      "LLMs generate text, but our applications need dataâ€”objects, lists, and validated fields. Structured outputs provide a reliable contract that forces the modelâ€™s free-form text into a predictable format. Mastering this is not just a nice-to-have, it is essential for any AI Engineer who wants to build systems that are reliable, testable, and maintainable. Without it, you are just building fragile applications on a foundation of hope.\n",
      "\n",
      "## Why Structured Outputs Are Critical\n",
      "\n",
      "Before we write any code, it is important to understand why forcing an LLM to return structured data is a non-negotiable best practice. At its core, it is about control and reliabi...\n",
      "</output>\n",
      "<expected_output>\n",
      "# Lesson 4: Structured Outputs\n",
      "\n",
      "In our previous lessons, we laid the groundwork for AI Engineering. We explored the AI agent landscape, looked at the difference between rule-based LLM workflows and autonomous AI agents, and covered context engineering: the art of feeding the right information to an LLM. In this lesson, we will tackle a fundamental challenge: getting structured and reliable informationÂ *out*Â of an LLM.\n",
      "\n",
      "![Figure 1: The goal of structured outputs is to fill in the gap between Software 3.0 (LLM workflows & AI Agents) and Software 1.0 (Traditional Applications). **Side note:** Software 2.0 is referred to as the Machine Learning/Deep Learning world.](Lesson%204%20Structured%20Outputs%20247f9b6f4270802194a3d7cad6e516e9/image.png)\n",
      "\n",
      "Figure 1: The goal of structured outputs is to fill in the gap between Software 3.0 (LLM workflows & AI Agents) and Software 1.0 (Traditional Applications). **Side note:** Software 2.0 is referred to as the Machine Learning/Deep Learning world.\n",
      "\n",
      "LL...\n",
      "</expected_output>\n",
      "<article_scores>\n",
      "\t\n",
      "<section_scores>\n",
      "    <section_title>Introduction</section_title>\n",
      "    <content>\n",
      "        <score>1</score>\n",
      "        <reason>Both sections cover the same core subjects and ideas, discussing the purpose of structured outputs as a bridge between LLMs and traditional applications.</reason>\n",
      "    </content>\n",
      "    <flow>\n",
      "        <score>0</score>\n",
      "        <reason>The generated section lacks the first sentence, which is used as a smooth transition into the article. Also, it misses the diagram present in the expected output, labeled as Figure 1.</reason>\n",
      "    </flow>\n",
      "    <structure>\n",
      "        <score>1</score>\n",
      "        <reason>Both sections use the same paragraph length patterns.</reason>\n",
      "    </structure>\n",
      "</section_scores>\n",
      "\n",
      "\t\n",
      "<section_scores>\n",
      "    <section_title>Why Structured Outputs Are Critical</section_title>\n",
      "    <content>\n",
      "        <score>0</score>\n",
      "        <reason>Both sections cover the same reasons why structured outputs are critical, including ease of parsing, data validation with Pydantic, and common use cases. Still, the generated section has a section on GraphRAG, which is not related to the specific topic of the section.</reason>\n",
      "    </content>\n",
      "    <flow>\n",
      "        <score>0</score>\n",
      "        <reason>Follows a similar logical flow, starting with the importance, detailing benefits, discussing use cases, and concluding with a diagram. Still, the generated section contains an additional paragraph on GraphRAG, which doesn't fit with the expected flow. Additionally, it omits the last sentence, which is necessary for a smooth transition to the next section.</reason>\n",
      "    </flow>\n",
      "    <structure>\n",
      "        <score>1</score>\n",
      "        <reason>Both sections use the same paragraph length patterns and have the same usage pattern for backticks and citation references across sentences. Also, the figures and their corresponding citations use the same formatting rules.</reason>\n",
      "    </structure>\n",
      "</section_scores>\n",
      "\n",
      "\t\n",
      "<section_scores>\n",
      "    <section_title>Implementing Structured Outputs From Scratch Using JSON</section_title>\n",
      "    <content>\n",
      "        <score>1</score>\n",
      "        <reason>Both sections provide a step-by-step guide on implementing structured outputs using JSON from scratch, covering client setup, document definition, prompt crafting, and parsing.</reason>\n",
      "    </content>\n",
      "    <flow>\n",
      "        <score>0</score>\n",
      "        <reason>The generated section omits the Note callout box present in the expected output.</reason>\n",
      "    </flow>\n",
      "    <structure>\n",
      "        <score>0</score>\n",
      "        <reason>The generated section incorrectly formats the JSON code block under point 4), where it misses the closing ```. Also, in the last section, where it outputs the final JSON structure, it doesn't enclose the JSON into Python backticks as expected: ```python <content> ```</reason>\n",
      "    </structure>\n",
      "</section_scores>\n",
      "\n",
      "\t\n",
      "<section_scores>\n",
      "    <section_title>Implementing Structured Outputs From Scratch Using Pydantic</section_title>\n",
      "    <content>\n",
      "        <score>0</score>\n",
      "        <reason>Both sections accurately explain the benefits of Pydantic for structured outputs, demonstrate defining models, generating schemas, and validating responses, and compare it with other Python types. Still, the generated section uses different code examples, using a RedditThread Pydantic Python class instead of the expected DocumentMetadata class.</reason>\n",
      "    </content>\n",
      "    <flow>\n",
      "        <score>1</score>\n",
      "        <reason>Even if the sections use different code examples, from the point of view of the flow of ideas, both sections follow a similar logical flow, introducing Pydantic, demonstrating its implementation through steps with code, and concluding with a comparison to other data validation methods.</reason>\n",
      "    </flow>\n",
      "    <structure>\n",
      "        <score>1</score>\n",
      "        <reason>Both sections maintain similar introductory paragraphs, numbered steps with code blocks, and a concluding comparison. The formatting of the Python code and JSON blocks is the same. Also, the use of backticks and formatting of citation references is the same.</reason>\n",
      "    </structure>\n",
      "</section_scores>\n",
      "\n",
      "\t\n",
      "<section_scores>\n",
      "    <section_title>Implementing Structured Outputs Using Gemini and Pydantic</section_title>\n",
      "    <content>\n",
      "        <score>0</score>\n",
      "        <reason>Both sections accurately describe the native implementation of structured outputs using the Gemini API and Pydantic. Still, the generated section lacks some key code block examples (points 3 and 4 on calling the Gemini API), which are necessary to fully illustrate the concept.</reason>\n",
      "    </content>\n",
      "    <flow>\n",
      "        <score>0</score>\n",
      "        <reason>Both sections follow a similar logical flow, introducing native API support and then demonstrating its implementation through numbered steps with code and outputs. Still, the generated section misses the first sentence used to make the transition from the previous sentence, and also it misses points 3) and 4) from the code walkthrough numbered list.</reason>\n",
      "    </flow>\n",
      "    <structure>\n",
      "        <score>0</score>\n",
      "        <reason>In both sections, the use of citation references and backticks is the same. Also, the structure of the introductory paragraph, division of code blocks and conclusion follow the same pattern. Still, the generated section uses a bulleted list to divide the code blocks instead of a numbered list as expected.</reason>\n",
      "    </structure>\n",
      "</section_scores>\n",
      "\n",
      "\t\n",
      "<section_scores>\n",
      "    <section_title>Structured Outputs Are Everywhere</section_title>\n",
      "    <content>\n",
      "        <score>0</score>\n",
      "        <reason>Both sections serve as a conclusion, summarizing the importance of structured outputs as a fundamental pattern. Still, the generated section misses the last paragraph that presents how structured outputs fit in the course and the AI Engineering field, which is critical for the conclusion.</reason>\n",
      "    </content>\n",
      "    <flow>\n",
      "        <score>0</score>\n",
      "        <reason>Both sections follow a similar flow, summarizing the key takeaway. Still, the generated section misses the last paragraph on looking ahead to future lessons in the course.</reason>\n",
      "    </flow>\n",
      "    <structure>\n",
      "        <score>0</score>\n",
      "        <reason>Both sections use the same paragraph length patterns. Still, the number formatting of the citation reference from the first paragraph misses the square brackets.</reason>\n",
      "    </structure>\n",
      "</section_scores>\n",
      "\n",
      "\t\n",
      "<section_scores>\n",
      "    <section_title>References</section_title>\n",
      "    <content>\n",
      "        <score>1</score>\n",
      "        <reason>Both sections contain a list of references, similar in purpose.</reason>\n",
      "    </content>\n",
      "    <flow>\n",
      "        <score>1</score>\n",
      "        <reason>Both sections follow the same flow for referencing the sources, as a numbered list from 1 to n.</reason>\n",
      "    </flow>\n",
      "    <structure>\n",
      "        <score>1</score>\n",
      "        <reason>Both sections use the same pattern to structure the references, as a bulleted list, where each element is structured as [<reference_number>] [<reference_name>](<reference_url>)</reason>\n",
      "    </structure>\n",
      "</section_scores>\n",
      "\n",
      "\n",
      "</article_scores>\n",
      "\n",
      "</example_1>\n",
      "\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "pretty_print.wrapped(cropped_few_shot_examples.to_context(), title=\"FollowsGT LLM Judge Few-Shot Examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Running the FollowsGT Metric\n",
    "\n",
    "Let's run the `FollowsGTMetricLLMJudge` on a sample to see it in action. We'll use the test sample from `inputs/tests/01_sample_small` as our expected output and create a variant with some modifications as the generated output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m----------------------------------------- Expected Article -----------------------------------------\u001b[0m\n",
      "  Expected article length: 6,982 characters\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m------------------------------------- Expected Article Preview -------------------------------------\u001b[0m\n",
      "  # Workflows vs. Agents: The Critical Decision Every AI Engineer Faces\n",
      "### How to choose between predictable control and autonomous flexibility when building AI applications.\n",
      "\n",
      "When building AI applications, engineers face a critical architectural decision early on. Should you create a predictable, step-by-step workflow where you control every action, or build an autonomous agent that can think and decide for itself? This choice impacts everything from development time and cost to reliability and user experience. It is a fundamental decision that often determines if an AI application will be successful in production.\n",
      "\n",
      "By the end of this lesson, you will understand the fundamental differences between LLM workflows and AI agents, know when to use each, and recognize how to combine their strengths in hybrid approaches.\n",
      "\n",
      "## Understanding the Spectrum: From Workflows to Agents\n",
      "\n",
      "To make the right choice, you first need to understand what LLM workflows and AI agents are. We will look at their core properties and how they are used, rather than their technical specifics.\n",
      "\n",
      "### LLM Workflows\n",
      "\n",
      "An LLM workflow is a sequence of tasks orchestrated by developer-written code. It can include LLM calls, but also other operations like reading from a database or calling an API. Think of it like a recipe where each step is explicitly defined. The key characteristic is that the path is determined in advance, resulting in a deterministic or rule-based system. This gives you predictable execution, explicit control over the application's flow, and makes the system easier to test and debug. Because you control every step, you know exactly where a failure occurred and how to fix it.\n",
      "\n",
      "```mermaid\n",
      "graph TD\n",
      "    A[\"Start\"] --> B[\"LLM Call\"]\n",
      "    B --> C[\"Process Data\"]\n",
      "    C --> D[\"Store Data\"]\n",
      "    D --> E[\"End\"]\n",
      "```\n",
      "Image 1: A flowchart illustrating a deterministic LLM workflow with clear start and end points, including an LLM call and data operations.\n",
      "\n",
      "### AI Agents\n",
      "\n",
      "AI agents are systems where an LLM dynamically decides the sequence of steps, reasoning, and actions to achieve a goal. The path is not predefined. Instead, the agent uses a reasoning process to plan its actions based on the task and the current state of its environment. This process is often modeled on frameworks like ReAct (Reason, Act, Observe). This allows agents to be adaptive and capable of handling new or unexpected situations through LLM-driven autonomy. They can select tools, execute actions, evaluate the outcomes, and correct their course until the goal is achieved [[1]](https://www.youtube.com/watch?v=kQxr-uOxw2o&t=1s).\n",
      "\n",
      "```mermaid\n",
      "graph TD\n",
      "    A[\"Start\"] --> B[\"Agent (LLM) Receives Goal\"]\n",
      "    B --> C[\"Plan/Reason (LLM)\"]\n",
      "    C --> D[\"Select Tool\"]\n",
      "    D --> E[\"Execute Action (Tool Call)\"]\n",
      "    E --> F[\"Observe Environment/Feedback\"]\n",
      "    F --> G{\"Evaluate Outcome\"}\n",
      "    G -->|\"Satisfactory\"| H[\"Stop/Achieve Goal\"]\n",
      "    G -->|\"Needs Adjustment\"| C\n",
      "```\n",
      "Image 2: Flowchart illustrating an AI agent's dynamic decision-making process driven by an LLM.\n",
      "\n",
      "## Choosing Your Path\n",
      "\n",
      "The core difference between these two approaches lies in a single trade-off: developer-defined logic versus LLM-driven autonomy [[2]](https://decodingml.substack.com/p/llmops-for-production-agentic-rag), [[3]](https://towardsdatascience.com/a-developers-guide-to-building-scalable-ai-workflows-vs-agents/). Workflows offer high reliability at the cost of flexibility, while agents offer high flexibility at the cost of reliability.\n",
      "\n",
      "https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5e64d5e0-7ef1-4e7f-b441-3bf1fef4ff9a_1276x818.png \n",
      "Image 3: The trade-off between an agent's level of control and application reliability. (Image by Iusztin, P. from [Exploring the difference between agents and workflows [2]](https://decodingml.substack.com/p/llmops-for-production-agentic-rag))\n",
      "\n",
      "### When to use LLM workflows\n",
      "\n",
      "Workflows are ideal for repeatable tasks with defined steps, like data extraction, report generation, or content repurposing. Their strength is predictability, ensuring reliable results, easier debugging, and lower costs by using specialized models. The main weakness is rigidity; they cannot handle unexpected scenarios, and adding features can become complex.\n",
      "\n",
      "### When to use AI agents\n",
      "\n",
      "Agents excel at dynamic problem-solving like open-ended research or complex customer support [[3]](https://towardsdatascience.com/a-developers-guide-to-building-scalable-ai-workflows-vs-agents/). Their strength is flexibility in handling ambiguity. However, this autonomy makes them less reliable, harder to debug, and costlier due to non-deterministic behavior. Without proper guardrails, they also pose security risks, especially with operations that can modify or delete data.\n",
      "\n",
      "### Hybrid Approaches\n",
      "\n",
      "Most real-world systems are not purely one or the other. They often blend elements of both, creating a hybrid system. A common pattern is to use a workflow for predictable parts of a task and delegate ambiguous steps to an agent. For example, a system might use a human-in-the-loop workflow, where the agent proposes an action, and a human verifies it before execution.\n",
      "\n",
      "```mermaid\n",
      "graph TD\n",
      "    A[\"Human Input\"] --> B[\"LLM Call (AI Generation)\"]\n",
      "    B --> C[\"Action in Environment\"]\n",
      "    C --> D[\"Feedback from Environment\"]\n",
      "    D --> E{\"Human Review/Verification\"}\n",
      "    E -->|\"Approved\"| G[\"Stop/Final Output\"]\n",
      "    E -->|\"Rejected\"| F[\"Continue/Refine\"]\n",
      "    F --> A\n",
      "```\n",
      "Image 4: A flowchart illustrating an AI generation and human verification loop with iterative refinement.\n",
      "\n",
      "## The Challenges of Every AI Engineer\n",
      "\n",
      "Understanding the spectrum from workflows to agents is a core part of AI engineering. This choice helps determine if your application will succeed in production. Building robust AI systems means navigating recurring challenges daily. These include building pipelines to pull information from Slack, web APIs, SQL databases, and data lakes; managing the cost-performance trap where sophisticated agents become too expensive per interaction; and mitigating security risks from autonomous agents that could send wrong emails or delete critical files.\n",
      "\n",
      "In our next lesson, we will explore a foundational skill for building both workflows and agents: context engineering.\n",
      "\n",
      "## References\n",
      "\n",
      "1. Bouchard, L-F. (n.d.). Real agents vs. workflows: The truth behind AI 'agents'. YouTube. https://www.youtube.com/watch?v=kQxr-uOxw2o&t=1s\n",
      "2. Iusztin, P. (n.d.). Exploring the difference between agents and workflows. Decoding AI Magazine. https://decodingml.substack.com/p/llmops-for-production-agentic-rag\n",
      "3. (n.d.). A developerâ€™s guide to building scalable AI: Workflows vs agents. Towards Data Science. https://towardsdatascience.com/a-developers-guide-to-building-scalable-ai-workflows-vs-agents/\n",
      "4. Google. (n.d.). Gemini CLI. GitHub. https://github.com/google-gemini/gemini-cli/blob/main/README.md\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "EXPECTED_ARTICLE = (SAMPLE_DIR / \"article.md\").read_text()\n",
    "\n",
    "pretty_print.wrapped(f\"Expected article length: {len(EXPECTED_ARTICLE):,} characters\", title=\"Expected Article\")\n",
    "pretty_print.wrapped(EXPECTED_ARTICLE, title=\"Expected Article Preview\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is a noisy version of the same article where we deleted or changed parts of the article from above to trigger the LLM judge metrics. This will serve as our generated output from the Brown workflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m---------------------------------------- Generated Article ----------------------------------------\u001b[0m\n",
      "  Generated article length: 6,024 characters\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m------------------------------------ Generated Article Preview ------------------------------------\u001b[0m\n",
      "  # Workflows vs. Agents: The Critical Decision Every AI Engineer Faces\n",
      "### How to choose between predictable control and autonomous flexibility when building AI applications.\n",
      "\n",
      "By the end of this lesson, you will understand the fundamental differences between LLM workflows and AI agents, know when to use each, and recognize how to combine their strengths in hybrid approaches.\n",
      "\n",
      "When building AI applications, engineers face a critical architectural decision early on. Should you create a predictable, step-by-step workflow where you control every action, or build an autonomous agent that can think and decide for itself? This choice impacts everything from development time and cost to reliability and user experience. It is a fundamental decision that often determines if an AI application will be successful in production.\n",
      "\n",
      "## Understanding the Spectrum: From Workflows to Agents\n",
      "\n",
      "To make the right choice, you first need to understand what LLM workflows and AI agents are. We will look at their core properties and how they are used, rather than their technical specifics.\n",
      "\n",
      "### LLM Workflows\n",
      "\n",
      "An LLM workflow is a sequence of tasks orchestrated by developer-written code.\n",
      "\n",
      "```mermaid\n",
      "graph TD\n",
      "    A[\"Start\"] --> B[\"LLM Call\"]\n",
      "    B --> C[\"Process Data\"]\n",
      "    C --> D[\"Store Data\"]\n",
      "    D --> E[\"End\"]\n",
      "```\n",
      "Image 1: A flowchart illustrating a deterministic LLM workflow with clear start and end points, including an LLM call and data operations.\n",
      "\n",
      "### AI Agents\n",
      "\n",
      "AI agents are systems where an LLM dynamically decides the sequence of steps, reasoning, and actions to achieve a goal. The path is not predefined. Instead, the agent uses a reasoning process to plan its actions based on the task and the current state of its environment. This process is often modeled on frameworks like ReAct (Reason, Act, Observe). This allows agents to be adaptive and capable of handling new or unexpected situations through LLM-driven autonomy. They can select tools, execute actions, evaluate the outcomes, and correct their course until the goal is achieved [[1]](https://www.youtube.com/watch?v=kQxr-uOxw2o&t=1s).\n",
      "\n",
      "```mermaid\n",
      "graph TD\n",
      "    A[\"Start\"] --> B[\"Agent (LLM) Receives Goal\"]\n",
      "    B --> C[\"Plan/Reason (LLM)\"]\n",
      "    C --> D[\"Select Tool\"]\n",
      "    D --> E[\"Execute Action (Tool Call)\"]\n",
      "    E --> F[\"Observe Environment/Feedback\"]\n",
      "    F --> G{\"Evaluate Outcome\"}\n",
      "    G -->|\"Satisfactory\"| H[\"Stop/Achieve Goal\"]\n",
      "    G -->|\"Needs Adjustment\"| C\n",
      "```\n",
      "Image 2: Flowchart illustrating an AI agent's dynamic decision-making process driven by an LLM.\n",
      "\n",
      "## Choosing Your Path\n",
      "\n",
      "The core difference between these two approaches lies in a single trade-off: use RAG or not.\n",
      "\n",
      "https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5e64d5e0-7ef1-4e7f-b441-3bf1fef4ff9a_1276x818.png \n",
      "Image 3: The trade-off between an agent's level of control and application reliability. (Image by Iusztin, P. from [Exploring the difference between agents and workflows [2]](https://decodingml.substack.com/p/llmops-for-production-agentic-rag))\n",
      "\n",
      "- **When to use LLM workflows:** Workflows are ideal for repeatable tasks with defined steps, like data extraction, report generation, or content repurposing. Their strength is predictability, ensuring reliable results, easier debugging, and lower costs by using specialized models. The main weakness is rigidity; they cannot handle unexpected scenarios, and adding features can become complex.\n",
      "\n",
      "- **When to use AI agents:** Agents excel at dynamic problem-solving like open-ended research or complex customer support [[3]](https://towardsdatascience.com/a-developers-guide-to-building-scalable-ai-workflows-vs-agents/). Their strength is flexibility in handling ambiguity. However, this autonomy makes them less reliable, harder to debug, and costlier due to non-deterministic behavior. Without proper guardrails, they also pose security risks, especially with operations that can modify or delete data.\n",
      "\n",
      "- **Hybrid Approaches:** Most real-world systems are not purely one or the other. They often blend elements of both, creating a hybrid system. A common pattern is to use a workflow for predictable parts of a task and delegate ambiguous steps to an agent. For example, a system might use a human-in-the-loop workflow, where the agent proposes an action, and a human verifies it before execution.\n",
      "\n",
      "```mermaid\n",
      "graph TD\n",
      "    A[\"Human Input\"] --> B[\"LLM Call (AI Generation)\"]\n",
      "    B --> C[\"Action in Environment\"]\n",
      "    C --> D[\"Feedback from Environment\"]\n",
      "    D --> E{\"Human Review/Verification\"}\n",
      "    E -->|\"Approved\"| G[\"Stop/Final Output\"]\n",
      "    E -->|\"Rejected\"| F[\"Continue/Refine\"]\n",
      "    F --> A\n",
      "```\n",
      "Image 4: A flowchart illustrating an AI generation and human verification loop with iterative refinement.\n",
      "\n",
      "## The Challenges of Every AI Engineer\n",
      "\n",
      "Understanding the spectrum from workflows to agents is a core part of AI engineering. This choice helps determine if your application will succeed in production. Building robust AI systems means navigating recurring challenges daily. These include building pipelines to pull information from Slack, web APIs, SQL databases, and data lakes; managing the cost-performance trap where sophisticated agents become too expensive per interaction; and mitigating security risks from autonomous agents that could send wrong emails or delete critical files.\n",
      "\n",
      "## References\n",
      "\n",
      "1. Bouchard, L-F. (n.d.). Real agents vs. workflows: The truth behind AI 'agents'. YouTube. https://www.youtube.com/watch?v=kQxr-uOxw2o&t=1s\n",
      "2. Iusztin, P. (n.d.). Exploring the difference between agents and workflows. Decoding AI Magazine. https://decodingml.substack.com/p/llmops-for-production-agentic-rag\n",
      "3. (n.d.). A developerâ€™s guide to building scalable AI: Workflows vs agents. Towards Data Science. https://towardsdatascience.com/a-developers-guide-to-building-scalable-ai-workflows-vs-agents/\n",
      "4. Google. (n.d.). Gemini CLI. GitHub. https://github.com/google-gemini/gemini-cli/blob/main/README.md\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_ARTICLE = (SAMPLE_DIR / \"article_noisy.md\").read_text()\n",
    "\n",
    "pretty_print.wrapped(f\"Generated article length: {len(OUTPUT_ARTICLE):,} characters\", title=\"Generated Article\")\n",
    "pretty_print.wrapped(OUTPUT_ARTICLE, title=\"Generated Article Preview\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this noise version of the article, we:\n",
    "- Changed the order of the paragraphs in the introduction.\n",
    "- Deleted most of the paragraph from the `### LLM Workflows` section.\n",
    "- Added incorrect information at the beginning of the `## Choosing Your Path` section.\n",
    "- Mapped the `Choosing Your Path` section from using `###` sections to using a bullet list.\n",
    "- Deleted the last paragraph from the conclusion.\n",
    "\n",
    "Let's see how our LLM judge can detect these changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running FollowsGT metric...\n",
      "This may take a minute...\n",
      "\n",
      "âœ“ Generated 3 score results\n"
     ]
    }
   ],
   "source": [
    "from brown.evals.metrics import FollowsGTMetricLLMJudge\n",
    "from brown.models import SupportedModels\n",
    "\n",
    "follows_gt_metric = FollowsGTMetricLLMJudge(\n",
    "    model=SupportedModels.GOOGLE_GEMINI_25_FLASH,\n",
    "    track=False,  # Disable tracking for this demo\n",
    ")\n",
    "\n",
    "print(\"Running FollowsGT metric...\")\n",
    "print(\"This may take a minute...\")\n",
    "\n",
    "results = await follows_gt_metric.ascore(\n",
    "    output=OUTPUT_ARTICLE,\n",
    "    expected_output=EXPECTED_ARTICLE,\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ“ Generated {len(results)} score results\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m---------------------------------------- follows_gt_content ----------------------------------------\u001b[0m\n",
      "  follows_gt_content - Score: 0.40\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "*** REASON ***\n",
      "\n",
      "Introduction:\n",
      "**1:** Both sections introduce the core dilemma of choosing between LLM workflows and AI agents, and outline the learning objectives of the lesson.\n",
      "\n",
      "Understanding the Spectrum: From Workflows to Agents:\n",
      "**0:** The generated section's 'LLM Workflows' sub-section is significantly less detailed than the expected output, omitting crucial descriptions about predictability, control, testing, and debugging.\n",
      "\n",
      "Choosing Your Path:\n",
      "**0:** The generated section incorrectly identifies the core ...\n",
      "\n",
      "\n",
      "\u001b[93m----------------------------------------- follows_gt_flow -----------------------------------------\u001b[0m\n",
      "  follows_gt_flow - Score: 0.20\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "*** REASON ***\n",
      "\n",
      "Introduction:\n",
      "**0:** The generated section reverses the order of the introductory paragraphs compared to the expected output, starting with the learning objectives instead of the problem statement.\n",
      "\n",
      "Understanding the Spectrum: From Workflows to Agents:\n",
      "**0:** While the overall order of sub-sections and diagrams is maintained, the generated output's 'LLM Workflows' sub-section is missing substantial descriptive content, which disrupts the expected flow of information.\n",
      "\n",
      "Choosing Your Path:\n",
      "**0:** ...\n",
      "\n",
      "\n",
      "\u001b[93m--------------------------------------- follows_gt_structure ---------------------------------------\u001b[0m\n",
      "  follows_gt_structure - Score: 0.40\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "*** REASON ***\n",
      "\n",
      "Introduction:\n",
      "**1:** Both sections maintain a similar paragraph structure and length.\n",
      "\n",
      "Understanding the Spectrum: From Workflows to Agents:\n",
      "**0:** The generated section's 'LLM Workflows' sub-section has a much shorter paragraph than expected, and the content within the paragraph is less detailed. The generated section also uses a bulleted list for the 'AI Agents' sub-section, while the expected output uses a paragraph.\n",
      "\n",
      "Choosing Your Path:\n",
      "**0:** The generated section uses bullet points for 'Wh...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for result in results:\n",
    "    pretty_print.wrapped(f\"{result.name} - Score: {result.value:.2f}\", title=result.name)\n",
    "    print(\"*** REASON ***\\n\")\n",
    "    print(f\"{result.reason[:500]}...\" if len(result.reason) > 500 else result.reason)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that even if we have a final aggregate score within the `[0, 1]` interval within the `REASON` attribute, we can see the binary score and the reason behind each score for each section. The final aggregate score is the average of the binary scores in the `REASON` field.\n",
    "\n",
    "Now, that we understood how the `FollowsGTMetricLLMJudge` metric works, let's look over the `UserIntentMetricLLMJudge` metric, which follows a similar strategy starting from how the metric class is defined to how we wrote the prompt and few-shot examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. The UserIntent Metric: Checking Guideline Adherence and Research Anchoring\n",
    "\n",
    "Now let's explore the `UserIntentMetricLLMJudge`, our second LLM judge metric. Since we already covered the base abstractions, we'll focus on the particularities of this metric.\n",
    "\n",
    "### 6.1 The UserIntentMetricLLMJudge Implementation\n",
    "\n",
    "Source: `brown.evals.metrics.user_intent.metric`\n",
    "\n",
    "```python\n",
    "class UserIntentMetricLLMJudge(BrownBaseMetric):\n",
    "    \"\"\"A metric that evaluates how well generated articles follow article guidelines and are anchored in research.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: SupportedModels = SupportedModels.GOOGLE_GEMINI_25_FLASH,\n",
    "        name: str = \"user_intent\",\n",
    "        model_config: ModelConfig | None = None,\n",
    "        track: bool = True,\n",
    "        project_name: str | None = None,\n",
    "    ) -> None:\n",
    "        model_config = model_config or ModelConfig(\n",
    "            temperature=0.0, thinking_budget=1024 * 4, include_thoughts=False, max_retries=3\n",
    "        )\n",
    "        super().__init__(\n",
    "            model=model,\n",
    "            name=name,\n",
    "            structured_output_type=UserIntentArticleScores,\n",
    "            few_shot_examples=prompts.DEFAULT_FEW_SHOT_EXAMPLES,\n",
    "            model_config=model_config,\n",
    "            track=track,\n",
    "            project_name=project_name,\n",
    "        )\n",
    "\n",
    "    async def ascore(\n",
    "        self,\n",
    "        input: str,\n",
    "        context: dict[str, Any],\n",
    "        output: str,\n",
    "        **ignored_kwargs: Any,\n",
    "    ) -> score_result.ScoreResult | list[score_result.ScoreResult]:\n",
    "        # Extract research from context\n",
    "        if \"research\" not in context:\n",
    "            raise ValueError(\"Context must contain a 'research' key with research content\")\n",
    "\n",
    "        research_content = context[\"research\"]\n",
    "        model_client = self.init_model()\n",
    "\n",
    "        llm_query = prompts.get_eval_prompt(\n",
    "            input=input,\n",
    "            context=research_content,\n",
    "            output=output,\n",
    "            few_shot_examples=self.few_shot_examples,\n",
    "        )\n",
    "\n",
    "        user_intent_response = cast(\n",
    "            UserIntentArticleScores,\n",
    "            await model_client.ainvoke([{\"role\": \"user\", \"content\": llm_query}]),\n",
    "        )\n",
    "\n",
    "        if not user_intent_response:\n",
    "            raise ValueError(\"Model failed to return a structured response.\")\n",
    "\n",
    "        return user_intent_response.to_score_result(self.name)\n",
    "```\n",
    "\n",
    "**Key Differences from FollowsGT:**\n",
    "- Takes `input` (article guideline), `context` (with research), and `output` (generated article)\n",
    "- Uses `UserIntentArticleScores` as the structured output type\n",
    "- Evaluates two dimensions: `guideline_adherence` and `research_anchoring`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 The Score Types\n",
    "\n",
    "Source: `brown.evals.metrics.user_intent.types`\n",
    "\n",
    "```python\n",
    "class UserIntentCriteriaScores(CriteriaScores):\n",
    "    \"\"\"Represents scores for both evaluation dimensions of a section.\"\"\"\n",
    "\n",
    "    guideline_adherence: CriterionScore\n",
    "    research_anchoring: CriterionScore\n",
    "\n",
    "\n",
    "class UserIntentSectionScores(pydantic.BaseModel):\n",
    "    \"\"\"Section evaluation with scores across all UserIntent dimensions.\"\"\"\n",
    "\n",
    "    title: str = pydantic.Field(description=\"The title of the section being evaluated.\")\n",
    "    scores: UserIntentCriteriaScores = pydantic.Field(description=\"The scores of the section.\")\n",
    "\n",
    "\n",
    "class UserIntentArticleScores(pydantic.BaseModel):\n",
    "    \"\"\"Article-level scores for the UserIntent evaluation metric.\"\"\"\n",
    "\n",
    "    sections: list[UserIntentSectionScores]\n",
    "\n",
    "    def to_score_result(self, prefix: str) -> list[score_result.ScoreResult]:\n",
    "        return aggregate_section_scores_to_results(self.sections, prefix)\n",
    "```\n",
    "\n",
    "The structure mirrors `FollowsGT`, but with different dimensions:\n",
    "- **guideline_adherence**: Does the section follow the article guideline?\n",
    "- **research_anchoring**: Is the content based on the provided research?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 The Few-Shot Examples\n",
    "\n",
    "Source: `brown.evals.metrics.user_intent.types`\n",
    "\n",
    "```python\n",
    "class UserIntentMetricFewShotExample(BaseExample):\n",
    "    \"\"\"Represents a single example for the user intent evaluation.\"\"\"\n",
    "\n",
    "    input: str  # Article guideline\n",
    "    context: str  # Research\n",
    "    output: str  # Generated article\n",
    "    scores: UserIntentArticleScores\n",
    "\n",
    "    @classmethod\n",
    "    def from_markdown(\n",
    "        cls,\n",
    "        input_file: Path,\n",
    "        context_file: Path,\n",
    "        output_file: Path,\n",
    "        scores: UserIntentArticleScores,\n",
    "    ) -> \"UserIntentMetricFewShotExample\":\n",
    "        input_content = input_file.read_text()\n",
    "        context_content = context_file.read_text()\n",
    "        output_content = output_file.read_text()\n",
    "        return cls(input=input_content, context=context_content, output=output_content, scores=scores)\n",
    "\n",
    "    def to_context(self) -> str:\n",
    "        return f\"\"\"\n",
    "<input>\n",
    "{self.input}\n",
    "</input>\n",
    "<context>\n",
    "{self.context}\n",
    "</context>\n",
    "<output>\n",
    "{self.output}\n",
    "</output>\n",
    "{self.scores.to_context()}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class UserIntentMetricFewShotExamples(BaseFewShotExamples[UserIntentMetricFewShotExample]):\n",
    "    \"\"\"Collection of few-shot examples for the UserIntent evaluation metric.\"\"\"\n",
    "    pass\n",
    "```\n",
    "\n",
    "As you can see, the metric and entity classes are almost identical with the ones for the follows GT metric, but scoped on different metrics and inputs. Now, let's also look over the system prompt and concrete few-shot examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 The System Prompt\n",
    "\n",
    "The UserIntent prompt focuses on guideline adherence and research anchoring:\n",
    "\n",
    "Source: `brown.evals.metrics.user_intent.prompts`\n",
    "\n",
    "**1. Introduction:**\n",
    "```python\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are an expert in Natural Language Processing (NLP) evaluation metrics, specifically trained to \n",
    "assess how well generated content follows specific guidelines and incorporates provided research material. \n",
    "\n",
    "The AI system you will evaluate takes as input an article guideline (<input>) and research material (<context>), and \n",
    "based on these inputs, another large language model (LLM) generates an article (<output>).\n",
    "\n",
    "Your task is to evaluate whether the generated article (<output>) adheres to the given\n",
    "user intent structured as an article guideline and is properly anchored in the provided research.\n",
    "...\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "**2. Instructions - Defining the scoring business logic for the guideline adherence and research anchoring binary metrics:**\n",
    "```python\n",
    "\"\"\"\n",
    "...\n",
    "## INSTRUCTIONS \n",
    "\n",
    "1. You must analyze the given article guideline (<input>), research material (<context>), and \n",
    "generated article (<output>) to determine how well the output follows the input guidelines and utilizes the research.\n",
    "2. The input, context, and output are in Markdown format.\n",
    "3. Instead of comparing the outputs as a whole, you will divide the outputs into sections and compare each section \n",
    "individually. \n",
    "4. Since the article guideline reflects the user expectations and intent, you will always use it as the reference point \n",
    "to understand which sections from the generated article should be evaluated. To find the expected sections, look within \n",
    "the given article guideline for the keyword \"Outline\" or actual section titles often marked with H2 headers prefixed\n",
    "with \"Section\". You will always use these as the anchor points, the expected sections, when making comparisons between\n",
    "sections. \n",
    "5. When associating a section from the article guideline with one from the generated article, you will first look\n",
    "for a matching or similar title. If there is no match based solely on the title, you will try to make associations based\n",
    "on the content of the section. For example, if the expected section has 3 points on how RAG works and\n",
    "the generated section has 3 points on how RAG works as well, then they are associated even if the titles are different.\n",
    "If a required section mentioned in the guideline is missing from the generated article, you will assign a score of \n",
    "0 to all evaluation criteria.\n",
    "6. Using the article guideline as an anchor, you will divide the generated article into sections and evaluate each section \n",
    "individually against all given criteria.\n",
    "7. Sections are divided by H2 headers, marked as \"##\" in Markdown. You will use the headers as separators. \n",
    "Anything between two H2 headers constitutes a section. The only valid exception to this rule is the first section, \n",
    "the introduction, which sometimes appears between the title and the first H2 header. You will never include the title or \n",
    "subtitle as part of the first section.\n",
    "8. The score can only be an integer value of 0 or 1. For each section, you will assign a binary integer score (0 or 1) based on \n",
    "two criteria:\n",
    "   1. **Guideline Adherence**: For each expected section in the article guideline, you will evaluate whether the generated \n",
    "   section follows the specific section requirements outlined in the article guideline:\n",
    "        - We expect a perfect match between the expected section and the generated section. Intuitively, you can\n",
    "        think of the section guideline as a sketch, a compressed version of the generated section.\n",
    "        - Less: If any topic from the expected article guideline is missing from the generated article, you will assign \n",
    "        a score of 0.\n",
    "        - More: If the generated section has any additional topics that are not in the expected article guideline, \n",
    "        you will assign a score of 0.\n",
    "        - Different Order: If the order of ideas from the expected article guideline is not followed in the \n",
    "        generated article, you will assign a score of 0.\n",
    "        - If section constraints are provided, we are looking only for a rough approximation of the length. The exact\n",
    "        section length criterias are present in the article guideline. Errors of Â±100 units are acceptable. Units can\n",
    "        be words, characters, or reading time. For example, if the expected section length is 100 words and the generated section length \n",
    "        is 190 words, you will assign a score of 1. But if the generated section is 230 words, as it exceeds the tolerance range,\n",
    "        you will assign a score of 0.\n",
    "   2. **Research Anchoring**: For each expected section in the article guideline, you will evaluate whether the generated \n",
    "   section content is based on or derived from the provided research:\n",
    "        - We expect each section from the generated article to be generated entirely based on the ideas provided\n",
    "        in the article guideline and research. Thus, you can consider both the context and article guideline as the \n",
    "        \"research\", the single source of truth.\n",
    "        - If any idea from the generated section is not present in the research, you will assign a score of 0.\n",
    "        - The generated section does not have to contain all the ideas from the research, just a subset of them.\n",
    "        - If no research is explicitly referenced through citations, you will manually check if the generated section content\n",
    "        it's based solely on the research. Missing explicit citations is valid. What it's critical is all the ideas to\n",
    "        adhere to the research. Thus, if the generated section content is based solely on the research, while missing citations,\n",
    "        you will assign a score of 1.\n",
    "9. Along with the binary score, you will provide a brief and concise explanation containing the reasoning behind \n",
    "the score for each criterion. The score will be used to debug and monitor the evaluation process. Therefore, it is\n",
    "important to provide thorough reasoning for the score. Since we provide binary scores, the reasoning should always \n",
    "contain what is good and what is problematic about the generated section, regardless of the score. For example, if the score \n",
    "is 0, the reasoning should also contain what is good about the generated section, such as \"the generated section \n",
    "contains all the bulleted points from the expected section guideline,\" and what is problematic, such as \"however, it contains an \n",
    "additional section on AI Evals that is not present in the guideline.\" Also, when generating the reasoning for the\n",
    "research anchoring criterion, you will always mention if the topic comes from the article guideline, context, or both, while\n",
    "supporting every single claim with evidence from the research. For example, the generated section is correctly anchored in the research,\n",
    "while the fundamentals on RAG are based on the context, while the specific details on the RAG architecture are based on the article\n",
    "guideline.\n",
    "10. Important rules when evaluating:\n",
    "   - Focus on substance, not superficial formatting differences\n",
    "   - When comparing **media**, you only care about the placement and the caption of the media. \n",
    "    Since media can take many forms such as Mermaid diagrams, images, or URLs, you will completely ignore the \n",
    "    content of the media. Based on the section guideline, you will check whether the media is present in the \n",
    "    correct place. Based on the caption of the media, you will check whether it is properly anchored in the research.\n",
    "...\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "**3. Chain of Thought:**\n",
    "```python\n",
    "\"\"\"\n",
    "...\n",
    "## CHAIN OF THOUGHT\n",
    "\n",
    "**Understanding Input:**\n",
    "1.1. Read and understand the article guideline (<input>) to identify specific requirements, structure, \n",
    "content expectations, constraints, and most importantly the expected sections.\n",
    "1.2. Read and understand the context (<context>) to identify available information, sources, and key findings.\n",
    "1.3. Label the article guideline and context as the \"research\", the single source of truth.\n",
    "1.4. Read and understand the generated article (<output>) and split it into sections using H2 headers as separators.\n",
    "1.5. Connect the expected sections from the article guideline to the sections from the generated article.\n",
    "\n",
    "**Section-by-Section Evaluation:**\n",
    "2.1. For each section identified in the article guideline, locate its associated section in the generated article, and \n",
    "evaluate it against both criteria. If a section is found in the article guideline and is missing in the generated \n",
    "article, you will assign a score of 0 to all evaluation criteria.\n",
    "2.2. Evaluate guideline adherence between each expected section from the article guideline and the associated section \n",
    "from the generated article.\n",
    "2.3. Evaluate research anchoring by first selecting the sections to evaluate from the article guideline and then \n",
    "comparing the generated section to the research, found in the context and the associated section guideline.\n",
    "\n",
    "**Assigning Scores:**\n",
    "3.1. Based on each section expected from the article guideline, assign a binary score of either 0 or 1 \n",
    "for all evaluation criteria listed in the instructions:\n",
    "    - Score 1 if the section clearly follows the requirements detailed in the instructions.\n",
    "    - Score 0 if it fails to follow the requirements detailed in the instructions.\n",
    "3.2. Justify why you assigned a score of 0 or 1 with a brief explanation that highlights the reasoning behind the score\n",
    "based on the given criterion.\n",
    "...\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "**4. What to Avoid:**\n",
    "```python\n",
    "\"\"\"\n",
    "...\n",
    "## WHAT TO AVOID\n",
    "\n",
    "- Do not provide scores using the generated output as the reference point to divide into sections. You must always \n",
    "use the article guideline as the reference point to divide into sections.\n",
    "- Do not let other sections influence the score of a section. The score of each section must be determined in complete \n",
    "isolation from any other section.\n",
    "...\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "**5. Few-shot examples and input:**\n",
    "```python\n",
    "\"\"\"\n",
    "...\n",
    "## FEW-SHOT EXAMPLES\n",
    "\n",
    "Here are few-shot examples demonstrating how to compute the scores for each section and criterion:\n",
    "<few-shot-examples>\n",
    "{examples}\n",
    "</few-shot-examples>\n",
    "\n",
    "## INPUTS\n",
    "\n",
    "<input>\n",
    "{input}\n",
    "</input>\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "<output>\n",
    "{output}\n",
    "</output>\n",
    "...\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "**6. Final anchoring to the requested task:**\n",
    "```python\n",
    "\"\"\"\n",
    "...\n",
    "Think through your answer step by step, and provide the requested evaluation.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5 The Default Few-Shot Examples\n",
    "\n",
    "Similar to FollowsGT, the few-shot examples are manually labeled and statically loaded within the `UserIntentMetricFewShotExamples`, `UserIntentMetricFewShotExample` and `UserIntentArticleScores` entity classes.\n",
    "\n",
    "Notice how, for each section, for each metric type (guideline adherence and research anchoring), we manually added a binary score and the reason behind WHY we added that particular binary score as a 1-2 sentence explanation.\n",
    "\n",
    "```python\n",
    "EXAMPLES_DIR = Path(__file__).parent / \"examples\"\n",
    "DEFAULT_FEW_SHOT_EXAMPLES = UserIntentMetricFewShotExamples(\n",
    "    examples=[\n",
    "        UserIntentMetricFewShotExample.from_markdown(\n",
    "            input_file=EXAMPLES_DIR / \"04_structured_outputs\" / \"article_guideline.md\",\n",
    "            context_file=EXAMPLES_DIR / \"04_structured_outputs\" / \"research.md\",\n",
    "            output_file=EXAMPLES_DIR / \"04_structured_outputs\" / \"article_generated.md\",\n",
    "            scores=UserIntentArticleScores(\n",
    "                sections=[\n",
    "                    UserIntentSectionScores(\n",
    "                        title=\"Introduction\",\n",
    "                        scores=UserIntentCriteriaScores(\n",
    "                            guideline_adherence=CriterionScore(\n",
    "                                score=0,\n",
    "                                reason=(\n",
    "                                    'The section correctly introduces the \"bridge\" concept but violates three guidelines: '\n",
    "                                    'it uses the first-person singular (\"I remember...\") against the point-of-view rule, '\n",
    "                                    \"adds a new anecdote about sentiment analysis and at ~250 words, it exceeds the \"\n",
    "                                    \"150-word length constraint.\"\n",
    "                                ),\n",
    "                            ),\n",
    "                            research_anchoring=CriterionScore(\n",
    "                                score=0,\n",
    "                                reason=(\n",
    "                                    'While the core \"bridge\" concept between the LLM (software 3.0) and Python '\n",
    "                                    \"(software 1.0) worlds' is anchored in the guideline, the section introduces a \"\n",
    "                                    'significant, un-anchored claim that mastering structured outputs is \"the key to '\n",
    "                                    'unlocking true Artificial General Intelligence (AGI),\" which is not supported by '\n",
    "                                    \"the research.\"\n",
    "                                ),\n",
    "                            ),\n",
    "                        ),\n",
    "                    ),\n",
    "                    UserIntentSectionScores(\n",
    "                        title=\"Understanding why structured outputs are critical\",\n",
    "                        scores=UserIntentCriteriaScores(\n",
    "                            guideline_adherence=CriterionScore(\n",
    "                                score=0,\n",
    "                                reason=(\n",
    "                                    \"The section correctly covers the required benefits but violates the \"\n",
    "                                    '\"Different Order\" rule by presenting them in a different sequence than the '\n",
    "                                    'guideline. It also adds an unrequested benefit (\"Improved Token Efficiency\"), '\n",
    "                                    'violating the \"More\" rule.'\n",
    "                                ),\n",
    "                            ),\n",
    "                            research_anchoring=CriterionScore(\n",
    "                                score=1,\n",
    "                                reason=(\n",
    "                                    \"The section is well-anchored. The benefits of parsing, Pydantic validation, \"\n",
    "                                    \"avoiding fragile parsing and the GraphRAG/knowledge-graph use-case are all \"\n",
    "                                    \"supported in the research and the added point on token efficiency are all \"\n",
    "                                    \"directly supported by claims in the provided research material\"\n",
    "                                ),\n",
    "                            ),\n",
    "                        ),\n",
    "                    ),\n",
    "                    ... # The rest of the sections\n",
    "                    UserIntentSectionScores(\n",
    "                        title=\"Conclusion: Structured Outputs Are Everywhere\",\n",
    "                        scores=UserIntentCriteriaScores(\n",
    "                            guideline_adherence=CriterionScore(\n",
    "                                score=0,\n",
    "                                reason=(\n",
    "                                    \"The section correctly emphasizes the ubiquity of structured outputs and \"\n",
    "                                    \"name-checks later topics as the guideline asks for. However, it does not \"\n",
    "                                    \"provide a transition to the requested next Lesson 5.\"\n",
    "                                ),\n",
    "                            ),\n",
    "                            research_anchoring=CriterionScore(\n",
    "                                score=1,\n",
    "                                reason=(\n",
    "                                    \"The section's claims are general and consistent with the course context. \"\n",
    "                                    \"It does not introduce new factual claims that require specific anchoring, \"\n",
    "                                    \"and therefore contains no information that contradicts the provided research \"\n",
    "                                    \"material.\"\n",
    "                                ),\n",
    "                            ),\n",
    "                        ),\n",
    "                    ),\n",
    "                ]\n",
    "            ),\n",
    "        ),\n",
    "        UserIntentMetricFewShotExample.from_markdown(\n",
    "            input_file=EXAMPLES_DIR / \"07_reasoning_planning\" / \"article_guideline.md\",\n",
    "            context_file=EXAMPLES_DIR / \"07_reasoning_planning\" / \"research.md\",\n",
    "            output_file=EXAMPLES_DIR / \"07_reasoning_planning\" / \"article_generated.md\",\n",
    "            scores=UserIntentArticleScores(\n",
    "                sections=[\n",
    "                    UserIntentSectionScores(\n",
    "                        title=\"Introduction\",\n",
    "                        scores=UserIntentCriteriaScores(\n",
    "                            guideline_adherence=CriterionScore(\n",
    "                                score=1,\n",
    "                                reason=(\n",
    "                                    \"The introduction aligns with the guideline. It sets the stage by introducing \"\n",
    "                                    \"planning and reasoning, explains the problem with standard LLMs, previews ReAct \"\n",
    "                                    \"and Plan-and-Execute, and mentions the advanced capabilities, all while staying \"\n",
    "                                    \"within the specified word count.\"\n",
    "                                ),\n",
    "                            ),\n",
    "                            research_anchoring=CriterionScore(\n",
    "                                score=1,\n",
    "                                reason=(\n",
    "                                    \"The section is fully anchored in the research. All the high-level concepts it \"\n",
    "                                    \"introducesâ€”planning, reasoning, ReAct, Plan-and-Execute, and self-correctionâ€”are \"\n",
    "                                    \"the central themes of the provided research material and are presented accurately.\"\n",
    "                                ),\n",
    "                            ),\n",
    "                        ),\n",
    "                    ),\n",
    "                    UserIntentSectionScores(\n",
    "                        title=\"What a Non-Reasoning Model Does And Why It Fails on Complex Tasks\",\n",
    "                        scores=UserIntentCriteriaScores(\n",
    "                            guideline_adherence=CriterionScore(\n",
    "                                score=1,\n",
    "                                reason=(\n",
    "                                    \"The section meets all requirements from the outline, including the correct agent \"\n",
    "                                    \"example, consequences, and transitions. The addition of context on the 'black box \"\n",
    "                                    \"problem' is brief and does not detract from or replace any of the mandated points.\"\n",
    "                                ),\n",
    "                            ),\n",
    "                            research_anchoring=CriterionScore(\n",
    "                                score=0,\n",
    "                                reason=(\n",
    "                                    \"While the core concepts are based on the guideline, the section's added discussion \"\n",
    "                                    \"on the 'black box problem' and its relation to 'enterprise adoption' is not \"\n",
    "                                    \"supported by any evidence within the provided research material.\"\n",
    "                                ),\n",
    "                            ),\n",
    "                        ),\n",
    "                    ),\n",
    "                    UserIntentSectionScores(\n",
    "                        title='Teaching Models to \"Think\": Chain-of-Thought and Its Limits',\n",
    "                        scores=UserIntentCriteriaScores(\n",
    "                            guideline_adherence=CriterionScore(\n",
    "                                score=0,\n",
    "                                reason=(\n",
    "                                    \"The section uses the correct prompt example and most request topics but it fails \"\n",
    "                                    \"to address the specific limitation required by the guideline, which is that the \"\n",
    "                                    \"'plan and the answer appear in the same text'. Instead, it substitutes this with \"\n",
    "                                    \"an unrequested point about computational cost.\"\n",
    "                                ),\n",
    "                            ),\n",
    "                            research_anchoring=CriterionScore(\n",
    "                                score=1,\n",
    "                                reason=(\n",
    "                                    \"The section's description of Chain-of-Thought is correctly aligned with the \"\n",
    "                                    \"foundational concepts presented in the research materials, such as the canonical \"\n",
    "                                    \"paper Sourced.\"\n",
    "                                ),\n",
    "                            ),\n",
    "                        ),\n",
    "                    ),\n",
    "                    ... # The rest of the sections\n",
    "                    UserIntentSectionScores(\n",
    "                        title=\"Plan-and-Execute in Depth: Structuring the Path from Goal to Action\",\n",
    "                        scores=UserIntentCriteriaScores(\n",
    "                            guideline_adherence=CriterionScore(\n",
    "                                score=0,\n",
    "                                reason=(\n",
    "                                    \"Includes mermaid diagram and maps out the plan and execute example as requested. \"\n",
    "                                    \"The section fails to adhere to the content guideline for its 'Pros'. It incorrectly \"\n",
    "                                    \"emphasizes 'fostering model creativity' instead of the specified advantages of \"\n",
    "                                    \"efficiency and reliability for structured tasks.\"\n",
    "                                ),\n",
    "                            ),\n",
    "                            research_anchoring=CriterionScore(\n",
    "                                score=0,\n",
    "                                reason=(\n",
    "                                    \"A source for inflexibility is found correctly but the primary advantage \"\n",
    "                                    \"presentedâ€”creativityâ€”is not supported by the research set. The provided sources \"\n",
    "                                    \"frame the benefits of this pattern in terms of cost, reliability, and efficiency.\"\n",
    "                                ),\n",
    "                            ),\n",
    "                        ),\n",
    "                    ),\n",
    "                    ... # The rest of the sections\n",
    "                    UserIntentSectionScores(\n",
    "                        title=\"Conclusion\",\n",
    "                        scores=UserIntentCriteriaScores(\n",
    "                            guideline_adherence=CriterionScore(\n",
    "                                score=1,\n",
    "                                reason=(\n",
    "                                    \"The conclusion adheres to the guideline. It effectively summarizes the lesson's \"\n",
    "                                    \"key takeaways, recaps the ReAct and Plan-and-Execute patterns, and provides a \"\n",
    "                                    \"clear, accurate preview of the next lessons (8, 9, and 10), all within the \"\n",
    "                                    \"specified length\"\n",
    "                                ),\n",
    "                            ),\n",
    "                            research_anchoring=CriterionScore(\n",
    "                                score=1,\n",
    "                                reason=(\n",
    "                                    \"The section is fully anchored. The summary accurately recaps the core concepts \"\n",
    "                                    \"supported by the research, and the preview of future lessons (8, 9, and 10) is \"\n",
    "                                    \"taken directly from the article guideline.\"\n",
    "                                ),\n",
    "                            ),\n",
    "                        ),\n",
    "                    ),\n",
    "                ]\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "```\n",
    "\n",
    "Let's look at more details about the few-shot examples:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of few-shot examples: 2\n",
      "\n",
      "Example 1:\n",
      "  - Input (guideline) length: 18,329 characters\n",
      "  - Context (research) length: 385,985 characters\n",
      "  - Output (article) length: 18,376 characters\n",
      "  - Number of sections scored: 6\n",
      "  - Section titles: ['Introduction', 'Understanding why structured outputs are critical', 'Implementing structured outputs from scratch using JSON', 'Implementing structured outputs from scratch using Pydantic', 'Implementing structured outputs using Gemini and Pydantic', 'Conclusion: Structured Outputs Are Everywhere']\n",
      "\n",
      "Example 2:\n",
      "  - Input (guideline) length: 20,085 characters\n",
      "  - Context (research) length: 255,315 characters\n",
      "  - Output (article) length: 19,238 characters\n",
      "  - Number of sections scored: 10\n",
      "  - Section titles: ['Introduction', 'What a Non-Reasoning Model Does And Why It Fails on Complex Tasks', 'Teaching Models to \"Think\": Chain-of-Thought and Its Limits', 'Separating Planning from Answering: Foundations of ReAct and Plan-and-Execute', 'ReAct in Depth: The Loop of Thought and Observation', 'Plan-and-Execute in Depth: Structuring the Path from Goal to Action', 'Where This Shows Up in Practice: Deep Research-Style Systems', 'Modern Reasoning Models: Thinking vs. Answer Streams and Interleaved Thinking', 'Advanced Agent Capabilities Enabled by Planning: Goal Decomposition and Self-Correction', 'Conclusion']\n"
     ]
    }
   ],
   "source": [
    "from brown.evals.metrics.user_intent.prompts import DEFAULT_FEW_SHOT_EXAMPLES as USER_INTENT_FEW_SHOT_EXAMPLES\n",
    "\n",
    "print(f\"Number of few-shot examples: {len(USER_INTENT_FEW_SHOT_EXAMPLES.examples)}\")\n",
    "for i, example in enumerate(USER_INTENT_FEW_SHOT_EXAMPLES.examples):\n",
    "    print(f\"\\nExample {i + 1}:\")\n",
    "    print(f\"  - Input (guideline) length: {len(example.input):,} characters\")\n",
    "    print(f\"  - Context (research) length: {len(example.context):,} characters\")\n",
    "    print(f\"  - Output (article) length: {len(example.output):,} characters\")\n",
    "    print(f\"  - Number of sections scored: {len(example.scores.sections)}\")\n",
    "    print(f\"  - Section titles: {[s.title for s in example.scores.sections]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.6 The get_eval_prompt Function\n",
    "\n",
    "To actually use the system prompt from above, together with the few-shot examples we defined, we defined a `get_eval_prompt` function that assembles the final prompt. This function is used within the `UserIntentMetricLLMJudge` metric class we defined at the beginning, right before calling the LLM:\n",
    "\n",
    "Source: `brown.evals.metrics.user_intent.prompts`\n",
    "\n",
    "```python\n",
    "def get_eval_prompt(\n",
    "    input: str,\n",
    "    context: str,\n",
    "    output: str,\n",
    "    few_shot_examples: UserIntentMetricFewShotExamples,\n",
    ") -> str:\n",
    "    \"\"\"Generate the evaluation prompt for the user intent metric.\"\"\"\n",
    "\n",
    "    return SYSTEM_PROMPT.format(\n",
    "        examples=few_shot_examples.to_context(),\n",
    "        input=input,\n",
    "        context=context,\n",
    "        output=output,\n",
    "    )\n",
    "```\n",
    "\n",
    "This function takes the article guideline, research, generated output, and few-shot examples, and formats them into the complete system prompt for the LLM judge.\n",
    "\n",
    "Here is how the system prompt looks like:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m-------------------------------- UserIntent LLM Judge System Prompt --------------------------------\u001b[0m\n",
      "  You are an expert in Natural Language Processing (NLP) evaluation metrics, specifically trained to \n",
      "assess how well generated content follows specific guidelines and incorporates provided research material. \n",
      "\n",
      "The AI system you will evaluate takes as input an article guideline (<input>) and research material (<context>), and \n",
      "based on these inputs, another large language model (LLM) generates an article (<output>).\n",
      "\n",
      "Your task is to evaluate whether the generated article (<output>) adheres to the given\n",
      "user intent structured as an article guideline and is properly anchored in the provided research.\n",
      "\n",
      "## INSTRUCTIONS \n",
      "\n",
      "1. You must analyze the given article guideline (<input>), research material (<context>), and \n",
      "generated article (<output>) to determine how well the output follows the input guidelines and utilizes the research.\n",
      "2. The input, context, and output are in Markdown format.\n",
      "3. Instead of comparing the outputs as a whole, you will divide the outputs into sections and compare each section \n",
      "individually. \n",
      "4. Since the article guideline reflects the user expectations and intent, you will always use it as the reference point \n",
      "to understand which sections from the generated article should be evaluated. To find the expected sections, look within \n",
      "the given article guideline for the keyword \"Outline\" or actual section titles often marked with H2 headers prefixed\n",
      "with \"Section\". You will always use these as the anchor points, the expected sections, when making comparisons between\n",
      "sections. \n",
      "5. When associating a section from the article guideline with one from the generated article, you will first look\n",
      "for a matching or similar title. If there is no match based solely on the title, you will try to make associations based\n",
      "on the content of the section. For example, if the expected section has 3 points on how RAG works and\n",
      "the generated section has 3 points on how RAG works as well, then they are associated even if the titles are different.\n",
      "If a required section mentioned in the guideline is missing from the generated article, you will assign a score of \n",
      "0 to all evaluation criteria.\n",
      "6. Using the article guideline as an anchor, you will divide the generated article into sections and evaluate each section \n",
      "individually against all given criteria.\n",
      "7. Sections are divided by H2 headers, marked as \"##\" in Markdown. You will use the headers as separators. \n",
      "Anything between two H2 headers constitutes a section. The only valid exception to this rule is the first section, \n",
      "the introduction, which sometimes appears between the title and the first H2 header. You will never include the title or \n",
      "subtitle as part of the first section.\n",
      "8. The score can only be an integer value of 0 or 1. For each section, you will assign a binary integer score (0 or 1) based on \n",
      "two criteria:\n",
      "   1. **Guideline Adherence**: For each expected section in the article guideline, you will evaluate whether the generated \n",
      "   section follows the specific section requirements outlined in the article guideline:\n",
      "        - We expect a perfect match between the expected section and the generated section. Intuitively, you can\n",
      "        think of the section guideline as a sketch, a compressed version of the generated section.\n",
      "        - Less: If any topic from the expected article guideline is missing from the generated article, you will assign \n",
      "        a score of 0.\n",
      "        - More: If the generated section has any additional topics that are not in the expected article guideline, \n",
      "        you will assign a score of 0.\n",
      "        - Different Order: If the order of ideas from the expected article guideline is not followed in the \n",
      "        generated article, you will assign a score of 0.\n",
      "        - If section constraints are provided, we are looking only for a rough approximation of the length. The exact\n",
      "        section length criterias are present in the article guideline. Errors of Â±100 units are acceptable. Units can\n",
      "        be words, characters, or reading time. For example, if the expected section length is 100 words and the generated section length \n",
      "        is 190 words, you will assign a score of 1. But if the generated section is 230 words, as it exceeds the tolerance range,\n",
      "        you will assign a score of 0.\n",
      "   2. **Research Anchoring**: For each expected section in the article guideline, you will evaluate whether the generated \n",
      "   section content is based on or derived from the provided research:\n",
      "        - We expect each section from the generated article to be generated entirely based on the ideas provided\n",
      "        in the article guideline and research. Thus, you can consider both the context and article guideline as the \n",
      "        \"research\", the single source of truth.\n",
      "        - If any idea from the generated section is not present in the research, you will assign a score of 0.\n",
      "        - The generated section does not have to contain all the ideas from the research, just a subset of them.\n",
      "        - If no research is explicitly referenced through citations, you will manually check if the generated section content\n",
      "        it's based solely on the research. Missing explicit citations is valid. What it's critical is all the ideas to\n",
      "        adhere to the research. Thus, if the generated section content is based solely on the research, while missing citations,\n",
      "        you will assign a score of 1.\n",
      "9. Along with the binary score, you will provide a brief and concise explanation containing the reasoning behind \n",
      "the score for each criterion. The score will be used to debug and monitor the evaluation process. Therefore, it is\n",
      "important to provide thorough reasoning for the score. Since we provide binary scores, the reasoning should always \n",
      "contain what is good and what is problematic about the generated section, regardless of the score. For example, if the score \n",
      "is 0, the reasoning should also contain what is good about the generated section, such as \"the generated section \n",
      "contains all the bulleted points from the expected section guideline,\" and what is problematic, such as \"however, it contains an \n",
      "additional section on AI Evals that is not present in the guideline.\" Also, when generating the reasoning for the\n",
      "research anchoring criterion, you will always mention if the topic comes from the article guideline, context, or both, while\n",
      "supporting every single claim with evidence from the research. For example, the generated section is correctly anchored in the research,\n",
      "while the fundamentals on RAG are based on the context, while the specific details on the RAG architecture are based on the article\n",
      "guideline.\n",
      "10. Important rules when evaluating:\n",
      "   - Focus on substance, not superficial formatting differences\n",
      "   - When comparing **media**, you only care about the placement and the caption of the media. \n",
      "    Since media can take many forms such as Mermaid diagrams, images, or URLs, you will completely ignore the \n",
      "    content of the media. Based on the section guideline, you will check whether the media is present in the \n",
      "    correct place. Based on the caption of the media, you will check whether it is properly anchored in the research.\n",
      "\n",
      "## CHAIN OF THOUGHT\n",
      "\n",
      "**Understanding Input:**\n",
      "1.1. Read and understand the article guideline (<input>) to identify specific requirements, structure, \n",
      "content expectations, constraints, and most importantly the expected sections.\n",
      "1.2. Read and understand the context (<context>) to identify available information, sources, and key findings.\n",
      "1.3. Label the article guideline and context as the \"research\", the single source of truth.\n",
      "1.4. Read and understand the generated article (<output>) and split it into sections using H2 headers as separators.\n",
      "1.5. Connect the expected sections from the article guideline to the sections from the generated article.\n",
      "\n",
      "**Section-by-Section Evaluation:**\n",
      "2.1. For each section identified in the article guideline, locate its associated section in the generated article, and \n",
      "evaluate it against both criteria. If a section is found in the article guideline and is missing in the generated \n",
      "article, you will assign a score of 0 to all evaluation criteria.\n",
      "2.2. Evaluate guideline adherence between each expected section from the article guideline and the associated section \n",
      "from the generated article.\n",
      "2.3. Evaluate research anchoring by first selecting the sections to evaluate from the article guideline and then \n",
      "comparing the generated section to the research, found in the context and the associated section guideline.\n",
      "\n",
      "**Assigning Scores:**\n",
      "3.1. Based on each section expected from the article guideline, assign a binary score of either 0 or 1 \n",
      "for all evaluation criteria listed in the instructions:\n",
      "    - Score 1 if the section clearly follows the requirements detailed in the instructions.\n",
      "    - Score 0 if it fails to follow the requirements detailed in the instructions.\n",
      "3.2. Justify why you assigned a score of 0 or 1 with a brief explanation that highlights the reasoning behind the score\n",
      "based on the given criterion.\n",
      "\n",
      "## WHAT TO AVOID\n",
      "\n",
      "- Do not provide scores using the generated output as the reference point to divide into sections. You must always \n",
      "use the article guideline as the reference point to divide into sections.\n",
      "- Do not let other sections influence the score of a section. The score of each section must be determined in complete \n",
      "isolation from any other section.\n",
      "\n",
      "## FEW-SHOT EXAMPLES\n",
      "\n",
      "Here are few-shot examples demonstrating how to compute the scores for each section and criterion:\n",
      "<few-shot-examples>\n",
      "<example_1>\n",
      "\t\n",
      "<input>\n",
      "## Global Context of the Lesson\n",
      "\n",
      "### What We Are Planning to Share\n",
      "\n",
      "In this lesson, we will talk about structured outputs. We'll introduce **Structured Outputs** as a way to achieve reliable, controlled, and formatted data extraction, ensuring type and data quality checks. Common structured output types are JSON, XML, and YAML, which can further be translated into Python dictionaries, lists, classes, or most commonly Pydantic models. This approach makes manipulating LLM outputs much easier as it serves as the bridge between the LLM (software 3.0) and Python (software 1.0) worlds. First, we will implement structured outputs from scratch, then show how to do it directly with a popular API such as Gemini. Ultimately, we will highlight how structured outputs are used everywhere when building LLM workflows or AI agents.\n",
      "\n",
      "### Why We Think It's Valuable\n",
      "\n",
      "For an AI Engineer, it's critical to understand how to implement the bridge between the LLM (software 3.0) and Python (software 1.0) worlds,...\n",
      "</input>\n",
      "<context>\n",
      "# Research\n",
      "\n",
      "## Research Results\n",
      "\n",
      "<details>\n",
      "<summary>What are the pros and cons of forcing JSON output via prompt engineering versus using a native model API like Gemini's or OpenAI's structured output features?</summary>\n",
      "\n",
      "### Source [1]: https://www.youtube.com/watch?v=npQx-11SwqU\n",
      "\n",
      "Query: What are the pros and cons of forcing JSON output via prompt engineering versus using a native model API like Gemini's or OpenAI's structured output features?\n",
      "\n",
      "Answer: The video tutorial emphasizes the utility of structured output in APIs powered by GCP Gemini AI. Structured outputâ€”like JSONâ€”enables efficient, precise data handling, easier parsing, and compatibility with other systems. Using Gemini AI's native structured output, developers can ensure that API responses are consistent and valid for downstream applications, reducing the need for error-prone post-processing. Implementing structured output via the API, rather than relying on prompt engineering, supports scalability and robust data validat...\n",
      "</context>\n",
      "<output>\n",
      "# LLM Structured Outputs: The Definitive Guide\n",
      "### From brittle strings to validated Pydantic objects\n",
      "\n",
      "## Introduction\n",
      "\n",
      "In our previous lessons, we laid the groundwork for AI Engineering. We explored the AI agent landscape, distinguished between rigid workflows and autonomous agents, and covered the essentials of Context Engineering. Now, we've reached a critical junction: how do we make the fluid, often unpredictable outputs of a Large Language Model (LLM) work with the structured, type-safe world of our Python applications? The answer is structured outputs.\n",
      "\n",
      "I remember a project I worked on early in my career, trying to build a sentiment analysis pipeline for customer reviews. We were getting raw text from the LLM, and my life became a nightmare of writing increasingly complex regular expressions to parse out the sentiment score, the key topics, and the customer's name. Every time the model updated, its phrasing would change slightly, and my regex would break. The system was so britt...\n",
      "</output>\n",
      "<article_scores>\n",
      "\t\n",
      "<section_scores>\n",
      "    <section_title>Introduction</section_title>\n",
      "    <guideline_adherence>\n",
      "        <score>0</score>\n",
      "        <reason>The section correctly introduces the \"bridge\" concept but violates three guidelines: it uses the first-person singular (\"I remember...\") against the point-of-view rule, adds a new anecdote about sentiment analysis and at ~250 words, it exceeds the 150-word length constraint.</reason>\n",
      "    </guideline_adherence>\n",
      "    <research_anchoring>\n",
      "        <score>0</score>\n",
      "        <reason>While the core \"bridge\" concept between the LLM (software 3.0) and Python (software 1.0) worlds' is anchored in the guideline, the section introduces a significant, un-anchored claim that mastering structured outputs is \"the key to unlocking true Artificial General Intelligence (AGI),\" which is not supported by the research.</reason>\n",
      "    </research_anchoring>\n",
      "</section_scores>\n",
      "\n",
      "\t\n",
      "<section_scores>\n",
      "    <section_title>Understanding why structured outputs are critical</section_title>\n",
      "    <guideline_adherence>\n",
      "        <score>0</score>\n",
      "        <reason>The section correctly covers the required benefits but violates the \"Different Order\" rule by presenting them in a different sequence than the guideline. It also adds an unrequested benefit (\"Improved Token Efficiency\"), violating the \"More\" rule.</reason>\n",
      "    </guideline_adherence>\n",
      "    <research_anchoring>\n",
      "        <score>1</score>\n",
      "        <reason>The section is well-anchored. The benefits of parsing, Pydantic validation, avoiding fragile parsing and the GraphRAG/knowledge-graph use-case are all supported in the research and the added point on token efficiency are all directly supported by claims in the provided research material</reason>\n",
      "    </research_anchoring>\n",
      "</section_scores>\n",
      "\n",
      "\t\n",
      "<section_scores>\n",
      "    <section_title>Implementing structured outputs from scratch using JSON</section_title>\n",
      "    <guideline_adherence>\n",
      "        <score>1</score>\n",
      "        <reason>This section adheres to the guideline. It follows the step list exactly: client/model setup, sample DOCUMENT, XML-tagged prompt, model call, raw output, helper to extract JSON, parsed output, and display. It meets the length and content expectations.</reason>\n",
      "    </guideline_adherence>\n",
      "    <research_anchoring>\n",
      "        <score>1</score>\n",
      "        <reason>The section is correctly anchored. All code examples are taken directly from the required lesson notebook, and the explanation of using XML tags for clarity is supported by prompt engineering best practices found in the research</reason>\n",
      "    </research_anchoring>\n",
      "</section_scores>\n",
      "\n",
      "\t\n",
      "<section_scores>\n",
      "    <section_title>Implementing structured outputs from scratch using Pydantic</section_title>\n",
      "    <guideline_adherence>\n",
      "        <score>0</score>\n",
      "        <reason>The section correctly explains Pydantic's general benefits. However, it critically fails to follow the guideline by omitting the mandatory step of generating the JSON schema from the Pydantic model and injecting it into the prompt. It also adds an unrequested comment on YAML.</reason>\n",
      "    </guideline_adherence>\n",
      "    <research_anchoring>\n",
      "        <score>0</score>\n",
      "        <reason>The section introduces a factual contradiction with the provided guideline. It incorrectly states that type hints can be used directly \"Starting with Python 10,\" whereas the guideline explicitly specifies this feature is available from \"Python 11,\" failing the anchoring criterion.</reason>\n",
      "    </research_anchoring>\n",
      "</section_scores>\n",
      "\n",
      "\t\n",
      "<section_scores>\n",
      "    <section_title>Implementing structured outputs using Gemini and Pydantic</section_title>\n",
      "    <guideline_adherence>\n",
      "        <score>0</score>\n",
      "        <reason>The section provides the correct code for using Gemini's native features. However, it fails the guideline by completely omitting the required explanation of the pros of this approach (that it is \"easier, more accurate and cheaper\").</reason>\n",
      "    </guideline_adherence>\n",
      "    <research_anchoring>\n",
      "        <score>0</score>\n",
      "        <reason>The section fails to incorporate key research findings. The provided sources clearly state that native API features enhance reliability and reduce development costs (Sources [37], [58]), but none of this required evidence is mentioned.</reason>\n",
      "    </research_anchoring>\n",
      "</section_scores>\n",
      "\n",
      "\t\n",
      "<section_scores>\n",
      "    <section_title>Conclusion: Structured Outputs Are Everywhere</section_title>\n",
      "    <guideline_adherence>\n",
      "        <score>0</score>\n",
      "        <reason>The section correctly emphasizes the ubiquity of structured outputs and name-checks later topics as the guideline asks for. However, it does not provide a transition to the requested next Lesson 5.</reason>\n",
      "    </guideline_adherence>\n",
      "    <research_anchoring>\n",
      "        <score>1</score>\n",
      "        <reason>The section's claims are general and consistent with the course context. It does not introduce new factual claims that require specific anchoring, and therefore contains no information that contradicts the provided research material.</reason>\n",
      "    </research_anchoring>\n",
      "</section_scores>\n",
      "\n",
      "\n",
      "</article_scores>\n",
      "\n",
      "</example_1>\n",
      "\n",
      "</few-shot-examples>\n",
      "\n",
      "## INPUTS\n",
      "\n",
      "<input>\n",
      "Dummy input guideline\n",
      "</input>\n",
      "\n",
      "<context>\n",
      "Dummy research\n",
      "</context>\n",
      "\n",
      "<output>\n",
      "Dummy output article\n",
      "</output>\n",
      "\n",
      "Think through your answer step by step, and provide the requested evaluation.\n",
      "\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from brown.evals.metrics.user_intent.prompts import DEFAULT_FEW_SHOT_EXAMPLES, get_eval_prompt\n",
    "\n",
    "# Crop the few-shot examples to 1 example and cropped dummy output and expected output to\n",
    "# easily see the prompt structure\n",
    "cropped_few_shot_examples = DEFAULT_FEW_SHOT_EXAMPLES.model_copy(deep=True)\n",
    "cropped_few_shot_examples.examples = cropped_few_shot_examples.examples[:1]\n",
    "cropped_few_shot_examples.examples[0].output = f\"{cropped_few_shot_examples.examples[0].output[:1000]}...\"\n",
    "cropped_few_shot_examples.examples[0].input = f\"{cropped_few_shot_examples.examples[0].input[:1000]}...\"\n",
    "cropped_few_shot_examples.examples[0].context = f\"{cropped_few_shot_examples.examples[0].context[:1000]}...\"\n",
    "\n",
    "pretty_print.wrapped(\n",
    "    get_eval_prompt(\n",
    "        output=\"Dummy output article\",\n",
    "        input=\"Dummy input guideline\",\n",
    "        context=\"Dummy research\",\n",
    "        few_shot_examples=cropped_few_shot_examples,\n",
    "    ),\n",
    "    title=\"UserIntent LLM Judge System Prompt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this is how the few-shot examples data structure looks like when we map it to context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m----------------------------- User Intent LLM Judge Few-Shot Examples -----------------------------\u001b[0m\n",
      "  <example_1>\n",
      "\t\n",
      "<input>\n",
      "## Global Context of the Lesson\n",
      "\n",
      "### What We Are Planning to Share\n",
      "\n",
      "In this lesson, we will talk about structured outputs. We'll introduce **Structured Outputs** as a way to achieve reliable, controlled, and formatted data extraction, ensuring type and data quality checks. Common structured output types are JSON, XML, and YAML, which can further be translated into Python dictionaries, lists, classes, or most commonly Pydantic models. This approach makes manipulating LLM outputs much easier as it serves as the bridge between the LLM (software 3.0) and Python (software 1.0) worlds. First, we will implement structured outputs from scratch, then show how to do it directly with a popular API such as Gemini. Ultimately, we will highlight how structured outputs are used everywhere when building LLM workflows or AI agents.\n",
      "\n",
      "### Why We Think It's Valuable\n",
      "\n",
      "For an AI Engineer, it's critical to understand how to implement the bridge between the LLM (software 3.0) and Python (software 1.0) worlds,...\n",
      "</input>\n",
      "<context>\n",
      "# Research\n",
      "\n",
      "## Research Results\n",
      "\n",
      "<details>\n",
      "<summary>What are the pros and cons of forcing JSON output via prompt engineering versus using a native model API like Gemini's or OpenAI's structured output features?</summary>\n",
      "\n",
      "### Source [1]: https://www.youtube.com/watch?v=npQx-11SwqU\n",
      "\n",
      "Query: What are the pros and cons of forcing JSON output via prompt engineering versus using a native model API like Gemini's or OpenAI's structured output features?\n",
      "\n",
      "Answer: The video tutorial emphasizes the utility of structured output in APIs powered by GCP Gemini AI. Structured outputâ€”like JSONâ€”enables efficient, precise data handling, easier parsing, and compatibility with other systems. Using Gemini AI's native structured output, developers can ensure that API responses are consistent and valid for downstream applications, reducing the need for error-prone post-processing. Implementing structured output via the API, rather than relying on prompt engineering, supports scalability and robust data validat...\n",
      "</context>\n",
      "<output>\n",
      "# LLM Structured Outputs: The Definitive Guide\n",
      "### From brittle strings to validated Pydantic objects\n",
      "\n",
      "## Introduction\n",
      "\n",
      "In our previous lessons, we laid the groundwork for AI Engineering. We explored the AI agent landscape, distinguished between rigid workflows and autonomous agents, and covered the essentials of Context Engineering. Now, we've reached a critical junction: how do we make the fluid, often unpredictable outputs of a Large Language Model (LLM) work with the structured, type-safe world of our Python applications? The answer is structured outputs.\n",
      "\n",
      "I remember a project I worked on early in my career, trying to build a sentiment analysis pipeline for customer reviews. We were getting raw text from the LLM, and my life became a nightmare of writing increasingly complex regular expressions to parse out the sentiment score, the key topics, and the customer's name. Every time the model updated, its phrasing would change slightly, and my regex would break. The system was so britt...\n",
      "</output>\n",
      "<article_scores>\n",
      "\t\n",
      "<section_scores>\n",
      "    <section_title>Introduction</section_title>\n",
      "    <guideline_adherence>\n",
      "        <score>0</score>\n",
      "        <reason>The section correctly introduces the \"bridge\" concept but violates three guidelines: it uses the first-person singular (\"I remember...\") against the point-of-view rule, adds a new anecdote about sentiment analysis and at ~250 words, it exceeds the 150-word length constraint.</reason>\n",
      "    </guideline_adherence>\n",
      "    <research_anchoring>\n",
      "        <score>0</score>\n",
      "        <reason>While the core \"bridge\" concept between the LLM (software 3.0) and Python (software 1.0) worlds' is anchored in the guideline, the section introduces a significant, un-anchored claim that mastering structured outputs is \"the key to unlocking true Artificial General Intelligence (AGI),\" which is not supported by the research.</reason>\n",
      "    </research_anchoring>\n",
      "</section_scores>\n",
      "\n",
      "\t\n",
      "<section_scores>\n",
      "    <section_title>Understanding why structured outputs are critical</section_title>\n",
      "    <guideline_adherence>\n",
      "        <score>0</score>\n",
      "        <reason>The section correctly covers the required benefits but violates the \"Different Order\" rule by presenting them in a different sequence than the guideline. It also adds an unrequested benefit (\"Improved Token Efficiency\"), violating the \"More\" rule.</reason>\n",
      "    </guideline_adherence>\n",
      "    <research_anchoring>\n",
      "        <score>1</score>\n",
      "        <reason>The section is well-anchored. The benefits of parsing, Pydantic validation, avoiding fragile parsing and the GraphRAG/knowledge-graph use-case are all supported in the research and the added point on token efficiency are all directly supported by claims in the provided research material</reason>\n",
      "    </research_anchoring>\n",
      "</section_scores>\n",
      "\n",
      "\t\n",
      "<section_scores>\n",
      "    <section_title>Implementing structured outputs from scratch using JSON</section_title>\n",
      "    <guideline_adherence>\n",
      "        <score>1</score>\n",
      "        <reason>This section adheres to the guideline. It follows the step list exactly: client/model setup, sample DOCUMENT, XML-tagged prompt, model call, raw output, helper to extract JSON, parsed output, and display. It meets the length and content expectations.</reason>\n",
      "    </guideline_adherence>\n",
      "    <research_anchoring>\n",
      "        <score>1</score>\n",
      "        <reason>The section is correctly anchored. All code examples are taken directly from the required lesson notebook, and the explanation of using XML tags for clarity is supported by prompt engineering best practices found in the research</reason>\n",
      "    </research_anchoring>\n",
      "</section_scores>\n",
      "\n",
      "\t\n",
      "<section_scores>\n",
      "    <section_title>Implementing structured outputs from scratch using Pydantic</section_title>\n",
      "    <guideline_adherence>\n",
      "        <score>0</score>\n",
      "        <reason>The section correctly explains Pydantic's general benefits. However, it critically fails to follow the guideline by omitting the mandatory step of generating the JSON schema from the Pydantic model and injecting it into the prompt. It also adds an unrequested comment on YAML.</reason>\n",
      "    </guideline_adherence>\n",
      "    <research_anchoring>\n",
      "        <score>0</score>\n",
      "        <reason>The section introduces a factual contradiction with the provided guideline. It incorrectly states that type hints can be used directly \"Starting with Python 10,\" whereas the guideline explicitly specifies this feature is available from \"Python 11,\" failing the anchoring criterion.</reason>\n",
      "    </research_anchoring>\n",
      "</section_scores>\n",
      "\n",
      "\t\n",
      "<section_scores>\n",
      "    <section_title>Implementing structured outputs using Gemini and Pydantic</section_title>\n",
      "    <guideline_adherence>\n",
      "        <score>0</score>\n",
      "        <reason>The section provides the correct code for using Gemini's native features. However, it fails the guideline by completely omitting the required explanation of the pros of this approach (that it is \"easier, more accurate and cheaper\").</reason>\n",
      "    </guideline_adherence>\n",
      "    <research_anchoring>\n",
      "        <score>0</score>\n",
      "        <reason>The section fails to incorporate key research findings. The provided sources clearly state that native API features enhance reliability and reduce development costs (Sources [37], [58]), but none of this required evidence is mentioned.</reason>\n",
      "    </research_anchoring>\n",
      "</section_scores>\n",
      "\n",
      "\t\n",
      "<section_scores>\n",
      "    <section_title>Conclusion: Structured Outputs Are Everywhere</section_title>\n",
      "    <guideline_adherence>\n",
      "        <score>0</score>\n",
      "        <reason>The section correctly emphasizes the ubiquity of structured outputs and name-checks later topics as the guideline asks for. However, it does not provide a transition to the requested next Lesson 5.</reason>\n",
      "    </guideline_adherence>\n",
      "    <research_anchoring>\n",
      "        <score>1</score>\n",
      "        <reason>The section's claims are general and consistent with the course context. It does not introduce new factual claims that require specific anchoring, and therefore contains no information that contradicts the provided research material.</reason>\n",
      "    </research_anchoring>\n",
      "</section_scores>\n",
      "\n",
      "\n",
      "</article_scores>\n",
      "\n",
      "</example_1>\n",
      "\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "pretty_print.wrapped(cropped_few_shot_examples.to_context(), title=\"User Intent LLM Judge Few-Shot Examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Running the UserIntent Metric\n",
    "\n",
    "Let's run the `UserIntentMetricLLMJudge` on a sample. We'll use the same sample tests as for the follows GT metric.\n",
    "\n",
    "Here is the output article as a refresher:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m------------------------------------ Generated Article Preview ------------------------------------\u001b[0m\n",
      "  # Workflows vs. Agents: The Critical Decision Every AI Engineer Faces\n",
      "### How to choose between predictable control and autonomous flexibility when building AI applications.\n",
      "\n",
      "By the end of this lesson, you will understand the fundamental differences between LLM workflows and AI agents, know when to use each, and recognize how to combine their strengths in hybrid approaches.\n",
      "\n",
      "When building AI applications, engineers face a critical architectural decision early on. Should you create a predictable, step-by-step workflow where you control every action, or build an autonomous agent that can think and decide for itself? This choice impacts everything from development time and cost to reliability and user experience. It is a fundamental decision that often determines if an AI application will be successful in production.\n",
      "\n",
      "## Understanding the Spectrum: From Workflows to Agents\n",
      "\n",
      "To make the right choice, you first need to understand what LLM workflows and AI agents are. We will look at their core properties and how they are used, rather than their technical specifics.\n",
      "\n",
      "### LLM Workflows\n",
      "\n",
      "An LLM workflow is a sequence of tasks orchestrated by developer-written code.\n",
      "\n",
      "```mermaid\n",
      "graph TD\n",
      "    A[\"Start\"] --> B[\"LLM Call\"]\n",
      "    B --> C[\"Process Data\"]\n",
      "    C --> D[\"Store Data\"]\n",
      "    D --> E[\"End\"]\n",
      "```\n",
      "Image 1: A flowchart illustrating a deterministic LLM workflow with clear start and end points, including an LLM call and data operations.\n",
      "\n",
      "### AI Agents\n",
      "\n",
      "AI agents are systems where an LLM dynamically decides the sequence of steps, reasoning, and actions to achieve a goal. The path is not predefined. Instead, the agent uses a reasoning process to plan its actions based on the task and the current state of its environment. This process is often modeled on frameworks like ReAct (Reason, Act, Observe). This allows agents to be adaptive and capable of handling new or unexpected situations through LLM-driven autonomy. They can select tools, execute actions, evaluate the outcomes, and correct their course until the goal is achieved [[1]](https://www.youtube.com/watch?v=kQxr-uOxw2o&t=1s).\n",
      "\n",
      "```mermaid\n",
      "graph TD\n",
      "    A[\"Start\"] --> B[\"Agent (LLM) Receives Goal\"]\n",
      "    B --> C[\"Plan/Reason (LLM)\"]\n",
      "    C --> D[\"Select Tool\"]\n",
      "    D --> E[\"Execute Action (Tool Call)\"]\n",
      "    E --> F[\"Observe Environment/Feedback\"]\n",
      "    F --> G{\"Evaluate Outcome\"}\n",
      "    G -->|\"Satisfactory\"| H[\"Stop/Achieve Goal\"]\n",
      "    G -->|\"Needs Adjustment\"| C\n",
      "```\n",
      "Image 2: Flowchart illustrating an AI agent's dynamic decision-making process driven by an LLM.\n",
      "\n",
      "## Choosing Your Path\n",
      "\n",
      "The core difference between these two approaches lies in a single trade-off: use RAG or not.\n",
      "\n",
      "https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5e64d5e0-7ef1-4e7f-b441-3bf1fef4ff9a_1276x818.png \n",
      "Image 3: The trade-off between an agent's level of control and application reliability. (Image by Iusztin, P. from [Exploring the difference between agents and workflows [2]](https://decodingml.substack.com/p/llmops-for-production-agentic-rag))\n",
      "\n",
      "- **When to use LLM workflows:** Workflows are ideal for repeatable tasks with defined steps, like data extraction, report generation, or content repurposing. Their strength is predictability, ensuring reliable results, easier debugging, and lower costs by using specialized models. The main weakness is rigidity; they cannot handle unexpected scenarios, and adding features can become complex.\n",
      "\n",
      "- **When to use AI agents:** Agents excel at dynamic problem-solving like open-ended research or complex customer support [[3]](https://towardsdatascience.com/a-developers-guide-to-building-scalable-ai-workflows-vs-agents/). Their strength is flexibility in handling ambiguity. However, this autonomy makes them less reliable, harder to debug, and costlier due to non-deterministic behavior. Without proper guardrails, they also pose security risks, especially with operations that can modify or delete data.\n",
      "\n",
      "- **Hybrid Approaches:** Most real-world systems are not purely one or the other. They often blend elements of both, creating a hybrid system. A common pattern is to use a workflow for predictable parts of a task and delegate ambiguous steps to an agent. For example, a system might use a human-in-the-loop workflow, where the agent proposes an action, and a human verifies it before execution.\n",
      "\n",
      "```mermaid\n",
      "graph TD\n",
      "    A[\"Human Input\"] --> B[\"LLM Call (AI Generation)\"]\n",
      "    B --> C[\"Action in Environment\"]\n",
      "    C --> D[\"Feedback from Environment\"]\n",
      "    D --> E{\"Human Review/Verification\"}\n",
      "    E -->|\"Approved\"| G[\"Stop/Final Output\"]\n",
      "    E -->|\"Rejected\"| F[\"Continue/Refine\"]\n",
      "    F --> A\n",
      "```\n",
      "Image 4: A flowchart illustrating an AI generation and human verification loop with iterative refinement.\n",
      "\n",
      "## The Challenges of Every AI Engineer\n",
      "\n",
      "Understanding the spectrum from workflows to agents is a core part of AI engineering. This choice helps determine if your application will succeed in production. Building robust AI systems means navigating recurring challenges daily. These include building pipelines to pull information from Slack, web APIs, SQL databases, and data lakes; managing the cost-performance trap where sophisticated agents become too expensive per interaction; and mitigating security risks from autonomous agents that could send wrong emails or delete critical files.\n",
      "\n",
      "## References\n",
      "\n",
      "1. Bouchard, L-F. (n.d.). Real agents vs. workflows: The truth behind AI 'agents'. YouTube. https://www.youtube.com/watch?v=kQxr-uOxw2o&t=1s\n",
      "2. Iusztin, P. (n.d.). Exploring the difference between agents and workflows. Decoding AI Magazine. https://decodingml.substack.com/p/llmops-for-production-agentic-rag\n",
      "3. (n.d.). A developerâ€™s guide to building scalable AI: Workflows vs agents. Towards Data Science. https://towardsdatascience.com/a-developers-guide-to-building-scalable-ai-workflows-vs-agents/\n",
      "4. Google. (n.d.). Gemini CLI. GitHub. https://github.com/google-gemini/gemini-cli/blob/main/README.md\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "pretty_print.wrapped(OUTPUT_ARTICLE, title=\"Generated Article Preview\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The article guideline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m---------------------------------------- Article Guideline ----------------------------------------\u001b[0m\n",
      "  ## Outline\n",
      "\n",
      "1. Introduction: The Critical Decision Every AI Engineer Faces\n",
      "2. Understanding the Spectrum: From Workflows to Agents\n",
      "3. Choosing Your Path\n",
      "4. Conclusion: The Challenges of Every AI Engineer\n",
      "\n",
      "## Section 1 - Introduction: The Critical Decision Every AI Engineer Faces\n",
      "\n",
      "- **The Problem:** When building AI applications, engineers face a critical architectural decision early in their development process. Should they create a predictable, step-by-step workflow where they control every action, or should they build an autonomous agent that can think and decide for itself? This is one of the key decisions that will impact everything from the product such as development time and costs to reliability and user experience.\n",
      "- Quick walkthrough of what we'll learn by the end of this lesson\n",
      "\n",
      "- **Section length:** 100 words\n",
      "\n",
      "## Section 2 - Understanding the Spectrum: From Workflows to Agents\n",
      "\n",
      "- In this section we want to take a brief look at what LLM workflows and AI agents are. At this point we don't focus on the technical specifics of each, but rather on their properties and how they are used.\n",
      "- On **LLM workflows** we care about:\n",
      "\t- Definition: A sequence of tasks involving LLM calls or other operations such as reading/writing data to a database or file system. It is largely predefined and orchestrated by developer-written code.\n",
      "\t- Characteristics: The steps are defined in advance, resulting in deterministic or rule-based paths with predictable execution and explicit control flow. \n",
      "\t- Add a Mermaid diagram \n",
      "- On **AI agents** we care about:\n",
      "\t- Definition: Systems where an LLM (or multiple LLMs) plays a central role in dynamically deciding (planning) the sequence of steps, reasoning, and actions to achieve a goal. The steps are not defined in advance, but are dynamically planned based on the task and current state of the environment.\n",
      "\t- Characteristics: Adaptive, capable of handling novelty, LLM-driven autonomy in decision-making and execution path.\n",
      "\t- Add a Mermaid diagram\n",
      "- **Section length:** 200 words (without the mermaid diagram code)\n",
      "\n",
      "## Section 3: Choosing Your Path\n",
      "\n",
      "- In the previous section we defined the LLM workflows and AI agents independently, now we want to explore their core differences: Developer-defined logic vs LLM-driven autonomy in reasoning and action selection.\n",
      "- Attach an image from the research showing the gradient between LLM workflows and AI agents.\n",
      "- **When to use LLM workflows:**\n",
      "\t- Examples where the structure is well-defined:\n",
      "\t\t- Pipelines for data extraction and transformation from sources such as the web, messaging tools like Slack, video calls from Zoom, project management tools like Notion, and cloud storage tools like Google Drive\n",
      "\t\t- Automated report or emails generation from multiple data sources\n",
      "\t\t- Repetitive daily tasks: Sending emails, posting social media updates, responding to messages\n",
      "\t\t- Content generation or repurposing, such as transforming articles into social media posts \n",
      "\t- Strengths: Predictability, reliability for well-defined tasks, easier debugging of fixed paths, potentially lower operational costs as we can leverage simpler and smaller models specialized in given sub-tasks. \n",
      "\t- Weaknesses: Potentially more development time required as each step is manually engineered. The user experience is rigid as it cannot handle unexpected scenarios. Adding new features can get complex when the application grows, similar to developing standard software tools.\n",
      "- **When to use AI agents:**\n",
      "\t- Examples: \n",
      "\t\t- Open-ended research and synthesis (e.g., researching about WW2)\n",
      "\t\t- Dynamic problem-solving (e.g., debugging code, complex customer support)\n",
      "\t\t- Interactive task completion in unfamiliar environments (e.g., booking a flight, where we don't specify the exact sites to use)\n",
      "\t- Strengths: Adaptability to new situations and the flexibility to handle ambiguity and complexity as the steps are dynamically decided.\n",
      "\t- Weaknesses: The system is more prone to errors. As the agent is non-deterministic, the performance, latency and costs can vary with each call of the agent, making agents often unreliable. As agents require LLMs that can generalize better, which are bigger, hence more costly, adopting an agentic solution usually ends up being more costly. AI agents usually require more LLM calls to understand the user intent and take various actions, which can result again in bigger costs per call. If not designed well, there can be huge security concerns, especially on write operations, where it can delete all our data or send inappropriate emails. Ultimately, a huge disadvantage of AI agents is that they are hard to debug and evaluate.\n",
      "\t\n",
      "- **Hybrid Approaches:** Most real-world systems blend elements of both approaches. Thus, in reality, we have a spectrum, a gradient between LLM workflows and AI agents, where a system adopts what's best from both worlds depending on its use cases.\n",
      "- Generate a mermaid to illustrate the AI generation and human verification loop\n",
      "\n",
      "- **Section length:** 200 words\n",
      "\n",
      "## Section 4 - Conclusion: The Challenges of Every AI Engineer\n",
      "\n",
      "- **The Reality of AI Engineering:** Now that you understand the spectrum from LLM workflows to AI agents, it's important to recognize that these are one of the core decisions that determine whether your AI application succeeds in production or fails spectacularly.\n",
      "- To set the scene for future lessons and patterns we will learn, present some daily challenges every AI engineer battles:\n",
      "\t- **Data Integration:** Building pipelines to pull information from Slack, web APIs, SQL databases, and data lakes while ensuring only high-quality data is passed to your AI system (garbage-in, garbage-out principle).\n",
      "\t- **Cost-Performance Trap:** Sophisticated agents deliver impressive results but cost a fortune per user interaction, making them economically unfeasible for many applications.\n",
      "\t- **Security Concerns:** Autonomous agents with powerful write permissions could send wrong emails, delete critical files, or expose sensitive data.\n",
      "\n",
      "- To transition from this lesson to the next, specify that in the future lesson (lesson 3) we will dig more into the foundations of AI agents and workflows, which is context engineering\n",
      "\n",
      "- **Section length:** 100 words\n",
      "\n",
      "## Golden Sources\n",
      "\n",
      "- [Real Agents vs. Workflows: The Truth Behind AI 'Agents'](https://www.youtube.com/watch?v=kQxr-uOxw2o&t=1s)\n",
      "- [Exploring the difference between agents and workflows](https://decodingml.substack.com/p/llmops-for-production-agentic-rag)\n",
      "- [A Developerâ€™s Guide to Building Scalable AI: Workflows vs Agents](https://towardsdatascience.com/a-developers-guide-to-building-scalable-ai-workflows-vs-agents/)\n",
      "- [Gemini CLI README.md](https://github.com/google-gemini/gemini-cli/blob/main/README.md)\n",
      "\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "ARTICLE_GUIDELINE = (SAMPLE_DIR / \"article_guideline.md\").read_text()\n",
    "\n",
    "pretty_print.wrapped(ARTICLE_GUIDELINE, title=\"Article Guideline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the research:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m--------------------------------------------- Research ---------------------------------------------\u001b[0m\n",
      "  # Research\n",
      "\n",
      "## Code Sources\n",
      "\n",
      "<details>\n",
      "<summary>Repository analysis for https://github.com/google-gemini/gemini-cli/blob/main/README.md</summary>\n",
      "\n",
      "# Repository analysis for https://github.com/google-gemini/gemini-cli/blob/main/README.md\n",
      "\n",
      "## Summary\n",
      "Repository: google-gemini/gemini-cli\n",
      "File: README.md\n",
      "Lines: 211\n",
      "\n",
      "Estimated tokens: 1.6k\n",
      "\n",
      "## File tree\n",
      "```Directory structure:\n",
      "â””â”€â”€ README.md\n",
      "\n",
      "```\n",
      "\n",
      "## Extracted content\n",
      "================================================\n",
      "FILE: README.md\n",
      "================================================\n",
      "# Gemini CLI\n",
      "\n",
      "[![Gemini CLI CI](https://github.com/google-gemini/gemini-cli/actions/workflows/ci.yml/badge.svg)](https://github.com/google-gemini/gemini-cli/actions/workflows/ci.yml)\n",
      "\n",
      "![Gemini CLI Screenshot](./docs/assets/gemini-screenshot.png)\n",
      "\n",
      "This repository contains the Gemini CLI, a command-line AI workflow tool that connects to your\n",
      "tools, understands your code and accelerates your workflows.\n",
      "\n",
      "With the Gemini CLI you can:\n",
      "\n",
      "- Query and edit large codebases in ...\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "RESEARCH = (SAMPLE_DIR / \"research.md\").read_text()\n",
    "\n",
    "pretty_print.wrapped(f\"{RESEARCH[:1000]}...\", title=\"Research\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's call the LLM judge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running UserIntent metric...\n",
      "This may take a minute...\n",
      "\n",
      "âœ“ Generated 2 score results\n"
     ]
    }
   ],
   "source": [
    "from brown.evals.metrics import UserIntentMetricLLMJudge\n",
    "\n",
    "user_intent_metric = UserIntentMetricLLMJudge(\n",
    "    model=SupportedModels.GOOGLE_GEMINI_25_FLASH,\n",
    "    track=False,  # Disable tracking for this demo\n",
    ")\n",
    "\n",
    "print(\"Running UserIntent metric...\")\n",
    "print(\"This may take a minute...\")\n",
    "\n",
    "results = await user_intent_metric.ascore(\n",
    "    input=ARTICLE_GUIDELINE,\n",
    "    context={\"research\": RESEARCH},\n",
    "    output=OUTPUT_ARTICLE,\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ“ Generated {len(results)} score results\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m--------------------------------- user_intent_guideline_adherence ---------------------------------\u001b[0m\n",
      "  user_intent_guideline_adherence - Score: 0.50\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "*** REASON ***\n",
      "\n",
      "Introduction: The Critical Decision Every AI Engineer Faces:\n",
      "**1:** The section correctly introduces the problem of architectural decisions in AI, and provides a quick walkthrough of what will be learned. The length is approximately 90 words, which is within the 100-word limit (allowing for a 10% tolerance).\n",
      "\n",
      "Understanding the Spectrum: From Workflows to Agents:\n",
      "**1:** The section correctly defines LLM workflows and AI agents, focusing on their properties and usage rather than technical specifics. It includes both required Mermaid diagrams and adheres to the approximate 200-word length constraint (excluding diagrams).\n",
      "\n",
      "Choosing Your Path:\n",
      "**0:** The section fails to adhere to the guideline's instruction to explore the core difference as 'Developer-defined logic vs LLM-driven autonomy in reasoning and action selection.' Instead, it incorrectly states the core difference is 'use RAG or not.' While it includes the required image and discusses hybrid approaches, this fundamental misreprese...\n",
      "\n",
      "\n",
      "\u001b[93m---------------------------------- user_intent_research_anchoring ----------------------------------\u001b[0m\n",
      "  user_intent_research_anchoring - Score: 0.75\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "*** REASON ***\n",
      "\n",
      "Introduction: The Critical Decision Every AI Engineer Faces:\n",
      "**1:** The content of this introductory section is high-level and aligns with the overall framing of the lesson as presented in the guideline. It does not introduce any specific factual claims that require external research anchoring, and therefore contains no information that contradicts the provided research material.\n",
      "\n",
      "Understanding the Spectrum: From Workflows to Agents:\n",
      "**1:** The definitions and characteristics of LLM workflows and AI agents are well-anchored in the provided research. The concept of workflows as 'predefined code paths' and agents as 'dynamically decides' is consistently supported by sources [1], [2], and [3] in the output's references, which are derived from the research material.\n",
      "\n",
      "Choosing Your Path:\n",
      "**0:** The section introduces a factual inaccuracy by stating the core difference between workflows and agents is 'use RAG or not.' This contradicts the research, which consistently defines the core differe...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for result in results:\n",
    "    pretty_print.wrapped(f\"{result.name} - Score: {result.value:.2f}\", title=result.name)\n",
    "    print(\"*** REASON ***\\n\")\n",
    "    print(f\"{result.reason[:1000]}...\" if len(result.reason) > 1000 else result.reason)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. The Metrics Factory Function\n",
    "\n",
    "To simplify metric creation, we provide a factory function:\n",
    "\n",
    "Source: `brown.evals.metrics.__init__`\n",
    "\n",
    "```python\n",
    "def build_evaluation_metrics(\n",
    "    metrics: list[str], model: SupportedModels, model_config: ModelConfig | None = None\n",
    ") -> list[base_metric.BaseMetric]:\n",
    "    \"\"\"Get evaluation metrics based on the provided metric names.\n",
    "\n",
    "    Args:\n",
    "        metrics: List of metric names to use. Valid values are:\n",
    "            - \"user_intent\": Evaluates if article follows guidelines and is anchored in research\n",
    "            - \"follows_gt\": Evaluates article structure and content\n",
    "        model: The language model to use for evaluation.\n",
    "        model_config: Configuration for the model.\n",
    "\n",
    "    Returns:\n",
    "        List of metric instances\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If an unknown metric name is provided\n",
    "    \"\"\"\n",
    "    metrics_mapping = {\n",
    "        \"user_intent\": UserIntentMetricLLMJudge(model=model, model_config=model_config),\n",
    "        \"follows_gt\": FollowsGTMetricLLMJudge(model=model, model_config=model_config),\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        return [metrics_mapping[metric] for metric in metrics]\n",
    "    except KeyError as e:\n",
    "        raise ValueError(f\"Unknown metric name: {e}\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, let's build both metrics using the factory function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created metrics:\n",
      "  - user_intent: UserIntentMetricLLMJudge\n",
      "  - follows_gt: FollowsGTMetricLLMJudge\n"
     ]
    }
   ],
   "source": [
    "from brown.evals.metrics import build_evaluation_metrics\n",
    "\n",
    "all_metrics = build_evaluation_metrics(\n",
    "    metrics=[\"user_intent\", \"follows_gt\"],\n",
    "    model=SupportedModels.GOOGLE_GEMINI_25_FLASH,\n",
    ")\n",
    "\n",
    "print(\"Created metrics:\")\n",
    "for metric in all_metrics:\n",
    "    print(f\"  - {metric.name}: {metric.__class__.__name__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Running Evaluations on the Dataset\n",
    "\n",
    "As we have to run the LLM judges on the whole AI evals dataset, we need a way to automate running Brown on multiple dataset samples on our business metrics.\n",
    "\n",
    "We need several components working together.\n",
    "\n",
    "### 9.1 The Evaluation Task\n",
    "\n",
    "The `evaluation_task` function generates an article for a single dataset sample:\n",
    "\n",
    "Source: `brown.evals.tasks`\n",
    "\n",
    "```python\n",
    "@a.as_sync\n",
    "async def evaluation_task(\n",
    "    sample: EvalSampleDict,\n",
    "    cache_dir: Path,\n",
    "    read_from_cache: bool = False,\n",
    "    clean_cache: bool = False,\n",
    "    debug: bool = False,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Generate an article using the Brown agent for a single evaluation sample.\"\"\"\n",
    "    logger.info(f\"Processing evaluation sample: {sample['name']}\")\n",
    "\n",
    "    cache_dir = cache_dir / sample[\"directory\"]\n",
    "    cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    try:\n",
    "        article_guideline_path = cache_dir / \"article_guideline.md\"\n",
    "        research_path = cache_dir / \"research.md\"\n",
    "\n",
    "        article_guideline_path.write_text(sample[\"article_guideline\"], encoding=\"utf-8\")\n",
    "        research_path.write_text(sample[\"research\"], encoding=\"utf-8\")\n",
    "\n",
    "        thread_id = str(uuid.uuid4())\n",
    "        inputs = {\"dir_path\": cache_dir}\n",
    "        tracer = tracing.build_handler(thread_id, tags=[\"generate-evaluation\"])\n",
    "        config = RunnableConfig(\n",
    "            configurable={\"thread_id\": thread_id},\n",
    "            callbacks=[tracer],\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            generated_article = await __run(config, inputs, read_from_cache)\n",
    "        except Exception:\n",
    "            generated_article = \"ERROR: Failed to generate article\"\n",
    "            logger.exception(f\"Failed to generate article for sample `{sample['name']}`\")\n",
    "\n",
    "        return {\n",
    "            \"input\": sample[\"article_guideline\"],\n",
    "            \"context\": {\"research\": sample[\"research\"], \"debug\": debug},\n",
    "            \"output\": generated_article,\n",
    "            \"expected_output\": sample[\"ground_truth_article\"],\n",
    "            \"reference\": sample[\"ground_truth_article\"],\n",
    "            \"name\": sample[\"name\"],\n",
    "        }\n",
    "    finally:\n",
    "        if clean_cache and cache_dir.exists():\n",
    "            shutil.rmtree(cache_dir)\n",
    "```\n",
    "\n",
    "**Key Points:**\n",
    "- Takes an `EvalSampleDict` from the dataset\n",
    "- Writes inputs to a cache directory\n",
    "- Runs the Brown workflow to generate an article\n",
    "- Returns a dictionary with all the data needed for scoring\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 The __run Function\n",
    "\n",
    "The `__run` function handles the actual workflow execution:\n",
    "\n",
    "Source: `brown.evals.tasks`\n",
    "\n",
    "```python\n",
    "async def __run(config: RunnableConfig, inputs: Dict[str, Any], read_from_cache: bool = False) -> str:\n",
    "    \"\"\"Run the article generation workflow and extract the final article.\"\"\"\n",
    "\n",
    "    app_config = get_app_config()\n",
    "\n",
    "    article_path = inputs[\"dir_path\"] / \"article.md\"\n",
    "    if read_from_cache:\n",
    "        assert article_path.exists(), f\"Article file not found in cache at `{article_path}`\"\n",
    "        logger.success(f\"Successfully read article from cache at `{article_path}`\")\n",
    "    else:\n",
    "        async with build_short_term_memory(app_config) as checkpointer:\n",
    "            generate_article_workflow = build_generate_article_workflow(checkpointer=checkpointer)\n",
    "            await generate_article_workflow.ainvoke(inputs, config)\n",
    "\n",
    "        logger.success(f\"Successfully generated article at `{article_path}`\")\n",
    "\n",
    "    article = article_path.read_text(encoding=\"utf-8\")\n",
    "\n",
    "    return article\n",
    "```\n",
    "\n",
    "**Key Features:**\n",
    "- Supports caching: Can read from cache or generate fresh\n",
    "- Uses the same workflow as production Brown\n",
    "- Returns the generated article as a string\n",
    "\n",
    "ðŸ’¡ Note how easy it is to reuse our Brown code due to our clean architecture design."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Hooking to Opik for Full Evaluation\n",
    "\n",
    "To run the full evaluation pipeline, we need glue code that connects our metrics to the `brown-course-lessons` dataset stored in Opik.\n",
    "\n",
    "### 10.1 The evaluate Function\n",
    "\n",
    "Source: `brown.observability.evaluation`\n",
    "\n",
    "```python\n",
    "from typing import Any, Callable, Literal\n",
    "\n",
    "from loguru import logger\n",
    "from opik import evaluation\n",
    "from opik.evaluation.metrics import base_metric\n",
    "\n",
    "from brown.config import get_settings\n",
    "from brown.config_app import get_app_config as load_app_config\n",
    "\n",
    "from .opik_utils import get_dataset\n",
    "\n",
    "app_config = load_app_config()\n",
    "\n",
    "def evaluate(\n",
    "    dataset_name: str,\n",
    "    metrics: list[base_metric.BaseMetric],\n",
    "    evaluation_task: Callable,\n",
    "    llm_judge_config: dict[str, Any],\n",
    "    workers: int = 2,\n",
    "    nb_samples: int | None = None,\n",
    "    dataset_item_names: list[str] | None = None,\n",
    ") -> None:\n",
    "    \"\"\"Run an Opik evaluation with the provided metrics and task.\"\"\"\n",
    "\n",
    "    assert get_settings().OPIK_API_KEY, \"OPIK_API_KEY is not set.\"\n",
    "\n",
    "    dataset = get_dataset(dataset_name)\n",
    "    if not dataset:\n",
    "        raise ValueError(f\"Dataset '{dataset_name}' does not exist.\")\n",
    "    \n",
    "    if dataset_item_names:\n",
    "        all_dataset_items = dataset.get_items()\n",
    "        dataset_item_ids = [item[\"id\"] for item in all_dataset_items if item[\"name\"] in dataset_item_names]\n",
    "        logger.info(f\"Evaluating {len(dataset_item_ids)}/{len(all_dataset_items)} dataset items.\")\n",
    "    else:\n",
    "        dataset_item_ids = None\n",
    "\n",
    "    logger.info(\"Starting evaluation...\")\n",
    "\n",
    "    llm_judge_config = {\n",
    "        \"dataset_name\": dataset.name,\n",
    "        \"llm_judge_config\": llm_judge_config,\n",
    "        \"app_config\": app_config.model_dump(mode=\"json\"),\n",
    "    }\n",
    "\n",
    "    evaluation.evaluate(\n",
    "        dataset=dataset,\n",
    "        task=evaluation_task,\n",
    "        scoring_metrics=metrics,\n",
    "        experiment_config=llm_judge_config,\n",
    "        task_threads=workers,\n",
    "        nb_samples=nb_samples,\n",
    "        dataset_item_ids=dataset_item_ids,\n",
    "    )\n",
    "```\n",
    "\n",
    "This function:\n",
    "- Fetches the dataset from Opik\n",
    "- Optionally filters to specific dataset items\n",
    "- Runs the evaluation task on each sample\n",
    "- Scores results with the provided metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2 Helper Functions\n",
    "\n",
    "Source: `brown.observability.opik_utils`\n",
    "\n",
    "```python\n",
    "def get_dataset(name: str) -> opik.Dataset | None:\n",
    "    \"\"\"Get a dataset by name.\"\"\"\n",
    "    client = opik.Opik()\n",
    "    try:\n",
    "        dataset = client.get_dataset(name=name)\n",
    "    except Exception:\n",
    "        dataset = None\n",
    "    return dataset\n",
    "```\n",
    "\n",
    "Source: `brown.evals.tasks`\n",
    "\n",
    "```python\n",
    "def create_evaluation_task(\n",
    "    cache_dir: Path,\n",
    "    read_from_cache: bool = False,\n",
    "    clean_cache: bool = False,\n",
    "    debug: bool = False,\n",
    ") -> Callable:\n",
    "    \"\"\"Create a reusable evaluation task with fixed runtime parameters.\n",
    "    \n",
    "    We need this because Opik's evaluate function doesn't allow passing extra\n",
    "    parameters to the task function. We use Python's `partial` to pin the\n",
    "    cache_dir and other options.\n",
    "    \"\"\"\n",
    "    return partial(\n",
    "        evaluation_task,\n",
    "        cache_dir=cache_dir,\n",
    "        read_from_cache=read_from_cache,\n",
    "        clean_cache=clean_cache,\n",
    "        debug=debug,\n",
    "    )\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Running the End-to-End Evaluation\n",
    "\n",
    "Now, let's glue everything together into the AI evals pipeline and run the end-to-end code we presented so far.\n",
    "\n",
    "ðŸ’¡ You can run the exact same code using the `scripts/brown_run_eval.py` script from the `lessons/writing_workflow` Brown project and the `brown-run-eval-flash` / `brown-run-eval-pro` Make commands.\n",
    "\n",
    "> âš ï¸ **Warning**: Running the full evaluation generates articles for each dataset sample, which takes significant time (several minutes per sample) and API costs. For this demo, we'll use cached articles from `outputs/evals-pro`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define constants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = \"brown-course-lessons\"\n",
    "CACHE_DIR = OUTPUTS_DIR / \"evals-pro\"  # Here, we store the generated articles from the entire dataset\n",
    "# to avoid recomputing them for this example.\n",
    "METRICS_TO_USE = [\"follows_gt\", \"user_intent\"]\n",
    "WORKERS = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Articles cached at:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('outputs/evals-pro')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CACHE_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's define the evaluation task and evaluation metrics as we've learnt in this Notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Configuration:\n",
      "  - Dataset: brown-course-lessons\n",
      "  - Cache directory: outputs/evals-pro\n",
      "  - Metrics: ['follows_gt', 'user_intent']\n",
      "  - Workers: 1\n"
     ]
    }
   ],
   "source": [
    "from brown.evals.tasks import create_evaluation_task\n",
    "from brown.models.config import ModelConfig\n",
    "from brown.observability.evaluation import evaluate\n",
    "\n",
    "evaluation_task = create_evaluation_task(\n",
    "    cache_dir=CACHE_DIR,\n",
    "    read_from_cache=True,  # Use cached articles from the `CACHE_DIR`\n",
    "    clean_cache=False,\n",
    "    debug=False,\n",
    ")\n",
    "\n",
    "model = SupportedModels.GOOGLE_GEMINI_25_FLASH\n",
    "model_config = ModelConfig(temperature=0.0, thinking_budget=int(1024 * 0.5), include_thoughts=False, max_retries=3)\n",
    "evaluation_metrics = build_evaluation_metrics(METRICS_TO_USE, model, model_config)\n",
    "\n",
    "print(\"Evaluation Configuration:\")\n",
    "print(f\"  - Dataset: {DATASET_NAME}\")\n",
    "print(f\"  - Cache directory: {CACHE_DIR}\")\n",
    "print(f\"  - Metrics: {METRICS_TO_USE}\")\n",
    "print(f\"  - Workers: {WORKERS}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define splits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset_item_names = [\"Lesson 10: Memory\"]\n",
    "test_dataset_item_names = [\n",
    "    \"Lesson 2: Workflows vs. Agents\",\n",
    "    \"Lesson 3: Context Engineering\",\n",
    "    \"Lesson 5: Workflow Patterns\",\n",
    "    \"Lesson 6: Tools\",\n",
    "    \"Lesson 8: ReAct Practice\",\n",
    "    \"Lesson 9: Retrieval-Augmented Generation (RAG)\",\n",
    "    \"Lesson 11: Multimodal Data\",\n",
    "]\n",
    "\n",
    "SPLITS = {\n",
    "    \"val\": val_dataset_item_names,\n",
    "    \"test\": test_dataset_item_names,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Computing the Alignment Score on the Validation Split\n",
    "\n",
    "Now that we have all the pieces in place, let's run the AI evals pipeline on the validation split to compute the alignment score, which checks whether the LLM judge aligns with a domain expert's human judgment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-03 15:36:51.176\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m5\u001b[0m - \u001b[1mStarting evaluation with dataset: brown-course-lessons on the `val` split\u001b[0m\n",
      "\u001b[32m2026-01-03 15:36:53.391\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mbrown.observability.evaluation\u001b[0m:\u001b[36mevaluate\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mSuccessfuly filtered dataset based on the provided dataset item names.\u001b[0m\n",
      "\u001b[32m2026-01-03 15:36:53.392\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mbrown.observability.evaluation\u001b[0m:\u001b[36mevaluate\u001b[0m:\u001b[36m57\u001b[0m - \u001b[1mEvaluating 1/8 dataset items.\u001b[0m\n",
      "\u001b[32m2026-01-03 15:36:53.392\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mbrown.observability.evaluation\u001b[0m:\u001b[36mevaluate\u001b[0m:\u001b[36m61\u001b[0m - \u001b[1mStarting evaluation...\u001b[0m\n",
      "\u001b[32m2026-01-03 15:36:53.394\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mbrown.observability.evaluation\u001b[0m:\u001b[36mevaluate\u001b[0m:\u001b[36m70\u001b[0m - \u001b[1mEvaluation details:\u001b[0m\n",
      "\u001b[32m2026-01-03 15:36:53.395\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mbrown.observability.evaluation\u001b[0m:\u001b[36mevaluate\u001b[0m:\u001b[36m71\u001b[0m - \u001b[1mDataset: brown-course-lessons\u001b[0m\n",
      "\u001b[32m2026-01-03 15:36:53.395\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mbrown.observability.evaluation\u001b[0m:\u001b[36mevaluate\u001b[0m:\u001b[36m72\u001b[0m - \u001b[1mMetrics: ['FollowsGTMetricLLMJudge', 'UserIntentMetricLLMJudge']\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea590047bca24465acb9cdfe7737fbf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-03 15:36:54.202\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mbrown.evals.tasks\u001b[0m:\u001b[36mevaluation_task\u001b[0m:\u001b[36m60\u001b[0m - \u001b[1mProcessing evaluation sample: Lesson 10: Memory\u001b[0m\n",
      "\u001b[32m2026-01-03 15:36:54.204\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mbrown.evals.tasks\u001b[0m:\u001b[36m__run\u001b[0m:\u001b[36m136\u001b[0m - \u001b[32m\u001b[1mSuccessfully read article from cache at `outputs/evals-pro/data/10_memory_knowledge_access/article.md`\u001b[0m\n",
      "OPIK: Started logging traces to the \"Default Project\" project at https://www.comet.com/opik/api/v1/session/redirect/projects/?trace_id=019b8413-45bf-7fb9-9da6-e764ab01a9cb&path=aHR0cHM6Ly93d3cuY29tZXQuY29tL29waWsvYXBpLw==.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â•­â”€ brown-course-lessons (1 samples) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
       "â”‚                                               â”‚\n",
       "â”‚ <span style=\"font-weight: bold\">Total time:       </span> 00:00:26                   â”‚\n",
       "â”‚ <span style=\"font-weight: bold\">Number of samples:</span> 1                          â”‚\n",
       "â”‚                                               â”‚\n",
       "â”‚ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">follows_gt_content: 0.8750 (avg)</span>              â”‚\n",
       "â”‚ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">follows_gt_flow: 0.5000 (avg)</span>                 â”‚\n",
       "â”‚ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">follows_gt_structure: 0.2500 (avg)</span>            â”‚\n",
       "â”‚ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">user_intent_guideline_adherence: 0.8571 (avg)</span> â”‚\n",
       "â”‚ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">user_intent_research_anchoring: 1.0000 (avg)</span>  â”‚\n",
       "â”‚                                               â”‚\n",
       "â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â•­â”€ brown-course-lessons (1 samples) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
       "â”‚                                               â”‚\n",
       "â”‚ \u001b[1mTotal time:       \u001b[0m 00:00:26                   â”‚\n",
       "â”‚ \u001b[1mNumber of samples:\u001b[0m 1                          â”‚\n",
       "â”‚                                               â”‚\n",
       "â”‚ \u001b[1;32mfollows_gt_content: 0.8750 (avg)\u001b[0m              â”‚\n",
       "â”‚ \u001b[1;32mfollows_gt_flow: 0.5000 (avg)\u001b[0m                 â”‚\n",
       "â”‚ \u001b[1;32mfollows_gt_structure: 0.2500 (avg)\u001b[0m            â”‚\n",
       "â”‚ \u001b[1;32muser_intent_guideline_adherence: 0.8571 (avg)\u001b[0m â”‚\n",
       "â”‚ \u001b[1;32muser_intent_research_anchoring: 1.0000 (avg)\u001b[0m  â”‚\n",
       "â”‚                                               â”‚\n",
       "â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Uploading results to Opik <span style=\"color: #808000; text-decoration-color: #808000\">...</span> \n",
       "</pre>\n"
      ],
      "text/plain": [
       "Uploading results to Opik \u001b[33m...\u001b[0m \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">View the results <a href=\"https://www.comet.com/opik/api/v1/session/redirect/experiments/?experiment_id=019b8413-429b-7d74-a6b8-8b7acf9576b7&dataset_id=019b4b30-8c7d-7428-bb0a-6da2a0d8a30a&path=aHR0cHM6Ly93d3cuY29tZXQuY29tL29waWsvYXBpLw==\" target=\"_blank\">in your Opik dashboard</a>.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "View the results \u001b]8;id=570452;https://www.comet.com/opik/api/v1/session/redirect/experiments/?experiment_id=019b8413-429b-7d74-a6b8-8b7acf9576b7&dataset_id=019b4b30-8c7d-7428-bb0a-6da2a0d8a30a&path=aHR0cHM6Ly93d3cuY29tZXQuY29tL29waWsvYXBpLw==\u001b\\in your Opik dashboard\u001b]8;;\u001b\\.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-03 15:37:24.268\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m15\u001b[0m - \u001b[32m\u001b[1mEvaluation completed successfully!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from loguru import logger\n",
    "\n",
    "split = \"val\"\n",
    "\n",
    "logger.info(f\"Starting evaluation with dataset: {DATASET_NAME} on the `{split}` split\")\n",
    "evaluate(\n",
    "    dataset_name=DATASET_NAME,\n",
    "    metrics=evaluation_metrics,\n",
    "    evaluation_task=evaluation_task,\n",
    "    llm_judge_config={\"model\": str(model), **model_config.model_dump()},\n",
    "    workers=WORKERS,\n",
    "    dataset_item_names=SPLITS[split],\n",
    "    split=split,\n",
    ")\n",
    "logger.success(\"Evaluation completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now open the experiment in Opik using the dynamic link generated in the console output above, and inspect the results in Opik's dashboard.\n",
    "\n",
    "For convenience, here are the results we got when we ran the experiment on our end:\n",
    "```markdown\n",
    "â•­â”€ brown-course-lessons (1 samples) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
    "â”‚                                               â”‚\n",
    "â”‚ Total time:        00:00:34                   â”‚\n",
    "â”‚ Number of samples: 1                          â”‚\n",
    "â”‚                                               â”‚\n",
    "â”‚ follows_gt_content: 1.0000 (avg)              â”‚\n",
    "â”‚ follows_gt_flow: 0.5714 (avg)                 â”‚\n",
    "â”‚ follows_gt_structure: 0.5238 (avg)            â”‚\n",
    "â”‚ user_intent_guideline_adherence: 0.8571 (avg) â”‚\n",
    "â”‚ user_intent_research_anchoring: 1.0000 (avg)  â”‚\n",
    "â”‚                                               â”‚\n",
    "â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n",
    "```\n",
    "\n",
    "And here is how the experiment looks in Opik:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/iusztinpaul/agentic-ai-engineering-course-data/main/images/30_ai_evals_offline_metrics_opik_validation_score.png\" alt=\"30_ai_evals_offline_metrics_opik_validation_score\"/>\n",
    "\n",
    "And here is how the reasoning message looks in Opik, where you can visualize the results at the section level (you can hover over each sample and dimension to see it):\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/iusztinpaul/agentic-ai-engineering-course-data/main/images/30_ai_evals_offline_metrics_opik_validation_reasoning_message.png\" alt=\"30_ai_evals_offline_metrics_opik_validation_reasoning_message\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this step, using the validation split, we want to align the LLM Judges with our human judgment. For simplicity, let's assume that we want to do that only for the following GT metric.\n",
    "\n",
    "To do that, as the validation split contains only `Lesson 10`, you will open the [cached generated article](https://github.com/iusztinpaul/agentic-ai-engineering-course-data/blob/main/data/outputs/evals-pro/data/10_memory_knowledge_access/article.md) and the [ground article](https://github.com/iusztinpaul/agentic-ai-engineering-course-data/blob/main/data/inputs/evals/dataset/data/10_memory_knowledge_access/article_ground_truth.md) side by side.\n",
    "\n",
    "Then you will label this sample, writing down the scores and the reasons behind the scores for the content, flow, and structure metrics scoped under the GT metric.\n",
    "\n",
    "Next, you will open the experiment in Opik's dashboard using the dynamically generated link from the function above and add the scores and reason messages computed by the LLM judge next to the ones you manually labeled.\n",
    "\n",
    "Finally, you will compare your results with those from the LLM Judge. Since we use only binary metrics at the section level, we compute the alignment score using the following formula, which ranges from `[0, 1]`: Hence, we can consider it as an alignment percentage:\n",
    "\n",
    "$$ \\text{Agreement \\%} = \\frac{\\sum_{j=1}^{n} [manualScore_j = LLMJudgeScore_j]}{n} \\times 100 $$\n",
    "\n",
    "The formula computes the average number of matching scores between your manual labeling and the LLM judge. If the alignment is close to 100%, then congratulations, your LLM Judge is aligned with human judgment. Otherwise, you still have to improve the LLM judge until the scores align.\n",
    "\n",
    "Only when the alignment percentage approaches 100% can you start relying on your LLM Judge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the Alignment Scores\n",
    "\n",
    "To show you how aligning works in pratice, let's compute the alignment scores on `Lesson 10 - Memory` on the follows ground truth metric.\n",
    "\n",
    "To do that, we:\n",
    "- opened the [cached generated memory article](https://github.com/iusztinpaul/agentic-ai-engineering-course-data/blob/main/data/outputs/evals-pro/data/10_memory_knowledge_access/article.md) (we have to fix the generated article as well to have all the components fixed except the LLM Judge itself)\n",
    "- opened the [ground truth memory article](https://github.com/iusztinpaul/agentic-ai-engineering-course-data/blob/main/data/inputs/evals/dataset/data/10_memory_knowledge_access/article_ground_truth.md) from the AI evals dataset\n",
    "- labeled the difference between the two (both scores and reason messages)\n",
    "- ran an experiment using the validation split\n",
    "- added the LLM judge results along with the manual ones\n",
    "- computed the alignemnt score using the formula above\n",
    "\n",
    "Here are the results:\n",
    "\n",
    "#### Content Metric Alignment\n",
    "\n",
    "| Section                                                   |   Score Content | Reason Content                                                                                                                                                                     |   LLM Judge Score Content | LLM Judge Reason Content                                                                                                                                                                                                                                                                                                                                                                 |\n",
    "|:----------------------------------------------------------|----------------:|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------:|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| Introduction                                              |               1 | Even if the examples are a bit different it both sections cover the same introductory ideas about memory for AI agents.                                                            |                         1 | Both sections introduce the concept of memory in AI agents, highlighting the stateless nature of LLMs and the role of the context window. They both set the stage for the lesson by explaining why memory is a necessary engineering solution.                                                                                                                                           |\n",
    "| The Layers of Memory: Internal, Short-Term, and Long-Term |               0 | Even if it covers the same ideas about internal, short, and long-term memory, the generated section misses the ideas on combining them into a unified system and reference to RAG. |                         1 | Both sections accurately describe the three layers of memory: Internal Knowledge, Short-Term Memory (Context Window), and Long-Term Memory, explaining their distinct functions and interplay.                                                                                                                                                                                           |\n",
    "| Long-Term Memory: Semantic, Episodic, and Procedural      |               1 | Both sections cover the same ideas around semantic, episodic and procedural memory.                                                                                                |                         1 | Both sections correctly categorize long-term memory into Semantic, Episodic, and Procedural types, providing clear definitions and examples for each.                                                                                                                                                                                                                                    |\n",
    "| Storing Memories: Pros and Cons of Different Approaches   |               1 | Both sections cover the same ideas around storing the memory as raw strings, structured entities or knowledge graphs.                                                              |                         1 | Both sections discuss the same three approaches for storing memories: Raw Strings (Vector Databases), Structured Entities (JSON), and Knowledge Graphs, outlining the pros and cons for each.                                                                                                                                                                                            |\n",
    "| Memory Implementations With Code Examples                 |               1 | Both sections cover examples on implementing semantic, episodic and procedural memory with mem0.                                                                                   |                         1 | Both sections provide code examples for implementing Semantic, Episodic, and Procedural memory. However, the generated section uses the `mem0` library, while the expected output also uses `mem0` but presents the code and explanations in a slightly different structure and with more detailed comments. The core concepts demonstrated are the same.                                |\n",
    "| Real-World Challenges                                     |               0 | The generated section adds an extra topic on \"The Human Factor\" that's entirely missing within the expected section.                                                               |                         0 | The generated section covers similar themes, such as re-evaluating compression due to larger context windows and designing for the product. However, it presents these as two distinct points, whereas the expected output integrates them into a more cohesive narrative. The generated section also adds a \"The Human Factor\" subsection, which is not present in the expected output. |\n",
    "| Conclusion                                                |               1 | Both sections cover the same ideas presenting what we've learnt within the article.                                                                                                |                         1 | Both sections provide a summary of the lesson's key takeaways, emphasizing that memory transforms agents from stateless tools to adaptive partners and that current memory systems are an engineering workaround for the lack of continual learning in LLMs.                                                                                                                             |\n",
    "| References                                                |               0 | The generated section misses 10 links.                                                                                                                                             |                         1 | The generated section provides a list of references, similar in purpose to the expected output. However, the specific references cited are different.                                                                                                                                                                                                                                    |\n",
    "\n",
    "**LLM Judge Alignment - Content Metric: 75.00%**\n",
    "\n",
    "#### Flow Metric Alignment\n",
    "\n",
    "| Section                                                   |   Score Flow | Reason Flow                                                                                                                                                                                                                                                                                                                                                       |   LLM Judge Score Flow | LLM Judge Reason Flow                                                                                                                                                                                                                                                                                                                                                                                            |\n",
    "|:----------------------------------------------------------|-------------:|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------:|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| Introduction                                              |            0 | Covers the core ideas in the same order, such as LLMs forgetting past interactions, using the context window to anchor the conversation, and the limitations of the context window. Still, the last sentence acting as the transition differs: the generated section makes a quick outline, while the expected section highlights its characteristics in biology. |                      0 | The generated section follows a similar flow, starting with a personal anecdote, explaining the statelessness of LLMs, and introducing memory as the solution. However, it omits the specific mention of OpenAI's ChatGPT memory feature, which is present in the expected output.                                                                                                                               |\n",
    "| The Layers of Memory: Internal, Short-Term, and Long-Term |            0 | The generated section starts with a comparison to biology, while the expected section jumps straight to the three memory types, with a small transition.                                                                                                                                                                                                          |                      1 | The flow of ideas is consistent in both sections, starting with an introduction to the layers, defining each one, and concluding with a diagram that illustrates their relationship.                                                                                                                                                                                                                             |\n",
    "| Long-Term Memory: Semantic, Episodic, and Procedural      |            0 | The generated section misses the last transition sentence.                                                                                                                                                                                                                                                                                                        |                      1 | The flow is identical, with both sections introducing the three categories and then dedicating a subsection (using H3 headers) to explain each one in the same order: Semantic, Episodic, and Procedural.                                                                                                                                                                                                        |\n",
    "| Storing Memories: Pros and Cons of Different Approaches   |            0 | Even if both sections have an introductory paragraph, the generated section text misses most of the ideas such as the discussion on tools for agents to interact with the memory.                                                                                                                                                                                 |                      0 | The generated section follows a similar logical flow, introducing the topic and then detailing each storage approach. However, it omits the introductory paragraph from the expected output that discusses the challenge of updating memories and the role of LLMs in managing `ADD/UPDATE/DELETE` operations.                                                                                                   |\n",
    "| Memory Implementations With Code Examples                 |            0 | The generated section adds an extra \"extraction prompt\" example within the semantic memory subsection that is not present within the expected section.                                                                                                                                                                                                            |                      0 | The generated section's flow is logical, covering setup and then each memory type. However, it differs from the expected output, which integrates the helper function definitions within the setup and provides more detailed, step-by-step explanations for each code block's output. The generated section also misses the introductory paragraph that frames the code examples and links to a Colab notebook. |\n",
    "| Real-World Challenges                                     |            0 | The generated section adds an extra \"The Human Factor\" that's missing within the expected section.                                                                                                                                                                                                                                                                |                      0 | The flow is different. The generated section breaks the discussion into \"Re-evaluating Compression,\" \"Designing for the Product,\" and \"The Human Factor.\" The expected output has a more integrated flow, discussing compression and product design as interconnected challenges without the distinct separation.                                                                                                |\n",
    "| Conclusion                                                |            1 | Both sections follow the same flow, presenting what we've learnt within the article, wrapping up with a transition to the next lesson.                                                                                                                                                                                                                            |                      1 | Both sections follow a similar flow, summarizing the main points of the lesson and providing a forward-looking statement about the next topics in the course (RAG and Multimodal data).                                                                                                                                                                                                                          |\n",
    "| References                                                |            1 | -                                                                                                                                                                                                                                                                                                                                                                 |                      1 | Both sections present the references as a numbered list, which is a consistent flow.                                                                                                                                                                                                                                                                                                                             |\n",
    "\n",
    "**LLM Judge Alignment - Flow Metric: 75.00%**\n",
    "\n",
    "\n",
    "#### Structure Metric Alignment\n",
    "\n",
    "| Section                                                   |   Score Structure | Reason Structure                                                                                                                         |   LLM Judge Score Structure | LLM Judge Reason Structure                                                                                                                                                                                                                                                                                                         |\n",
    "|:----------------------------------------------------------|------------------:|:-----------------------------------------------------------------------------------------------------------------------------------------|----------------------------:|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| Introduction                                              |                 1 | Both sections are structured the same as simple text paragraphs.                                                                         |                           1 | Both sections use similar paragraph structures and bolding for emphasis. The generated section correctly uses citation formatting.                                                                                                                                                                                                 |\n",
    "| The Layers of Memory: Internal, Short-Term, and Long-Term |                 1 | Both sections enumerate the three memory types within simple paragraphs with bolded key words.                                           |                           1 | Both sections use bolding for key terms and include a Mermaid diagram to visualize the concept. The formatting of the diagram and its caption is consistent.                                                                                                                                                                       |\n",
    "| Long-Term Memory: Semantic, Episodic, and Procedural      |                 1 | Both sections structured the three long-term memory types under H3 headers.                                                              |                           1 | Both sections use H3 headers for the sub-categories, maintaining a consistent structure. The use of bolding and italics for emphasis is also similar.                                                                                                                                                                              |\n",
    "| Storing Memories: Pros and Cons of Different Approaches   |                 0 | The generated section uses numbered H3 headers instead of simple H3 headers as in the expected section.                                  |                           0 | The generated section uses numbered H3 headers for each approach, which differs from the expected output's use of unnumbered H3 headers. Additionally, the expected output includes a detailed comparison table, which is missing from the generated section                                                                       |\n",
    "| Memory Implementations With Code Examples                 |                 1 | Both sections use numebered lists to present the code and divide the impelemntation examples using H3 headers.                           |                           0 | The generated section uses H3 headers for each implementation, which is a different structure from the expected output's H3 headers for each memory type. The expected output also includes a \"Tip\" callout box, which is absent in the generated version. The formatting of code blocks and outputs is similar but not identical. |\n",
    "| Real-World Challenges                                     |                 1 | Both sections divide the ides under H3 headers and simple text paragraphs.                                                               |                           0 | The structure is different. The generated section uses H3 headers for its points, while the expected output uses a single H2 header for the entire section and presents the content as continuous prose.                                                                                                                           |\n",
    "| Conclusion                                                |                 0 | Both sections are structured as simple text paragraphs, but the generated sections bold the word \"Retrieval-Augmented-Generation (RAG)\". |                           1 | The paragraph structure and overall formatting are consistent between both sections.                                                                                                                                                                                                                                               |\n",
    "| References                                                |                 0 | The references from the generated section do not follow the same APA format.                                                             |                           0 | The formatting of the references is different. The generated output uses a format like \"1. Title. (Date). Source. URL\", while the expected output uses \"[<number>] [Title](URL)\". The generated section also includes an \"Images\" subsection at the end, which is not in the expected output.                                      |\n",
    "\n",
    "**LLM Judge Alignment - Structure Metric: 62.50%**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next Steps\n",
    "\n",
    "As you can see, the scores are far from 100%. So what next?\n",
    "\n",
    "We have to look where the scores do not match, adapt the LLM Judges by either improving the few-shot examples or the system prompt and retry until the scores get closer to 100%.\n",
    "\n",
    "Still, if you look only at the difference in scores, it's tough to figure out what to improve. That's why we labeled the reason message as well. Looking at it, we have a lot of signals on knowing what to improve.\n",
    "\n",
    "For example, if we look at `The Layers of Memory: Internal, Short-Term, and Long-Term` within the `flow metric` table, we can see that the LLM judge gave a score of 1 because it ignored the transition paragraph, which is wrong. Thus, we know that we should add this rule to the LLM judge system prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Computing the Baseline Score on the Test Split\n",
    "\n",
    "For the sake of this notebook, let's assume that this alignment score is acceptable.\n",
    "\n",
    "Thus, let's run the evaluation on the test split to compute the baseline score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-03 15:37:24.302\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m3\u001b[0m - \u001b[1mStarting evaluation with dataset: brown-course-lessons on the `test` split\u001b[0m\n",
      "\u001b[32m2026-01-03 15:37:36.332\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mbrown.observability.evaluation\u001b[0m:\u001b[36mevaluate\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mSuccessfuly filtered dataset based on the provided dataset item names.\u001b[0m\n",
      "\u001b[32m2026-01-03 15:37:36.334\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mbrown.observability.evaluation\u001b[0m:\u001b[36mevaluate\u001b[0m:\u001b[36m57\u001b[0m - \u001b[1mEvaluating 7/8 dataset items.\u001b[0m\n",
      "\u001b[32m2026-01-03 15:37:36.334\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mbrown.observability.evaluation\u001b[0m:\u001b[36mevaluate\u001b[0m:\u001b[36m61\u001b[0m - \u001b[1mStarting evaluation...\u001b[0m\n",
      "\u001b[32m2026-01-03 15:37:36.334\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mbrown.observability.evaluation\u001b[0m:\u001b[36mevaluate\u001b[0m:\u001b[36m70\u001b[0m - \u001b[1mEvaluation details:\u001b[0m\n",
      "\u001b[32m2026-01-03 15:37:36.335\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mbrown.observability.evaluation\u001b[0m:\u001b[36mevaluate\u001b[0m:\u001b[36m71\u001b[0m - \u001b[1mDataset: brown-course-lessons\u001b[0m\n",
      "\u001b[32m2026-01-03 15:37:36.338\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mbrown.observability.evaluation\u001b[0m:\u001b[36mevaluate\u001b[0m:\u001b[36m72\u001b[0m - \u001b[1mMetrics: ['FollowsGTMetricLLMJudge', 'UserIntentMetricLLMJudge']\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "847d85617dfe43b7b23495eb343ca26e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-03 15:37:39.885\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mbrown.evals.tasks\u001b[0m:\u001b[36mevaluation_task\u001b[0m:\u001b[36m60\u001b[0m - \u001b[1mProcessing evaluation sample: Lesson 11: Multimodal Data\u001b[0m\n",
      "\u001b[32m2026-01-03 15:37:39.887\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mbrown.evals.tasks\u001b[0m:\u001b[36m__run\u001b[0m:\u001b[36m136\u001b[0m - \u001b[32m\u001b[1mSuccessfully read article from cache at `outputs/evals-pro/data/11_multimodal/article.md`\u001b[0m\n",
      "\u001b[32m2026-01-03 15:38:09.216\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mbrown.evals.tasks\u001b[0m:\u001b[36mevaluation_task\u001b[0m:\u001b[36m60\u001b[0m - \u001b[1mProcessing evaluation sample: Lesson 9: Retrieval-Augmented Generation (RAG)\u001b[0m\n",
      "\u001b[32m2026-01-03 15:38:09.218\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mbrown.evals.tasks\u001b[0m:\u001b[36m__run\u001b[0m:\u001b[36m136\u001b[0m - \u001b[32m\u001b[1mSuccessfully read article from cache at `outputs/evals-pro/data/09_RAG/article.md`\u001b[0m\n",
      "\u001b[32m2026-01-03 15:38:30.887\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mbrown.evals.tasks\u001b[0m:\u001b[36mevaluation_task\u001b[0m:\u001b[36m60\u001b[0m - \u001b[1mProcessing evaluation sample: Lesson 8: ReAct Practice\u001b[0m\n",
      "\u001b[32m2026-01-03 15:38:30.889\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mbrown.evals.tasks\u001b[0m:\u001b[36m__run\u001b[0m:\u001b[36m136\u001b[0m - \u001b[32m\u001b[1mSuccessfully read article from cache at `outputs/evals-pro/data/08_react_practice/article.md`\u001b[0m\n",
      "\u001b[32m2026-01-03 15:38:54.085\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mbrown.evals.tasks\u001b[0m:\u001b[36mevaluation_task\u001b[0m:\u001b[36m60\u001b[0m - \u001b[1mProcessing evaluation sample: Lesson 6: Tools\u001b[0m\n",
      "\u001b[32m2026-01-03 15:38:54.087\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mbrown.evals.tasks\u001b[0m:\u001b[36m__run\u001b[0m:\u001b[36m136\u001b[0m - \u001b[32m\u001b[1mSuccessfully read article from cache at `outputs/evals-pro/data/06_tools/article.md`\u001b[0m\n",
      "\u001b[32m2026-01-03 15:39:20.703\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mbrown.evals.tasks\u001b[0m:\u001b[36mevaluation_task\u001b[0m:\u001b[36m60\u001b[0m - \u001b[1mProcessing evaluation sample: Lesson 5: Workflow Patterns\u001b[0m\n",
      "\u001b[32m2026-01-03 15:39:20.705\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mbrown.evals.tasks\u001b[0m:\u001b[36m__run\u001b[0m:\u001b[36m136\u001b[0m - \u001b[32m\u001b[1mSuccessfully read article from cache at `outputs/evals-pro/data/05_workflow_patterns/article.md`\u001b[0m\n",
      "\u001b[32m2026-01-03 15:39:45.044\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mbrown.evals.tasks\u001b[0m:\u001b[36mevaluation_task\u001b[0m:\u001b[36m60\u001b[0m - \u001b[1mProcessing evaluation sample: Lesson 3: Context Engineering\u001b[0m\n",
      "\u001b[32m2026-01-03 15:39:45.046\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mbrown.evals.tasks\u001b[0m:\u001b[36m__run\u001b[0m:\u001b[36m136\u001b[0m - \u001b[32m\u001b[1mSuccessfully read article from cache at `outputs/evals-pro/data/03_context_engineering/article.md`\u001b[0m\n",
      "\u001b[32m2026-01-03 15:40:10.408\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mbrown.evals.tasks\u001b[0m:\u001b[36mevaluation_task\u001b[0m:\u001b[36m60\u001b[0m - \u001b[1mProcessing evaluation sample: Lesson 2: Workflows vs. Agents\u001b[0m\n",
      "\u001b[32m2026-01-03 15:40:10.410\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mbrown.evals.tasks\u001b[0m:\u001b[36m__run\u001b[0m:\u001b[36m136\u001b[0m - \u001b[32m\u001b[1mSuccessfully read article from cache at `outputs/evals-pro/data/02_workflows_vs_agents/article.md`\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â•­â”€ brown-course-lessons (7 samples) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
       "â”‚                                               â”‚\n",
       "â”‚ <span style=\"font-weight: bold\">Total time:       </span> 00:03:03                   â”‚\n",
       "â”‚ <span style=\"font-weight: bold\">Number of samples:</span> 7                          â”‚\n",
       "â”‚                                               â”‚\n",
       "â”‚ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">follows_gt_content: 0.9597 (avg)</span>              â”‚\n",
       "â”‚ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">follows_gt_flow: 0.3091 (avg)</span>                 â”‚\n",
       "â”‚ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">follows_gt_structure: 0.1882 (avg)</span>            â”‚\n",
       "â”‚ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">user_intent_guideline_adherence: 0.4796 (avg)</span> â”‚\n",
       "â”‚ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">user_intent_research_anchoring: 0.8075 (avg)</span>  â”‚\n",
       "â”‚                                               â”‚\n",
       "â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â•­â”€ brown-course-lessons (7 samples) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
       "â”‚                                               â”‚\n",
       "â”‚ \u001b[1mTotal time:       \u001b[0m 00:03:03                   â”‚\n",
       "â”‚ \u001b[1mNumber of samples:\u001b[0m 7                          â”‚\n",
       "â”‚                                               â”‚\n",
       "â”‚ \u001b[1;32mfollows_gt_content: 0.9597 (avg)\u001b[0m              â”‚\n",
       "â”‚ \u001b[1;32mfollows_gt_flow: 0.3091 (avg)\u001b[0m                 â”‚\n",
       "â”‚ \u001b[1;32mfollows_gt_structure: 0.1882 (avg)\u001b[0m            â”‚\n",
       "â”‚ \u001b[1;32muser_intent_guideline_adherence: 0.4796 (avg)\u001b[0m â”‚\n",
       "â”‚ \u001b[1;32muser_intent_research_anchoring: 0.8075 (avg)\u001b[0m  â”‚\n",
       "â”‚                                               â”‚\n",
       "â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Uploading results to Opik <span style=\"color: #808000; text-decoration-color: #808000\">...</span> \n",
       "</pre>\n"
      ],
      "text/plain": [
       "Uploading results to Opik \u001b[33m...\u001b[0m \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">View the results <a href=\"https://www.comet.com/opik/api/v1/session/redirect/experiments/?experiment_id=019b8413-ea53-7e51-9c35-b45ed96134c3&dataset_id=019b4b30-8c7d-7428-bb0a-6da2a0d8a30a&path=aHR0cHM6Ly93d3cuY29tZXQuY29tL29waWsvYXBpLw==\" target=\"_blank\">in your Opik dashboard</a>.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "View the results \u001b]8;id=677661;https://www.comet.com/opik/api/v1/session/redirect/experiments/?experiment_id=019b8413-ea53-7e51-9c35-b45ed96134c3&dataset_id=019b4b30-8c7d-7428-bb0a-6da2a0d8a30a&path=aHR0cHM6Ly93d3cuY29tZXQuY29tL29waWsvYXBpLw==\u001b\\in your Opik dashboard\u001b]8;;\u001b\\.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-03 15:40:42.877\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m13\u001b[0m - \u001b[32m\u001b[1mEvaluation completed successfully!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "split = \"test\"\n",
    "\n",
    "logger.info(f\"Starting evaluation with dataset: {DATASET_NAME} on the `{split}` split\")\n",
    "evaluate(\n",
    "    dataset_name=DATASET_NAME,\n",
    "    metrics=evaluation_metrics,\n",
    "    evaluation_task=evaluation_task,\n",
    "    llm_judge_config={\"model\": str(model), **model_config.model_dump()},\n",
    "    workers=WORKERS,\n",
    "    dataset_item_names=SPLITS[split],\n",
    "    split=split,\n",
    ")\n",
    "logger.success(\"Evaluation completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now open the experiment in Opik using the dynamic link generated in the console output above, and inspect the results in Opik's dashboard.\n",
    "\n",
    "For convenience, here are the results we got when we ran the experiment on our end:\n",
    "```markdown\n",
    "â•­â”€ brown-course-lessons (7 samples) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
    "â”‚                                               â”‚\n",
    "â”‚ Total time:        00:03:03                   â”‚\n",
    "â”‚ Number of samples: 7                          â”‚\n",
    "â”‚                                               â”‚\n",
    "â”‚ follows_gt_content: 0.8962 (avg)              â”‚\n",
    "â”‚ follows_gt_flow: 0.4127 (avg)                 â”‚\n",
    "â”‚ follows_gt_structure: 0.1086 (avg)            â”‚\n",
    "â”‚ user_intent_guideline_adherence: 0.7468 (avg) â”‚\n",
    "â”‚ user_intent_research_anchoring: 0.8075 (avg)  â”‚\n",
    "â”‚                                               â”‚\n",
    "â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n",
    "```\n",
    "\n",
    "And here is how the experiment looks in Opik:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/iusztinpaul/agentic-ai-engineering-course-data/main/images/30_ai_evals_offline_metrics_opik_baseline_score.png\" alt=\"30_ai_evals_offline_metrics_opik_baseline_score\" height=600/>\n",
    "\n",
    "And here is how the reasoning message looks in Opik, where you can visualize the results at the section level (you can hover over each sample and dimension to see it):\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/iusztinpaul/agentic-ai-engineering-course-data/main/images/30_ai_evals_offline_metrics_opik_baseline_score_reasoning_message.png\" alt=\"30_ai_evals_offline_metrics_opik_baseline_score_reasoning_message\" height=600/>\n",
    "\n",
    "These scores are the baseline to compare against when we make future changes to our writing workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Checking the Stability of the LLM Judge\n",
    "\n",
    "Another criterion you should check before relying on your LLM judge is its stability. Why? Because we use LLMs to compute the metrics, we might get different scores after each run. Even if we did our best to stabilize these scores, for example, by using binary scores to make it easier for the LLM to choose one option, the LLM judge can still be unstable.\n",
    "\n",
    "So, how can we compute the stability of the LLM judge?\n",
    "\n",
    "As before, we fix all components on the [outputs/evals-pro](https://github.com/iusztinpaul/agentic-ai-engineering-course-data/tree/main/data/outputs/evals-pro/data) cached generated articles and evaluation dataset, and run the experiment multiple times to see how much the scores vary. The idea is that, by keeping everything the same within the AI evals pipeline, the results differ purely due to the stochastic nature of the LLMs.\n",
    "\n",
    "In our use case, we did that 5 times and got the following results:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/iusztinpaul/agentic-ai-engineering-course-data/main/images/30_ai_evals_offline_metrics_practice_llm_judge_variation.png\" alt=\"LLM Judge Variation\" height=600/>\n",
    "\n",
    "As you can see in the image above, the lines for each metric are pretty flat. It's not 100% flat, but it doesn't vary much, which means our LLM judges are stable, though not 100% stable.\n",
    "\n",
    "To quantify this variation, we computed the standard deviation for each metric:\n",
    "\n",
    "| Metric | Standard Deviation |\n",
    "| :--- | :--- |\n",
    "| follows_gt_content | 0.0316042 |\n",
    "| follows_gt_flow | 0.0432000 |\n",
    "| follows_gt_structure | 0.0180826 |\n",
    "| user_intent_guideline_adherence | 0.0380109 |\n",
    "| user_intent_research_anchoring | 0.0268774 |\n",
    "\n",
    "\n",
    "#### What Next?\n",
    "\n",
    "As you can see, the standard deviation is not 0, which means the LLM judges are not purely stable between runs. So what can you do?\n",
    "\n",
    "There are 2 options:\n",
    "1. You can use this standard deviation to understand when a difference in a particular metric is statistically significant. For example, for `follows_gt_content`, we have a standard deviation of ~0.031. Thus, if we have a difference of <=0.031 between the baseline and the change you made to the AI app, it's probably probabilistically insignificant.\n",
    "2. You can improve the LLM judge and stabilize it by trying to get this number close to 0. Which can be more time-consuming, but it makes your system more trustworthy. You can do that by making it easier for the LLM to decide between 0 and 1. To do this, you need to review the data, label more samples, and identify which scenarios are fuzzy and need clarification.\n",
    "\n",
    "Probably the best strategy is to start with option 1 to get started and strive for option 2 in the long run as you keep using the system.\n",
    "\n",
    "Now... How can we use these AI Evals in practice? How can we integrate them into our real-world processes? We do that through the optimization loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Optimization Loop\n",
    "\n",
    "What we just did is called running an **experiment**. For a particular state of our AI application and a given evaluation dataset, we computed scores reflecting how well our system performs.\n",
    "\n",
    "Thus, whenever you plan to do changes to your AI application you have to follow the next strategy, known as the **optimization loop:**\n",
    "\n",
    "1. Run an experiment to get baseline metrics\n",
    "2. Make changes to your AI app (prompts, models, parameters)\n",
    "3. Run another experiment\n",
    "4. Compare metrics to see if the change improved or degraded performance\n",
    "5. Repeat until satisfied\n",
    "\n",
    "Now that we have clarity on how our system works (or doesn't work), we can start tweaking our AI app while using the metrics as our north star."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Exploring the Experiments Inside Opik\n",
    "\n",
    "Here is a short video presenting in more detail how you can leverage Opik's dashboard to zoom in on each experiment or compare multiple experiments:\n",
    "\n",
    "<a href=\"https://youtu.be/6XDzz9v0pao\" target=\"_blank\">\n",
    "   <img src=\"https://img.youtube.com/vi/6XDzz9v0pao/maxresdefault.jpg\" alt=\"LLM Judge Variation\" height=\"400\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Conclusion\n",
    "\n",
    "AI evals are critical for building reliable AI applications. Without metrics, you're flying blind, making changes based on gut feeling and vibe checking rather than evidence.\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "In this lesson, we covered:\n",
    "\n",
    "- **LLM Judge Architecture**: How to build LLM judges that use structured output to evaluate articles at the section level with binary metrics\n",
    "- **FollowsGT Metric**: Compares generated articles against ground truth across content, flow, and structure dimensions\n",
    "- **UserIntent Metric**: Checks if articles follow the guideline and are anchored in the research (hallucination detection)\n",
    "- **Few-Shot Examples**: How to manually create labeled examples to \"train\" the LLM judges\n",
    "- **Evaluation Pipeline**: How to run experiments using Opik and the Brown agent\n",
    "\n",
    "### Wrapping Up AI Evals\n",
    "\n",
    "With this lesson, we've wrapped up the AI evals section of the course. Now we move on to other ops aspects such as deploying the AI apps and building CI/CD pipelines.\n",
    "\n",
    "### Practicing Ideas\n",
    "\n",
    "1. **Test LLM Judge Stability**: We showed the LLM Judge stability on the article generated with `gemini-pro` using the cached articles from [outputs/evals-pro](https://github.com/iusztinpaul/agentic-ai-engineering-course-data/tree/main/data/outputs/evals-pro/data). Now repeat the same process using the cached articles from the [outputs/evals-flash](https://github.com/iusztinpaul/agentic-ai-engineering-course-data/tree/main/data/outputs/evals-flash/data) directory. Run the LLM judges 5 times with all the inputs fixed. Measure the stability of the scores across runs. Are the results more stable or worse when using `flash`?\n",
    "\n",
    "2. **Compute the Alignment Score on the User Intent Metrics**: Use the Memory lesson from the validation split and manually score each section on the user intent metrics, such as article guideline adherence and research anchoring. Compare your scores with the LLM judge scores to see how aligned they are.\n",
    "\n",
    "3. **Iterate on Brown**: Make changes to the Brown AI app (prompts, model, temperature) and rerun the AI evals. See if your change improves or degrades the metrics?\n",
    "\n",
    "### Useful Resources\n",
    "\n",
    "- **Brown Package**: Explore `lessons/writing_workflow`\n",
    "- **Evaluation Script**: Check `lessons/writing_workflow/scripts/brown_run_eval.py`\n",
    "- **Configuration Examples**: Check `configs/` for different configurations\n",
    "- **Test Data**: Use `inputs/tests/` for additional testing scenarios\n",
    "\n",
    "### ðŸ’¡ Run Brown as a Standalone Python Project\n",
    "\n",
    "Remember that you can also run `brown` as a standalone Python project by going to `lessons/writing_workflow/` and following the instructions from there. We support running the evaluation within the `scripts/brown_run_eval.py` Python script.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
