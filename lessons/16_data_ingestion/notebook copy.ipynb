{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lesson 16: Initial Data Ingestion and Tooling\n",
        "\n",
        "In this lesson, we focus on building the first set of essential MCP tools for data gathering in our research agent. We'll implement tools that read article guideline files, extract web URLs programmatically, and scrape their content in parallel. This lesson demonstrates how file-based approaches can save tokens for the orchestrating agent, which only needs to process simple success or failure messages rather than large content blocks.\n",
        "\n",
        "Learning Objectives:\n",
        "- Learn how to build MCP tools that extract URLs and references from text files\n",
        "- Understand the benefits of file-based tool outputs for token efficiency\n",
        "- Implement robust web scraping tools using external services\n",
        "- Handle error cases gracefully thanks to appropriate policies in the MCP prompt instructions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup\n",
        "\n",
        "First, we define some standard Magic Python commands to autoreload Python packages whenever they change:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Set Up Python Environment\n",
        "\n",
        "To set up your Python virtual environment using `uv` and load it into the Notebook, follow the step-by-step instructions from the `Course Admin` lesson from the beginning of the course.\n",
        "\n",
        "**TL/DR:** Be sure the correct kernel pointing to your `uv` virtual environment is selected."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Configure Gemini API\n",
        "\n",
        "To run this lesson, you'll need several API keys configured:\n",
        "\n",
        "1. **Gemini API Key**, `GOOGLE_API_KEY` variable: Get your key from [Google AI Studio](https://aistudio.google.com/app/apikey).\n",
        "2. **Firecrawl API Key**, `FIRECRAWL_API_KEY` variable: Get your key from [Firecrawl](https://firecrawl.dev/). They have a free tier that allows you to scrape 500 pages, which is enough for testing the agent for free.\n",
        "3. **GitHub token (optional)**, `GITHUB_TOKEN` variable: If you want to process private GitHub repositories, you'll need a GitHub token with access to them. In case you want to test this functionality, you can get a token from [here](https://github.com/settings/personal-access-tokens). However, this is not required for the lesson, as we can easily use public repositories for explaining the functionalities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Environment variables loaded from `/Users/fabio/Desktop/course-ai-agents/.env`\n",
            "Environment variables loaded successfully.\n"
          ]
        }
      ],
      "source": [
        "from utils import env\n",
        "\n",
        "env.load(required_env_vars=[\"GOOGLE_API_KEY\", \"FIRECRAWL_API_KEY\", \"GITHUB_TOKEN\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Import Key Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply() # Allow nested async usage in notebooks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Understanding the Research Agent Workflow\n",
        "\n",
        "As we saw in the previous lesson, the research agent follows a systematic workflow for data ingestion. The MCP prompt defines a clear two-phase approach regarding the data ingestion:\n",
        "\n",
        "- **Step 1**: Extract URLs and file references from the article guidelines.\n",
        "- **Step 2**: Process all the extracted resources in parallel (local files, web URLs, GitHub repos, YouTube videos).\n",
        "\n",
        "Here's a snapshot of the MCP prompt that defines the first two steps of the workflow:\n",
        "```markdown\n",
        "1. Setup:\n",
        "\n",
        "    1.1. Explain to the user the numbered steps of the workflow. Be concise. Keep them numbered so that the user\n",
        "    can easily refer to them later.\n",
        "    \n",
        "    1.2. Ask the user for the research directory, if not provided. Ask the user if any modification is needed for the\n",
        "    workflow (e.g. running from a specific step, or adding user feedback to specific steps).\n",
        "\n",
        "    1.3 Extract the URLs from the ARTICLE_GUIDELINE_FILE with the \"extract_guidelines_urls\" tool. This tool reads the\n",
        "    ARTICLE_GUIDELINE_FILE and extracts three groups of references from the guidelines:\n",
        "    ‚Ä¢ \"github_urls\" - all GitHub links;\n",
        "    ‚Ä¢ \"youtube_videos_urls\" - all YouTube video links;\n",
        "    ‚Ä¢ \"other_urls\" - all remaining HTTP/HTTPS links;\n",
        "    ‚Ä¢ \"local_files\" - relative paths to local files mentioned in the guidelines (e.g. \"code.py\", \"src/main.py\").\n",
        "    Only extensions allowed are: \".py\", \".ipynb\", and \".md\".\n",
        "    The extracted data is saved to the GUIDELINES_FILENAMES_FILE within the NOVA_FOLDER directory.\n",
        "\n",
        "2. Process the extracted resources in parallel:\n",
        "\n",
        "    You can run the following sub-steps (2.1 to 2.4) in parallel. In a single turn, you can call all the\n",
        "    necessary tools for these steps.\n",
        "\n",
        "    2.1 Local files - run the \"process_local_files\" tool to read every file path listed under \"local_files\" in the\n",
        "    GUIDELINES_FILENAMES_FILE and copy its content into the LOCAL_FILES_FROM_RESEARCH_FOLDER subfolder within\n",
        "    NOVA_FOLDER, giving each copy an appropriate filename (path separators are replaced with underscores).\n",
        "\n",
        "    2.2 Other URL links - run the \"scrape_and_clean_other_urls\" tool to read the `other_urls` list from the\n",
        "    GUIDELINES_FILENAMES_FILE and scrape/clean them. The tool writes the cleaned markdown files inside the\n",
        "    URLS_FROM_GUIDELINES_FOLDER subfolder within NOVA_FOLDER.\n",
        "\n",
        "    2.3 GitHub URLs - run the \"process_github_urls\" tool to process the `github_urls` list from the\n",
        "    GUIDELINES_FILENAMES_FILE with gitingest and save a Markdown summary for each URL inside the\n",
        "    URLS_FROM_GUIDELINES_CODE_FOLDER subfolder within NOVA_FOLDER.\n",
        "\n",
        "    2.4 YouTube URLs - run the \"transcribe_youtube_urls\" tool to process the `youtube_videos_urls` list from the\n",
        "    GUIDELINES_FILENAMES_FILE, transcribe each video, and save the transcript as a Markdown file inside the\n",
        "    URLS_FROM_GUIDELINES_YOUTUBE_FOLDER subfolder within NOVA_FOLDER.\n",
        "        Note: Please be aware that video transcription can be a time-consuming process. For reference,\n",
        "        transcribing a 39-minute video can take approximately 4.5 minutes.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's examine the MCP tools involved in these first two steps of the workflow. As we saw in the previous lesson, the MCP tool endpoints are defined in the `src/routers/tools.py` file.\n",
        "\n",
        "Source: _research_agent_part_2/mcp_server/src/routers/tools.py_\n",
        "\n",
        "```python\n",
        "def register_mcp_tools(mcp: FastMCP) -> None:\n",
        "    \"\"\"Register all MCP tools with the server instance.\"\"\"\n",
        "    \n",
        "    # Step 1: Extract URLs and file references from guidelines\n",
        "    @mcp.tool()\n",
        "    async def extract_guidelines_urls(research_directory: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Extract URLs and local file references from article guidelines.\n",
        "        \n",
        "        Reads the ARTICLE_GUIDELINE_FILE file in the research directory and extracts:\n",
        "        - GitHub URLs\n",
        "        - Other HTTP/HTTPS URLs  \n",
        "        - Local file references (files mentioned in quotes with extensions)\n",
        "        \n",
        "        Results are saved to GUIDELINES_FILENAMES_FILE in the research directory.\n",
        "        \"\"\"\n",
        "        result = extract_guidelines_urls_tool(research_directory)\n",
        "        return result\n",
        "\n",
        "    # Step 2.1: Process local files\n",
        "    @mcp.tool()\n",
        "    async def process_local_files(research_directory: str) -> Dict[str, Any]:\n",
        "        \"\"\"Process local files referenced in the article guidelines.\"\"\"\n",
        "        result = process_local_files_tool(research_directory)\n",
        "        return result\n",
        "        \n",
        "    # Step 2.2: Scrape web URLs\n",
        "    @mcp.tool() \n",
        "    async def scrape_and_clean_other_urls(research_directory: str, concurrency_limit: int = 4) -> Dict[str, Any]:\n",
        "        \"\"\"Scrape and clean other URLs from GUIDELINES_FILENAMES_FILE.\"\"\"\n",
        "        result = await scrape_and_clean_other_urls_tool(research_directory, concurrency_limit)\n",
        "        return result\n",
        "\n",
        "    # Step 2.3: Process GitHub repositories\n",
        "    @mcp.tool()\n",
        "    async def process_github_urls(research_directory: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Process GitHub URLs from GUIDELINES_FILENAMES_FILE using gitingest.\n",
        "        \n",
        "        Reads the GUIDELINES_FILENAMES_FILE file and processes each URL listed\n",
        "        under 'github_urls' using gitingest to extract repository summaries, file trees,\n",
        "        and content. The results are saved as markdown files in the\n",
        "        URLS_FROM_GUIDELINES_CODE_FOLDER subfolder.\n",
        "        \"\"\"\n",
        "        result = await process_github_urls_tool(research_directory)\n",
        "        return result\n",
        "        \n",
        "    # Step 2.4: Transcribe YouTube videos\n",
        "    @mcp.tool()\n",
        "    async def transcribe_youtube_urls(research_directory: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Transcribe YouTube video URLs from GUIDELINES_FILENAMES_FILE using Gemini 2.5 Pro.\n",
        "        \n",
        "        Reads the GUIDELINES_FILENAMES_FILE file and processes each URL listed\n",
        "        under 'youtube_videos_urls'. Each video is transcribed, and the results are\n",
        "        saved as markdown files in the URLS_FROM_GUIDELINES_YOUTUBE_FOLDER subfolder.\n",
        "        \"\"\"\n",
        "        result = await transcribe_youtube_videos_tool(research_directory)\n",
        "        return result\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notice how this tool returns a concise summary rather than the full extracted content. We'll see the exact outputs in the next sections. This design choice has several advantages:\n",
        "\n",
        "1. **Token Efficiency**: The agent receives only essential information (counts, status, file path) rather than large content blocks.\n",
        "2. **Context Management**: Keeps the agent's context window manageable for complex workflows.\n",
        "3. **Selective Reading**: The agent can choose to read the output file only if needed for decision-making. However, the ability to read files must be implemented as a tool (or another MCP server) for the MCP client. To do this, it would be possible to add a separate MCP server to the MCP client, or to use an MCP client that has already this capability (e.g. Cursor).\n",
        "4. **Error Handling**: Clear status messages help the agent understand what succeeded or failed, and how to proceed.\n",
        "\n",
        "Let's now see how each of these MCP tools is implemented."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Extracting URLs from Guidelines\n",
        "\n",
        "The first tool in our data ingestion pipeline reads an article guideline file and programmatically extracts all URLs and file references it contains.\n",
        "\n",
        "Here's its implementation:\n",
        "\n",
        "Source: _research_agent_part_2/mcp_server/src/tools/extract_guidelines_urls_tool.py_\n",
        "\n",
        "```python\n",
        "def extract_guidelines_urls_tool(research_folder: str) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Extract URLs and local file references from the article guidelines in the research folder.\n",
        "    \n",
        "    Reads the ARTICLE_GUIDELINE_FILE file and extracts:\n",
        "    - GitHub URLs\n",
        "    - YouTube video URLs  \n",
        "    - Other HTTP/HTTPS URLs\n",
        "    - Local file references\n",
        "    \n",
        "    Results are saved to GUIDELINES_FILENAMES_FILE in the research folder.\n",
        "    \"\"\"\n",
        "    ...\n",
        "\n",
        "    # Convert to Path object\n",
        "    research_path = Path(research_folder)\n",
        "    nova_path = research_path / NOVA_FOLDER\n",
        "    guidelines_path = research_path / ARTICLE_GUIDELINE_FILE\n",
        "    ...\n",
        "    \n",
        "    # Read guidelines content\n",
        "    guidelines_content = read_file_safe(guidelines_path)\n",
        "    \n",
        "    # Extract URLs\n",
        "    urls = extract_urls(guidelines_content)\n",
        "    github_source_urls = [u for u in all_urls if \"github.com\" in u]\n",
        "    youtube_source_urls = [u for u in all_urls if \"youtube.com\" in u]\n",
        "    web_source_urls = [u for u in all_urls if \"github.com\" not in u and \"youtube.com\" not in u]\n",
        "\n",
        "    # Extract local file paths\n",
        "    local_paths = extract_local_paths(guidelines_content)\n",
        "    \n",
        "    # Prepare the extracted data structure\n",
        "    extracted_data = {\n",
        "        \"github_urls\": urls[\"github_urls\"],\n",
        "        \"youtube_videos_urls\": urls[\"youtube_videos_urls\"], \n",
        "        \"other_urls\": urls[\"other_urls\"],\n",
        "        \"local_file_paths\": local_paths,\n",
        "    }\n",
        "    \n",
        "    # Save to JSON file\n",
        "    output_path = nova_path / GUIDELINES_FILENAMES_FILE\n",
        "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(extracted_data, f, indent=2, ensure_ascii=False)\n",
        "    \n",
        "    return {\n",
        "        \"status\": \"success\",\n",
        "        \"github_sources_count\": len(urls[\"github_urls\"]),\n",
        "        \"youtube_sources_count\": len(urls[\"youtube_videos_urls\"]),\n",
        "        \"web_sources_count\": len(urls[\"other_urls\"]),\n",
        "        \"local_files_count\": len(local_paths),\n",
        "        \"output_path\": str(output_path),\n",
        "        \"message\": f\"Successfully extracted URLs from article guidelines in '{research_folder}'. \"\n",
        "                  f\"Found {len(urls['github_urls'])} GitHub URLs, {len(urls['youtube_videos_urls'])} YouTube videos URLs, \"\n",
        "                  f\"{len(urls['other_urls'])} other URLs, and {len(local_paths)} local file references. \"\n",
        "                  f\"Results saved to: {output_path}\"\n",
        "    }\n",
        "```\n",
        "\n",
        "The code:\n",
        "1. Identifies the location of the article guidelines file,\n",
        "2. Uses the `extract_urls` function to extract the URLs from the guidelines content,\n",
        "3. Extracts local file paths with the `extract_local_paths` function,\n",
        "4. Saves the extracted data to a JSON file, and\n",
        "5. Returns a summary of the results.\n",
        "\n",
        "Let's now see how the URLs are extracted from the guidelines content."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 URLs Extraction\n",
        "\n",
        "The `extract_urls` function from the guideline extractions handler finds all HTTP/HTTPS URLs:\n",
        "\n",
        "Source: _research_agent_part_2/mcp_server/src/app/guideline_extractions_handler.py_\n",
        "\n",
        "```python\n",
        "def extract_urls(text: str) -> list[str]:\n",
        "    \"\"\"Extract all HTTP/HTTPS URLs from the given text.\"\"\"\n",
        "    url_pattern = re.compile(r\"https?://[^\\s)>\\\"',]+\")\n",
        "    return url_pattern.findall(text)\n",
        "```\n",
        "\n",
        "This regular expression pattern:\n",
        "- `https?://` - Matches both HTTP and HTTPS protocols\n",
        "- `[^\\s)>\\\"',]+` - Matches any characters except whitespace, closing parentheses, greater-than signs, quotes, or commas\n",
        "- This ensures URLs are extracted cleanly from markdown links, plain text, and various formatting contexts\n",
        "\n",
        "After extraction, the URLs are categorized by domain to enable specialized processing for each type of content source."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Local File Path Extraction\n",
        "\n",
        "The `extract_local_paths` function is used to extract local file paths from the guidelines content, and it is defined in the `app/guideline_extractions_handler.py` file.\n",
        "\n",
        "We won't show its code here as it's not interesting for teaching how AI agents work. You can check how it works in the code if you're curious. You only need to know the following:\n",
        "- It only looks for specific file extensions (`.py`, `.ipynb`, `.md`)\n",
        "- It excludes anything that looks like a URL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.3 Running the Tool\n",
        "\n",
        "Let's test this tool programmatically to get an idea of its output:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/fabio/Desktop/course-ai-agents/.venv/lib/python3.12/site-packages/pydantic/_internal/_fields.py:198: UserWarning: Field name \"schema\" in \"FewShotExampleStructuredOutputCompliance\" shadows an attribute in parent \"BaseModel\"\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'status': 'success', 'github_sources_count': 1, 'youtube_sources_count': 2, 'web_sources_count': 6, 'local_files_count': 0, 'output_path': '/Users/fabio/Desktop/course-ai-agents/lessons/research_agent_part_2/data/sample_research_folder/.nova/guidelines_filenames.json', 'message': \"Successfully extracted URLs from article guidelines in '/Users/fabio/Desktop/course-ai-agents/lessons/research_agent_part_2/data/sample_research_folder'. Found 1 GitHub URLs, 2 YouTube videos URLs, 6 other URLs, and 0 local file references. Results saved to: /Users/fabio/Desktop/course-ai-agents/lessons/research_agent_part_2/data/sample_research_folder/.nova/guidelines_filenames.json\"}\n"
          ]
        }
      ],
      "source": [
        "from research_agent_part_2.mcp_server.src.tools import extract_guidelines_urls_tool\n",
        "\n",
        "# Update this path to your actual sample research folder\n",
        "research_folder = \"/path/to/research_folder\"\n",
        "result = extract_guidelines_urls_tool(research_folder=research_folder)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The output will show a structured summary like:\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"status\": \"success\",\n",
        "  \"github_sources_count\": 1,\n",
        "  \"youtube_sources_count\": 2, \n",
        "  \"web_sources_count\": 6,\n",
        "  \"local_files_count\": 0,\n",
        "  \"output_path\": \"/path/to/research_folder/.nova/guidelines_filenames.json\",\n",
        "  \"message\": \"Successfully extracted URLs from article guidelines in '/path/to/research_folder'. Found 1 GitHub URLs, 2 YouTube videos URLs, 6 other URLs, and 0 local file references. Results saved to: /path/to/research_folder/.nova/guidelines_filenames.json\"\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With this summary, the agent can understand if everything worked fine or not, and how to proceed in case of errors (e.g. by asking the user for help)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Processing Local Files\n",
        "\n",
        "The `process_local_files_tool` tool handles local file references found in the guidelines. It copies referenced files to an organized folder structure and formats them for LLM consumption.\n",
        "\n",
        "Source: _research_agent_part_2/mcp_server/src/tools/process_local_files_tool.py_\n",
        "\n",
        "```python\n",
        "def process_local_files_tool(research_directory: str) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Process local files referenced in the article guidelines.\n",
        "\n",
        "    Reads the guidelines JSON file and copies each referenced local file\n",
        "    to the local files subfolder. Path separators in filenames are\n",
        "    replaced with underscores to avoid creating nested folders.\n",
        "\n",
        "    Args:\n",
        "        research_directory: Path to the research directory containing the guidelines JSON file\n",
        "\n",
        "    Returns:\n",
        "        Dict with status, processing results, and file paths\n",
        "    \"\"\"\n",
        "    ...\n",
        "\n",
        "    # Convert to Path object\n",
        "    research_path = Path(research_directory)\n",
        "    nova_path = research_path / NOVA_FOLDER\n",
        "\n",
        "    # Look for GUIDELINES_FILENAMES_FILE\n",
        "    metadata_path = nova_path / GUIDELINES_FILENAMES_FILE\n",
        "\n",
        "    ...\n",
        "\n",
        "    # Load JSON metadata\n",
        "    data = json.loads(metadata_path.read_text(encoding=\"utf-8\"))\n",
        "    local_files = data.get(\"local_files\", [])\n",
        "\n",
        "    if not local_files:\n",
        "        return {\n",
        "            \"status\": \"success\",\n",
        "            \"message\": f\"No local files to process in research folder '{research_directory}'.\",\n",
        "            \"files_processed\": 0,\n",
        "            \"files_total\": 0,\n",
        "            \"warnings\": [],\n",
        "            \"errors\": [],\n",
        "        }\n",
        "\n",
        "    # Create destination folder if it doesn't exist\n",
        "    dest_folder = nova_path / LOCAL_FILES_FROM_RESEARCH_FOLDER\n",
        "    dest_folder.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    processed = 0\n",
        "    warnings = []\n",
        "    errors = []\n",
        "    processed_files = []\n",
        "\n",
        "    # Initialize notebook converter for .ipynb files\n",
        "    notebook_converter = NotebookToMarkdownConverter(include_outputs=True, include_metadata=False)\n",
        "\n",
        "    for rel_path in local_files:\n",
        "        # Local files are relative to the research folder\n",
        "        src_path = research_path / rel_path\n",
        "        ...\n",
        "\n",
        "        # Sanitize destination filename (replace path separators with underscores)\n",
        "        dest_name = rel_path.replace(\"/\", \"_\").replace(\"\\\\\", \"_\")\n",
        "\n",
        "        try:\n",
        "            # Handle .ipynb files specially by converting to markdown\n",
        "            if src_path.suffix.lower() == \".ipynb\":\n",
        "                # Convert .ipynb to .md extension for destination\n",
        "                dest_name = dest_name.rsplit(\".ipynb\", 1)[0] + \".md\"\n",
        "                dest_path = dest_folder / dest_name\n",
        "\n",
        "                # Convert notebook to markdown string\n",
        "                markdown_content = notebook_converter.convert_notebook_to_string(src_path)\n",
        "\n",
        "                # Write markdown content to destination\n",
        "                dest_path.write_text(markdown_content, encoding=\"utf-8\")\n",
        "            else:\n",
        "                # For other file types, copy as before\n",
        "                dest_path = dest_folder / dest_name\n",
        "                shutil.copy2(src_path, dest_path)\n",
        "\n",
        "            processed += 1\n",
        "            processed_files.append(dest_name)\n",
        "        except Exception as e:\n",
        "            errors.append(f\"Failed to process {rel_path}: {str(e)}\")\n",
        "\n",
        "    # Build result message using the dedicated function\n",
        "    result_message = build_result_message(research_directory, processed, local_files, dest_folder, warnings, errors)\n",
        "\n",
        "    return {\n",
        "        \"status\": \"success\" if processed > 0 else \"warning\",\n",
        "        \"files_processed\": processed,\n",
        "        \"files_total\": len(local_files),\n",
        "        \"processed_files\": processed_files,\n",
        "        \"warnings\": warnings,\n",
        "        \"errors\": errors,\n",
        "        \"output_directory\": str(dest_folder.resolve()),\n",
        "        \"message\": result_message,\n",
        "    }\n",
        "```\n",
        "\n",
        "This local file processing tool looks for the local files extracted by the `extract_guidelines_urls_tool` tool and copies them to an organized folder structure. It distinguishes between different file types (where it copy its content as is) and notebooks (where it converts the content to markdown).\n",
        "\n",
        "The `NotebookToMarkdownConverter` class can be found in the `app/notebook_handler.py` file. We won't show its code here as it's not interesting for teaching how AI agents work. You can check how it works in the code if you're curious. You only need to know that it keeps both markdown cells and code cells, and it also keeps the outputs of the executed cells truncated to a maximum amount of characters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Web Scraping with Firecrawl and LLM Cleaning\n",
        "\n",
        "This is the most complex tool in our data ingestion pipeline. It scrapes web URLs and cleans the content using both external services and LLM processing. Here's its implementation:\n",
        "\n",
        "Source: _research_agent_part_2/mcp_server/src/tools/scrape_and_clean_other_urls_tool.py_\n",
        "\n",
        "```python\n",
        "async def scrape_and_clean_other_urls_tool(research_directory: str, concurrency_limit: int = 4) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Scrape and clean other URLs from guidelines file in the research folder.\n",
        "    \n",
        "    Reads the guidelines file and scrapes/cleans each URL listed\n",
        "    under 'other_urls'. The cleaned markdown content is saved to the\n",
        "    URLS_FROM_GUIDELINES_FOLDER subfolder with appropriate filenames.\n",
        "    \"\"\"    \n",
        "    # Convert to Path object\n",
        "    research_path = Path(research_directory)\n",
        "    nova_path = research_path / NOVA_FOLDER\n",
        "    \n",
        "    # Look for GUIDELINES_FILENAMES_FILE file\n",
        "    guidelines_file_path = nova_path / GUIDELINES_FILENAMES_FILE\n",
        "    \n",
        "    # Read the guidelines filenames file\n",
        "    guidelines_data = json.loads(read_file_safe(guidelines_file_path))\n",
        "    urls_to_scrape = guidelines_data.get(\"other_urls\", [])\n",
        "    \n",
        "    if not urls_to_scrape:\n",
        "        return {\n",
        "            \"status\": \"success\",\n",
        "            \"urls_processed\": [],\n",
        "            \"urls_failed\": [],\n",
        "            \"total_urls\": 0,\n",
        "            \"successful_urls_count\": 0,\n",
        "            \"failed_urls_count\": 0,\n",
        "            \"output_directory\": str(nova_path / URLS_FROM_GUIDELINES_FOLDER),\n",
        "            \"message\": \"No other URLs found to scrape in the guidelines filenames file.\"\n",
        "        }\n",
        "    \n",
        "    # Read article guidelines for context\n",
        "    guidelines_path = research_path / ARTICLE_GUIDELINE_FILE\n",
        "    guidelines_content = read_file_safe(guidelines_path)\n",
        "    \n",
        "    # Scrape URLs concurrently\n",
        "    completed_results = await scrape_urls_concurrently(\n",
        "        urls_to_scrape, \n",
        "        concurrency_limit, \n",
        "        guidelines_content\n",
        "    )\n",
        "    \n",
        "    # Write results to files\n",
        "    output_dir = nova_path / URLS_FROM_GUIDELINES_FOLDER\n",
        "    saved_files, successful_scrapes = write_scraped_results_to_files(completed_results, output_dir)\n",
        "    \n",
        "    # Calculate statistics\n",
        "    failed_urls = [res[\"url\"] for res in completed_results if not res.get(\"success\", False)]\n",
        "    successful_urls = [res[\"url\"] for res in completed_results if res.get(\"success\", False)]\n",
        "    \n",
        "    return {\n",
        "        \"status\": \"success\",\n",
        "        \"urls_processed\": successful_urls,\n",
        "        \"urls_failed\": failed_urls,\n",
        "        \"total_urls\": len(urls_to_scrape),\n",
        "        \"successful_urls_count\": successful_scrapes,\n",
        "        \"failed_urls_count\": len(failed_urls),\n",
        "        \"output_directory\": str(output_dir),\n",
        "        \"message\": f\"Successfully processed {successful_scrapes}/{len(urls_to_scrape)} URLs. \"\n",
        "                  f\"Results saved to: {output_dir}\"\n",
        "    }\n",
        "```\n",
        "\n",
        "Here's how it works:\n",
        "1. It looks for the URLs to scrape in the guidelines filenames file.\n",
        "2. It uses the `scrape_urls_concurrently` function to scrape the URLs concurrently using Firecrawl and clean the content using an LLM.\n",
        "3. It saves the cleaned content to the `URLS_FROM_GUIDELINES_FOLDER` folder.\n",
        "4. It returns a summary of the results.\n",
        "\n",
        "Let's see in more detail how the `scrape_urls_concurrently` function works."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.2 The Two-Stage Cleaning Process\n",
        "\n",
        "The scraping process uses a sophisticated two-stage approach:\n",
        "\n",
        "1. **Firecrawl for Initial Scraping**: Firecrawl is a specialized service that handles the complexity of modern web scraping, including:\n",
        "   - JavaScript rendering\n",
        "   - Dynamic content loading  \n",
        "   - Anti-bot protection\n",
        "   - Clean markdown extraction\n",
        "\n",
        "2. **LLM for Content Refinement**: After Firecrawl extracts the raw content, an LLM (Gemini 2.5 Flash) further cleans and structures the content by:\n",
        "   - Removing irrelevant sections (ads, navigation, footers)\n",
        "   - Focusing on content relevant to the article guidelines\n",
        "   - Maintaining proper markdown formatting\n",
        "   - Preserving important links and references\n",
        "\n",
        "Here's the implementation of the `scrape_urls_concurrently` function:\n",
        "\n",
        "Source: _research_agent_part_2/mcp_server/src/app/scraping_handler.py_\n",
        "\n",
        "```python\n",
        "async def scrape_urls_concurrently(urls: List[str], concurrency_limit: int, guidelines_content: str) -> List[dict]:\n",
        "    \"\"\"\n",
        "    Scrape multiple URLs concurrently using Firecrawl and clean with LLM.\n",
        "    \n",
        "    Args:\n",
        "        urls: List of URLs to scrape\n",
        "        concurrency_limit: Maximum concurrent requests\n",
        "        guidelines_content: Article guidelines for context-aware cleaning\n",
        "        \n",
        "    Returns:\n",
        "        List of scraping results with success status and cleaned content\n",
        "    \"\"\"\n",
        "    semaphore = asyncio.Semaphore(concurrency_limit)\n",
        "    \n",
        "    async def scrape_single_url(url: str) -> dict:\n",
        "        async with semaphore:\n",
        "            try:\n",
        "                # Stage 1: Use Firecrawl to scrape the URL\n",
        "                firecrawl_result = await scrape_with_firecrawl(url)\n",
        "                \n",
        "                if not firecrawl_result.get(\"success\", False):\n",
        "                    return {\n",
        "                        \"url\": url,\n",
        "                        \"success\": False,\n",
        "                        \"title\": \"\",\n",
        "                        \"markdown\": f\"Failed to scrape {url}: {firecrawl_result.get('error', 'Unknown error')}\"\n",
        "                    }\n",
        "                \n",
        "                raw_markdown = firecrawl_result.get(\"markdown\", \"\")\n",
        "                title = firecrawl_result.get(\"title\", \"\")\n",
        "                \n",
        "                # Stage 2: Clean the content with LLM\n",
        "                cleaned_markdown = await clean_scraped_content_with_llm(\n",
        "                    raw_markdown, \n",
        "                    url, \n",
        "                    guidelines_content\n",
        "                )\n",
        "                \n",
        "                return {\n",
        "                    \"url\": url,\n",
        "                    \"success\": True,\n",
        "                    \"title\": title,\n",
        "                    \"markdown\": cleaned_markdown\n",
        "                }\n",
        "                \n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error scraping {url}: {e}\")\n",
        "                return {\n",
        "                    \"url\": url,\n",
        "                    \"success\": False,\n",
        "                    \"title\": \"\",\n",
        "                    \"markdown\": f\"Error scraping {url}: {str(e)}\"\n",
        "                }\n",
        "    \n",
        "    # Execute all scraping tasks concurrently\n",
        "    tasks = [scrape_single_url(url) for url in urls]\n",
        "    results = await asyncio.gather(*tasks, return_exceptions=True)\n",
        "    \n",
        "    return [r for r in results if isinstance(r, dict)]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The individual URL processing combines Firecrawl scraping with LLM cleaning:\n",
        "\n",
        "```python\n",
        "async def scrape_and_clean(url: str, article_guidelines: str, firecrawl_app: AsyncFirecrawl, chat_model) -> dict:\n",
        "    \"\"\"Scrape and clean a single URL, returning dict with url, title, markdown.\"\"\"\n",
        "    scraped = await scrape_url(url, firecrawl_app)\n",
        "    status_marker = \"‚úì\" if scraped[\"success\"] else \"‚úó\"\n",
        "    number_of_tokens = chat_model.get_num_tokens(scraped[\"markdown\"])\n",
        "    token_info = f\" ({number_of_tokens} tokens)\"\n",
        "    logger.debug(f\"üì• Scraped: {url} {status_marker}{token_info}\")\n",
        "    if scraped[\"success\"]:\n",
        "        cleaned_md = await clean_markdown(scraped[\"markdown\"], article_guidelines, url, chat_model)\n",
        "        scraped[\"markdown\"] = cleaned_md\n",
        "        number_of_tokens = chat_model.get_num_tokens(scraped[\"markdown\"])\n",
        "        token_info = f\" (tokens reduced to {number_of_tokens})\"\n",
        "        logger.debug(f\"üßº Cleaned: {url} {token_info}\")\n",
        "    return scraped\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The Firecrawl scraping function handles the complexity of modern web scraping:\n",
        "\n",
        "```python\n",
        "async def scrape_url(url: str, firecrawl_app: AsyncFirecrawl) -> dict:\n",
        "    \"\"\"\n",
        "    Scrape a URL using Firecrawl with retries and return a dict with url, title, markdown.\n",
        "\n",
        "    Uses maxAge=1 week for 500% faster scraping by leveraging cached data when available.\n",
        "    This optimization significantly improves performance for documentation, articles, and\n",
        "    relatively static content while maintaining freshness within acceptable limits.\n",
        "    \"\"\"\n",
        "    max_retries = 3\n",
        "    base_delay = 5  # seconds\n",
        "    timeout_seconds = 120000  # 2 minutes timeout per request\n",
        "\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            # Add timeout to individual Firecrawl request\n",
        "            # Use maxAge=1 week for 500% faster scraping with cached data\n",
        "            res = await firecrawl_app.scrape(\n",
        "                url, formats=[\"markdown\"], maxAge=MAX_AGE_ONE_WEEK, timeout=timeout_seconds\n",
        "            )\n",
        "            title = res.metadata.title if res and res.metadata and res.metadata.title else \"N/A\"\n",
        "            markdown_content = res.markdown if res and res.markdown else \"\"\n",
        "            return {\"url\": url, \"title\": title, \"markdown\": markdown_content, \"success\": True}\n",
        "        except asyncio.TimeoutError:\n",
        "            error_msg = f\"‚ö†Ô∏è Firecrawl request timed out after {timeout_seconds}s for {url}\"\n",
        "            logger.warning(f\"{error_msg} (attempt {attempt + 1}/{max_retries})\")\n",
        "            if attempt < max_retries - 1:\n",
        "                delay = base_delay * (2**attempt)\n",
        "                logger.warning(f\"Retrying in {delay}s...\")\n",
        "                await asyncio.sleep(delay)\n",
        "            else:\n",
        "                logger.error(f\"{error_msg} after {max_retries} attempts\")\n",
        "                return {\n",
        "                    \"url\": url,\n",
        "                    \"title\": \"Scraping Timeout\",\n",
        "                    \"markdown\": f\"{error_msg} after {max_retries} attempts.\",\n",
        "                    \"success\": False,\n",
        "                }\n",
        "        except Exception as e:\n",
        "            # print the error with traceback\n",
        "            logger.error(f\"Error scraping {url}: {e}\", exc_info=True)\n",
        "\n",
        "            if attempt < max_retries - 1:\n",
        "                delay = base_delay * (2**attempt)\n",
        "                logger.warning(f\"‚ö†Ô∏è Error scraping {url} (attempt {attempt + 1}/{max_retries}). Retrying in {delay}s...\")\n",
        "                await asyncio.sleep(delay)\n",
        "            else:\n",
        "                msg = f\"‚ö†Ô∏è Error scraping {url} after {max_retries} attempts: {e}\"\n",
        "                logger.error(msg, exc_info=True)\n",
        "                return {\n",
        "                    \"url\": url,\n",
        "                    \"title\": \"Scraping Failed\",\n",
        "                    \"markdown\": msg,\n",
        "                    \"success\": False,\n",
        "                }\n",
        "    \n",
        "    return {\n",
        "        \"url\": url,\n",
        "        \"title\": \"Scraping Failed\",\n",
        "        \"markdown\": f\"‚ö†Ô∏è Error scraping {url} after {max_retries} attempts.\",\n",
        "        \"success\": False,\n",
        "    }\n",
        "```\n",
        "\n",
        "The LLM cleaning process is handled by the `clean_markdown` function:\n",
        "\n",
        "Source: _research_agent_part_2/mcp_server/src/app/scraping_handler.py_\n",
        "\n",
        "```python\n",
        "async def clean_markdown(\n",
        "    markdown_content: str, article_guidelines: str, url_for_log: str, chat_model: BaseChatModel\n",
        ") -> str:\n",
        "    \"\"\"Clean markdown content via LLM and convert image syntax to URLs.\"\"\"\n",
        "    if not markdown_content.strip():\n",
        "        return markdown_content\n",
        "\n",
        "    prompt_text = PROMPT_CLEAN_MARKDOWN.format(article_guidelines=article_guidelines, markdown_content=markdown_content)\n",
        "    timeout_seconds = 180  # 3 minutes timeout for LLM call\n",
        "\n",
        "    try:\n",
        "        # Add timeout to LLM API call\n",
        "        response = await asyncio.wait_for(chat_model.ainvoke(prompt_text), timeout=timeout_seconds)\n",
        "        cleaned_content = response.content if hasattr(response, \"content\") else str(response)\n",
        "\n",
        "        if isinstance(cleaned_content, list):\n",
        "            cleaned_content = \"\".join(str(part) for part in cleaned_content)\n",
        "\n",
        "        # Post-process: convert markdown images to just URLs\n",
        "        cleaned_content = convert_markdown_images_to_urls(cleaned_content)\n",
        "\n",
        "        return cleaned_content\n",
        "    except asyncio.TimeoutError:\n",
        "        logger.error(f\"LLM API call timed out after {timeout_seconds}s for {url_for_log}. Using original content.\")\n",
        "        return markdown_content\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error cleaning markdown for {url_for_log}: {e}. Using original content.\", exc_info=True)\n",
        "        return markdown_content\n",
        "```\n",
        "\n",
        "This function demonstrates several important patterns:\n",
        "\n",
        "1. **Context-Aware Cleaning**: Uses the article guidelines as context to help the LLM understand what content is relevant\n",
        "2. **Robust Error Handling**: Falls back to original content if LLM processing fails\n",
        "3. **Timeout Management**: Prevents hanging on slow LLM responses\n",
        "4. **Post-Processing**: Converts markdown image syntax to plain URLs for better LLM consumption\n",
        "\n",
        "The cleaning process significantly reduces token count while preserving the most relevant information for research purposes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Key features of the scraping architecture:\n",
        "\n",
        "1. **Concurrency Control**: Uses semaphores to limit concurrent requests and respect API limits\n",
        "2. **Retry Logic**: Implements exponential backoff for failed requests\n",
        "3. **Caching Optimization**: Uses week-long caching for 500% performance improvement on static content\n",
        "4. **Comprehensive Error Handling**: Gracefully handles timeouts, network errors, and API failures\n",
        "5. **Token Tracking**: Monitors content size before and after cleaning to show efficiency gains\n",
        "6. **Status Logging**: Provides detailed logging for debugging and monitoring"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.3 Why Use External Scraping Services?\n",
        "\n",
        "Web scraping is notoriously complex due to:\n",
        "\n",
        "- **Dynamic Content**: Modern websites heavily use JavaScript\n",
        "- **Anti-Bot Measures**: CAPTCHAs, rate limiting, IP blocking\n",
        "- **Varied Formats**: Inconsistent HTML structures across sites\n",
        "- **Performance Issues**: Slow loading, timeouts, redirects\n",
        "\n",
        "Rather than building a robust scraper from scratch (which would require significant effort and still fall short), using a specialized service like Firecrawl allows us to:\n",
        "\n",
        "- Focus on our core research logic\n",
        "- Get reliable results across diverse websites  \n",
        "- Benefit from ongoing improvements to the scraping infrastructure\n",
        "- Handle edge cases that would be time-consuming to solve ourselves\n",
        "\n",
        "Let's test the scraping tool:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from research_agent_part_2.mcp_server.src.tools import scrape_and_clean_other_urls_tool\n",
        "\n",
        "# Test the scraping tool\n",
        "result = await scrape_and_clean_other_urls_tool(research_directory=research_folder, concurrency_limit=2)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Step 2: Processing GitHub URLs\n",
        "\n",
        "For GitHub repositories, we use a different approach optimized for code analysis.\n",
        "\n",
        "### 6.1 Using GitIngest for Repository Processing\n",
        "\n",
        "The `process_github_urls_tool` leverages the `gitingest` library to extract comprehensive information from GitHub repositories, making code and documentation available for research purposes.\n",
        "\n",
        "Source: _research_agent_part_2/mcp_server/src/tools/process_github_urls_tool.py_\n",
        "\n",
        "```python\n",
        "async def process_github_urls_tool(research_directory: str) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Process GitHub URLs from guidelines file in the research folder.\n",
        "\n",
        "    Reads the guidelines file and processes each URL listed\n",
        "    under 'github_urls' using gitingest to extract repository summaries, file trees,\n",
        "    and content. The results are saved as markdown files in the\n",
        "    URLS_FROM_GUIDELINES_CODE_FOLDER subfolder.\n",
        "\n",
        "    Args:\n",
        "        research_directory: Path to the research folder containing the guidelines file\n",
        "\n",
        "    Returns:\n",
        "        Dict with status, processing results, and file paths\n",
        "    \"\"\"\n",
        "    logger.debug(f\"Processing GitHub URLs from research folder: {research_directory}\")\n",
        "\n",
        "    # Convert to Path object\n",
        "    research_path = Path(research_directory)\n",
        "    nova_path = research_path / NOVA_FOLDER\n",
        "\n",
        "    # Validate folders and files\n",
        "    validate_research_folder(research_path)\n",
        "\n",
        "    # Ensure the NOVA_FOLDER directory exists\n",
        "    nova_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Look for GUIDELINES_FILENAMES_FILE file\n",
        "    metadata_path = nova_path / GUIDELINES_FILENAMES_FILE\n",
        "\n",
        "    # Validate the guidelines filenames file\n",
        "    validate_guidelines_filenames_file(metadata_path)\n",
        "\n",
        "    # Read the guidelines JSON file\n",
        "    try:\n",
        "        data = json.loads(metadata_path.read_text(encoding=\"utf-8\"))\n",
        "    except (IOError, OSError, json.JSONDecodeError) as e:\n",
        "        msg = f\"Error reading {GUIDELINES_FILENAMES_FILE}: {str(e)}\"\n",
        "        logger.error(msg, exc_info=True)\n",
        "        raise ValueError(msg) from e\n",
        "\n",
        "    # Get the github_urls list\n",
        "    github_urls: list[str] = data.get(\"github_urls\", [])\n",
        "\n",
        "    if not github_urls:\n",
        "        return {\n",
        "            \"status\": \"success\",\n",
        "            \"message\": f\"No GitHub URLs found in {GUIDELINES_FILENAMES_FILE} in '{research_directory}'\",\n",
        "            \"urls_processed\": 0,\n",
        "            \"urls_total\": 0,\n",
        "            \"files_saved\": 0,\n",
        "        }\n",
        "\n",
        "    # Prepare output directory\n",
        "    dest_folder = nova_path / URLS_FROM_GUIDELINES_CODE_FOLDER\n",
        "    dest_folder.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    logger.debug(f\"Processing {len(github_urls)} GitHub URLs...\")\n",
        "\n",
        "    # Process GitHub URLs sequentially\n",
        "    success_count = 0\n",
        "    for url in github_urls:\n",
        "        try:\n",
        "            result = await process_github_url(url, dest_folder, settings.github_token.get_secret_value())\n",
        "            if result:\n",
        "                success_count += 1\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error processing GitHub URL {url}: {e}\")\n",
        "            continue\n",
        "\n",
        "    return {\n",
        "        \"status\": \"success\" if success_count > 0 else \"warning\",\n",
        "        \"urls_processed\": success_count,\n",
        "        \"urls_total\": len(github_urls),\n",
        "        \"files_saved\": success_count,\n",
        "        \"output_directory\": str(dest_folder.resolve()),\n",
        "        \"message\": (\n",
        "            f\"Processed {success_count}/{len(github_urls)} GitHub URLs from {GUIDELINES_FILENAMES_FILE} \"\n",
        "            f\"in '{research_directory}'. Saved markdown summaries to {URLS_FROM_GUIDELINES_CODE_FOLDER} folder.\"\n",
        "        ),\n",
        "    }\n",
        "```\n",
        "\n",
        "Key features of the GitHub processing tool:\n",
        "\n",
        "Source: _research_agent_part_2/mcp_server/src/tools/process_github_urls_tool.py_\n",
        "\n",
        "The `process_github_urls_tool` uses the `gitingest` library to:\n",
        "\n",
        "1. **Sequential Processing**: Unlike web scraping, GitHub processing is done sequentially to respect API rate limits\n",
        "2. **Token Authentication**: Uses GitHub tokens for accessing private repositories when available\n",
        "3. **Error Resilience**: Continues processing other URLs even if one fails\n",
        "4. **Comprehensive Output**: Each repository generates a detailed markdown file with:\n",
        "   - Repository metadata and structure\n",
        "   - File trees showing project organization\n",
        "   - Key source files with syntax highlighting\n",
        "   - Converted notebook content in LLM-friendly format\n",
        "   - Documentation and README files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from research_agent_part_2.mcp_server.src.tools import process_github_urls_tool\n",
        "\n",
        "# Test GitHub URL processing\n",
        "result = process_github_urls_tool(research_directory=research_folder)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `gitingest` library handles the complexity of:\n",
        "- Cloning repositories efficiently\n",
        "- Parsing different file types appropriately\n",
        "- Converting Jupyter notebooks to markdown\n",
        "- Organizing content by importance and relevance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Step 2: YouTube Video Transcription\n",
        "\n",
        "For YouTube videos referenced in guidelines, we use Gemini's multimodal capabilities.\n",
        "\n",
        "### 7.1 Gemini-Based Video Transcription\n",
        "\n",
        "Source: _research_agent_part_2/mcp_server/src/tools/transcribe_youtube_urls_tool.py_\n",
        "\n",
        "The `transcribe_youtube_videos_tool` leverages Gemini's multimodal capabilities to process video content directly and generate structured transcripts for research purposes.\n",
        "\n",
        "Source: _research_agent_part_2/mcp_server/src/tools/transcribe_youtube_videos_tool.py_\n",
        "\n",
        "```python\n",
        "async def transcribe_youtube_videos_tool(research_directory: str) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Transcribe YouTube video URLs from GUIDELINES_JSON_FILE using Gemini 2.5 Pro.\n",
        "\n",
        "    Reads the GUIDELINES_JSON_FILE file and processes each URL listed\n",
        "    under 'youtube_videos_urls'. Each video is transcribed, and the results are\n",
        "    saved as markdown files in the URLS_FROM_GUIDELINES_YOUTUBE_FOLDER subfolder.\n",
        "\n",
        "    Args:\n",
        "        research_directory: Path to the research directory containing GUIDELINES_JSON_FILE\n",
        "\n",
        "    Returns:\n",
        "        Dict with status, processing results, and file paths\n",
        "    \"\"\"\n",
        "    logger.debug(f\"Starting transcription of YouTube videos from {research_directory}\")\n",
        "\n",
        "    research_path = Path(research_directory)\n",
        "    nova_path = research_path / NOVA_FOLDER\n",
        "\n",
        "    # Validate folders and files\n",
        "    validate_research_folder(research_path)\n",
        "\n",
        "    # Look for GUIDELINES_FILENAMES_FILE\n",
        "    metadata_path = nova_path / GUIDELINES_FILENAMES_FILE\n",
        "\n",
        "    # Validate the guidelines filenames file\n",
        "    validate_guidelines_filenames_file(metadata_path)\n",
        "\n",
        "    try:\n",
        "        data = json.loads(metadata_path.read_text(encoding=\"utf-8\"))\n",
        "    except (IOError, OSError, json.JSONDecodeError) as e:\n",
        "        msg = f\"Error reading {GUIDELINES_FILENAMES_FILE}: {str(e)}\"\n",
        "        logger.error(msg, exc_info=True)\n",
        "        raise ValueError(msg) from e\n",
        "\n",
        "    youtube_urls: list[str] = data.get(\"youtube_videos_urls\", [])\n",
        "\n",
        "    if not youtube_urls:\n",
        "        return {\n",
        "            \"status\": \"success\",\n",
        "            \"videos_processed\": 0,\n",
        "            \"videos_total\": 0,\n",
        "            \"message\": f\"No YouTube URLs found in {GUIDELINES_FILENAMES_FILE} in '{research_directory}'\",\n",
        "        }\n",
        "\n",
        "    dest_folder = nova_path / URLS_FROM_GUIDELINES_YOUTUBE_FOLDER\n",
        "    dest_folder.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    logger.debug(f\"Processing {len(youtube_urls)} YouTube URL(s)...\")\n",
        "\n",
        "    semaphore = asyncio.Semaphore(YOUTUBE_TRANSCRIPTION_MAX_CONCURRENT_REQUESTS)\n",
        "    tasks = [process_youtube_url(url, dest_folder, semaphore) for url in youtube_urls]\n",
        "    await asyncio.gather(*tasks)\n",
        "\n",
        "    return {\n",
        "        \"status\": \"success\",\n",
        "        \"videos_processed\": len(youtube_urls),\n",
        "        \"videos_total\": len(youtube_urls),\n",
        "        \"output_directory\": str(dest_folder.resolve()),\n",
        "        \"message\": (\n",
        "            f\"Processed {len(youtube_urls)} YouTube URLs from {GUIDELINES_FILENAMES_FILE} \"\n",
        "            f\"in '{research_directory}'. Saved transcriptions to {dest_folder.name} folder.\"\n",
        "        ),\n",
        "    }\n",
        "```\n",
        "\n",
        "Key features of the YouTube transcription tool:\n",
        "\n",
        "1. **Concurrent Processing**: Uses a semaphore to limit concurrent requests while maintaining efficiency\n",
        "2. **Gemini Integration**: Leverages Gemini 2.0 Flash's ability to process video content directly\n",
        "3. **Structured Output**: Generates organized transcripts with:\n",
        "   - Video metadata (title, duration, channel)\n",
        "   - Timestamped sections for easy navigation\n",
        "   - Key points and topics identified\n",
        "   - Content relevant to the article guidelines\n",
        "4. **Rate Limiting**: Respects API limits with `YOUTUBE_TRANSCRIPTION_MAX_CONCURRENT_REQUESTS`\n",
        "\n",
        "The transcription process:\n",
        "1. **Extracts video content** using Gemini's multimodal input\n",
        "2. **Generates structured transcripts** with timestamps and key points  \n",
        "3. **Identifies relevant segments** based on article guidelines\n",
        "4. **Formats for research use** with clear section breaks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from research_agent_part_2.mcp_server.src.tools import transcribe_youtube_videos_tool\n",
        "\n",
        "# Test YouTube transcription (note: this can be time-consuming)\n",
        "result = await transcribe_youtube_videos_tool(research_directory=research_folder)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Note**: Video transcription is time-intensive. A 39-minute video typically takes about 4.5 minutes to process. The tool processes videos concurrently but with controlled concurrency to respect API limits and avoid overwhelming the service."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Running the Full Agent with MCP Prompt\n",
        "\n",
        "Now let's see how these tools work together in the complete research workflow using the MCP client."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from research_agent_part_2.mcp_client.src.client import main as client_main\n",
        "import sys\n",
        "\n",
        "async def run_client():\n",
        "    _argv_backup = sys.argv[:]\n",
        "    sys.argv = [\"client\"]\n",
        "    try:\n",
        "        await client_main()\n",
        "    finally:\n",
        "        sys.argv = _argv_backup\n",
        "\n",
        "# Start client with in-memory server \n",
        "await run_client()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Once the client is running, you can:\n",
        "\n",
        "1. **Start the workflow**: Type `/prompt/full_research_instructions_prompt` to load the complete research workflow\n",
        "2. **Provide research directory**: Give the path to your sample research folder\n",
        "3. **Watch the agent work**: Observe how it runs the tools in sequence\n",
        "4. **Examine outputs**: Check the `.nova` folder for generated files\n",
        "\n",
        "Try these commands in sequence:\n",
        "- `/prompt/full_research_instructions_prompt`\n",
        "- Provide your research directory path when asked\n",
        "- Watch the agent execute steps 1 and 2 of the workflow\n",
        "\n",
        "## 9. Error Handling and Failure Cases\n",
        "\n",
        "The research agent is designed to handle various error scenarios gracefully. Let's explore how it behaves with different types of failures.\n",
        "\n",
        "### 9.1 Non-Critical Failures\n",
        "\n",
        "When some URLs fail to scrape but others succeed, the agent continues the workflow:\n",
        "\n",
        "```python\n",
        "# Example of partial failure handling\n",
        "result = {\n",
        "    \"status\": \"success\",\n",
        "    \"urls_processed\": [\"https://example1.com\", \"https://example2.com\"],\n",
        "    \"urls_failed\": [\"https://broken-link.com\"],\n",
        "    \"total_urls\": 3,\n",
        "    \"successful_urls_count\": 2,\n",
        "    \"failed_urls_count\": 1,\n",
        "    \"message\": \"Successfully processed 2/3 URLs. Results saved to output directory.\"\n",
        "}\n",
        "```\n",
        "\n",
        "The agent recognizes this as a partial success and continues with available data.\n",
        "\n",
        "### 9.2 Critical Failures\n",
        "\n",
        "According to the MCP prompt instructions, the agent stops only for \"complete failures\" - when zero items are processed successfully:\n",
        "\n",
        "```python\n",
        "# Example of critical failure\n",
        "result = {\n",
        "    \"status\": \"success\", \n",
        "    \"urls_processed\": [],\n",
        "    \"urls_failed\": [\"https://url1.com\", \"https://url2.com\", \"https://url3.com\"],\n",
        "    \"total_urls\": 3,\n",
        "    \"successful_urls_count\": 0,\n",
        "    \"failed_urls_count\": 3,\n",
        "    \"message\": \"Failed to process any URLs. All 3 URLs failed to scrape.\"\n",
        "}\n",
        "```\n",
        "\n",
        "In this case, the agent would:\n",
        "1. State the exact tool that failed and quote the output message\n",
        "2. Announce that it's stopping the workflow per its instructions  \n",
        "3. Ask the user for guidance on how to proceed\n",
        "\n",
        "### 9.3 MCP Prompt Failure Policy\n",
        "\n",
        "The research agent follows specific instructions defined in the MCP prompt for handling failures. Let's examine the exact policy from the prompt:\n",
        "\n",
        "Source: _research_agent_part_2/mcp_server/src/prompts/research_instructions_prompt.py_\n",
        "\n",
        "```\n",
        "**Critical Failure Policy:**\n",
        "\n",
        "If a tool reports a complete failure, you are required to halt the entire workflow immediately. A complete failure\n",
        "is defined as processing zero items successfully (e.g., scraped 0/7 URLs, processed 0 files).\n",
        "\n",
        "If this occurs, your immediate and only action is to:\n",
        "    1. State the exact tool that failed and quote the output message.\n",
        "    2. Announce that you are stopping the workflow as per your instructions.\n",
        "    3. Ask the user for guidance on how to proceed.\n",
        "```\n",
        "\n",
        "This policy demonstrates several important design principles:\n",
        "\n",
        "1. **Clear Failure Definition**: \"Complete failure\" is precisely defined as processing zero items successfully, not just encountering some errors.\n",
        "\n",
        "2. **Immediate Halt**: The agent must stop the entire workflow immediately when a critical failure occurs, preventing wasted resources on subsequent steps.\n",
        "\n",
        "3. **Transparent Communication**: The agent must quote the exact error message and explain why it's stopping, ensuring the user understands the situation.\n",
        "\n",
        "4. **Human Escalation**: Rather than attempting to recover automatically, the agent asks for human guidance, recognizing that critical failures often require human judgment.\n",
        "\n",
        "5. **Graceful Degradation**: Partial failures (e.g., 5/7 URLs scraped successfully) are acceptable and allow the workflow to continue with available data.\n",
        "\n",
        "This approach balances automation with reliability‚Äîthe agent continues working through minor issues but escalates major problems that could compromise the entire research effort.\n",
        "\n",
        "### 9.4 Testing Error Scenarios\n",
        "\n",
        "To test error handling, you can modify the sample article guideline to include:\n",
        "- Non-existent local files\n",
        "- Invalid URLs\n",
        "- Private repositories without proper tokens\n",
        "\n",
        "The agent will demonstrate different responses based on the severity of failures:\n",
        "\n",
        "- **Non-Critical**: Some files fail to process, but others succeed ‚Üí workflow continues\n",
        "- **Critical**: All items in a processing step fail ‚Üí workflow halts and asks for guidance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Exploring Generated Files\n",
        "\n",
        "After running the tools, examine the organized file structure in your research directory:\n",
        "\n",
        "```\n",
        "research_directory/\n",
        "‚îú‚îÄ‚îÄ article_guideline.md                     # Input guidelines\n",
        "‚îú‚îÄ‚îÄ .nova/                                   # Hidden folder with all data\n",
        "‚îÇ   ‚îú‚îÄ‚îÄ guidelines_filenames.json           # Extracted URLs and files\n",
        "‚îÇ   ‚îú‚îÄ‚îÄ local_files_from_research/          # Copied local files  \n",
        "‚îÇ   ‚îú‚îÄ‚îÄ urls_from_guidelines/               # Scraped web content\n",
        "‚îÇ   ‚îú‚îÄ‚îÄ urls_from_guidelines_code/          # GitHub repo summaries\n",
        "‚îÇ   ‚îî‚îÄ‚îÄ urls_from_guidelines_youtube/       # Video transcripts\n",
        "```\n",
        "\n",
        "Each folder contains processed content ready for the next stages of the research workflow. The file-based approach ensures that:\n",
        "\n",
        "- **Content is persistent** across agent sessions\n",
        "- **Large content blocks** don't overwhelm the agent's context\n",
        "- **Selective access** allows the agent to read only relevant files\n",
        "- **Human inspection** is possible for debugging and verification\n",
        "\n",
        "## 11. Key Takeaways\n",
        "\n",
        "This lesson demonstrated several important principles for building robust data ingestion tools:\n",
        "\n",
        "1. **File-Based Outputs**: Keep tool responses concise and save detailed content to files\n",
        "2. **External Services**: Use specialized services (Firecrawl, GitIngest) for complex tasks\n",
        "3. **Parallel Processing**: Implement concurrency for efficient data collection\n",
        "4. **Graceful Degradation**: Handle partial failures without stopping the entire workflow\n",
        "5. **LLM Enhancement**: Use LLMs to clean and structure scraped content\n",
        "6. **Organized Storage**: Create clear folder structures for different content types\n",
        "\n",
        "These patterns form the foundation for scalable research automation and can be adapted for various data ingestion scenarios.\n",
        "\n",
        "In the next lesson, we'll explore how the agent continues the workflow by generating targeted research queries and using advanced search capabilities to fill knowledge gaps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
