{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lesson 15: FastMCP ‚Äî MCP Server and Client Quickstart\n",
        "\n",
        "In this lesson, you will run a Model Context Protocol (MCP) server and MCP client using the FastMCP library, then explore how our research agent exposes MCP tools, MCP resources, and MCP prompts. We‚Äôll start with a quick demo that runs the MCP client with an in-memory MCP server directly from this notebook, so you can get to try its capabilities immediately. Then, we‚Äôll examine the MCP server and MCP client code structure.\n",
        "\n",
        "Learning Objectives:\n",
        "- Learn how to create an MCP server using `fastmcp`\n",
        "- Learn how to create an MCP client using `fastmcp`\n",
        "- Learn how to use the `fastmcp` library to expose MCP tools, MCP resources, and MCP prompts\n",
        "- Learn how to use the `fastmcp` library to interact with an MCP server"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup\n",
        "\n",
        "First, we define some standard Magic Python commands to autoreload Python packages whenever they change:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Set Up Python Environment\n",
        "\n",
        "To set up your Python virtual environment using `uv` and load it into the Notebook, follow the step-by-step instructions from the `Course Admin` lesson from the beginning of the course.\n",
        "\n",
        "**TL/DR:** Be sure the correct kernel pointing to your `uv` virtual environment is selected."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Configure Gemini API\n",
        "\n",
        "To configure the Gemini API, follow the step-by-step instructions from the `Course Admin` lesson.\n",
        "\n",
        "But here is a quick check on what you need to run this Notebook:\n",
        "\n",
        "1.  Get your key from [Google AI Studio](https://aistudio.google.com/app/apikey).\n",
        "2.  From the root of your project, run: `cp .env.example .env` \n",
        "3.  Within the `.env` file, fill in the `GOOGLE_API_KEY` variable:\n",
        "\n",
        "Now, the code below will load the key from the `.env` file:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from utils import env\n",
        "\n",
        "env.load(required_env_vars=[\"GOOGLE_API_KEY\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Import Key Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Allow nested async usage in notebooks\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Try the MCP client (Quickstart)\n",
        "\n",
        "The research agent is made of an MCP server and an MCP client.\n",
        "\n",
        "The MCP server is a `fastmcp` server that registers MCP tools, MCP resources, and MCP prompt via router modules. The MCP client is a `fastmcp` client that connects to the MCP server and allows you to interact with it, along with interacting with the LLM agent.\n",
        "\n",
        "This quickstart runs the MCP client of the research agent inside the notebook kernel. It connects to the MCP server running in‚Äëmemory (same process), which is the only transport supported for running everything in the same notebook. So, we'll always run the MCP server in-memory in the notebooks.\n",
        "\n",
        "Run the next code cell to start the MCP client. You will see some texts and can type commands directly in the input box that appears. The input box will be in different locations depending on where you are running the notebook from.\n",
        "\n",
        "Once the client is running, you can type commands when prompted, such as:\n",
        "\n",
        "- `/tools`: list all available MCP tools with names and descriptions.\n",
        "- `/resources`: list all available MCP resources with their URIs.\n",
        "- `/prompts`: list all available MCP prompts by name and description.\n",
        "- `/prompt/full_research_instructions_prompt`: fetch the research workflow prompt and inject it into the conversation.\n",
        "- `/resource/system://memory`: read and print the server memory stats (an example of running an MCP resource).\n",
        "- `/model-thinking-switch`: toggle model ‚Äúthinking‚Äù traces on/off. By default it is true, which means that you'll see the agent's thoughts in the conversation before each answer or tool call.\n",
        "- Any other text: treated as a normal user message for the agent, which may use the MCP server tools for answering.\n",
        "- `/quit`: terminate the client.\n",
        "\n",
        "At first, try with the following commands and see what happens:\n",
        "- `Hello! Who are you?`\n",
        "- `/tools`\n",
        "- `/resource/system://memory`\n",
        "- `/quit`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/fabio/Desktop/course-ai-agents/.venv/lib/python3.12/site-packages/pydantic/_internal/_fields.py:198: UserWarning: Field name \"schema\" in \"FewShotExampleStructuredOutputCompliance\" shadows an attribute in parent \"BaseModel\"\n",
            "  warnings.warn(\n",
            "/Users/fabio/Desktop/course-ai-agents/.venv/lib/python3.12/site-packages/opik/error_tracking/shutdown_hooks.py:12: SentryHubDeprecationWarning: `sentry_sdk.Hub` is deprecated and will be removed in a future major release. Please consult our 1.x to 2.x migration guide for details on how to migrate `Hub` usage to the new API: https://docs.sentry.io/platforms/python/migration/1.x-to-2.x\n",
            "  client = sentry_sdk.Hub.current.client\n",
            "INFO:root:üìä Opik monitoring disabled (missing configuration)\n",
            "\u001b[32m2025-09-16 14:16:57.275\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlogging\u001b[0m:\u001b[36mcallHandlers\u001b[0m:\u001b[36m1762\u001b[0m | üöÄ Starting MCP client with in-memory transport...\n",
            "\u001b[32m2025-09-16 14:16:57.286\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlogging\u001b[0m:\u001b[36mcallHandlers\u001b[0m:\u001b[36m1762\u001b[0m | Processing request of type ListToolsRequest\n",
            "\u001b[32m2025-09-16 14:16:57.287\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlogging\u001b[0m:\u001b[36mcallHandlers\u001b[0m:\u001b[36m1762\u001b[0m | Processing request of type ListResourcesRequest\n",
            "\u001b[32m2025-09-16 14:16:57.288\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlogging\u001b[0m:\u001b[36mcallHandlers\u001b[0m:\u001b[36m1762\u001b[0m | Processing request of type ListPromptsRequest\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üõ†Ô∏è  Available tools: 11\n",
            "üìö Available resources: 2\n",
            "üí¨ Available prompts: 1\n",
            "\n",
            "Available Commands: /tools, /resources, /prompts, /prompt/<name>, /resource/<uri>, /model-thinking-switch, /quit\n",
            "\n",
            "\u001b[97m============================================================\u001b[0m\n",
            "\u001b[1m\u001b[96müõ†Ô∏è  Available Tools\u001b[0m\n",
            "\u001b[97m============================================================\u001b[0m\n",
            "\n",
            "\u001b[92m1. \u001b[0m\u001b[97mextract_guidelines_urls\u001b[0m\u001b[33m\n",
            "   Extract URLs and local file references from article guidelines.\n",
            "\n",
            "Reads the ARTICLE_GUIDELINE_FILE file in the research directory and extracts:\n",
            "- GitHub URLs\n",
            "- Other HTTP/HTTPS URLs\n",
            "- Local file references (files mentioned in quotes with extensions)\n",
            "\n",
            "Results are saved to GUIDELINES_FILENAMES_FILE in the research directory.\n",
            "\n",
            "Args:\n",
            "    research_directory: Path to the research directory containing ARTICLE_GUIDELINE_FILE\n",
            "\n",
            "Returns:\n",
            "    Dict[str, Any]: Dictionary containing:\n",
            "        - status: Operation status (\"success\")\n",
            "        - github_sources_count: Number of GitHub URLs found\n",
            "        - youtube_sources_count: Number of YouTube URLs found\n",
            "        - web_sources_count: Number of other web URLs found\n",
            "        - local_files_count: Number of local file references found\n",
            "        - output_path: Path to the generated GUIDELINES_FILENAMES_FILE\n",
            "        - message: Human-readable success message\u001b[0m\n",
            "\n",
            "\u001b[92m2. \u001b[0m\u001b[97mprocess_local_files\u001b[0m\u001b[33m\n",
            "   Process local files referenced in the article guidelines.\n",
            "\n",
            "Reads the GUIDELINES_FILENAMES_FILE file and copies each referenced local file\n",
            "to the LOCAL_FILES_FROM_RESEARCH_FOLDER subfolder. Path separators in filenames are\n",
            "replaced with underscores to avoid creating nested folders.\n",
            "\n",
            "Args:\n",
            "    research_directory: Path to the research directory containing GUIDELINES_FILENAMES_FILE\n",
            "\n",
            "Returns:\n",
            "    Dict[str, Any]: Dictionary containing:\n",
            "        - status: Operation status (\"success\")\n",
            "        - files_processed: List of successfully processed files\n",
            "        - files_failed: List of files that failed to process\n",
            "        - total_files: Total number of files processed\n",
            "        - files_copied_count: Number of files successfully copied\n",
            "        - files_failed_count: Number of files that failed to copy\n",
            "        - output_directory: Path to the output directory\n",
            "        - message: Human-readable success message with processing results\u001b[0m\n",
            "\n",
            "\u001b[92m3. \u001b[0m\u001b[97mscrape_and_clean_other_urls\u001b[0m\u001b[33m\n",
            "   Scrape and clean other URLs from GUIDELINES_FILENAMES_FILE.\n",
            "\n",
            "Reads the GUIDELINES_FILENAMES_FILE file and scrapes/cleans each URL listed\n",
            "under 'other_urls'. The cleaned markdown content is saved to the\n",
            "URLS_FROM_GUIDELINES_FOLDER subfolder with appropriate filenames.\n",
            "\n",
            "Args:\n",
            "    research_directory: Path to the research directory containing GUIDELINES_FILENAMES_FILE\n",
            "    concurrency_limit: Maximum number of concurrent tasks for scraping (default: 4)\n",
            "\n",
            "Returns:\n",
            "    Dict[str, Any]: Dictionary containing:\n",
            "        - status: Operation status (\"success\")\n",
            "        - urls_processed: List of successfully processed URLs\n",
            "        - urls_failed: List of URLs that failed to process\n",
            "        - total_urls: Total number of URLs processed\n",
            "        - successful_urls_count: Number of URLs successfully scraped\n",
            "        - failed_urls_count: Number of URLs that failed to scrape\n",
            "        - output_directory: Path to the output directory\n",
            "        - message: Human-readable success message with processing results\u001b[0m\n",
            "\n",
            "\u001b[92m4. \u001b[0m\u001b[97mprocess_github_urls\u001b[0m\u001b[33m\n",
            "   Process GitHub URLs from GUIDELINES_FILENAMES_FILE using gitingest.\n",
            "\n",
            "Reads the GUIDELINES_FILENAMES_FILE file and processes each URL listed\n",
            "under 'github_urls' using gitingest to extract repository summaries, file trees,\n",
            "and content. The results are saved as markdown files in the\n",
            "URLS_FROM_GUIDELINES_CODE_FOLDER subfolder.\n",
            "\n",
            "Args:\n",
            "    research_directory: Path to the research directory containing GUIDELINES_FILENAMES_FILE\n",
            "\n",
            "Returns:\n",
            "    Dict[str, Any]: Dictionary containing:\n",
            "        - status: Operation status (\"success\")\n",
            "        - urls_processed: List of successfully processed GitHub URLs\n",
            "        - urls_failed: List of GitHub URLs that failed to process\n",
            "        - total_urls: Total number of GitHub URLs processed\n",
            "        - successful_urls_count: Number of GitHub URLs successfully processed\n",
            "        - failed_urls_count: Number of GitHub URLs that failed to process\n",
            "        - output_directory: Path to the output directory\n",
            "        - message: Human-readable success message with processing results\u001b[0m\n",
            "\n",
            "\u001b[92m5. \u001b[0m\u001b[97mtranscribe_youtube_urls\u001b[0m\u001b[33m\n",
            "   Transcribe YouTube video URLs from GUIDELINES_FILENAMES_FILE using Gemini 2.5 Pro.\n",
            "\n",
            "Reads the GUIDELINES_FILENAMES_FILE file and processes each URL listed\n",
            "under 'youtube_videos_urls'. Each video is transcribed, and the results are\n",
            "saved as markdown files in the URLS_FROM_GUIDELINES_YOUTUBE_FOLDER subfolder.\n",
            "\n",
            "Args:\n",
            "    research_directory: Path to the research directory containing GUIDELINES_FILENAMES_FILE\n",
            "\n",
            "Returns:\n",
            "    Dict[str, Any]: Dictionary containing:\n",
            "        - status: Operation status (\"success\")\n",
            "        - urls_processed: List of successfully transcribed YouTube URLs\n",
            "        - urls_failed: List of YouTube URLs that failed to transcribe\n",
            "        - total_urls: Total number of YouTube URLs processed\n",
            "        - successful_urls_count: Number of YouTube URLs successfully transcribed\n",
            "        - failed_urls_count: Number of YouTube URLs that failed to transcribe\n",
            "        - output_directory: Path to the output directory\n",
            "        - message: Human-readable success message with processing results\u001b[0m\n",
            "\n",
            "\u001b[92m6. \u001b[0m\u001b[97mgenerate_next_queries\u001b[0m\u001b[33m\n",
            "   Generate candidate web-search queries for the next research round.\n",
            "\n",
            "Analyzes the article guidelines, already-scraped content, and existing Perplexity\n",
            "results to identify knowledge gaps and propose new web-search questions.\n",
            "Each query includes a rationale explaining why it's important for the article.\n",
            "Results are saved to next_queries.md in the research directory.\n",
            "\n",
            "Args:\n",
            "    research_directory: Path to the research directory containing article data\n",
            "    n_queries: Number of queries to generate (default: 5)\n",
            "\n",
            "Returns:\n",
            "    Dict[str, Any]: Dictionary containing:\n",
            "        - status: Operation status (\"success\")\n",
            "        - queries_generated: List of generated query dictionaries with 'query' and 'rationale' keys\n",
            "        - queries_count: Number of queries generated\n",
            "        - output_path: Path to the generated next_queries.md file\n",
            "        - message: Human-readable success message with generation results\u001b[0m\n",
            "\n",
            "\u001b[92m7. \u001b[0m\u001b[97mrun_perplexity_research\u001b[0m\u001b[33m\n",
            "   Run selected web-search queries with Perplexity and store the results.\n",
            "\n",
            "Executes the provided queries using Perplexity's Sonar-Pro model and appends\n",
            "the results to perplexity_results.md in the research directory. Each query\n",
            "result includes the answer and source citations.\n",
            "\n",
            "Args:\n",
            "    research_directory: Path to the research directory where results will be saved\n",
            "    queries: List of web-search queries to execute\n",
            "\n",
            "Returns:\n",
            "    Dict[str, Any]: Dictionary containing:\n",
            "        - status: Operation status (\"success\")\n",
            "        - queries_executed: List of queries that were successfully executed\n",
            "        - queries_failed: List of queries that failed to execute\n",
            "        - total_queries: Total number of queries processed\n",
            "        - successful_queries_count: Number of queries successfully executed\n",
            "        - failed_queries_count: Number of queries that failed to execute\n",
            "        - total_sources: Total number of sources collected across all queries\n",
            "        - output_path: Path to the updated perplexity_results.md file\n",
            "        - message: Human-readable success message with processing results\u001b[0m\n",
            "\n",
            "\u001b[92m8. \u001b[0m\u001b[97mselect_research_sources_to_keep\u001b[0m\u001b[33m\n",
            "   Automatically select high-quality sources from Perplexity results.\n",
            "\n",
            "Uses GPT-4.1 to evaluate each source in perplexity_results.md for trustworthiness,\n",
            "authority, and relevance based on the article guidelines. Writes the comma-separated\n",
            "IDs of accepted sources to perplexity_sources_selected.md and saves a filtered\n",
            "markdown file perplexity_results_selected.md containing only the accepted sources.\n",
            "\n",
            "Args:\n",
            "    research_directory: Path to the research directory (e.g., \"articles/1\")\n",
            "\n",
            "Returns:\n",
            "    Dict[str, Any]: Dictionary containing:\n",
            "        - status: Operation status (\"success\")\n",
            "        - sources_selected_count: Number of sources selected\n",
            "        - selected_source_ids: List of IDs of selected sources\n",
            "        - sources_selected_path: Path to the perplexity_sources_selected.md file\n",
            "        - results_selected_path: Path to the perplexity_results_selected.md file\n",
            "        - message: Human-readable success message with selection results\u001b[0m\n",
            "\n",
            "\u001b[92m9. \u001b[0m\u001b[97mselect_research_sources_to_scrape\u001b[0m\u001b[33m\n",
            "   Select up to max_sources priority research sources to scrape in full.\n",
            "\n",
            "Analyzes the filtered Perplexity results together with the article guidelines and\n",
            "the material already scraped from guideline URLs, then chooses up to max_sources diverse,\n",
            "authoritative sources whose full content will add most value. The chosen URLs are\n",
            "written (one per line) to urls_to_scrape_from_research.md.\n",
            "\n",
            "Args:\n",
            "    research_directory: Path to the research directory (e.g., \"articles/1\")\n",
            "    max_sources: Maximum number of sources to select (default: 5)\n",
            "\n",
            "Returns:\n",
            "    Dict[str, Any]: Dictionary containing:\n",
            "        - status: Operation status (\"success\")\n",
            "        - sources_selected: List of selected source URLs\n",
            "        - sources_selected_count: Number of sources selected\n",
            "        - output_path: Path to the urls_to_scrape_from_research.md file\n",
            "        - reasoning: AI reasoning for why these sources were selected\n",
            "        - message: Human-readable success message with selection results and reasoning\u001b[0m\n",
            "\n",
            "\u001b[92m10. \u001b[0m\u001b[97mscrape_research_urls\u001b[0m\u001b[33m\n",
            "   Scrape the selected research URLs for full content.\n",
            "\n",
            "Reads the URLs from urls_to_scrape_from_research.md and scrapes/cleans each URL's\n",
            "full content. The cleaned markdown files are saved to the urls_from_research\n",
            "subfolder with appropriate filenames.\n",
            "\n",
            "Args:\n",
            "    research_directory: Path to the research directory containing urls_to_scrape_from_research.md\n",
            "    concurrency_limit: Maximum number of concurrent tasks for scraping (default: 4)\n",
            "\n",
            "Returns:\n",
            "    Dict[str, Any]: Dictionary containing:\n",
            "        - status: Operation status (\"success\")\n",
            "        - urls_processed: List of successfully processed URLs\n",
            "        - urls_failed: List of URLs that failed to process\n",
            "        - total_urls: Total number of URLs processed\n",
            "        - successful_urls_count: Number of URLs successfully scraped\n",
            "        - failed_urls_count: Number of URLs that failed to scrape\n",
            "        - output_directory: Path to the output directory\n",
            "        - message: Human-readable success message with processing results\u001b[0m\n",
            "\n",
            "\u001b[92m11. \u001b[0m\u001b[97mcreate_research_file\u001b[0m\u001b[33m\n",
            "   Generate the final comprehensive research.md file.\n",
            "\n",
            "Combines all research data including filtered Perplexity results, scraped guideline\n",
            "sources, and full research sources into a comprehensive research.md file. The file\n",
            "is organized into sections with collapsible blocks for easy navigation.\n",
            "\n",
            "Args:\n",
            "    research_directory: Path to the research directory containing all research data\n",
            "\n",
            "Returns:\n",
            "    Dict[str, Any]: Dictionary containing:\n",
            "        - status: Operation status (\"success\")\n",
            "        - markdown_file: Path to the generated research.md file\n",
            "        - research_results_count: Number of research result sections\n",
            "        - scraped_sources_count: Number of scraped sources sections\n",
            "        - code_sources_count: Number of code source sections\n",
            "        - youtube_transcripts_count: Number of YouTube transcript sections\n",
            "        - additional_sources_count: Number of additional source sections\n",
            "        - message: Human-readable success message with file generation results\u001b[0m\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m2025-09-16 14:17:22.701\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlogging\u001b[0m:\u001b[36mcallHandlers\u001b[0m:\u001b[36m1762\u001b[0m | Processing request of type ReadResourceRequest\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[97m============================================================\u001b[0m\n",
            "\u001b[1m\u001b[96müìñ Resource Content: system://status\u001b[0m\n",
            "\u001b[97m============================================================\u001b[0m\n",
            "\n",
            "{\"timestamp\":\"2025-09-16T14:17:22.701632\",\"platform\":\"macOS-15.6.1-arm64-arm-64bit\",\"python_version\":\"3.12.11 (main, Sep  2 2025, 14:12:30) [Clang 20.1.4 ]\",\"working_directory\":\"/Users/fabio/Desktop/course-ai-agents/lessons/15_fastmcp\",\"environment_variables\":{\"BASH_SILENCE_DEPRECATION_WARNING\":\"1\",\"COMMAND_MODE\":\"unix2003\",\"CONDA_EXE\":\"/Users/fabio/miniconda3/bin/conda\",\"CONDA_PYTHON_EXE\":\"/Users/fabio/miniconda3/bin/python\",\"CONDA_SHLVL\":\"0\",\"CURSOR_TRACE_ID\":\"f58477329bbf43cbb7e19339eb71456f\",\"DISPLAY\":\"/private/tmp/com.apple.launchd.E751STUBnV/org.xquartz:0\",\"HOME\":\"/Users/fabio\",\"HOMEBREW_CELLAR\":\"/opt/homebrew/Cellar\",\"HOMEBREW_PREFIX\":\"/opt/homebrew\",\"HOMEBREW_REPOSITORY\":\"/opt/homebrew\",\"INFOPATH\":\"/opt/homebrew/share/info:\",\"JAVA_HOME\":\"/Library/Java/JavaVirtualMachines/jdk-18.0.1.1.jdk/Contents/Home\",\"LOGNAME\":\"fabio\",\"MallocNanoZone\":\"0\",\"NVM_BIN\":\"/Users/fabio/.nvm/versions/node/v23.10.0/bin\",\"NVM_DIR\":\"/Users/fabio/.nvm\",\"NVM_INC\":\"/Users/fabio/.nvm/versions/node/v23.10.0/include/node\",\"ORIGINAL_XDG_CURRENT_DESKTOP\":\"undefined\",\"PATH\":\"/Users/fabio/Desktop/course-ai-agents/.venv/bin:/Users/fabio/google-cloud-sdk/bin:/Users/fabio/.local/bin:/Users/fabio/miniconda3/condabin:/Users/fabio/.nvm/versions/node/v23.10.0/bin:/opt/homebrew/opt/libxml2/bin:/Users/fabio/.cargo/bin:/Library/Frameworks/Python.framework/Versions/3.11/bin:/Users/fabio/.npm-global/bin:/Users/fabio/path_tools:/opt/homebrew/opt/imagemagick@6/bin:/opt/homebrew/bin:/opt/homebrew/sbin:/usr/local/bin:/System/Cryptexes/App/usr/bin:/usr/bin:/bin:/usr/sbin:/sbin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/local/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/appleinternal/bin:/opt/X11/bin\",\"PLEX_CLAIM\":\"claim-_HBSJKp4GjDX7M-tZbys\",\"PWD\":\"/\",\"SHELL\":\"/bin/bash\",\"SHLVL\":\"2\",\"SSH_AUTH_SOCK\":\"/private/tmp/com.apple.launchd.Fd0hw5iTwm/Listeners\",\"TMPDIR\":\"/var/folders/6w/zxb4hbqn3p722sx3s_r_dms40000gn/T/\",\"USER\":\"fabio\",\"VSCODE_CODE_CACHE_PATH\":\"/Users/fabio/Library/Application Support/Cursor/CachedData/9b5f3f4f2368631e3455d37672ca61b6dce85430\",\"VSCODE_CRASH_REPORTER_PROCESS_TYPE\":\"extensionHost\",\"VSCODE_CWD\":\"/\",\"VSCODE_ESM_ENTRYPOINT\":\"vs/workbench/api/node/extensionHostProcess\",\"VSCODE_HANDLES_UNCAUGHT_ERRORS\":\"true\",\"VSCODE_IPC_HOOK\":\"/Users/fabio/Library/Application Support/Cursor/1.6.-main.sock\",\"VSCODE_NLS_CONFIG\":\"{\\\"userLocale\\\":\\\"en-gb\\\",\\\"osLocale\\\":\\\"en-it\\\",\\\"resolvedLanguage\\\":\\\"en\\\",\\\"defaultMessagesFile\\\":\\\"/Applications/Cursor.app/Contents/Resources/app/out/nls.messages.json\\\",\\\"locale\\\":\\\"en-gb\\\",\\\"availableLanguages\\\":{}}\",\"VSCODE_PID\":\"43296\",\"VSCODE_PROCESS_TITLE\":\"extension-host  [1-1]\",\"XPC_FLAGS\":\"0x0\",\"XPC_SERVICE_NAME\":\"0\",\"_\":\"/Users/fabio/Desktop/course-ai-agents/.venv/bin/python\",\"_CONDA_EXE\":\"/Users/fabio/miniconda3/bin/conda\",\"_CONDA_ROOT\":\"/Users/fabio/miniconda3\",\"__CFBundleIdentifier\":\"com.todesktop.230313mzl4w4u92\",\"__CF_USER_TEXT_ENCODING\":\"0x1F5:0x0:0x0\",\"ELECTRON_RUN_AS_NODE\":\"1\",\"VSCODE_L10N_BUNDLE_LOCATION\":\"\",\"PYTHONUNBUFFERED\":\"1\",\"PYTHONIOENCODING\":\"utf-8\",\"VIRTUAL_ENV\":\"/Users/fabio/Desktop/course-ai-agents/.venv\",\"PS1\":\"(agentic-ai-engineering-course) \",\"VIRTUAL_ENV_PROMPT\":\"agentic-ai-engineering-course\",\"LC_CTYPE\":\"C.UTF-8\",\"PYDEVD_IPYTHON_COMPATIBLE_DEBUGGING\":\"1\",\"PYTHON_FROZEN_MODULES\":\"on\",\"PYDEVD_USE_FRAME_EVAL\":\"NO\",\"TERM\":\"xterm-color\",\"CLICOLOR\":\"1\",\"FORCE_COLOR\":\"1\",\"CLICOLOR_FORCE\":\"1\",\"PAGER\":\"cat\",\"GIT_PAGER\":\"cat\",\"MPLBACKEND\":\"module://matplotlib_inline.backend_inline\"}}\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m2025-09-16 14:17:39.338\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlogging\u001b[0m:\u001b[36mcallHandlers\u001b[0m:\u001b[36m1762\u001b[0m | Processing request of type ReadResourceRequest\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[97m============================================================\u001b[0m\n",
            "\u001b[1m\u001b[96müìñ Resource Content: system://memory\u001b[0m\n",
            "\u001b[97m============================================================\u001b[0m\n",
            "\n",
            "{\"process_memory_mb\":214.328125,\"process_memory_percent\":1.3081550598144531,\"system_memory\":{\"total_gb\":16.0,\"available_gb\":5.9115142822265625,\"used_percent\":63.1}}\n",
            "\n",
            "\n",
            "\u001b[37müí¨ LLM Response: \u001b[0mI am a research agent specializing in article creation. I can perform various research tasks, including extracting information from guidelines, processing local files, scraping and transcribing URLs, running web searches, and generating comprehensive research reports.\n",
            "\n",
            "How can I assist you with your research today?\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m2025-09-16 14:18:08.641\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlogging\u001b[0m:\u001b[36mcallHandlers\u001b[0m:\u001b[36m1762\u001b[0m | üëã Terminating application...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m2025-09-16 15:05:36.529\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlogging\u001b[0m:\u001b[36mcallHandlers\u001b[0m:\u001b[36m1762\u001b[0m | üìä Opik monitoring disabled (missing configuration)\n",
            "\u001b[32m2025-09-16 15:05:36.530\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlogging\u001b[0m:\u001b[36mcallHandlers\u001b[0m:\u001b[36m1762\u001b[0m | üöÄ Starting MCP client with in-memory transport...\n",
            "\u001b[32m2025-09-16 15:05:36.545\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlogging\u001b[0m:\u001b[36mcallHandlers\u001b[0m:\u001b[36m1762\u001b[0m | Processing request of type ListToolsRequest\n",
            "\u001b[32m2025-09-16 15:05:36.551\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlogging\u001b[0m:\u001b[36mcallHandlers\u001b[0m:\u001b[36m1762\u001b[0m | Processing request of type ListResourcesRequest\n",
            "\u001b[32m2025-09-16 15:05:36.555\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlogging\u001b[0m:\u001b[36mcallHandlers\u001b[0m:\u001b[36m1762\u001b[0m | Processing request of type ListPromptsRequest\n",
            "\u001b[32m2025-09-16 15:05:55.010\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlogging\u001b[0m:\u001b[36mcallHandlers\u001b[0m:\u001b[36m1762\u001b[0m | Processing request of type CallToolRequest\n",
            "\u001b[32m2025-09-16 15:05:55.013\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36mlogging\u001b[0m:\u001b[36mcallHandlers\u001b[0m:\u001b[36m1762\u001b[0m | Research folder does not exist: data/sample_research_folder\n",
            "\u001b[32m2025-09-16 15:05:55.013\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36mlogging\u001b[0m:\u001b[36mcallHandlers\u001b[0m:\u001b[36m1762\u001b[0m | Error calling tool 'extract_guidelines_urls'\n",
            "\u001b[33m\u001b[1mTraceback (most recent call last):\u001b[0m\n",
            "\n",
            "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/Users/fabio/Desktop/course-ai-agents/.venv/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
            "    app.launch_new_instance()\n",
            "    ‚îÇ   ‚îî <bound method Application.launch_instance of <class 'ipykernel.kernelapp.IPKernelApp'>>\n",
            "    ‚îî <module 'ipykernel.kernelapp' from '/Users/fabio/Desktop/course-ai-agents/.venv/lib/python3.12/site-packages/ipykernel/kernel...\n",
            "  File \"/Users/fabio/Desktop/course-ai-agents/.venv/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
            "    app.start()\n",
            "    ‚îÇ   ‚îî <function IPKernelApp.start at 0x103692a20>\n",
            "    ‚îî <ipykernel.kernelapp.IPKernelApp object at 0x1007207d0>\n",
            "  File \"/Users/fabio/Desktop/course-ai-agents/.venv/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
            "    self.io_loop.start()\n",
            "    ‚îÇ    ‚îÇ       ‚îî <function BaseAsyncIOLoop.start at 0x1036937e0>\n",
            "    ‚îÇ    ‚îî <tornado.platform.asyncio.AsyncIOMainLoop object at 0x1036a66c0>\n",
            "    ‚îî <ipykernel.kernelapp.IPKernelApp object at 0x1007207d0>\n",
            "  File \"/Users/fabio/Desktop/course-ai-agents/.venv/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 211, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "    ‚îÇ    ‚îÇ            ‚îî <function BaseEventLoop.run_forever at 0x100c32e80>\n",
            "    ‚îÇ    ‚îî <_UnixSelectorEventLoop running=True closed=False debug=False>\n",
            "    ‚îî <tornado.platform.asyncio.AsyncIOMainLoop object at 0x1036a66c0>\n",
            "  File \"/Users/fabio/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/asyncio/base_events.py\", line 645, in run_forever\n",
            "    self._run_once()\n",
            "    ‚îÇ    ‚îî <function BaseEventLoop._run_once at 0x100c3ccc0>\n",
            "    ‚îî <_UnixSelectorEventLoop running=True closed=False debug=False>\n",
            "  File \"/Users/fabio/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/asyncio/base_events.py\", line 1999, in _run_once\n",
            "    handle._run()\n",
            "    ‚îÇ      ‚îî <function Handle._run at 0x100bd8680>\n",
            "    ‚îî <Handle <_asyncio.TaskStepMethWrapper object at 0x103c783d0>()>\n",
            "  File \"/Users/fabio/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "    ‚îÇ    ‚îÇ            ‚îÇ    ‚îÇ           ‚îÇ    ‚îî <member '_args' of 'Handle' objects>\n",
            "    ‚îÇ    ‚îÇ            ‚îÇ    ‚îÇ           ‚îî <Handle <_asyncio.TaskStepMethWrapper object at 0x103c783d0>()>\n",
            "    ‚îÇ    ‚îÇ            ‚îÇ    ‚îî <member '_callback' of 'Handle' objects>\n",
            "    ‚îÇ    ‚îÇ            ‚îî <Handle <_asyncio.TaskStepMethWrapper object at 0x103c783d0>()>\n",
            "    ‚îÇ    ‚îî <member '_context' of 'Handle' objects>\n",
            "    ‚îî <Handle <_asyncio.TaskStepMethWrapper object at 0x103c783d0>()>\n",
            "  File \"/Users/fabio/Desktop/course-ai-agents/.venv/lib/python3.12/site-packages/mcp/server/lowlevel/server.py\", line 608, in _handle_message\n",
            "    await self._handle_request(message, req, session, lifespan_context, raise_exceptions)\n",
            "          ‚îÇ    ‚îÇ               ‚îÇ        ‚îÇ    ‚îÇ        ‚îÇ                 ‚îî False\n",
            "          ‚îÇ    ‚îÇ               ‚îÇ        ‚îÇ    ‚îÇ        ‚îî {}\n",
            "          ‚îÇ    ‚îÇ               ‚îÇ        ‚îÇ    ‚îî <mcp.server.session.ServerSession object at 0x134deb110>\n",
            "          ‚îÇ    ‚îÇ               ‚îÇ        ‚îî CallToolRequest(method='tools/call', params=CallToolRequestParams(meta=Meta(progressToken=1), name='extract_guidelines_urls',...\n",
            "          ‚îÇ    ‚îÇ               ‚îî <mcp.shared.session.RequestResponder object at 0x134daf770>\n",
            "          ‚îÇ    ‚îî <function Server._handle_request at 0x106453100>\n",
            "          ‚îî <fastmcp.server.low_level.LowLevelServer object at 0x134de9ca0>\n",
            "  File \"/Users/fabio/Desktop/course-ai-agents/.venv/lib/python3.12/site-packages/mcp/server/lowlevel/server.py\", line 645, in _handle_request\n",
            "    response = await handler(req)\n",
            "                     ‚îÇ       ‚îî CallToolRequest(method='tools/call', params=CallToolRequestParams(meta=Meta(progressToken=1), name='extract_guidelines_urls',...\n",
            "                     ‚îî <function Server.call_tool.<locals>.decorator.<locals>.handler at 0x134dd2c00>\n",
            "  File \"/Users/fabio/Desktop/course-ai-agents/.venv/lib/python3.12/site-packages/mcp/server/lowlevel/server.py\", line 461, in handler\n",
            "    results = await func(tool_name, arguments)\n",
            "                    ‚îÇ    ‚îÇ          ‚îî {'research_directory': 'data/sample_research_folder'}\n",
            "                    ‚îÇ    ‚îî 'extract_guidelines_urls'\n",
            "                    ‚îî <bound method FastMCP._mcp_call_tool of FastMCP('Nova Research MCP Server')>\n",
            "  File \"/Users/fabio/Desktop/course-ai-agents/.venv/lib/python3.12/site-packages/fastmcp/server/server.py\", line 722, in _mcp_call_tool\n",
            "    result = await self._call_tool(key, arguments)\n",
            "                   ‚îÇ    ‚îÇ          ‚îÇ    ‚îî {'research_directory': 'data/sample_research_folder'}\n",
            "                   ‚îÇ    ‚îÇ          ‚îî 'extract_guidelines_urls'\n",
            "                   ‚îÇ    ‚îî <function FastMCP._call_tool at 0x106c67420>\n",
            "                   ‚îî FastMCP('Nova Research MCP Server')\n",
            "  File \"/Users/fabio/Desktop/course-ai-agents/.venv/lib/python3.12/site-packages/fastmcp/server/server.py\", line 752, in _call_tool\n",
            "    return await self._apply_middleware(mw_context, _handler)\n",
            "                 ‚îÇ    ‚îÇ                 ‚îÇ           ‚îî <function FastMCP._call_tool.<locals>._handler at 0x134dd2ca0>\n",
            "                 ‚îÇ    ‚îÇ                 ‚îî MiddlewareContext(message=CallToolRequestParams(meta=None, name='extract_guidelines_urls', arguments={'research_directory': '...\n",
            "                 ‚îÇ    ‚îî <function FastMCP._apply_middleware at 0x106c66700>\n",
            "                 ‚îî FastMCP('Nova Research MCP Server')\n",
            "  File \"/Users/fabio/Desktop/course-ai-agents/.venv/lib/python3.12/site-packages/fastmcp/server/server.py\", line 406, in _apply_middleware\n",
            "    return await chain(context)\n",
            "                 ‚îÇ     ‚îî MiddlewareContext(message=CallToolRequestParams(meta=None, name='extract_guidelines_urls', arguments={'research_directory': '...\n",
            "                 ‚îî <function FastMCP._call_tool.<locals>._handler at 0x134dd2ca0>\n",
            "  File \"/Users/fabio/Desktop/course-ai-agents/.venv/lib/python3.12/site-packages/fastmcp/server/server.py\", line 741, in _handler\n",
            "    return await self._tool_manager.call_tool(\n",
            "                 ‚îÇ    ‚îÇ             ‚îî <function ToolManager.call_tool at 0x106bb9760>\n",
            "                 ‚îÇ    ‚îî <fastmcp.tools.tool_manager.ToolManager object at 0x134de98b0>\n",
            "                 ‚îî FastMCP('Nova Research MCP Server')\n",
            "> File \"/Users/fabio/Desktop/course-ai-agents/.venv/lib/python3.12/site-packages/fastmcp/tools/tool_manager.py\", line 224, in call_tool\n",
            "    return await tool.run(arguments)\n",
            "                 ‚îÇ    ‚îÇ   ‚îî {'research_directory': 'data/sample_research_folder'}\n",
            "                 ‚îÇ    ‚îî <function FunctionTool.run at 0x106b7b6a0>\n",
            "                 ‚îî FunctionTool(name='extract_guidelines_urls', title=None, description='Extract URLs and local file references from article gui...\n",
            "  File \"/Users/fabio/Desktop/course-ai-agents/.venv/lib/python3.12/site-packages/fastmcp/tools/tool.py\", line 317, in run\n",
            "    result = await result\n",
            "                   ‚îî <coroutine object register_mcp_tools.<locals>.extract_guidelines_urls at 0x134d50930>\n",
            "\n",
            "  File \"\u001b[32m/Users/fabio/Desktop/course-ai-agents/lessons/research_agent_part_2/mcp_server/src/routers/\u001b[0m\u001b[32m\u001b[1mtools.py\u001b[0m\", line \u001b[33m54\u001b[0m, in \u001b[35mextract_guidelines_urls\u001b[0m\n",
            "    \u001b[1mresult\u001b[0m \u001b[35m\u001b[1m=\u001b[0m \u001b[1mextract_guidelines_urls_tool\u001b[0m\u001b[1m(\u001b[0m\u001b[1mresearch_directory\u001b[0m\u001b[1m)\u001b[0m\n",
            "    \u001b[36m         ‚îÇ                            ‚îî \u001b[0m\u001b[36m\u001b[1m'data/sample_research_folder'\u001b[0m\n",
            "    \u001b[36m         ‚îî \u001b[0m\u001b[36m\u001b[1m<function extract_guidelines_urls_tool at 0x134d414e0>\u001b[0m\n",
            "\n",
            "  File \"\u001b[32m/Users/fabio/Desktop/course-ai-agents/lessons/research_agent_part_2/mcp_server/src/tools/\u001b[0m\u001b[32m\u001b[1mextract_guidelines_urls_tool.py\u001b[0m\", line \u001b[33m45\u001b[0m, in \u001b[35mextract_guidelines_urls_tool\u001b[0m\n",
            "    \u001b[1mvalidate_research_folder\u001b[0m\u001b[1m(\u001b[0m\u001b[1mresearch_path\u001b[0m\u001b[1m)\u001b[0m\n",
            "    \u001b[36m‚îÇ                        ‚îî \u001b[0m\u001b[36m\u001b[1mPosixPath('data/sample_research_folder')\u001b[0m\n",
            "    \u001b[36m‚îî \u001b[0m\u001b[36m\u001b[1m<function validate_research_folder at 0x1330bbba0>\u001b[0m\n",
            "\n",
            "  File \"\u001b[32m/Users/fabio/Desktop/course-ai-agents/lessons/research_agent_part_2/mcp_server/src/utils/\u001b[0m\u001b[32m\u001b[1mfile_utils.py\u001b[0m\", line \u001b[33m38\u001b[0m, in \u001b[35mvalidate_research_folder\u001b[0m\n",
            "    \u001b[35m\u001b[1mraise\u001b[0m \u001b[1mValueError\u001b[0m\u001b[1m(\u001b[0m\u001b[1mmsg\u001b[0m\u001b[1m)\u001b[0m\n",
            "    \u001b[36m                 ‚îî \u001b[0m\u001b[36m\u001b[1m'Research folder does not exist: data/sample_research_folder'\u001b[0m\n",
            "\n",
            "\u001b[31m\u001b[1mValueError\u001b[0m:\u001b[1m Research folder does not exist: data/sample_research_folder\u001b[0m\n",
            "\u001b[32m2025-09-16 15:08:25.195\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlogging\u001b[0m:\u001b[36mcallHandlers\u001b[0m:\u001b[36m1762\u001b[0m | Processing request of type CallToolRequest\n",
            "\u001b[32m2025-09-16 15:08:25.200\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlogging\u001b[0m:\u001b[36mcallHandlers\u001b[0m:\u001b[36m1762\u001b[0m | Processing request of type ListToolsRequest\n",
            "\u001b[32m2025-09-16 15:09:29.499\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlogging\u001b[0m:\u001b[36mcallHandlers\u001b[0m:\u001b[36m1762\u001b[0m | üëã Terminating application...\n",
            "\u001b[32m2025-09-16 15:22:18.939\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlogging\u001b[0m:\u001b[36mcallHandlers\u001b[0m:\u001b[36m1762\u001b[0m | Exception in execute request:\n",
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
            "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 3\u001b[39m\n",
            "\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mresearch_agent_part_2\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmcp_server\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtools\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m extract_guidelines_urls_tool\n",
            "\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mextract_guidelines_urls_tool\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresearch_directory\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresearch_agent_part_2/data/sample_research_folder\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "\n",
            "\u001b[31mTypeError\u001b[39m: extract_guidelines_urls_tool() got an unexpected keyword argument 'research_directory'\n",
            "\u001b[32m2025-09-16 15:22:28.493\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36mlogging\u001b[0m:\u001b[36mcallHandlers\u001b[0m:\u001b[36m1762\u001b[0m | Research folder does not exist: research_agent_part_2/data/sample_research_folder\n",
            "\u001b[32m2025-09-16 15:22:28.572\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mlogging\u001b[0m:\u001b[36mcallHandlers\u001b[0m:\u001b[36m1762\u001b[0m | Exception in execute request:\n",
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
            "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 3\u001b[39m\n",
            "\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mresearch_agent_part_2\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmcp_server\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtools\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m extract_guidelines_urls_tool\n",
            "\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mextract_guidelines_urls_tool\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresearch_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresearch_agent_part_2/data/sample_research_folder\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/course-ai-agents/lessons/research_agent_part_2/mcp_server/src/tools/extract_guidelines_urls_tool.py:45\u001b[39m, in \u001b[36mextract_guidelines_urls_tool\u001b[39m\u001b[34m(research_folder)\u001b[39m\n",
            "\u001b[32m     42\u001b[39m guidelines_path = research_path / ARTICLE_GUIDELINE_FILE\n",
            "\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# Validate folders and files\u001b[39;00m\n",
            "\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m \u001b[43mvalidate_research_folder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresearch_path\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[32m     46\u001b[39m validate_guidelines_file(guidelines_path)\n",
            "\u001b[32m     48\u001b[39m \u001b[38;5;66;03m# Create NOVA_FOLDER directory if it doesn't exist\u001b[39;00m\n",
            "\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/course-ai-agents/lessons/research_agent_part_2/mcp_server/src/utils/file_utils.py:38\u001b[39m, in \u001b[36mvalidate_research_folder\u001b[39m\u001b[34m(research_path)\u001b[39m\n",
            "\u001b[32m     36\u001b[39m     msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mResearch folder does not exist: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresearch_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n",
            "\u001b[32m     37\u001b[39m     logger.error(msg)\n",
            "\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n",
            "\u001b[32m     40\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m research_path.is_dir():\n",
            "\u001b[32m     41\u001b[39m     msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPath is not a directory: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresearch_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n",
            "\n",
            "\u001b[31mValueError\u001b[39m: Research folder does not exist: research_agent_part_2/data/sample_research_folder\n"
          ]
        }
      ],
      "source": [
        "# Run the MCP client in-kernel\n",
        "from research_agent_part_2.mcp_client.src.client import main as client_main\n",
        "import sys\n",
        "\n",
        "async def run_client():\n",
        "    _argv_backup = sys.argv[:]\n",
        "    sys.argv = [\"client\"]\n",
        "    try:\n",
        "        await client_main()\n",
        "    finally:\n",
        "        sys.argv = _argv_backup\n",
        "\n",
        "# Start client with in-memory server \n",
        "await run_client()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Whenever you want, you can run the previous cell again to try the client.\n",
        "\n",
        "Now, let's see how the MCP server works."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. MCP Server Overview\n",
        "\n",
        "The purpose of this section is to show how the MCP server is created with `fastmcp` and how it wires MCP tools, MCP resources, and MCP prompts.\n",
        "\n",
        "The MCP server is a `fastmcp` server that registers MCP tools (actions with side effects like scraping webpages, transcribing videos, etc.), MCP resources (read-only endpoints for information like system status or memory), and MCP prompts (reusable instruction blocks, such as our agent workflow) via router modules.\n",
        "\n",
        "The MCP server follows a FastAPI‚Äëlike layout for clarity and scalability. It is structured as follows:\n",
        "\n",
        "- `server.py`: Entry point exposing `create_mcp_server()` and a `__main__` runner.\n",
        "- `routers/`: Functions that attach endpoints to the FastMCP instance.\n",
        "  - `tools.py`: registers all MCP tools.\n",
        "  - `resources.py`: registers all MCP resources.\n",
        "  - `prompts.py`: registers all MCP prompts.\n",
        "- `tools/`: MCP tools implementations.\n",
        "- `resources/`: MCP resources implementations.\n",
        "- `prompts/`: MCP prompts implementations (e.g. full workflow instructions for the agent).\n",
        "- `app/`: Functions implementing business logic.\n",
        "- `utils/`: Utility functions.\n",
        "- `config/`: Pydantic settings (`settings.py`) for server name/version, logging, model choices, and API keys.\n",
        "\n",
        "This separation keeps orchestration thin at the server boundary while allowing each capability (tool/resource/prompt) to evolve independently.\n",
        "\n",
        "Let's see now how the MCP server is created.\n",
        "\n",
        "Source:\n",
        "_mcp_server/src/server.py_\n",
        "\n",
        "```python\n",
        "from fastmcp import FastMCP\n",
        "\n",
        "from .config.settings import settings\n",
        "from .routers.prompts import register_mcp_prompts\n",
        "from .routers.resources import register_mcp_resources\n",
        "from .routers.tools import register_mcp_tools\n",
        "\n",
        "\n",
        "def create_mcp_server() -> FastMCP:\n",
        "    \"\"\"\n",
        "    Create and configure the MCP server instance.\n",
        "\n",
        "    This function can be imported to get a configured MCP server\n",
        "    for use with in-memory transport in clients.\n",
        "\n",
        "    Returns:\n",
        "        FastMCP: Configured MCP server instance\n",
        "    \"\"\"\n",
        "    # Create the FastMCP server instance\n",
        "    mcp = FastMCP(\n",
        "        name=settings.server_name,\n",
        "        version=settings.version,\n",
        "    )\n",
        "\n",
        "    # Register all MCP endpoints\n",
        "    register_mcp_tools(mcp)\n",
        "    register_mcp_resources(mcp)\n",
        "    register_mcp_prompts(mcp)\n",
        "\n",
        "    return mcp\n",
        "```\n",
        "\n",
        "Notice how the `FastMCP` instance is created and how the `mcp` object is passed to the `register_mcp_tools`, `register_mcp_resources`, and `register_mcp_prompts` functions. It is pretty similar to how you would create a FastAPI app and attach endpoints to it!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 Registering MCP Tools\n",
        "\n",
        "Let's see now in particular how to register an MCP tool with `fastmcp`. This specific tool reads the article guidelines and extracts relevant references. Its implementation is in the `tools/extract_guidelines_urls_tool.py` file, along with other business logic functions in the `app/` folder. You can read the full file `mcp_server/src/routers/tools.py` to see all the 11 available MCP tools.\n",
        "\n",
        "Source: _mcp_server/src/routers/tools.py_\n",
        "\n",
        "```python\n",
        "@mcp.tool()\n",
        "async def extract_guidelines_urls(research_directory: str) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Extract URLs and local file references from article guidelines.\n",
        "\n",
        "    Reads the ARTICLE_GUIDELINE_FILE file in the research directory and extracts:\n",
        "    - GitHub URLs\n",
        "    - Other HTTP/HTTPS URLs\n",
        "    - Local file references (files mentioned in quotes with extensions)\n",
        "\n",
        "    Results are saved to GUIDELINES_FILENAMES_FILE in the research directory.\n",
        "    \"\"\"\n",
        "    result = extract_guidelines_urls_tool(research_directory)\n",
        "    return result\n",
        "```\n",
        "\n",
        "This tool is the first step in the workflow. It reads the article guideline and writes a structured file containing URLs and local references. Notice how it requires a `research_directory` input, which is the path to the research directory containing a `article_guideline.md` file.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's test it with a sample article guideline. In the research agent folder, there's a `data/sample_research_folder` folder with an `article_guideline.md` file. Let's use it as input for the `extract_guidelines_urls` tool.\n",
        "\n",
        "Here is how it is structured:\n",
        "\n",
        "```md\n",
        "## Global Context of the Lesson\n",
        "\n",
        "...\n",
        "\n",
        "## Lesson Outline\n",
        "\n",
        "## Section 1: Introduction\n",
        "\n",
        "...\n",
        "\n",
        "## Section 2: Understanding why agents need tools\n",
        "\n",
        "...\n",
        "\n",
        "## Section N: Conclusion\n",
        "\n",
        "...\n",
        "\n",
        "## Article code\n",
        "\n",
        "Links to code that will be used to support the article. Always prioritize this code over every other piece of code found in the sources: \n",
        "\n",
        "- [Notebook 1](https://github.com/path/to/notebook.ipynb)\n",
        "\n",
        "## Sources\n",
        "\n",
        "- [Function calling with the Gemini API](https://ai.google.dev/gemini-api/docs/function-calling)\n",
        "- [Function calling with OpenAI's API](https://platform.openai.com/docs/guides/function-calling)\n",
        "- [Tool Calling Agent From Scratch](https://www.youtube.com/watch?v=ApoDzZP8_ck)\n",
        "- [Efficient Tool Use with Chain-of-Abstraction Reasoning](https://arxiv.org/pdf/2401.17464v3)\n",
        "- [Building AI Agents from scratch - Part 1: Tool use](https://www.newsletter.swirlai.com/p/building-ai-agents-from-scratch-part)\n",
        "- [What is Tool Calling? Connecting LLMs to Your Data](https://www.youtube.com/watch?v=h8gMhXYAv1k)\n",
        "- [ReAct vs Plan-and-Execute: A Practical Comparison of LLM Agent Patterns](https://dev.to/jamesli/react-vs-plan-and-execute-a-practical-comparison-of-llm-agent-patterns-4gh9)\n",
        "- [Agentic Design Patterns Part 3, Tool Use](https://www.deeplearning.ai/the-batch/agentic-design-patterns-part-3-tool-use/)\n",
        "```\n",
        "\n",
        "Normally, an `article_guideline.md` file would contain detailed information about the article to write, including the outline, the sections, the sources, and the code, as the research agent needs this information to look for the best content to include in the article. In this sample file, we have a simplified version of an article guideline.\n",
        "\n",
        "Now, run the next code cell to run the research agent MCP client again, and give it the following command. Make sure to replace the folder path with tour actual absolute folder path, otherwise the tool will not find the file.\n",
        "- Command to give to the client: `Run the \"extract_guidelines_urls\" tool with the \"data/sample_research_folder\" directory as research folder`.\n",
        "\n",
        "In case you provide the wrong path, notice how the tool will return an error and how the agent will ask you to provide a valid path and how to proceed.\n",
        "\n",
        "*Important*: the agent will manage every message starting with the `/` as a command, so, if you want to provide the folder path in a message, you need to write something like this: `Here is the folder path: /absolute/path/to/the/folder`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üõ†Ô∏è  Available tools: 11\n",
            "üìö Available resources: 2\n",
            "üí¨ Available prompts: 1\n",
            "\n",
            "Available Commands: /tools, /resources, /prompts, /prompt/<name>, /resource/<uri>, /model-thinking-switch, /quit\n",
            "\n",
            "\n",
            "\u001b[1m\u001b[95mü§î LLM's Thoughts:\u001b[0m\n",
            "\u001b[35m**Let's Get Those URLs!**\n",
            "\n",
            "Okay, so I'm ready to get this project moving. The first step is running the `extract_guidelines_urls` tool. I've got my data all set up in the \"data/sample_research_folder\" directory, which I know is the appropriate structure.  Since the tool specifically requires a `research_directory` argument, and I've given it \"data/sample_research_folder,\" I should be all set. It's time to fire it up and see what we get!\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m\u001b[36müîß Function Call (Tool):\u001b[0m\n",
            "\u001b[36m  Tool: \u001b[0m\u001b[1m\u001b[36mextract_guidelines_urls\u001b[0m\n",
            "\u001b[36m  Arguments: \u001b[0m\u001b[36m{\n",
            "  \"research_directory\": \"data/sample_research_folder\"\n",
            "}\u001b[0m\n",
            "\n",
            "\u001b[36m‚ö° Executing tool 'extract_guidelines_urls' via MCP server...\u001b[0m\n",
            "\u001b[91m‚ùå Tool 'extract_guidelines_urls' execution failed: Error calling tool 'extract_guidelines_urls': Research folder does not exist: data/sample_research_folder\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[95mü§î LLM's Thoughts:\u001b[0m\n",
            "\u001b[35m**Addressing a Research Directory Issue**\n",
            "\n",
            "Okay, so I was trying to execute the `extract_guidelines_urls` tool, but it looks like I hit a snag.  It seems the specified research folder, `data/sample_research_folder`, is missing.  I really need to clarify this with the user.  They need a valid research directory for this to work, so I'll reach out and ask them to provide the correct path.  Hopefully, it's just a simple oversight.\u001b[0m\n",
            "\n",
            "\u001b[37müí¨ LLM Response: \u001b[0mThe research folder `data/sample_research_folder` does not exist. Please provide a valid research directory.\n",
            "\n",
            "\u001b[1m\u001b[91m‚ùå Unknown command: '/Users/fabio/Desktop/course-ai-agents/lessons/research_agent_part_2/data/sample_research_folder'\u001b[0m\n",
            "Available commands: /tools, /resources, /prompts, /prompt/<name>, /resource/<uri>, /model-thinking-switch, /quit\n",
            "\n",
            "\u001b[1m\u001b[95mü§î LLM's Thoughts:\u001b[0m\n",
            "\u001b[35m**Updated Research Path & Next Steps**\n",
            "\n",
            "Okay, so the user has given me a fresh direction for the research directory. That's great! Now that I have the updated path, I need to immediately put it to work. My next step is clear: I need to call the `extract_guidelines_urls` tool, but this time, I'll be sure to pass in this shiny, new research directory path they just provided. That's the logical progression here. Let's get that tool running with the correct input.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m\u001b[36müîß Function Call (Tool):\u001b[0m\n",
            "\u001b[36m  Tool: \u001b[0m\u001b[1m\u001b[36mextract_guidelines_urls\u001b[0m\n",
            "\u001b[36m  Arguments: \u001b[0m\u001b[36m{\n",
            "  \"research_directory\": \"/Users/fabio/Desktop/course-ai-agents/lessons/research_agent_part_2/data/sample_research_folder\"\n",
            "}\u001b[0m\n",
            "\n",
            "\u001b[36m‚ö° Executing tool 'extract_guidelines_urls' via MCP server...\u001b[0m\n",
            "\u001b[36m‚úÖ Tool execution successful!\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[95mü§î LLM's Thoughts:\u001b[0m\n",
            "\u001b[35m**Next Steps After URL Extraction**\n",
            "\n",
            "Okay, so I've successfully pulled all those URLs out of the article guidelines. That's a solid first step. Now, the next obvious move is to actually *do* something with them. Thinking about the available tools, I've got a good range to work with: `process_github_urls`, `transcribe_youtube_urls`, and `scrape_and_clean_other_urls`. Given that the output from `extract_guidelines_urls` shows a mix of all three URL types, it makes perfect sense to loop back and ask if I want to kick off those processing workflows. There's no need to consider `process_local_files` here since there aren't any local files to speak of. So, I'll go ahead and propose processing these URLs to the user, seeing as they were successfully extracted.\u001b[0m\n",
            "\n",
            "\u001b[37müí¨ LLM Response: \u001b[0mURLs have been successfully extracted from the article guidelines.\n",
            "\n",
            "Here's a summary of what was found:\n",
            "- 1 GitHub URL\n",
            "- 2 YouTube video URLs\n",
            "- 6 other web URLs\n",
            "- 0 local file references\n",
            "\n",
            "The results have been saved to `/Users/fabio/Desktop/course-ai-agents/lessons/research_agent_part_2/data/sample_research_folder/.nova/guidelines_filenames.json`.\n",
            "\n",
            "Would you like to proceed with processing these URLs (e.g., processing GitHub URLs, transcribing YouTube videos, or scraping other web URLs)?\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Run the MCP client in-kernel\n",
        "from research_agent_part_2.mcp_client.src.client import main as client_main\n",
        "import sys\n",
        "\n",
        "async def run_client():\n",
        "    _argv_backup = sys.argv[:]\n",
        "    sys.argv = [\"client\"]\n",
        "    try:\n",
        "        await client_main()\n",
        "    finally:\n",
        "        sys.argv = _argv_backup\n",
        "\n",
        "# Start client with in-memory server \n",
        "await run_client()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notice the agent's thoughts. If everything ran correctly, you'll see the text \"Tool execution successful\". If so, notice that there is a new folder named `.nova` in the research directory, with a file `guidelines_filenames.json` inside. This file contains the URLs and local references extracted from the article guideline.\n",
        "\n",
        "Its content should be like this:\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"github_urls\": [\n",
        "    \"https://github.com/path/to/notebook.ipynb\"\n",
        "  ],\n",
        "  \"youtube_videos_urls\": [\n",
        "    \"https://www.youtube.com/watch?v=ApoDzZP8_ck\",\n",
        "    \"https://www.youtube.com/watch?v=h8gMhXYAv1k\"\n",
        "  ],\n",
        "  \"other_urls\": [\n",
        "    \"https://ai.google.dev/gemini-api/docs/function-calling\",\n",
        "    \"https://platform.openai.com/docs/guides/function-calling\",\n",
        "    \"https://arxiv.org/pdf/2401.17464v3\",\n",
        "    \"https://www.newsletter.swirlai.com/p/building-ai-agents-from-scratch-part\",\n",
        "    \"https://dev.to/jamesli/react-vs-plan-and-execute-a-practical-comparison-of-llm-agent-patterns-4gh9\",\n",
        "    \"https://www.deeplearning.ai/the-batch/agentic-design-patterns-part-3-tool-use/\"\n",
        "  ],\n",
        "  \"local_file_paths\": []\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So, the tool has extracted those URLs from the `article_guideline.md` file and categorized them into the groups you see above.\n",
        "\n",
        "We can run the above tool also programmatically as follows. The output shows the result of running it from the local setup of the author of this notebook. To run it, update the path of the `research_folder` variable with your absolute path to the `sample_research_folder` folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'status': 'success',\n",
              " 'github_sources_count': 1,\n",
              " 'youtube_sources_count': 2,\n",
              " 'web_sources_count': 6,\n",
              " 'local_files_count': 0,\n",
              " 'output_path': '/Users/fabio/Desktop/course-ai-agents/lessons/research_agent_part_2/data/sample_research_folder/.nova/guidelines_filenames.json',\n",
              " 'message': \"Successfully extracted URLs from article guidelines in '/Users/fabio/Desktop/course-ai-agents/lessons/research_agent_part_2/data/sample_research_folder'. Found 1 GitHub URLs, 2 YouTube videos URLs, 6 other URLs, and 0 local file references. Results saved to: /Users/fabio/Desktop/course-ai-agents/lessons/research_agent_part_2/data/sample_research_folder/.nova/guidelines_filenames.json\"}"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from research_agent_part_2.mcp_server.src.tools import extract_guidelines_urls_tool\n",
        "\n",
        "research_folder = \"your/absolute/path/to/sample_research_folder\"\n",
        "extract_guidelines_urls_tool(research_folder=research_folder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We'll comment the output of this tool in the next lesson. In the next lessons, we'll run each tool one by one like in the above code cell, so you can see the output of each tool and understand how the research agent works."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Registering MCP Resources\n",
        "\n",
        "Let's see now how to register an MCP resource endpoint using `fastmcp`.\n",
        "\n",
        "Source:\n",
        "_lessons/research_agent_part_2/mcp_server/src/routers/resources.py_\n",
        "\n",
        "```python\n",
        "@mcp.resource(\"system://memory\")\n",
        "async def memory_usage() -> Dict[str, Any]:\n",
        "    \"\"\"Monitor memory usage of the server.\"\"\"\n",
        "    return await get_memory_usage_resource()\n",
        "```\n",
        "\n",
        "It's very similar to how tools are registered, except that the `@mcp.resource()` decorator is used instead of the `@mcp.tool()` decorator.\n",
        "\n",
        "Let's now run the `get_memory_usage_resource` function to see the memory usage of the server."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'process_memory_mb': 63.59375,\n",
              " 'process_memory_percent': 0.3882408142089844,\n",
              " 'system_memory': {'total_gb': 16.0,\n",
              "  'available_gb': 4.650054931640625,\n",
              "  'used_percent': 70.9}}"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from research_agent_part_2.mcp_server.src.resources import get_memory_usage_resource\n",
        "\n",
        "await get_memory_usage_resource()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This output is the same output that an MCP client would get if it uses this MCP resource.\n",
        "\n",
        "*Important*: in the research agent MCP client, we have only implemented the use of tools by the agent LLM, but we could have implemented the use of resources as well. Most MCP clients do not support resources yet, but their support is increasing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.3 Registering MCP Prompts\n",
        "\n",
        "This section shows how MCP prompts are implemented with `fastmcp`. This specific prompt defines the agentic workflow for the research agent.\n",
        "\n",
        "Source:\n",
        "_mcp_server/src/routers/prompts.py_\n",
        "\n",
        "```python\n",
        "@mcp.prompt()\n",
        "async def full_research_instructions_prompt() -> str:\n",
        "    \"\"\"Complete Nova research agent workflow instructions.\"\"\"\n",
        "    return await _get_research_instructions()\n",
        "```\n",
        "\n",
        "The prompt content encodes the full workflow orchestration the agent should follow when started via a prompt.\n",
        "\n",
        "In practice, MCP prompts are triggered by users from an MCP client, not by the agent LLM. When a user triggers an MCP prompt, the MCP client would retrieve that prompt and load it to instruct the LLM on how to run the available tools in sequence (and sometimes in parallel) according to the workflow described in it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For reference, here is the full prompt content of the only MCP prompt implemented in the research agent, which is the `full_research_instructions_prompt` prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Your job is to execute the workflow below.\n",
            "\n",
            "All the tools require a research directory as input.\n",
            "If the user doesn't provide a research directory, you should ask for it before executing any tool.\n",
            "\n",
            "**Workflow:**\n",
            "\n",
            "1. Setup:\n",
            "\n",
            "    1.1. Explain to the user the numbered steps of the workflow. Be concise. Keep them numbered so that the user\n",
            "    can easily refer to them later.\n",
            "    \n",
            "    1.2. Ask the user for the research directory, if not provided. Ask the user if any modification is needed for the\n",
            "    workflow (e.g. running from a specific step, or adding user feedback to specific steps).\n",
            "\n",
            "    1.3 Extract the URLs from the ARTICLE_GUIDELINE_FILE with the \"extract_guidelines_urls\" tool. This tool reads the\n",
            "    ARTICLE_GUIDELINE_FILE and extracts three groups of references from the guidelines:\n",
            "    ‚Ä¢ \"github_urls\" - all GitHub links;\n",
            "    ‚Ä¢ \"youtube_videos_urls\" - all YouTube video links;\n",
            "    ‚Ä¢ \"other_urls\" - all remaining HTTP/HTTPS links;\n",
            "    ‚Ä¢ \"local_files\" - relative paths to local files mentioned in the guidelines (e.g. \"code.py\", \"src/main.py\").\n",
            "    Only extensions allowed are: \".py\", \".ipynb\", and \".md\".\n",
            "    The extracted data is saved to the GUIDELINES_FILENAMES_FILE within the NOVA_FOLDER directory.\n",
            "\n",
            "2. Process the extracted resources in parallel:\n",
            "\n",
            "    You can run the following sub-steps (2.1 to 2.4) in parallel. In a single turn, you can call all the\n",
            "    necessary tools for these steps.\n",
            "\n",
            "    2.1 Local files - run the \"process_local_files\" tool to read every file path listed under \"local_files\" in the\n",
            "    GUIDELINES_FILENAMES_FILE and copy its content into the LOCAL_FILES_FROM_RESEARCH_FOLDER subfolder within\n",
            "    NOVA_FOLDER, giving each copy an appropriate filename (path separators are replaced with underscores).\n",
            "\n",
            "    2.2 Other URL links - run the \"scrape_and_clean_other_urls\" tool to read the `other_urls` list from the\n",
            "    GUIDELINES_FILENAMES_FILE and scrape/clean them. The tool writes the cleaned markdown files inside the\n",
            "    URLS_FROM_GUIDELINES_FOLDER subfolder within NOVA_FOLDER.\n",
            "\n",
            "    2.3 GitHub URLs - run the \"process_github_urls\" tool to process the `github_urls` list from the\n",
            "    GUIDELINES_FILENAMES_FILE with gitingest and save a Markdown summary for each URL inside the\n",
            "    URLS_FROM_GUIDELINES_CODE_FOLDER subfolder within NOVA_FOLDER.\n",
            "\n",
            "    2.4 YouTube URLs - run the \"transcribe_youtube_urls\" tool to process the `youtube_videos_urls` list from the\n",
            "    GUIDELINES_FILENAMES_FILE, transcribe each video, and save the transcript as a Markdown file inside the\n",
            "    URLS_FROM_GUIDELINES_YOUTUBE_FOLDER subfolder within NOVA_FOLDER.\n",
            "        Note: Please be aware that video transcription can be a time-consuming process. For reference,\n",
            "        transcribing a 39-minute video can take approximately 4.5 minutes.\n",
            "\n",
            "3. Repeat the following research loop for 3 rounds:\n",
            "\n",
            "    3.1. Run the \"generate_next_queries\" tool to analyze the ARTICLE_GUIDELINE_FILE, the already-scraped guideline\n",
            "    URLs, and the existing PERPLEXITY_RESULTS_FILE. The tool identifies knowledge gaps, proposes new web-search\n",
            "    questions, and writes them - together with a short justification for each - to the NEXT_QUERIES_FILE within\n",
            "    NOVA_FOLDER.\n",
            "\n",
            "    3.2. Run the \"run_perplexity_research\" tool with the new queries. This tool executes the queries with\n",
            "    Perplexity and appends the results to the PERPLEXITY_RESULTS_FILE within NOVA_FOLDER.\n",
            "\n",
            "4. Filter Perplexity results by quality:\n",
            "\n",
            "    4.1 Run the \"select_research_sources_to_keep\" tool. The tool reads the ARTICLE_GUIDELINE_FILE and the\n",
            "    PERPLEXITY_RESULTS_FILE, automatically evaluates each source for trustworthiness, authority and relevance,\n",
            "    writes the comma-separated IDs of the accepted sources to the PERPLEXITY_SOURCES_SELECTED_FILE **and** saves a\n",
            "    filtered markdown file PERPLEXITY_RESULTS_SELECTED_FILE that contains only the full content blocks of the accepted\n",
            "    sources. Both files are saved within NOVA_FOLDER.\n",
            "\n",
            "5. Identify which of the accepted sources deserve a *full* scrape:\n",
            "\n",
            "    5.1 Run the \"select_research_sources_to_scrape\" tool. It analyses the PERPLEXITY_RESULTS_SELECTED_FILE together\n",
            "    with the ARTICLE_GUIDELINE_FILE and the material already scraped from guideline URLs, then chooses up to 5 diverse,\n",
            "    authoritative sources whose full content will add most value. The chosen URLs are written (one per line) to the\n",
            "    URLS_TO_SCRAPE_FROM_RESEARCH_FILE within NOVA_FOLDER.\n",
            "\n",
            "    5.2 Run the \"scrape_research_urls\" tool. The tool reads the URLs from URLS_TO_SCRAPE_FROM_RESEARCH_FILE and\n",
            "    scrapes/cleans each URL's full content. The cleaned markdown files are saved to the\n",
            "    URLS_FROM_RESEARCH_FOLDER subfolder within NOVA_FOLDER with appropriate filenames.\n",
            "\n",
            "6. Write final research file:\n",
            "\n",
            "    6.1 Run the \"create_research_file\" tool. The tool combines all research data including filtered Perplexity results\n",
            "    from PERPLEXITY_RESULTS_SELECTED_FILE, scraped guideline sources from URLS_FROM_GUIDELINES_FOLDER,\n",
            "    URLS_FROM_GUIDELINES_CODE_FOLDER, and URLS_FROM_GUIDELINES_YOUTUBE_FOLDER, and full research sources from\n",
            "    URLS_FROM_RESEARCH_FOLDER into a comprehensive RESEARCH_MD_FILE organized into sections with collapsible blocks\n",
            "    for easy navigation. The final RESEARCH_MD_FILE is saved in the root of the research directory.\n",
            "\n",
            "Depending on the results of previous steps, you may want to skip running a tool if not necessary.\n",
            "\n",
            "**Critical Failure Policy:**\n",
            "\n",
            "If a tool reports a complete failure, you are required to halt the entire workflow immediately. A complete failure\n",
            "is defined as processing zero items successfully (e.g., scraped 0/7 URLs, processed 0 files).\n",
            "\n",
            "If this occurs, your immediate and only action is to:\n",
            "    1. State the exact tool that failed and quote the output message.\n",
            "    2. Announce that you are stopping the workflow as per your instructions.\n",
            "    3. Ask the user for guidance on how to proceed.\n",
            "\n",
            "**File and Folder Structure:**\n",
            "\n",
            "After running the complete workflow, the research directory will contain the following structure:\n",
            "\n",
            "```\n",
            "research_directory/\n",
            "‚îú‚îÄ‚îÄ ARTICLE_GUIDELINE_FILE                           # Input: Article guidelines and requirements\n",
            "‚îú‚îÄ‚îÄ NOVA_FOLDER/                                     # Hidden directory containing all research data\n",
            "‚îÇ   ‚îú‚îÄ‚îÄ GUIDELINES_FILENAMES_FILE                    # Extracted URLs and local files from guidelines\n",
            "‚îÇ   ‚îú‚îÄ‚îÄ LOCAL_FILES_FROM_RESEARCH_FOLDER/           # Copied local files referenced in guidelines\n",
            "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ [processed_local_files...]\n",
            "‚îÇ   ‚îú‚îÄ‚îÄ URLS_FROM_GUIDELINES_FOLDER/               # Scraped content from other URLs in guidelines\n",
            "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ [scraped_web_pages...]\n",
            "‚îÇ   ‚îú‚îÄ‚îÄ URLS_FROM_GUIDELINES_CODE_FOLDER/          # GitHub repository summaries and code analysis\n",
            "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ [github_repo_summaries...]\n",
            "‚îÇ   ‚îú‚îÄ‚îÄ URLS_FROM_GUIDELINES_YOUTUBE_FOLDER/       # YouTube video transcripts\n",
            "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ [youtube_transcripts...]\n",
            "‚îÇ   ‚îú‚îÄ‚îÄ NEXT_QUERIES_FILE                           # Generated web-search queries with justifications\n",
            "‚îÇ   ‚îú‚îÄ‚îÄ PERPLEXITY_RESULTS_FILE                     # Complete results from all Perplexity research rounds\n",
            "‚îÇ   ‚îú‚îÄ‚îÄ PERPLEXITY_SOURCES_SELECTED_FILE            # Comma-separated IDs of quality sources selected\n",
            "‚îÇ   ‚îú‚îÄ‚îÄ PERPLEXITY_RESULTS_SELECTED_FILE            # Filtered Perplexity results (only selected sources)\n",
            "‚îÇ   ‚îú‚îÄ‚îÄ URLS_TO_SCRAPE_FROM_RESEARCH_FILE          # URLs selected for full content scraping\n",
            "‚îÇ   ‚îî‚îÄ‚îÄ URLS_FROM_RESEARCH_FOLDER/                 # Fully scraped content from selected research URLs\n",
            "‚îÇ       ‚îî‚îÄ‚îÄ [full_research_sources...]\n",
            "‚îî‚îÄ‚îÄ RESEARCH_MD_FILE                                 # Final comprehensive research compilation\n",
            "```\n",
            "\n",
            "This organized structure ensures all research artifacts are systematically collected, processed, and made easily\n",
            "accessible for article writing and future reference.\n"
          ]
        }
      ],
      "source": [
        "from research_agent_part_2.mcp_server.src.prompts import full_research_instructions_prompt\n",
        "\n",
        "prompt = await full_research_instructions_prompt()\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is the instruction block that defines the agentic workflow for the research agent. In the next lessons, we'll go through each step defined in the workflow, learn how it is implemented, and run it in isolation.\n",
        "\n",
        "Let's now see how the MCP client works."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. MCP Client Overview\n",
        "\n",
        "The MCP client can run with two transports:\n",
        "\n",
        "- **in-memory**: The client imports the server factory (`create_mcp_server`) and instantiates the server inside the same Python process. This is fast, simple to debug, and is what we use in this notebook.\n",
        "- **stdio**: The client launches the server as a separate process and communicates using the MCP stdio transport. This mirrors how external MCP clients (e.g., editors) connect to servers and provides process isolation.\n",
        "\n",
        "How the code works:\n",
        "1) Parse the `--transport` flag.\n",
        "2) If in-memory, build a `Client` with the FastMCP server object. If stdio, pass a config that tells FastMCP how to exec the server via `uv`.\n",
        "3) Query capabilities (tools/resources/prompts) and print them.\n",
        "4) Enter the interactive loop: read input, parse it, and dispatch handling.\n",
        "\n",
        "Source:\n",
        "_lessons/research_agent_part_2/mcp_client/src/client.py_\n",
        "_Snippet: transport selection + startup + input handling_\n",
        "```python\n",
        "if args.transport == \"in-memory\":\n",
        "    project_root = Path(__file__).parent.parent.parent\n",
        "    sys.path.insert(0, str(project_root))\n",
        "    from mcp_server.src.server import create_mcp_server\n",
        "    mcp_server = create_mcp_server()\n",
        "    mcp_client = Client(mcp_server)\n",
        "\n",
        "elif args.transport == \"stdio\":\n",
        "    config = {\n",
        "        \"mcpServers\": {\n",
        "            \"research-agent\": {\n",
        "                \"transport\": \"stdio\",\n",
        "                \"command\": \"uv\",\n",
        "                \"args\": [\n",
        "                    \"--directory\", str(settings.server_main_path),\n",
        "                    \"run\", \"-m\", \"src.server\",\n",
        "                    \"--transport\", \"stdio\",\n",
        "                ],\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "    mcp_client = Client(config)\n",
        "\n",
        "# At startup\n",
        "tools, resources, prompts = await get_capabilities_from_mcp_client(mcp_client)\n",
        "print_startup_info(tools, resources, prompts)\n",
        "\n",
        "# Input and handling\n",
        "parsed_input = parse_user_input(user_input)\n",
        "should_continue, thinking_enabled = await handle_user_message(...)\n",
        "```\n",
        "\n",
        "We‚Äôll explore input parsing and command handlers next.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.1 Parsing Input and Commands\n",
        "\n",
        "Purpose: The client supports a small command language. Input can be either a command (starting with `/`) or a freeform user message. One function parses input; others handle effects.\n",
        "\n",
        "Commands (also listed in Section 2):\n",
        "- `/tools`, `/resources`, `/prompts`\n",
        "- `/prompt/<name>` (e.g., `/prompt/full_research_instructions_prompt`)\n",
        "- `/resource/<uri>` (e.g., `/resource/system://status`)\n",
        "- `/model-thinking-switch`\n",
        "- `/quit`\n",
        "\n",
        "How pieces fit together:\n",
        "- `parse_user_input`: pure parser that classifies the input (no side effects). It returns a `ProcessedInput` with metadata.\n",
        "- `handle_user_message`: orchestrates the conversation, calling the appropriate helper for the parsed command, or appending a normal message and running the agent loop.\n",
        "- `handle_command`: prints tools/resources/prompts (for info commands).\n",
        "- `handle_prompt_command`: fetches the prompt text from the server so it can be injected into the conversation.\n",
        "- `handle_resource_command`: fetches and prints a resource.\n",
        "\n",
        "Source:\n",
        "_lessons/research_agent_part_2/mcp_client/src/utils/parse_message_utils.py_\n",
        "_Function: parse_user_input_\n",
        "```python\n",
        "def parse_user_input(user_input: str) -> ProcessedInput:\n",
        "    # /prompt/<name>, /resource/<uri>, /tools, /resources, /prompts,\n",
        "    # /model-thinking-switch, /quit, or a normal message\n",
        "    ...\n",
        "```\n",
        "\n",
        "Example mappings:\n",
        "- Input `/tools` ‚Üí parsed as COMMAND_INFO_TOOLS ‚Üí `handle_command` prints the tools list.\n",
        "- Input `/prompt/full_research_instructions_prompt` ‚Üí parsed as COMMAND_PROMPT ‚Üí `handle_prompt_command` returns the prompt text; the client appends it to conversation and runs the agent loop.\n",
        "- Input `/resource/system://status` ‚Üí parsed as COMMAND_RESOURCE ‚Üí `handle_resource_command` prints the resource body.\n",
        "- Input `/model-thinking-switch` ‚Üí parsed as COMMAND_MODEL_THINKING_SWITCH ‚Üí toggles thoughts on/off.\n",
        "- Input `Hello` ‚Üí parsed as NORMAL_MESSAGE ‚Üí appended to conversation; the agent loop runs once.\n",
        "\n",
        "Source:\n",
        "_lessons/research_agent_part_2/mcp_client/src/utils/command_utils.py_\n",
        "_Function: handle_command_\n",
        "```python\n",
        "def handle_command(processed_input: ProcessedInput, tools: List, resources: List, prompts: List):\n",
        "    # Pretty-prints lists of tools/resources/prompts\n",
        "    ...\n",
        "```\n",
        "\n",
        "Source:\n",
        "_lessons/research_agent_part_2/mcp_client/src/utils/command_utils.py_\n",
        "_Function: handle_prompt_command_\n",
        "```python\n",
        "async def handle_prompt_command(prompt_name: str, prompts: List, client: Client) -> str | None:\n",
        "    # Fetches prompt content from the server by name\n",
        "    ...\n",
        "```\n",
        "\n",
        "Source:\n",
        "_lessons/research_agent_part_2/mcp_client/src/utils/command_utils.py_\n",
        "_Function: handle_resource_command_\n",
        "```python\n",
        "async def handle_resource_command(resource_uri: str, resources: List, client: Client) -> None:\n",
        "    # Reads a resource by URI and prints it\n",
        "    ...\n",
        "```\n",
        "\n",
        "Source:\n",
        "_lessons/research_agent_part_2/mcp_client/src/utils/handle_message_utils.py_\n",
        "_Function: handle_user_message_\n",
        "```python\n",
        "async def handle_user_message(...):\n",
        "    # Routes by parsed input type, updates conversation, and runs agent loop if needed\n",
        "    ...\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Client utils and configuration quick tour\n",
        "\n",
        "This section briefly summarizes the purpose of the client utilities. You don‚Äôt need to memorize the code ‚Äî focus on what each module is responsible for.\n",
        "\n",
        "- `utils/mcp_startup_utils.py`: lists capabilities from the server and prints a concise startup banner. Used immediately after the client connects to show Tools/Resources/Prompts.\n",
        "- `utils/parse_message_utils.py`: parses raw user input into a structured `ProcessedInput` (command vs normal message). Pure function with no side effects.\n",
        "- `utils/command_utils.py`: handlers for informational commands. Prints lists of tools/resources/prompts; loads a prompt by name; reads a resource by URI.\n",
        "- `utils/handle_message_utils.py`: routes parsed input to the correct behavior, updates the conversation, and invokes the agent loop when a message or prompt should be processed by the LLM.\n",
        "- `utils/handle_agent_loop_utils.py`: runs the ReAct‚Äëstyle loop with Gemini function calling, executes MCP tools, and appends tool results back to the conversation.\n",
        "- `utils/llm_utils.py`: wraps `google-genai` client, builds tool configs (function declarations) from MCP Tools, extracts thoughts/function calls/final answers.\n",
        "- `utils/print_utils.py`: colorized console output helpers.\n",
        "- `utils/types.py`: small types for the input parsing pipeline (`InputType`, `ProcessedInput`).\n",
        "- `utils/logging_utils.py`: logger setup for readable logs.\n",
        "- `utils/opik_handler.py`: optional Opik instrumentation for tracing LLM calls.\n",
        "\n",
        "Settings and configuration:\n",
        "- `src/settings.py`: centralizes configuration using `pydantic_settings.BaseSettings`. It loads `.env` values (e.g., `GOOGLE_API_KEY`) and exposes defaults like `model_id`, `thinking_budget`, log levels, and server paths. Using Pydantic settings keeps configuration declarative and environment‚Äëdriven while giving type safety.\n",
        "\n",
        "Where used:\n",
        "- The client imports these modules in `src/client.py` and wires them into the main loop so the top‚Äëlevel file stays small and readable.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
